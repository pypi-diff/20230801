# Comparing `tmp/diveplane_reactor_api-5.0.18-py3-none-any.whl.zip` & `tmp/diveplane_reactor_api-6.5.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,72 +1,72 @@
-Zip file size: 277923 bytes, number of entries: 70
--rw-r--r--  2.0 unx   171737 b- defN 23-Jul-04 19:26 diveplane/client/LICENSE-3RD-PARTY.txt
--rw-r--r--  2.0 unx     2146 b- defN 23-Jul-04 19:26 diveplane/client/__init__.py
--rw-r--r--  2.0 unx    14410 b- defN 23-Jul-04 19:25 diveplane/client/base.py
--rw-r--r--  2.0 unx     3080 b- defN 23-Jul-04 19:25 diveplane/client/cache.py
--rw-r--r--  2.0 unx    16092 b- defN 23-Jul-04 19:25 diveplane/client/client.py
--rw-r--r--  2.0 unx     4594 b- defN 23-Jul-04 19:25 diveplane/client/configuration.py
--rw-r--r--  2.0 unx     4329 b- defN 23-Jul-04 19:25 diveplane/client/exceptions.py
--rw-r--r--  2.0 unx      958 b- defN 23-Jul-04 19:25 diveplane/client/protocols.py
--rw-r--r--  2.0 unx     1900 b- defN 23-Jul-04 19:26 diveplane/client/requirements-3.10.txt
--rw-r--r--  2.0 unx     1900 b- defN 23-Jul-04 19:26 diveplane/client/requirements-3.11.txt
--rw-r--r--  2.0 unx     1919 b- defN 23-Jul-04 19:26 diveplane/client/requirements-3.8.txt
--rw-r--r--  2.0 unx     1898 b- defN 23-Jul-04 19:26 diveplane/client/requirements-3.9.txt
--rw-r--r--  2.0 unx     3309 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev-3.10.txt
--rw-r--r--  2.0 unx     3060 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev-3.11.txt
--rw-r--r--  2.0 unx     3296 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev-3.8.txt
--rw-r--r--  2.0 unx     3296 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev-3.9.txt
--rw-r--r--  2.0 unx      210 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev.in
--rw-r--r--  2.0 unx      371 b- defN 23-Jul-04 19:26 diveplane/client/requirements.in
--rw-r--r--  2.0 unx      175 b- defN 23-Jul-04 19:25 diveplane/client/pandas/__init__.py
--rw-r--r--  2.0 unx    10935 b- defN 23-Jul-04 19:25 diveplane/client/pandas/client.py
--rw-r--r--  2.0 unx      845 b- defN 23-Jul-04 19:25 diveplane/client/tests/__init__.py
--rw-r--r--  2.0 unx    50937 b- defN 23-Jul-04 19:25 diveplane/client/tests/test_client.py
--rw-r--r--  2.0 unx    13534 b- defN 23-Jul-04 19:25 diveplane/client/tests/test_infer_feature_attributes.py
--rw-r--r--  2.0 unx     8783 b- defN 23-Jul-04 19:25 diveplane/client/tests/test_scikit.py
--rw-r--r--  2.0 unx      193 b- defN 23-Jul-04 19:25 diveplane/direct/__init__.py
--rw-r--r--  2.0 unx     4538 b- defN 23-Jul-04 19:25 diveplane/direct/_utilities.py
--rw-r--r--  2.0 unx   213772 b- defN 23-Jul-04 19:25 diveplane/direct/client.py
--rw-r--r--  2.0 unx    66259 b- defN 23-Jul-04 19:25 diveplane/direct/core.py
--rw-r--r--  2.0 unx      923 b- defN 23-Jul-04 19:25 diveplane/direct/tests/test_standalone.py
--rw-r--r--  2.0 unx      768 b- defN 23-Jul-04 19:25 diveplane/reactor/__init__.py
--rw-r--r--  2.0 unx     1282 b- defN 23-Jul-04 19:25 diveplane/reactor/client.py
--rw-r--r--  2.0 unx    10613 b- defN 23-Jul-04 19:25 diveplane/reactor/project.py
--rw-r--r--  2.0 unx     9637 b- defN 23-Jul-04 19:25 diveplane/reactor/session.py
--rw-r--r--  2.0 unx   126345 b- defN 23-Jul-04 19:25 diveplane/reactor/trainee.py
--rw-r--r--  2.0 unx      302 b- defN 23-Jul-04 19:25 diveplane/reactor/tests/conftest.py
--rw-r--r--  2.0 unx     3243 b- defN 23-Jul-04 19:25 diveplane/reactor/tests/test_reactor.py
--rw-r--r--  2.0 unx      426 b- defN 23-Jul-04 19:25 diveplane/scikit/__init__.py
--rw-r--r--  2.0 unx    54659 b- defN 23-Jul-04 19:25 diveplane/scikit/scikit.py
--rw-r--r--  2.0 unx     1741 b- defN 23-Jul-04 19:25 diveplane/utilities/__init__.py
--rw-r--r--  2.0 unx     3655 b- defN 23-Jul-04 19:25 diveplane/utilities/eula_helper.py
--rw-r--r--  2.0 unx    22403 b- defN 23-Jul-04 19:25 diveplane/utilities/features.py
--rw-r--r--  2.0 unx      698 b- defN 23-Jul-04 19:25 diveplane/utilities/guess_feature_attributes.py
--rw-r--r--  2.0 unx    39658 b- defN 23-Jul-04 19:25 diveplane/utilities/installation_verification.py
--rw-r--r--  2.0 unx    25873 b- defN 23-Jul-04 19:25 diveplane/utilities/internals.py
--rw-r--r--  2.0 unx     2126 b- defN 23-Jul-04 19:25 diveplane/utilities/json_wrapper.py
--rw-r--r--  2.0 unx     2424 b- defN 23-Jul-04 19:25 diveplane/utilities/locale.py
--rw-r--r--  2.0 unx     4697 b- defN 23-Jul-04 19:25 diveplane/utilities/monitors.py
--rw-r--r--  2.0 unx     2341 b- defN 23-Jul-04 19:25 diveplane/utilities/posix.py
--rw-r--r--  2.0 unx      380 b- defN 23-Jul-04 19:25 diveplane/utilities/random.py
--rw-r--r--  2.0 unx    40637 b- defN 23-Jul-04 19:25 diveplane/utilities/utilities.py
--rw-r--r--  2.0 unx      194 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/__init__.py
--rw-r--r--  2.0 unx    39499 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/base.py
--rw-r--r--  2.0 unx    16407 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/infer_feature_attributes.py
--rw-r--r--  2.0 unx    23164 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/pandas.py
--rw-r--r--  2.0 unx     6359 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/protocols.py
--rw-r--r--  2.0 unx    37406 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/relational.py
--rw-r--r--  2.0 unx    31809 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/time_series.py
--rw-r--r--  2.0 unx    24901 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py
--rw-r--r--  2.0 unx     6442 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/tests/test_infer_time_series_attributes.py
--rw-r--r--  2.0 unx     1321 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/__init__.py
--rw-r--r--  2.0 unx       51 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/conftest.py
--rw-r--r--  2.0 unx    13611 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/test_features.py
--rw-r--r--  2.0 unx     8189 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/test_internals.py
--rw-r--r--  2.0 unx    11237 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/test_utilities.py
--rw-r--r--  2.0 unx    12104 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     3192 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/WHEEL
--rw-r--r--  2.0 unx      157 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       10 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6634 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/RECORD
-70 files, 1206086 bytes uncompressed, 267199 bytes compressed:  77.8%
+Zip file size: 282847 bytes, number of entries: 70
+-rw-r--r--  2.0 unx   171816 b- defN 23-Jul-30 17:28 diveplane/client/LICENSE-3RD-PARTY.txt
+-rw-r--r--  2.0 unx     2145 b- defN 23-Jul-30 17:28 diveplane/client/__init__.py
+-rw-r--r--  2.0 unx    14668 b- defN 23-Jul-30 17:27 diveplane/client/base.py
+-rw-r--r--  2.0 unx     3080 b- defN 23-Jul-30 17:27 diveplane/client/cache.py
+-rw-r--r--  2.0 unx    16092 b- defN 23-Jul-30 17:27 diveplane/client/client.py
+-rw-r--r--  2.0 unx     4594 b- defN 23-Jul-30 17:27 diveplane/client/configuration.py
+-rw-r--r--  2.0 unx     4329 b- defN 23-Jul-30 17:27 diveplane/client/exceptions.py
+-rw-r--r--  2.0 unx      958 b- defN 23-Jul-30 17:27 diveplane/client/protocols.py
+-rw-r--r--  2.0 unx     1903 b- defN 23-Jul-30 17:28 diveplane/client/requirements-3.10.txt
+-rw-r--r--  2.0 unx     1903 b- defN 23-Jul-30 17:28 diveplane/client/requirements-3.11.txt
+-rw-r--r--  2.0 unx     1936 b- defN 23-Jul-30 17:28 diveplane/client/requirements-3.8.txt
+-rw-r--r--  2.0 unx     1901 b- defN 23-Jul-30 17:28 diveplane/client/requirements-3.9.txt
+-rw-r--r--  2.0 unx     3346 b- defN 23-Jul-30 17:28 diveplane/client/requirements-dev-3.10.txt
+-rw-r--r--  2.0 unx     3097 b- defN 23-Jul-30 17:28 diveplane/client/requirements-dev-3.11.txt
+-rw-r--r--  2.0 unx     3332 b- defN 23-Jul-30 17:28 diveplane/client/requirements-dev-3.8.txt
+-rw-r--r--  2.0 unx     3332 b- defN 23-Jul-30 17:28 diveplane/client/requirements-dev-3.9.txt
+-rw-r--r--  2.0 unx      210 b- defN 23-Jul-30 17:28 diveplane/client/requirements-dev.in
+-rw-r--r--  2.0 unx      372 b- defN 23-Jul-30 17:28 diveplane/client/requirements.in
+-rw-r--r--  2.0 unx      175 b- defN 23-Jul-30 17:27 diveplane/client/pandas/__init__.py
+-rw-r--r--  2.0 unx    10935 b- defN 23-Jul-30 17:27 diveplane/client/pandas/client.py
+-rw-r--r--  2.0 unx      845 b- defN 23-Jul-30 17:27 diveplane/client/tests/__init__.py
+-rw-r--r--  2.0 unx    50923 b- defN 23-Jul-30 17:27 diveplane/client/tests/test_client.py
+-rw-r--r--  2.0 unx    13534 b- defN 23-Jul-30 17:27 diveplane/client/tests/test_infer_feature_attributes.py
+-rw-r--r--  2.0 unx     8783 b- defN 23-Jul-30 17:27 diveplane/client/tests/test_scikit.py
+-rw-r--r--  2.0 unx      193 b- defN 23-Jul-30 17:27 diveplane/direct/__init__.py
+-rw-r--r--  2.0 unx     4538 b- defN 23-Jul-30 17:27 diveplane/direct/_utilities.py
+-rw-r--r--  2.0 unx   228745 b- defN 23-Jul-30 17:27 diveplane/direct/client.py
+-rw-r--r--  2.0 unx    66788 b- defN 23-Jul-30 17:27 diveplane/direct/core.py
+-rw-r--r--  2.0 unx      923 b- defN 23-Jul-30 17:27 diveplane/direct/tests/test_standalone.py
+-rw-r--r--  2.0 unx      768 b- defN 23-Jul-30 17:27 diveplane/reactor/__init__.py
+-rw-r--r--  2.0 unx     1282 b- defN 23-Jul-30 17:27 diveplane/reactor/client.py
+-rw-r--r--  2.0 unx    10613 b- defN 23-Jul-30 17:27 diveplane/reactor/project.py
+-rw-r--r--  2.0 unx     9637 b- defN 23-Jul-30 17:27 diveplane/reactor/session.py
+-rw-r--r--  2.0 unx   141124 b- defN 23-Jul-30 17:27 diveplane/reactor/trainee.py
+-rw-r--r--  2.0 unx      302 b- defN 23-Jul-30 17:27 diveplane/reactor/tests/conftest.py
+-rw-r--r--  2.0 unx     3891 b- defN 23-Jul-30 17:27 diveplane/reactor/tests/test_reactor.py
+-rw-r--r--  2.0 unx      426 b- defN 23-Jul-30 17:27 diveplane/scikit/__init__.py
+-rw-r--r--  2.0 unx    54642 b- defN 23-Jul-30 17:27 diveplane/scikit/scikit.py
+-rw-r--r--  2.0 unx     1741 b- defN 23-Jul-30 17:27 diveplane/utilities/__init__.py
+-rw-r--r--  2.0 unx     3655 b- defN 23-Jul-30 17:27 diveplane/utilities/eula_helper.py
+-rw-r--r--  2.0 unx    22403 b- defN 23-Jul-30 17:27 diveplane/utilities/features.py
+-rw-r--r--  2.0 unx      698 b- defN 23-Jul-30 17:27 diveplane/utilities/guess_feature_attributes.py
+-rw-r--r--  2.0 unx    39939 b- defN 23-Jul-30 17:27 diveplane/utilities/installation_verification.py
+-rw-r--r--  2.0 unx    25873 b- defN 23-Jul-30 17:27 diveplane/utilities/internals.py
+-rw-r--r--  2.0 unx     2126 b- defN 23-Jul-30 17:27 diveplane/utilities/json_wrapper.py
+-rw-r--r--  2.0 unx     2424 b- defN 23-Jul-30 17:27 diveplane/utilities/locale.py
+-rw-r--r--  2.0 unx     4697 b- defN 23-Jul-30 17:27 diveplane/utilities/monitors.py
+-rw-r--r--  2.0 unx     2341 b- defN 23-Jul-30 17:27 diveplane/utilities/posix.py
+-rw-r--r--  2.0 unx      380 b- defN 23-Jul-30 17:27 diveplane/utilities/random.py
+-rw-r--r--  2.0 unx    40637 b- defN 23-Jul-30 17:27 diveplane/utilities/utilities.py
+-rw-r--r--  2.0 unx      194 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/__init__.py
+-rw-r--r--  2.0 unx    41791 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/base.py
+-rw-r--r--  2.0 unx    16407 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/infer_feature_attributes.py
+-rw-r--r--  2.0 unx    23179 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/pandas.py
+-rw-r--r--  2.0 unx     6359 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/protocols.py
+-rw-r--r--  2.0 unx    37406 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/relational.py
+-rw-r--r--  2.0 unx    31809 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/time_series.py
+-rw-r--r--  2.0 unx    24917 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py
+-rw-r--r--  2.0 unx     6442 b- defN 23-Jul-30 17:27 diveplane/utilities/feature_attributes/tests/test_infer_time_series_attributes.py
+-rw-r--r--  2.0 unx     1321 b- defN 23-Jul-30 17:27 diveplane/utilities/tests/__init__.py
+-rw-r--r--  2.0 unx       51 b- defN 23-Jul-30 17:27 diveplane/utilities/tests/conftest.py
+-rw-r--r--  2.0 unx    13611 b- defN 23-Jul-30 17:27 diveplane/utilities/tests/test_features.py
+-rw-r--r--  2.0 unx     8189 b- defN 23-Jul-30 17:27 diveplane/utilities/tests/test_internals.py
+-rw-r--r--  2.0 unx    11237 b- defN 23-Jul-30 17:27 diveplane/utilities/tests/test_utilities.py
+-rw-r--r--  2.0 unx    12104 b- defN 23-Jul-30 17:28 diveplane_reactor_api-6.5.8.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     3192 b- defN 23-Jul-30 17:28 diveplane_reactor_api-6.5.8.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-30 17:28 diveplane_reactor_api-6.5.8.dist-info/WHEEL
+-rw-r--r--  2.0 unx      157 b- defN 23-Jul-30 17:28 diveplane_reactor_api-6.5.8.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       10 b- defN 23-Jul-30 17:28 diveplane_reactor_api-6.5.8.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6628 b- defN 23-Jul-30 17:28 diveplane_reactor_api-6.5.8.dist-info/RECORD
+70 files, 1240091 bytes uncompressed, 272135 bytes compressed:  78.1%
```

## zipnote {}

```diff
@@ -186,26 +186,26 @@
 
 Filename: diveplane/utilities/tests/test_internals.py
 Comment: 
 
 Filename: diveplane/utilities/tests/test_utilities.py
 Comment: 
 
-Filename: diveplane_reactor_api-5.0.18.dist-info/LICENSE.txt
+Filename: diveplane_reactor_api-6.5.8.dist-info/LICENSE.txt
 Comment: 
 
-Filename: diveplane_reactor_api-5.0.18.dist-info/METADATA
+Filename: diveplane_reactor_api-6.5.8.dist-info/METADATA
 Comment: 
 
-Filename: diveplane_reactor_api-5.0.18.dist-info/WHEEL
+Filename: diveplane_reactor_api-6.5.8.dist-info/WHEEL
 Comment: 
 
-Filename: diveplane_reactor_api-5.0.18.dist-info/entry_points.txt
+Filename: diveplane_reactor_api-6.5.8.dist-info/entry_points.txt
 Comment: 
 
-Filename: diveplane_reactor_api-5.0.18.dist-info/top_level.txt
+Filename: diveplane_reactor_api-6.5.8.dist-info/top_level.txt
 Comment: 
 
-Filename: diveplane_reactor_api-5.0.18.dist-info/RECORD
+Filename: diveplane_reactor_api-6.5.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## diveplane/client/LICENSE-3RD-PARTY.txt

```diff
@@ -1,14 +1,14 @@
 Faker
-18.10.1
+19.1.0
 MIT License
 joke2k
 https://github.com/joke2k/faker
 Faker is a Python package that generates fake data for you.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/Faker-18.10.1.dist-info/LICENSE.txt
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/Faker-19.1.0.dist-info/LICENSE.txt
 Copyright (c) 2012 Daniele Faraglia
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
 to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 copies of the Software, and to permit persons to whom the Software is
@@ -28,15 +28,15 @@
 
 PyJWT
 2.7.0
 MIT License
 Jose Padilla
 https://github.com/jpadilla/pyjwt
 JSON Web Token implementation in Python
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/PyJWT-2.7.0.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/PyJWT-2.7.0.dist-info/LICENSE
 The MIT License (MIT)
 
 Copyright (c) 2015-2022 José Padilla
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
@@ -53,20 +53,20 @@
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
 
 
 PyYAML
-6.0
+6.0.1
 MIT License
 Kirill Simonov
 https://pyyaml.org/
 YAML parser and emitter for Python
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/PyYAML-6.0.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/PyYAML-6.0.1.dist-info/LICENSE
 Copyright (c) 2017-2021 Ingy döt Net
 Copyright (c) 2006-2016 Kirill Simonov
 
 Permission is hereby granted, free of charge, to any person obtaining a copy of
 this software and associated documentation files (the "Software"), to deal in
 the Software without restriction, including without limitation the rights to
 use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
@@ -87,15 +87,15 @@
 
 Pygments
 2.15.1
 BSD License
 Georg Brandl <georg@python.org>
 https://pygments.org
 Pygments is a syntax highlighting package written in Python.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/Pygments-2.15.1.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/Pygments-2.15.1.dist-info/LICENSE
 Copyright (c) 2006-2022 by the respective authors (see AUTHORS file).
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are
 met:
 
@@ -121,15 +121,15 @@
 
 certifi
 2023.5.7
 Mozilla Public License 2.0 (MPL 2.0)
 Kenneth Reitz
 https://github.com/certifi/python-certifi
 Python package for providing Mozilla's CA Bundle.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/certifi-2023.5.7.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/certifi-2023.5.7.dist-info/LICENSE
 This package contains a modified version of ca-bundle.crt:
 
 ca-bundle.crt -- Bundle of CA Root Certificates
 
 Certificate data from Mozilla as of: Thu Nov  3 19:04:19 2011#
 This is a bundle of X.509 certificates of public Certificate Authorities
 (CA). These were automatically extracted from Mozilla's root certificates
@@ -151,15 +151,15 @@
 
 cffi
 1.15.1
 MIT License
 Armin Rigo, Maciej Fijalkowski
 http://cffi.readthedocs.org
 Foreign Function Interface for Python calling C code.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/cffi-1.15.1.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/cffi-1.15.1.dist-info/LICENSE
 
 Except when otherwise stated (look for LICENSE files in directories or
 information at the beginning of each file) all software and
 documentation is licensed as follows: 
 
     The MIT License
 
@@ -181,32 +181,32 @@
     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING 
     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER 
     DEALINGS IN THE SOFTWARE.
 
 
 
 cryptography
-41.0.1
+41.0.2
 Apache Software License; BSD License
 The Python Cryptographic Authority and individual contributors <cryptography-dev@python.org>
 https://github.com/pyca/cryptography
 cryptography is a package which provides cryptographic recipes and primitives to Python developers.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/cryptography-41.0.1.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/cryptography-41.0.2.dist-info/LICENSE
 This software is made available under the terms of *either* of the licenses
 found in LICENSE.APACHE or LICENSE.BSD. Contributions to cryptography are made
 under the terms of *both* these licenses.
 
 
 deprecation
 2.1.0
 Apache Software License
 Brian Curtin
 http://deprecation.readthedocs.io/
 A library to handle automated deprecations
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/deprecation-2.1.0.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/deprecation-2.1.0.dist-info/LICENSE
                                  Apache License
                            Version 2.0, January 2004
                         http://www.apache.org/licenses/
 
    TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
 
    1. Definitions.
@@ -403,20 +403,20 @@
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
 
 
 diveplane-amalgam-api
-2.3.7
-UNKNOWN
-Diveplane Corp
-UNKNOWN
+2.3.10
+Other/Proprietary License
+Diveplane Corporation
+https://www.diveplane.com
 A direct interface with Amalgam compiled dll or so.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/diveplane_amalgam_api-2.3.7.dist-info/LICENSE.txt
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/diveplane_amalgam_api-2.3.10.dist-info/LICENSE.txt
 Diveplane Corporation
 
 Free Software License Terms
 
 PLEASE READ THIS LICENSE AGREEMENT (“Agreement”) CAREFULLY. This Agreement is a
 legally binding agreement between you (sometimes referred to as “You” or
 “Your”) and Diveplane Corporation (referred to as “Diveplane”, “Us”, “Our”, or
@@ -607,20 +607,20 @@
 
    f. Compliance with Laws. You shall comply with all applicable laws,
       regulations, rules, orders, and other requirements, now or hereafter in
       effect, of any applicable governmental authority, in its performance of
       activities hereunder.
 
 diveplane-openapi-client
-22.2.3
+24.0.1
 UNKNOWN
 Diveplane Corporation
 UNKNOWN
 Diveplane API
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/diveplane/openapi/LICENSE-3RD-PARTY.txt
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/diveplane/openapi/LICENSE-3RD-PARTY.txt
 python-dateutil
 2.8.2
 Apache Software License; BSD License
 Gustavo Niemeyer
 https://github.com/dateutil/dateutil
 Extensions to the standard Python datetime module
 /opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/python_dateutil-2.8.2.dist-info/LICENSE
@@ -736,20 +736,20 @@
 SOFTWARE.
 
 
 
 
 
 humanize
-4.6.0
+4.7.0
 MIT License
 Jason Moiron <jmoiron@jmoiron.net>
 https://github.com/python-humanize/humanize
 Python humanize utilities
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/humanize-4.6.0.dist-info/licenses/LICENCE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/humanize-4.7.0.dist-info/licenses/LICENCE
 Copyright (c) 2010-2020 Jason Moiron and Contributors
 
 Permission is hereby granted, free of charge, to any person obtaining
 a copy of this software and associated documentation files (the
 "Software"), to deal in the Software without restriction, including
 without limitation the rights to use, copy, modify, merge, publish,
 distribute, sublicense, and/or sell copies of the Software, and to
@@ -765,20 +765,20 @@
 NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
 LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
 OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
 WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
 
 joblib
-1.2.0
+1.3.1
 BSD License
-Gael Varoquaux
+Gael Varoquaux <gael.varoquaux@normalesup.org>
 https://joblib.readthedocs.io
 Lightweight pipelining with Python functions
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/joblib-1.2.0.dist-info/LICENSE.txt
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/joblib-1.3.1.dist-info/LICENSE.txt
 BSD 3-Clause License
 
 Copyright (c) 2008-2021, The joblib developers.
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
@@ -803,20 +803,20 @@
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 
 markdown-it-py
-2.2.0
+3.0.0
 MIT License
 Chris Sewell <chrisj_sewell@hotmail.com>
 https://github.com/executablebooks/markdown-it-py
 Python port of markdown-it. Markdown parsing, done right!
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/markdown_it_py-2.2.0.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/markdown_it_py-3.0.0.dist-info/LICENSE
 MIT License
 
 Copyright (c) 2020 ExecutableBookProject
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
@@ -838,15 +838,15 @@
 
 mdurl
 0.1.2
 MIT License
 Taneli Hukkinen <hukkin@users.noreply.github.com>
 https://github.com/executablebooks/mdurl
 Markdown URL utilities
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/mdurl-0.1.2.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/mdurl-0.1.2.dist-info/LICENSE
 Copyright (c) 2015 Vitaly Puzrin, Alex Kocharin.
 Copyright (c) 2021 Taneli Hukkinen
 
 Permission is hereby granted, free of charge, to any person
 obtaining a copy of this software and associated documentation
 files (the "Software"), to deal in the Software without
 restriction, including without limitation the rights to use,
@@ -888,20 +888,20 @@
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 IN THE SOFTWARE.
 
 
 mmh3
-4.0.0
+4.0.1
 MIT License
 Hajime Senuma <hajime.senuma@gmail.com>
 https://pypi.org/project/mmh3/
 Python extension for MurmurHash (MurmurHash3), a set of fast and robust hash functions.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/mmh3-4.0.0.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/mmh3-4.0.1.dist-info/LICENSE
 MIT License
 
 Copyright (c) 2011-2023 Hajime Senuma
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
@@ -917,21 +917,21 @@
 FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
 
 numpy
-1.24.3
+1.25.1
 BSD License
 Travis E. Oliphant et al.
 https://www.numpy.org
 Fundamental package for array computing in Python
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/numpy-1.24.3.dist-info/LICENSE.txt
-Copyright (c) 2005-2022, NumPy Developers.
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/numpy-1.25.1.dist-info/LICENSE.txt
+Copyright (c) 2005-2023, NumPy Developers.
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are
 met:
 
     * Redistributions of source code must retain the above copyright
@@ -1841,27 +1841,27 @@
 
 packaging
 23.1
 Apache Software License; BSD License
 Donald Stufft <donald@stufft.io>
 https://github.com/pypa/packaging
 Core utilities for Python packages
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/packaging-23.1.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/packaging-23.1.dist-info/LICENSE
 This software is made available under the terms of *either* of the licenses
 found in LICENSE.APACHE or LICENSE.BSD. Contributions to this software is made
 under the terms of *both* these licenses.
 
 
 pandas
-2.0.2
+2.0.3
 BSD License
 The Pandas Development Team <pandas-dev@python.org>
 https://pandas.pydata.org
 Powerful data structures for data analysis, time series, and statistics
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/pandas-2.0.2.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/pandas-2.0.3.dist-info/LICENSE
 BSD 3-Clause License
 
 Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team
 All rights reserved.
 
 Copyright (c) 2011-2023, Open source contributors.
 
@@ -1893,15 +1893,15 @@
 
 pycparser
 2.21
 BSD License
 Eli Bendersky
 https://github.com/eliben/pycparser
 C parser in Python
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/pycparser-2.21.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/pycparser-2.21.dist-info/LICENSE
 pycparser -- A C parser in Python
 
 Copyright (c) 2008-2020, Eli Bendersky
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without modification,
 are permitted provided that the following conditions are met:
@@ -1929,15 +1929,15 @@
 
 python-dateutil
 2.8.2
 Apache Software License; BSD License
 Gustavo Niemeyer
 https://github.com/dateutil/dateutil
 Extensions to the standard Python datetime module
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/python_dateutil-2.8.2.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/python_dateutil-2.8.2.dist-info/LICENSE
 Copyright 2017- Paul Ganssle <paul@ganssle.io>
 Copyright 2017- dateutil contributors (see AUTHORS file)
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
@@ -1991,15 +1991,15 @@
 
 pytz
 2023.3
 MIT License
 Stuart Bishop
 http://pythonhosted.org/pytz
 World timezone definitions, modern and historical
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/pytz-2023.3.dist-info/LICENSE.txt
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/pytz-2023.3.dist-info/LICENSE.txt
 Copyright (c) 2003-2019 Stuart Bishop <stuart@stuartbishop.net>
 
 Permission is hereby granted, free of charge, to any person obtaining a
 copy of this software and associated documentation files (the "Software"),
 to deal in the Software without restriction, including without limitation
 the rights to use, copy, modify, merge, publish, distribute, sublicense,
 and/or sell copies of the Software, and to permit persons to whom the
@@ -2014,20 +2014,20 @@
 THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 DEALINGS IN THE SOFTWARE.
 
 
 rich
-13.4.1
+13.4.2
 MIT License
 Will McGugan
 https://github.com/Textualize/rich
 Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/rich-13.4.1.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/rich-13.4.2.dist-info/LICENSE
 Copyright (c) 2020 Will McGugan
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
 to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 copies of the Software, and to permit persons to whom the Software is
@@ -2042,23 +2042,23 @@
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
 
 
 scikit-learn
-1.2.2
+1.3.0
 BSD License
 UNKNOWN
 http://scikit-learn.org
 A set of python modules for machine learning and data mining
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/scikit_learn-1.2.2.dist-info/COPYING
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/scikit_learn-1.3.0.dist-info/COPYING
 BSD 3-Clause License
 
-Copyright (c) 2007-2022 The scikit-learn developers.
+Copyright (c) 2007-2023 The scikit-learn developers.
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
 
 * Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
@@ -2080,21 +2080,21 @@
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 
 scipy
-1.10.1
+1.11.1
 BSD License
 UNKNOWN
 https://scipy.org/
 Fundamental algorithms for scientific computing in Python
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/scipy-1.10.1.dist-info/LICENSE.txt
-Copyright (c) 2001-2002 Enthought, Inc. 2003-2022, SciPy Developers.
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/scipy-1.11.1.dist-info/LICENSE.txt
+Copyright (c) 2001-2002 Enthought, Inc. 2003-2023, SciPy Developers.
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions
 are met:
 
 1. Redistributions of source code must retain the above copyright
@@ -3004,15 +3004,15 @@
 
 semantic-version
 2.10.0
 BSD License
 Raphaël Barrois
 https://github.com/rbarrois/python-semanticversion
 A library implementing the 'SemVer' scheme.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/semantic_version-2.10.0.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/semantic_version-2.10.0.dist-info/LICENSE
 Copyright (c) The python-semanticversion project
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met: 
 
 1. Redistributions of source code must retain the above copyright notice, this
@@ -3035,15 +3035,15 @@
 
 six
 1.16.0
 MIT License
 Benjamin Peterson
 https://github.com/benjaminp/six
 Python 2 and 3 compatibility utilities
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/six-1.16.0.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/six-1.16.0.dist-info/LICENSE
 Copyright (c) 2010-2020 Benjamin Peterson
 
 Permission is hereby granted, free of charge, to any person obtaining a copy of
 this software and associated documentation files (the "Software"), to deal in
 the Software without restriction, including without limitation the rights to
 use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
 the Software, and to permit persons to whom the Software is furnished to do so,
@@ -3057,20 +3057,20 @@
 FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
 COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
 IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
 CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
 
 threadpoolctl
-3.1.0
+3.2.0
 BSD License
 Thomas Moreau
 https://github.com/joblib/threadpoolctl
 threadpoolctl
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/threadpoolctl-3.1.0.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/threadpoolctl-3.2.0.dist-info/LICENSE
 Copyright (c) 2019, threadpoolctl contributors
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
 
     * Redistributions of source code must retain the above copyright notice,
       this list of conditions and the following disclaimer.
@@ -3089,20 +3089,20 @@
 DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 typing_extensions
-4.6.3
+4.7.1
 Python Software Foundation License
 "Guido van Rossum, Jukka Lehtosalo, Łukasz Langa, Michael Lee" <levkivskyi@gmail.com>
 https://github.com/python/typing_extensions
 Backported and Experimental Type Hints for Python 3.7+
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/typing_extensions-4.6.3.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/typing_extensions-4.7.1.dist-info/LICENSE
 A. HISTORY OF THE SOFTWARE
 ==========================
 
 Python was created in the early 1990s by Guido van Rossum at Stichting
 Mathematisch Centrum (CWI, see https://www.cwi.nl) in the Netherlands
 as a successor of a language called ABC.  Guido remains Python's
 principal author, although it includes many contributions from others.
@@ -3382,15 +3382,15 @@
 
 tzdata
 2023.3
 Apache Software License
 Python Software Foundation
 https://github.com/python/tzdata
 Provider of IANA time zone data
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/tzdata-2023.3.dist-info/LICENSE
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/tzdata-2023.3.dist-info/LICENSE
 Apache Software License 2.0
 
 Copyright (c) 2020, Paul Ganssle (Google)
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
@@ -3406,15 +3406,15 @@
 
 urllib3
 1.26.16
 MIT License
 Andrey Petrov
 https://urllib3.readthedocs.io/
 HTTP library with thread-safe connection pooling, file post, and more.
-/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/urllib3-1.26.16.dist-info/LICENSE.txt
+/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/urllib3-1.26.16.dist-info/LICENSE.txt
 MIT License
 
 Copyright (c) 2008-2020 Andrey Petrov and contributors (see CONTRIBUTORS.txt)
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
```

## diveplane/client/__init__.py

```diff
@@ -63,8 +63,8 @@
 ]
 
 
 # The version number is automatically incremented by the pipeline
 # It should not be manually changed.
 # To change the major/minor number change the number in azure-pipelines.yml
 
-__version__ = "5.0.18"
+__version__ = "6.5.8"
```

## diveplane/client/base.py

```diff
@@ -77,15 +77,16 @@
         ablatement_params=None,
         accumulate_weight_feature=None,
         batch_size=None,
         derived_features=None,
         input_is_substituted=False,
         progress_callback=None,
         series=None,
-        train_weights_only=False
+        train_weights_only=False,
+        validate=True,
     ):
         """Train a trainee with sessions containing training cases."""
 
     @abstractmethod
     def impute(self, trainee_id, features=None, features_to_impute=None,
                batch_size=1):
         """Impute the missing values for the specified features_to_impute."""
@@ -182,14 +183,18 @@
     ):
         """Get cached feature residuals."""
 
     @abstractmethod
     def get_prediction_stats(
         self, trainee_id, *,
         action_feature=None,
+        condition=None,
+        num_cases=None,
+        num_robust_influence_samples_per_case=None,
+        precision=None,
         robust=None,
         robust_hyperparameters=None,
         stats=None,
         weight_feature=None,
     ):
         """Get cached feature prediction stats."""
 
@@ -258,14 +263,17 @@
         contributions=None,
         contributions_robust=None,
         hyperparameter_param_path=None,
         mda=None,
         mda_permutation=None,
         mda_robust=None,
         mda_robust_permutation=None,
+        num_robust_influence_samples=None,
+        num_robust_residual_samples=None,
+        num_robust_influence_samples_per_case=None,
         num_samples=None,
         residuals=None,
         residuals_robust=None,
         sample_model_fraction=None,
         sub_model_size=None,
         use_case_weights=False,
         weight_feature=None,
@@ -322,56 +330,56 @@
         """Send a `react` to the Diveplane engine."""
 
     @abstractmethod
     def evaluate(self, trainee_id, features_to_code_map, *, aggregation_code=None):
         """Evaluate custom code on case values within the trainee."""
 
     @abstractmethod
-    def optimize(
+    def analyze(
         self,
         trainee_id,
         context_features=None,
         action_features=None,
         *,
         bypass_calculate_feature_residuals=None,
         bypass_calculate_feature_weights=None,
-        bypass_hyperparameter_optimization=None,
-        dwe_values=None,
+        bypass_hyperparameter_analysis=None,
+        dt_values=None,
         inverse_residuals_as_weights=None,
         k_folds=None,
         k_values=None,
-        num_optimization_samples=None,
+        num_analysis_samples=None,
         num_samples=None,
-        optimization_sub_model_size=None,
-        optimize_level=None,
+        analysis_sub_model_size=None,
+        analyze_level=None,
         p_values=None,
         targeted_model=None,
         use_case_weights=None,
         use_deviations=None,
         weight_feature=None,
         **kwargs
     ):
-        """Optimizes a trainee."""
+        """Analyzes a trainee."""
 
     @abstractmethod
-    def auto_optimize(self, trainee_id):
-        """Auto-optimize the trainee model."""
+    def auto_analyze(self, trainee_id):
+        """Auto-analyze the trainee model."""
 
     @abstractmethod
-    def set_auto_optimize_params(
+    def set_auto_analyze_params(
         self,
         trainee_id,
-        auto_optimize_enabled=False,
-        optimize_threshold=None,
+        auto_analyze_enabled=False,
+        analyze_threshold=None,
         *,
-        auto_optimize_limit_size=None,
-        optimize_growth_factor=None,
+        auto_analyze_limit_size=None,
+        analyze_growth_factor=None,
         **kwargs
     ):
-        """Set trainee parameters for auto optimization."""
+        """Set trainee parameters for auto analysis."""
 
     @abstractmethod
     def get_cases(self, trainee_id, session=None, case_indices=None,
                   indicate_imputed=False, features=None, condition=None,
                   num_cases=None, precision=None):
         """Retrieve cases from a trainee."""
```

## diveplane/client/requirements-3.10.txt

```diff
@@ -1,79 +1,79 @@
 # NOTE - this file is automatically generated during CICD builds, do not update manually
 #
 # This file is autogenerated by pip-compile with Python 3.10
 # by the following command:
 #
 #    ./bin/build.sh gen_requirements 3.10
 #
-certifi==2023.5.7
+certifi==2023.7.22
     # via -r requirements.in
 cffi==1.15.1
     # via cryptography
-cryptography==41.0.1
+cryptography==41.0.2
     # via -r requirements.in
 deprecation==2.1.0
     # via -r requirements.in
-diveplane-amalgam-api==2.3.9
+diveplane-amalgam-api==2.3.12
     # via -r requirements.in
-diveplane-openapi-client==23.0.5
+diveplane-openapi-client==24.0.8
     # via -r requirements.in
-faker==18.11.2
+faker==19.2.0
     # via -r requirements.in
 humanize==4.7.0
     # via -r requirements.in
 joblib==1.3.1
     # via scikit-learn
 markdown-it-py==3.0.0
     # via rich
 mdurl==0.1.2
     # via markdown-it-py
-mmh3==4.0.0
+mmh3==4.0.1
     # via -r requirements.in
-numpy==1.25.0
+numpy==1.25.1
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
     #   pandas
     #   scikit-learn
     #   scipy
 packaging==23.1
     # via deprecation
 pandas==2.0.3
     # via -r requirements.in
 pycparser==2.21
     # via cffi
 pygments==2.15.1
     # via rich
-pyjwt==2.7.0
+pyjwt==2.8.0
     # via -r requirements.in
 python-dateutil==2.8.2
     # via
     #   diveplane-openapi-client
     #   faker
     #   pandas
 pytz==2023.3
     # via pandas
-pyyaml==6.0
+pyyaml==6.0.1
     # via -r requirements.in
-rich==13.4.2
+rich==13.5.0
     # via -r requirements.in
 scikit-learn==1.3.0
     # via -r requirements.in
 scipy==1.11.1
     # via scikit-learn
 semantic-version==2.10.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
 six==1.16.0
     # via
     #   diveplane-openapi-client
     #   python-dateutil
-threadpoolctl==3.1.0
+threadpoolctl==3.2.0
     # via scikit-learn
 typing-extensions==4.7.1
     # via -r requirements.in
 tzdata==2023.3
     # via pandas
 urllib3==1.26.16
     # via
```

## diveplane/client/requirements-3.11.txt

```diff
@@ -1,79 +1,79 @@
 # NOTE - this file is automatically generated during CICD builds, do not update manually
 #
 # This file is autogenerated by pip-compile with Python 3.11
 # by the following command:
 #
 #    ./bin/build.sh gen_requirements 3.11
 #
-certifi==2023.5.7
+certifi==2023.7.22
     # via -r requirements.in
 cffi==1.15.1
     # via cryptography
-cryptography==41.0.1
+cryptography==41.0.2
     # via -r requirements.in
 deprecation==2.1.0
     # via -r requirements.in
-diveplane-amalgam-api==2.3.9
+diveplane-amalgam-api==2.3.12
     # via -r requirements.in
-diveplane-openapi-client==23.0.5
+diveplane-openapi-client==24.0.8
     # via -r requirements.in
-faker==18.11.2
+faker==19.2.0
     # via -r requirements.in
 humanize==4.7.0
     # via -r requirements.in
 joblib==1.3.1
     # via scikit-learn
 markdown-it-py==3.0.0
     # via rich
 mdurl==0.1.2
     # via markdown-it-py
-mmh3==4.0.0
+mmh3==4.0.1
     # via -r requirements.in
-numpy==1.25.0
+numpy==1.25.1
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
     #   pandas
     #   scikit-learn
     #   scipy
 packaging==23.1
     # via deprecation
 pandas==2.0.3
     # via -r requirements.in
 pycparser==2.21
     # via cffi
 pygments==2.15.1
     # via rich
-pyjwt==2.7.0
+pyjwt==2.8.0
     # via -r requirements.in
 python-dateutil==2.8.2
     # via
     #   diveplane-openapi-client
     #   faker
     #   pandas
 pytz==2023.3
     # via pandas
-pyyaml==6.0
+pyyaml==6.0.1
     # via -r requirements.in
-rich==13.4.2
+rich==13.5.0
     # via -r requirements.in
 scikit-learn==1.3.0
     # via -r requirements.in
 scipy==1.11.1
     # via scikit-learn
 semantic-version==2.10.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
 six==1.16.0
     # via
     #   diveplane-openapi-client
     #   python-dateutil
-threadpoolctl==3.1.0
+threadpoolctl==3.2.0
     # via scikit-learn
 typing-extensions==4.7.1
     # via -r requirements.in
 tzdata==2023.3
     # via pandas
 urllib3==1.26.16
     # via
```

## diveplane/client/requirements-3.8.txt

```diff
@@ -1,37 +1,37 @@
 # NOTE - this file is automatically generated during CICD builds, do not update manually
 #
 # This file is autogenerated by pip-compile with Python 3.8
 # by the following command:
 #
 #    ./bin/build.sh gen_requirements 3.8
 #
-certifi==2023.5.7
+certifi==2023.7.22
     # via -r requirements.in
 cffi==1.15.1
     # via cryptography
-cryptography==41.0.1
+cryptography==41.0.2
     # via -r requirements.in
 deprecation==2.1.0
     # via -r requirements.in
-diveplane-amalgam-api==2.3.9
+diveplane-amalgam-api==2.3.12
     # via -r requirements.in
-diveplane-openapi-client==23.0.5
+diveplane-openapi-client==24.0.8
     # via -r requirements.in
-faker==18.11.2
+faker==19.2.0
     # via -r requirements.in
 humanize==4.7.0
     # via -r requirements.in
 joblib==1.3.1
     # via scikit-learn
 markdown-it-py==3.0.0
     # via rich
 mdurl==0.1.2
     # via markdown-it-py
-mmh3==4.0.0
+mmh3==4.0.1
     # via -r requirements.in
 numpy==1.24.4
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
     #   pandas
     #   scikit-learn
@@ -40,44 +40,45 @@
     # via deprecation
 pandas==2.0.3
     # via -r requirements.in
 pycparser==2.21
     # via cffi
 pygments==2.15.1
     # via rich
-pyjwt==2.7.0
+pyjwt==2.8.0
     # via -r requirements.in
 python-dateutil==2.8.2
     # via
     #   diveplane-openapi-client
     #   faker
     #   pandas
 pytz==2023.3
     # via pandas
-pyyaml==6.0
+pyyaml==6.0.1
     # via -r requirements.in
-rich==13.4.2
+rich==13.5.0
     # via -r requirements.in
 scikit-learn==1.3.0
     # via -r requirements.in
 scipy==1.10.1
     # via scikit-learn
 semantic-version==2.10.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
 six==1.16.0
     # via
     #   diveplane-openapi-client
     #   python-dateutil
-threadpoolctl==3.1.0
+threadpoolctl==3.2.0
     # via scikit-learn
 typing-extensions==4.7.1
     # via
     #   -r requirements.in
+    #   faker
     #   rich
 tzdata==2023.3
     # via pandas
 urllib3==1.26.16
     # via
     #   -r requirements.in
     #   diveplane-openapi-client
```

## diveplane/client/requirements-3.9.txt

```diff
@@ -1,79 +1,79 @@
 # NOTE - this file is automatically generated during CICD builds, do not update manually
 #
 # This file is autogenerated by pip-compile with Python 3.9
 # by the following command:
 #
 #    ./bin/build.sh gen_requirements 3.9
 #
-certifi==2023.5.7
+certifi==2023.7.22
     # via -r requirements.in
 cffi==1.15.1
     # via cryptography
-cryptography==41.0.1
+cryptography==41.0.2
     # via -r requirements.in
 deprecation==2.1.0
     # via -r requirements.in
-diveplane-amalgam-api==2.3.9
+diveplane-amalgam-api==2.3.12
     # via -r requirements.in
-diveplane-openapi-client==23.0.5
+diveplane-openapi-client==24.0.8
     # via -r requirements.in
-faker==18.11.2
+faker==19.2.0
     # via -r requirements.in
 humanize==4.7.0
     # via -r requirements.in
 joblib==1.3.1
     # via scikit-learn
 markdown-it-py==3.0.0
     # via rich
 mdurl==0.1.2
     # via markdown-it-py
-mmh3==4.0.0
+mmh3==4.0.1
     # via -r requirements.in
-numpy==1.25.0
+numpy==1.25.1
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
     #   pandas
     #   scikit-learn
     #   scipy
 packaging==23.1
     # via deprecation
 pandas==2.0.3
     # via -r requirements.in
 pycparser==2.21
     # via cffi
 pygments==2.15.1
     # via rich
-pyjwt==2.7.0
+pyjwt==2.8.0
     # via -r requirements.in
 python-dateutil==2.8.2
     # via
     #   diveplane-openapi-client
     #   faker
     #   pandas
 pytz==2023.3
     # via pandas
-pyyaml==6.0
+pyyaml==6.0.1
     # via -r requirements.in
-rich==13.4.2
+rich==13.5.0
     # via -r requirements.in
 scikit-learn==1.3.0
     # via -r requirements.in
 scipy==1.11.1
     # via scikit-learn
 semantic-version==2.10.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
 six==1.16.0
     # via
     #   diveplane-openapi-client
     #   python-dateutil
-threadpoolctl==3.1.0
+threadpoolctl==3.2.0
     # via scikit-learn
 typing-extensions==4.7.1
     # via -r requirements.in
 tzdata==2023.3
     # via pandas
 urllib3==1.26.16
     # via
```

## diveplane/client/requirements-dev-3.10.txt

```diff
@@ -1,29 +1,29 @@
 # NOTE - this file is automatically generated during CICD builds, do not update manually
 #
 # This file is autogenerated by pip-compile with Python 3.10
 # by the following command:
 #
 #    ./bin/build.sh gen_requirements 3.10
 #
-astroid==2.15.5
+astroid==2.15.6
     # via pylint
 build==0.10.0
     # via pip-tools
-certifi==2023.5.7
+certifi==2023.7.22
     # via
     #   -c requirements-3.10.txt
     #   requests
-charset-normalizer==3.1.0
+charset-normalizer==3.2.0
     # via requests
-click==8.1.3
+click==8.1.6
     # via pip-tools
 coverage[toml]==7.2.7
     # via pytest-cov
-dill==0.3.6
+dill==0.3.7
     # via
     #   -r requirements-dev-3.10.in
     #   pylint
 diveplane-build-artifacts==1.0.10
     # via -r requirements-dev-3.10.in
 dohq-artifactory==0.9.0
     # via diveplane-build-artifacts
@@ -45,51 +45,51 @@
     # via pylint
 lazy-object-proxy==1.9.0
     # via astroid
 mccabe==0.6.1
     # via
     #   flake8
     #   pylint
-numpy==1.25.0
+numpy==1.25.1
     # via
     #   -c requirements-3.10.txt
     #   pandas
 packaging==23.1
     # via
     #   -c requirements-3.10.txt
     #   build
     #   pytest
 pandas==2.0.3
     # via
     #   -c requirements-3.10.txt
     #   pmlb
-pip-tools==6.14.0
+pip-tools==7.1.0
     # via -r requirements-dev-3.10.in
-pipdeptree==2.9.3
+pipdeptree==2.12.0
     # via -r requirements-dev-3.10.in
-platformdirs==3.8.0
+platformdirs==3.10.0
     # via pylint
 pluggy==1.2.0
     # via pytest
 pmlb==1.0.1.post3
     # via -r requirements-dev-3.10.in
 pycodestyle==2.7.0
     # via
     #   -r requirements-dev-3.10.in
     #   flake8
     #   flake8-import-order
 pydocstyle==6.3.0
     # via flake8-docstrings
 pyflakes==2.3.1
     # via flake8
-pyjwt==2.7.0
+pyjwt==2.8.0
     # via
     #   -c requirements-3.10.txt
     #   dohq-artifactory
-pylint==2.17.4
+pylint==2.17.5
     # via -r requirements-dev-3.10.in
 pyproject-hooks==1.0.0
     # via build
 pytest==7.4.0
     # via
     #   -r requirements-dev-3.10.in
     #   pytest-cov
@@ -103,15 +103,15 @@
     #   -c requirements-3.10.txt
     #   dohq-artifactory
     #   pandas
 pytz==2023.3
     # via
     #   -c requirements-3.10.txt
     #   pandas
-pyyaml==6.0
+pyyaml==6.0.1
     # via
     #   -c requirements-3.10.txt
     #   diveplane-build-artifacts
     #   pmlb
 requests==2.31.0
     # via
     #   dohq-artifactory
@@ -130,29 +130,30 @@
     # via
     #   build
     #   coverage
     #   pip-tools
     #   pylint
     #   pyproject-hooks
     #   pytest
-tomlkit==0.11.8
+tomlkit==0.12.1
     # via pylint
 typing-extensions==4.7.1
     # via
     #   -c requirements-3.10.txt
     #   astroid
 tzdata==2023.3
     # via
     #   -c requirements-3.10.txt
     #   pandas
 urllib3==1.26.16
     # via
+    #   -c requirements-3.10.txt
     #   -r requirements-dev-3.10.in
     #   requests
-wheel==0.40.0
+wheel==0.41.0
     # via pip-tools
 wrapt==1.15.0
     # via astroid
 
 # The following packages are considered to be unsafe in a requirements file:
 # pip
 # setuptools
```

## diveplane/client/requirements-dev-3.11.txt

```diff
@@ -1,29 +1,29 @@
 # NOTE - this file is automatically generated during CICD builds, do not update manually
 #
 # This file is autogenerated by pip-compile with Python 3.11
 # by the following command:
 #
 #    ./bin/build.sh gen_requirements 3.11
 #
-astroid==2.15.5
+astroid==2.15.6
     # via pylint
 build==0.10.0
     # via pip-tools
-certifi==2023.5.7
+certifi==2023.7.22
     # via
     #   -c requirements-3.11.txt
     #   requests
-charset-normalizer==3.1.0
+charset-normalizer==3.2.0
     # via requests
-click==8.1.3
+click==8.1.6
     # via pip-tools
 coverage[toml]==7.2.7
     # via pytest-cov
-dill==0.3.6
+dill==0.3.7
     # via
     #   -r requirements-dev-3.11.in
     #   pylint
 diveplane-build-artifacts==1.0.10
     # via -r requirements-dev-3.11.in
 dohq-artifactory==0.9.0
     # via diveplane-build-artifacts
@@ -43,51 +43,51 @@
     # via pylint
 lazy-object-proxy==1.9.0
     # via astroid
 mccabe==0.6.1
     # via
     #   flake8
     #   pylint
-numpy==1.25.0
+numpy==1.25.1
     # via
     #   -c requirements-3.11.txt
     #   pandas
 packaging==23.1
     # via
     #   -c requirements-3.11.txt
     #   build
     #   pytest
 pandas==2.0.3
     # via
     #   -c requirements-3.11.txt
     #   pmlb
-pip-tools==6.14.0
+pip-tools==7.1.0
     # via -r requirements-dev-3.11.in
-pipdeptree==2.9.3
+pipdeptree==2.12.0
     # via -r requirements-dev-3.11.in
-platformdirs==3.8.0
+platformdirs==3.10.0
     # via pylint
 pluggy==1.2.0
     # via pytest
 pmlb==1.0.1.post3
     # via -r requirements-dev-3.11.in
 pycodestyle==2.7.0
     # via
     #   -r requirements-dev-3.11.in
     #   flake8
     #   flake8-import-order
 pydocstyle==6.3.0
     # via flake8-docstrings
 pyflakes==2.3.1
     # via flake8
-pyjwt==2.7.0
+pyjwt==2.8.0
     # via
     #   -c requirements-3.11.txt
     #   dohq-artifactory
-pylint==2.17.4
+pylint==2.17.5
     # via -r requirements-dev-3.11.in
 pyproject-hooks==1.0.0
     # via build
 pytest==7.4.0
     # via
     #   -r requirements-dev-3.11.in
     #   pytest-cov
@@ -101,15 +101,15 @@
     #   -c requirements-3.11.txt
     #   dohq-artifactory
     #   pandas
 pytz==2023.3
     # via
     #   -c requirements-3.11.txt
     #   pandas
-pyyaml==6.0
+pyyaml==6.0.1
     # via
     #   -c requirements-3.11.txt
     #   diveplane-build-artifacts
     #   pmlb
 requests==2.31.0
     # via
     #   dohq-artifactory
@@ -120,25 +120,26 @@
     #   diveplane-build-artifacts
 six==1.16.0
     # via
     #   -c requirements-3.11.txt
     #   python-dateutil
 snowballstemmer==2.2.0
     # via pydocstyle
-tomlkit==0.11.8
+tomlkit==0.12.1
     # via pylint
 tzdata==2023.3
     # via
     #   -c requirements-3.11.txt
     #   pandas
 urllib3==1.26.16
     # via
+    #   -c requirements-3.11.txt
     #   -r requirements-dev-3.11.in
     #   requests
-wheel==0.40.0
+wheel==0.41.0
     # via pip-tools
 wrapt==1.15.0
     # via astroid
 
 # The following packages are considered to be unsafe in a requirements file:
 # pip
 # setuptools
```

## diveplane/client/requirements-dev-3.8.txt

```diff
@@ -1,29 +1,29 @@
 # NOTE - this file is automatically generated during CICD builds, do not update manually
 #
 # This file is autogenerated by pip-compile with Python 3.8
 # by the following command:
 #
 #    ./bin/build.sh gen_requirements 3.8
 #
-astroid==2.15.5
+astroid==2.15.6
     # via pylint
 build==0.10.0
     # via pip-tools
-certifi==2023.5.7
+certifi==2023.7.22
     # via
     #   -c requirements-3.8.txt
     #   requests
-charset-normalizer==3.1.0
+charset-normalizer==3.2.0
     # via requests
-click==8.1.3
+click==8.1.6
     # via pip-tools
 coverage[toml]==7.2.7
     # via pytest-cov
-dill==0.3.6
+dill==0.3.7
     # via
     #   -r requirements-dev-3.8.in
     #   pylint
 diveplane-build-artifacts==1.0.10
     # via -r requirements-dev-3.8.in
 dohq-artifactory==0.9.0
     # via diveplane-build-artifacts
@@ -58,38 +58,38 @@
     #   -c requirements-3.8.txt
     #   build
     #   pytest
 pandas==2.0.3
     # via
     #   -c requirements-3.8.txt
     #   pmlb
-pip-tools==6.14.0
+pip-tools==7.1.0
     # via -r requirements-dev-3.8.in
-pipdeptree==2.9.3
+pipdeptree==2.12.0
     # via -r requirements-dev-3.8.in
-platformdirs==3.8.0
+platformdirs==3.10.0
     # via pylint
 pluggy==1.2.0
     # via pytest
 pmlb==1.0.1.post3
     # via -r requirements-dev-3.8.in
 pycodestyle==2.7.0
     # via
     #   -r requirements-dev-3.8.in
     #   flake8
     #   flake8-import-order
 pydocstyle==6.3.0
     # via flake8-docstrings
 pyflakes==2.3.1
     # via flake8
-pyjwt==2.7.0
+pyjwt==2.8.0
     # via
     #   -c requirements-3.8.txt
     #   dohq-artifactory
-pylint==2.17.4
+pylint==2.17.5
     # via -r requirements-dev-3.8.in
 pyproject-hooks==1.0.0
     # via build
 pytest==7.4.0
     # via
     #   -r requirements-dev-3.8.in
     #   pytest-cov
@@ -103,15 +103,15 @@
     #   -c requirements-3.8.txt
     #   dohq-artifactory
     #   pandas
 pytz==2023.3
     # via
     #   -c requirements-3.8.txt
     #   pandas
-pyyaml==6.0
+pyyaml==6.0.1
     # via
     #   -c requirements-3.8.txt
     #   diveplane-build-artifacts
     #   pmlb
 requests==2.31.0
     # via
     #   dohq-artifactory
@@ -130,30 +130,31 @@
     # via
     #   build
     #   coverage
     #   pip-tools
     #   pylint
     #   pyproject-hooks
     #   pytest
-tomlkit==0.11.8
+tomlkit==0.12.1
     # via pylint
 typing-extensions==4.7.1
     # via
     #   -c requirements-3.8.txt
     #   astroid
     #   pylint
 tzdata==2023.3
     # via
     #   -c requirements-3.8.txt
     #   pandas
 urllib3==1.26.16
     # via
+    #   -c requirements-3.8.txt
     #   -r requirements-dev-3.8.in
     #   requests
-wheel==0.40.0
+wheel==0.41.0
     # via pip-tools
 wrapt==1.15.0
     # via astroid
 
 # The following packages are considered to be unsafe in a requirements file:
 # pip
 # setuptools
```

## diveplane/client/requirements-dev-3.9.txt

```diff
@@ -1,29 +1,29 @@
 # NOTE - this file is automatically generated during CICD builds, do not update manually
 #
 # This file is autogenerated by pip-compile with Python 3.9
 # by the following command:
 #
 #    ./bin/build.sh gen_requirements 3.9
 #
-astroid==2.15.5
+astroid==2.15.6
     # via pylint
 build==0.10.0
     # via pip-tools
-certifi==2023.5.7
+certifi==2023.7.22
     # via
     #   -c requirements-3.9.txt
     #   requests
-charset-normalizer==3.1.0
+charset-normalizer==3.2.0
     # via requests
-click==8.1.3
+click==8.1.6
     # via pip-tools
 coverage[toml]==7.2.7
     # via pytest-cov
-dill==0.3.6
+dill==0.3.7
     # via
     #   -r requirements-dev-3.9.in
     #   pylint
 diveplane-build-artifacts==1.0.10
     # via -r requirements-dev-3.9.in
 dohq-artifactory==0.9.0
     # via diveplane-build-artifacts
@@ -45,51 +45,51 @@
     # via pylint
 lazy-object-proxy==1.9.0
     # via astroid
 mccabe==0.6.1
     # via
     #   flake8
     #   pylint
-numpy==1.25.0
+numpy==1.25.1
     # via
     #   -c requirements-3.9.txt
     #   pandas
 packaging==23.1
     # via
     #   -c requirements-3.9.txt
     #   build
     #   pytest
 pandas==2.0.3
     # via
     #   -c requirements-3.9.txt
     #   pmlb
-pip-tools==6.14.0
+pip-tools==7.1.0
     # via -r requirements-dev-3.9.in
-pipdeptree==2.9.3
+pipdeptree==2.12.0
     # via -r requirements-dev-3.9.in
-platformdirs==3.8.0
+platformdirs==3.10.0
     # via pylint
 pluggy==1.2.0
     # via pytest
 pmlb==1.0.1.post3
     # via -r requirements-dev-3.9.in
 pycodestyle==2.7.0
     # via
     #   -r requirements-dev-3.9.in
     #   flake8
     #   flake8-import-order
 pydocstyle==6.3.0
     # via flake8-docstrings
 pyflakes==2.3.1
     # via flake8
-pyjwt==2.7.0
+pyjwt==2.8.0
     # via
     #   -c requirements-3.9.txt
     #   dohq-artifactory
-pylint==2.17.4
+pylint==2.17.5
     # via -r requirements-dev-3.9.in
 pyproject-hooks==1.0.0
     # via build
 pytest==7.4.0
     # via
     #   -r requirements-dev-3.9.in
     #   pytest-cov
@@ -103,15 +103,15 @@
     #   -c requirements-3.9.txt
     #   dohq-artifactory
     #   pandas
 pytz==2023.3
     # via
     #   -c requirements-3.9.txt
     #   pandas
-pyyaml==6.0
+pyyaml==6.0.1
     # via
     #   -c requirements-3.9.txt
     #   diveplane-build-artifacts
     #   pmlb
 requests==2.31.0
     # via
     #   dohq-artifactory
@@ -130,30 +130,31 @@
     # via
     #   build
     #   coverage
     #   pip-tools
     #   pylint
     #   pyproject-hooks
     #   pytest
-tomlkit==0.11.8
+tomlkit==0.12.1
     # via pylint
 typing-extensions==4.7.1
     # via
     #   -c requirements-3.9.txt
     #   astroid
     #   pylint
 tzdata==2023.3
     # via
     #   -c requirements-3.9.txt
     #   pandas
 urllib3==1.26.16
     # via
+    #   -c requirements-3.9.txt
     #   -r requirements-dev-3.9.in
     #   requests
-wheel==0.40.0
+wheel==0.41.0
     # via pip-tools
 wrapt==1.15.0
     # via astroid
 
 # The following packages are considered to be unsafe in a requirements file:
 # pip
 # setuptools
```

## diveplane/client/requirements.in

```diff
@@ -3,15 +3,15 @@
 deprecation>=2.1.0,<3
 Faker>=4.1.1
 humanize>=4.0.0
 mmh3
 numpy
 pandas
 pyjwt>=2.6.0,<3
-pyyaml>=5.4.1,<7.0
+pyyaml>=6.0.1,<7.0
 rich>=12.5.1
 scikit-learn
 semantic-version>=2.8.5,<3
 typing-extensions>=4.1.0,<5.0
 urllib3>=1.26.2,<=2
-diveplane-openapi-client==23.0.6 # Added by automation - please leave
-diveplane-amalgam-api==2.3.9 # Added by automation - please leave
+diveplane-openapi-client==24.0.9 # Added by automation - please leave
+diveplane-amalgam-api==2.3.12 # Added by automation - please leave
```

## diveplane/client/tests/test_client.py

```diff
@@ -790,21 +790,21 @@
 
         Test for verbose output expected when get_trainee_sessions is called.
         """
         client.get_trainee_sessions(trainee.id)
         out, err = capsys.readouterr()
         assert f'Getting sessions from trainee with id: {trainee.id}' in out
 
-    def test_optimize_verbose(self, trainee, capsys):
-        """Test for verbose output expected when optimize is called."""
+    def test_analyze_verbose(self, trainee, capsys):
+        """Test for verbose output expected when analyze is called."""
         context_features = ['class']
-        client.optimize(trainee.id, context_features)
+        client.analyze(trainee.id, context_features)
         out, err = capsys.readouterr()
-        assert f'Optimizing trainee with id: {trainee.id}' in out
-        assert 'Optimizing trainee with parameters: ' in out
+        assert f'Analyzing trainee with id: {trainee.id}' in out
+        assert 'Analyzing trainee with parameters: ' in out
 
     @pytest.mark.skipif('WEB' not in TEST_OPTIONS, reason='Web client only')
     def test_wait_for_action_exception(self, mocker):
         """
         Test that wait_for_action raises when expected.
 
         Tests for expected exception to be raised when the max_wait_time
@@ -969,22 +969,22 @@
     def test_set_params_verbose(self, trainee, capsys):
         """Test for the verbose output expected when set_params is called."""
         params = {
             "hyperparameter_map": {
                 ".targetless": {
                     "robust": {
                         ".none": {
-                            "dwe": -1, "p": .1, "k": 8
+                            "dt": -1, "p": .1, "k": 8
                         }
                     }
                 }
             },
-            "optimize_threshold": 100,
-            "optimize_growth_factor": 3,
-            "auto_optimize_limit_size": 100000
+            "analyze_threshold": 100,
+            "analyze_growth_factor": 3,
+            "auto_analyze_limit_size": 100000
         }
         client.set_params(trainee.id, params)
         out, err = capsys.readouterr()
         assert (f'Setting model attributes for trainee with '
                 f'id: {trainee.id}') in out
 
     def test_get_configuration_path_exceptions(self, mocker):
@@ -1006,29 +1006,29 @@
         DiveplaneClient is created with an invalid path
         """
         with pytest.raises(DiveplaneConfigurationError) as exc:
             DiveplaneClient(config_path="Fake/Path", verbose=True)
         assert "Specified configuration file was not found" in str(exc)
 
     @pytest.mark.skipif('WEB' not in TEST_OPTIONS, reason='Web client only')
-    def test_train_optimizes(self, dp_client, trainee, mocker):
+    def test_train_analyzes(self, dp_client, trainee, mocker):
         """
-        Test that traine optimizes when expected.
+        Test that traine analyzes when expected.
 
-        Test that auto_optimize is also called when test_train is called
+        Test that auto_analyze is also called when test_train is called
         using mocking.
         """
         df = pd.read_csv(IRIS_FILE_PATH)
         data = df.values
         # Ensure we don't trigger batch_train conditions...
         data_train = data[:dp_client._train_batch_threshold]
-        return_value = TrainResponse(status="optimize")
+        return_value = TrainResponse(status="analyze")
         dp_client._train = mocker.Mock(return_value=return_value)
 
-        spy = mocker.spy(dp_client, 'auto_optimize')
+        spy = mocker.spy(dp_client, 'auto_analyze')
         dp_client.train(trainee.id, cases=data_train,
                         features=df.columns.tolist(), batch_size=None)
         spy.assert_called_once_with(trainee.id)
 
     def test_set_get_substitute_feature_values(self, trainee, capsys):
         """
         Test that set_substitute_feature_values works as expected.
```

## diveplane/direct/client.py

```diff
@@ -1,16 +1,18 @@
 from copy import deepcopy
 from datetime import datetime
 import json
 import logging
+import multiprocessing
 import operator
 import os
 from pathlib import Path
 import platform
 from typing import (
+    Any,
     Callable,
     Dict,
     Iterable,
     List,
     Literal,
     Optional,
     Sequence,
@@ -704,15 +706,15 @@
                 f"{new_trainee_name} trainee already exists."
             )
 
     def load_trainee(self, trainee_id: str):
         """
         Load a Trainee that was persisted on the Diveplane service.
 
-        .. deprecated::
+        .. deprecated:: 1.0.0
             Use :meth:`DiveplaneDirectClient.acquire_trainee_resources` instead.
 
         Parameters
         ----------
         trainee_id : str
             The ID of the Trainee load.
         """
@@ -722,15 +724,15 @@
             'instead.', DeprecationWarning)
         self.acquire_trainee_resources(trainee_id)
 
     def unload_trainee(self, trainee_id: str):
         """
         Unload a Trainee from the Diveplane service.
 
-        .. deprecated::
+        .. deprecated:: 1.0.0
             Use :meth:`DiveplaneDirectClient.release_trainee_resources` instead.
 
         Parameters
         ----------
         trainee_id : str
             The ID of the Trainee unload.
         """
@@ -1038,15 +1040,16 @@
         ablatement_params: Optional[Dict[str, List[object]]] = None,
         accumulate_weight_feature: Optional[str] = None,
         batch_size: Optional[int] = None,
         derived_features: Optional[Iterable[str]] = None,
         input_is_substituted: bool = False,
         progress_callback: Optional[Callable] = None,
         series: Optional[str] = None,
-        train_weights_only: bool = False
+        train_weights_only: bool = False,
+        validate: bool = True,
     ):
         """
         Train one or more cases into a trainee (model).
 
         Parameters
         ----------
         trainee_id : str
@@ -1108,34 +1111,38 @@
             the value(s) of this case are appended to all cases in the series.
             If cases is the same length as the series, the value of each case
             in cases is applied in order to each of the cases in the series.
         train_weights_only : bool, default False
             When true, and accumulate_weight_feature is provided,
             will accumulate all of the cases' neighbor weights instead of
             training the cases into the model.
+        validate : bool, default True
+            Whether to validate the data against the provided feature
+            attributes. Issues warnings if there are any discrepancies between
+            the data and the features dictionary.
         """
         self._auto_resolve_trainee(trainee_id)
         feature_attributes = self.trainee_cache.get(trainee_id).features
 
         # Check to see if the feature attributes still generally describe
         # the data, and warn the user if they do not
-        if isinstance(cases, DataFrame):
+        if isinstance(cases, DataFrame) and validate:
             attrs_obj = SingleTableFeatureAttributes(feature_attributes, {})
             attrs_obj.validate(cases)
 
         validate_list_shape(features, 1, "features", "str")
         if self.verbose:
             print(f'Training session(s) on trainee with id: {trainee_id}')
 
         validate_list_shape(cases, 2, "cases", "list", allow_none=False)
         if features is None:
             features = internals.get_features_from_data(cases)
         cases = serialize_cases(cases, features, feature_attributes, warn=True)
 
-        auto_optimize = False
+        auto_analyze = False
 
         with ProgressTimer(len(cases)) as progress:
             gen_batch_size = None
             if series is not None:
                 # If training series, always send full size
                 batch_size = len(cases)
             if not batch_size:
@@ -1158,16 +1165,16 @@
                     features=features,
                     input_cases=cases[start:end],
                     input_is_substituted=input_is_substituted,
                     series=series,
                     session=self.active_session.id,
                     train_weights_only=train_weights_only
                 )
-                if response and response.get('status') == 'optimize':
-                    auto_optimize = True
+                if response and response.get('status') == 'analyze':
+                    auto_analyze = True
                 if gen_batch_size is None:
                     progress.update(batch_size)
                 else:
                     batch_size = next(gen_batch_size, None)
 
         # Final call to batch callback on completion
         if isinstance(progress_callback, Callable):
@@ -1178,17 +1185,17 @@
             trainee_id,
             self.active_session.id,
             self.active_session
         )
 
         self._auto_persist_trainee(trainee_id)
 
-        # kick off auto-optimize if the train response requests it
-        if auto_optimize:
-            self.auto_optimize(trainee_id)
+        # kick off auto-analyze if the train response requests it
+        if auto_analyze:
+            self.auto_analyze(trainee_id)
 
     def impute(
         self,
         trainee_id: str,
         features: Optional[Iterable[str]] = None,
         features_to_impute: Optional[Iterable[str]] = None,
         batch_size: int = 1
@@ -2124,15 +2131,19 @@
         case_indices = react_params.get('case_indices')
         initial_values = react_params.get('initial_values')
         max_series_lengths = react_params.get('max_series_lengths')
         series_context_values = react_params.get('series_context_values')
         series_stop_maps = react_params.get('series_stop_maps')
 
         with ProgressTimer(total_size) as progress:
-            batch_scaler = self.batch_scaler_class(1, progress)
+            if self.dp.amlg.library_postfix[1:] == 'mt':
+                start_batch_size = max(multiprocessing.cpu_count(), 1)
+            else:
+                start_batch_size = 1
+            batch_scaler = self.batch_scaler_class(start_batch_size, progress)
             gen_batch_size = batch_scaler.gen_batch_size()
             batch_size = next(gen_batch_size, None)
 
             while not progress.is_complete and batch_size is not None:
                 if isinstance(progress_callback, Callable):
                     progress_callback(progress, temp_result)
                 batch_start = progress.current_tick
@@ -2398,21 +2409,27 @@
             - num_most_similar_cases: int, optional
                 Outputs this manually specified number of most similar cases,
                 which will first include the influential cases.
 
                 NOTE: The maximum number of cases that can be queried is
                 `1000`.
 
-            - num_most_similar_case_indices: int, optional
+            - num_most_similar_case_indices : int, optional
                 Outputs the specified number of most similar case indices when
                 'distance_ratio' is also set to True.
 
                 NOTE: The maximum number of cases that can be queried is
                 '1000'.
 
+            - num_robust_influence_samples_per_case : int, optional
+                Specifies the number of robust samples to use for each case.
+                Applicable only for computing robust feature contributions or
+                robust case feature contributions. Defaults to 2000. Higher
+                values will take longer but provide more stable results.
+
             - boundary_cases: bool, optional
                 If True outputs an automatically determined (when
                 'num_boundary_cases' is not specified) relevant number of
                 boundary cases. Uses both context and action features of the
                 reacted case to determine the counterfactual boundary based on
                 action features, which maximize the dissimilarity of action
                 features while maximizing the similarity of context features.
@@ -3570,26 +3587,26 @@
         Get cached feature residuals.
 
         All keyword arguments are optional, when not specified will auto-select
         cached residuals for output, when specified will attempt to
         output the cached residuals best matching the requested parameters,
         if no cached match is found.
 
-        .. deprecated::
-            Use :meth:`DiveplaneDirectClient.get_feature_residuals` instead.
+        .. deprecated:: 1.0.0
+            Use :meth:`DiveplaneDirectClient.get_prediction_stats` instead.
 
         Parameters
         ----------
         trainee_id : str
             The id or name of the trainee.
         action_feature : str, optional
             When specified, will attempt to return residuals that
             were computed for this specified action_feature.
             Note: ".targetless" is the action feature used during targetless
-            optimization.
+            analysis.
         robust : bool, optional
             When specified, will attempt to return residuals that
             were computed with the specified robust or non-robust type.
         robust_hyperparameters : bool, optional
             When specified, will attempt to return residuals that were
             computed using hyperpparameters with the specified robust or
             non-robust type.
@@ -3633,16 +3650,16 @@
         Get cached feature Mean Decrease In Accuracy (MDA).
 
         All keyword arguments are optional, when not specified will auto-select
         cached MDA for output, when specified will attempt to
         output the cached MDA best matching the requested parameters,
         if no cached match is found.
 
-        .. deprecated::
-            Use :meth:`DiveplaneDirectClient.get_feature_residuals` instead.
+        .. deprecated:: 1.0.0
+            Use :meth:`DiveplaneDirectClient.get_prediction_stats` instead.
 
         Parameters
         ----------
         trainee_id : str
             The id or name of the trainee.
         action_feature : str
             Will attempt to return MDA that was
@@ -3693,16 +3710,16 @@
         Get cached feature contributions.
 
         All keyword arguments are optional. When not specified, will
         auto-select cached contributions for output. When specified, will
         attempt to output the cached contributions best matching the requested
         parameters, if no cached match is found.
 
-        .. deprecated::
-            Use :meth:`DiveplaneDirectClient.get_feature_residuals` instead.
+        .. deprecated:: 1.0.0
+            Use :meth:`DiveplaneDirectClient.get_prediction_stats` instead.
 
         Parameters
         ----------
         trainee_id : str
             The id or name of the trainee.
         action_feature : str
             Will attempt to return contributions that were
@@ -3737,53 +3754,101 @@
         return contributions
 
     def get_prediction_stats(
         self,
         trainee_id: str,
         *,
         action_feature: Optional[str] = None,
+        condition: Optional[Dict[str, Any]] = None,
+        num_cases: Optional[int] = None,
+        num_robust_influence_samples_per_case=None,
+        precision: Optional[str] = None,
         robust: Optional[bool] = None,
         robust_hyperparameters: Optional[bool] = None,
         stats: Optional[Iterable[str]] = None,
         weight_feature: Optional[str] = None,
     ) -> Dict[str, Dict[str, float]]:
         """
-        Get cached feature prediction stats.
+        Get feature prediction stats.
+
+        Gets cached stats when condition is None.
+        If condition is not None, then uses the condition to select cases and
+        computes prediction stats for that set of cases.
 
         All keyword arguments are optional, when not specified will auto-select
         all cached stats for output, when specified will attempt to
         output the cached stats best matching the requested parameters,
         if no cached match is found.
 
         Parameters
         ----------
         trainee_id : str
             The id or name of the trainee.
         action_feature : str, optional
             When specified, will attempt to return stats that
             were computed for this specified action_feature.
             Note: ".targetless" is the action feature used during targetless
-            optimization.
+            analysis.
+
+            .. NOTE::
+                If get_prediction_stats is being used with time series analysis,
+                the action feature for which the prediction statistics information
+                is desired must be specified.
+        condition : dict or None, optional
+            A condition map to select which cases to compute prediction stats
+            for.
+
+            .. NOTE::
+                The dictionary keys are the feature name and values are one of:
+
+                    - None
+                    - A value, must match exactly.
+                    - An array of two numeric values, specifying an inclusive
+                      range. Only applicable to continuous and numeric ordinal
+                      features.
+                    - An array of string values, must match any of these values
+                      exactly. Only applicable to nominal and string ordinal
+                      features.
+        num_cases : int, default None
+            The maximum amount of cases to use to calculate prediction stats.
+            If not specified, the limit will be k cases if precision is
+            "similar", or 1000 cases if precision is "exact". Only used if
+            `condition` is not None.
+        num_robust_influence_samples_per_case : int, optional
+            Specifies the number of robust samples to use for each case for
+            robust contribution computations.
+            Defaults to 300 + 2 * (number of features).
+        precision : str, default None
+            The precision to use when selecting cases with the condition.
+            Options are 'exact' or 'similar'. If not specified "exact" will be
+            used. Only used if `condition` is not None.
         robust : bool, optional
             When specified, will attempt to return stats that
             were computed with the specified robust or non-robust type.
         robust_hyperparameters : bool, optional
             When specified, will attempt to return stats that were
             computed using hyperparameters with the specified robust or
             non-robust type.
         stats : iterable of str, optional
             List of stats to output. When unspecified, returns all.
             Allowed values:
 
-                - accuracy : Accuracy (1 - mean absolute error) value for
-                  nominal features only.
+                - accuracy : The number of correct predictions divided by the
+                  total number of predictions.
+                - confusion_matrix : A map of actual feature value to a map of
+                  predicted feature value to counts.
                 - contribution : Feature contributions to predicted value when
                   each feature is dropped from the model, applies to all
                   features.
-                - mae : Mean absolute error, applies to all features.
+                - mae : Mean absolute error. For continuous features, this is
+                  calculated as the mean of absolute values of the difference
+                  between the actual and predicted values. For nominal features,
+                  this is 1 - the average categorical action probability of each case's
+                  correct classes. Categorical action probabilities are the probabilities
+                  for each class for the action feature.
                 - mda : Mean decrease in accuracy when each feature is dropped
                   from the model, applies to all features.
                 - mda_permutation : Mean decrease in accuracy that used
                   scrambling of feature values instead of dropping each
                   feature, applies to all features.
                 - precision : Precision (positive predictive) value for nominal
                   features only.
@@ -3807,31 +3872,36 @@
 
         if self.verbose:
             print('Getting feature prediction stats for trainee with '
                   f'id: {trainee_id}')
 
         validate_list_shape(stats, 1, "stats", "str")
         valid_stats = {
-            "accuracy", "contribution", "mae", "mda", "mda_permutation",
-            "precision", "r2", "recall", "rmse", "spearman_coeff",
+            "accuracy", "contribution", "confusion_matrix", "mae", "mda",
+            "mda_permutation", "precision", "r2", "recall", "rmse", "spearman_coeff",
         }
 
         if stats is not None and not set(stats).issubset(valid_stats):
             raise ValueError(
                 'Invalid prediction stats. The following stats are supported: '
                 f'{", ".join(valid_stats)}'
             )
 
         stats = self.dp.get_prediction_stats(
             trainee_id,
             action_feature=action_feature,
             robust=robust,
             robust_hyperparameters=robust_hyperparameters,
             stats=stats,
-            weight_feature=weight_feature)
+            weight_feature=weight_feature,
+            condition=condition,
+            precision=precision,
+            num_cases=num_cases,
+            num_robust_influence_samples_per_case=num_robust_influence_samples_per_case,
+        )
         return stats
 
     def get_marginal_stats(
         self,
         trainee_id: str,
         *,
         weight_feature: Optional[str] = None
@@ -3872,14 +3942,17 @@
         contributions: Optional[bool] = None,
         contributions_robust: Optional[bool] = None,
         hyperparameter_param_path: Optional[Iterable[str]] = None,
         mda: Optional[bool] = None,
         mda_permutation: Optional[bool] = None,
         mda_robust: Optional[bool] = None,
         mda_robust_permutation: Optional[bool] = None,
+        num_robust_influence_samples: Optional[int] = None,
+        num_robust_residual_samples: Optional[int] = None,
+        num_robust_influence_samples_per_case: Optional[int] = None,
         num_samples: Optional[int] = None,
         residuals: Optional[bool] = None,
         residuals_robust: Optional[bool] = None,
         sample_model_fraction: Optional[float] = None,
         sub_model_size: Optional[int] = None,
         use_case_weights: bool = False,
         weight_feature: Optional[str] = None
@@ -3889,20 +3962,21 @@
 
         Parameters
         ----------
         trainee_id : str
             The ID of the Trainee to react to.
         action_feature : str, optional
             Name of target feature for which to do computations. Default is
-            whatever the model was optimized for, i.e., action feature for MDA
-            and contributions, or ".targetless" if optimized for targetless.
+            whatever the model was analyzed for, i.e., action feature for MDA
+            and contributions, or ".targetless" if analyzed for targetless.
             This parameter is required for MDA or contributions computations.
         context_features : iterable of str, optional
             List of features names to use as contexts for
-            computations. Default is all trained features if unspecified.
+            computations. Default is all trained non-unique features if
+            unspecified.
         contributions: bool, optional
             For each context_feature, use the full set of all other
             context_features to compute the mean absolute delta between
             prediction of action_feature with and without the context_feature
             in the model. False removes cached values.
         contributions_robust: bool, optional
             For each context_feature, use the robust (power set/permutation)
@@ -3927,19 +4001,30 @@
             Compute MDA by dropping each feature and using the
             robust (power set/permutations) set of remaining context features
             for each prediction. False removes cached values.
         mda_robust_permutation : bool, optional
             Compute MDA by scrambling each feature and using the
             robust (power set/permutations) set of remaining context features
             for each prediction. False removes cached values.
+        num_robust_influence_samples : int, optional
+            Total sample size of model to use (using sampling with replacement)
+            for robust contribution computation. Defaults to 300.
+        num_robust_residual_samples : int, optional
+            Total sample size of model to use (using sampling with replacement)
+            for robust mda and residual computation.
+            Defaults to 1000 * (1 + log(number of features)).  Note: robust mda
+            will be updated to use num_robust_influence_samples in a future release.
+        num_robust_influence_samples_per_case : int, optional
+            Specifies the number of robust samples to use for each case for
+            robust contribution computations.
+            Defaults to 300 + 2 * (number of features).
         num_samples : int, optional
-            Sample size of model to use (using sampling with replacement).
-            Defaults to 1000 for non-robust, for robust computations scales the
-            1000 up dynamically by (1 + log(number of features)). If specified
-            uses the unmodified value and overrides sample_model_fraction.
+            Total sample size of model to use (using sampling with replacement)
+            for all non-robust computation. Defaults to 1000.
+            If specified overrides sample_model_fraction.```
         residuals : bool, optional
             For each context_feature, use the full set of all other
             context_features to predict the feature. When True computes and
             caches MAE (mean absolute error), R^2, RMSE (root mean squared
             error), and Spearman Coefficient for continuous features, and
             MAE, accuracy, precision and recall for nominal features.
             False removes cached values.
@@ -3997,14 +4082,17 @@
             residuals_robust=residuals_robust,
             contributions=contributions,
             contributions_robust=contributions_robust,
             mda=mda,
             mda_permutation=mda_permutation,
             mda_robust=mda_robust,
             mda_robust_permutation=mda_robust_permutation,
+            num_robust_influence_samples=num_robust_influence_samples,
+            num_robust_residual_samples=num_robust_residual_samples,
+            num_robust_influence_samples_per_case=num_robust_influence_samples_per_case,
             hyperparameter_param_path=hyperparameter_param_path,
             sample_model_fraction=sample_model_fraction,
             sub_model_size=sub_model_size,
             action_feature=action_feature)
 
         self._auto_persist_trainee(trainee_id)
 
@@ -4259,16 +4347,16 @@
         trainee_id : str
             The ID of the Trainee get parameters from.
 
         Returns
         -------
         dict
            A nested dict of the trainee's hyperparameter_map, including
-           optimize_threshold, optimize_growth_factor and
-           auto_optimize_limit_size.
+           analyze_threshold, analyze_growth_factor and
+           auto_analyze_limit_size.
         """
         self._auto_resolve_trainee(trainee_id)
         if self.verbose:
             print(f'Getting model attributes from trainee with id: '
                   f'{trainee_id}')
         return self.dp.get_internal_parameters(trainee_id)
 
@@ -4330,29 +4418,46 @@
             Example::
 
                 {
                     "hyperparameter_map": {
                         ".targetless": {
                             "robust": {
                                 ".none": {
-                                    "dwe": -1, "p": .1, "k": 8
+                                    "dt": -1, "p": .1, "k": 8
                                 }
                             }
                         }
                     },
-                    "auto_optimize_enabled": False,
-                    "optimize_threshold": 100,
-                    "optimize_growth_factor": 7.389,
-                    "auto_optimize_limit_size": 100000
+                    "auto_analyze_enabled": False,
+                    "analyze_threshold": 100,
+                    "analyze_growth_factor": 7.389,
+                    "auto_analyze_limit_size": 100000
                 }
         """
         self._auto_resolve_trainee(trainee_id)
         if self.verbose:
             print(f'Setting model attributes for trainee with id: {trainee_id}')
 
+        deprecated_params = {
+            'auto_optimize_enabled': 'auto_analyze_enabled',
+            'optimize_threshold': 'analyze_threshold',
+            'optimize_growth_factor': 'analyze_growth_factor',
+            'auto_optimize_limit_size': 'auto_analyze_limit_size',
+        }
+
+        # replace any old params with new params and remove old param
+        for old_param, new_param in deprecated_params.items():
+            if old_param in params:
+                params[new_param] = params[old_param]
+                del params[old_param]
+                warnings.warn(
+                    f'The `{old_param}` parameter has been renamed to '
+                    f'`{new_param}`, please use the new parameter '
+                    'instead.', UserWarning)
+
         self.dp.set_internal_parameters(trainee_id, params)
         self._auto_persist_trainee(trainee_id)
 
     def get_num_training_cases(self, trainee_id: str) -> int:
         """
         Return the number of trained cases in the model.
 
@@ -4368,107 +4473,287 @@
         """
         self._auto_resolve_trainee(trainee_id)
         ret = self.dp.get_num_training_cases(trainee_id)
         if isinstance(ret, dict):
             return ret.get('count', 0)
         return 0
 
-    def set_auto_optimize_params(
+    def set_auto_analyze_params(
         self,
         trainee_id: str,
-        auto_optimize_enabled: bool = False,
-        optimize_threshold: Optional[int] = None,
+        auto_analyze_enabled: bool = False,
+        analyze_threshold: Optional[int] = None,
         *,
-        auto_optimize_limit_size: Optional[int] = None,
-        optimize_growth_factor: Optional[float] = None,
+        auto_analyze_limit_size: Optional[int] = None,
+        analyze_growth_factor: Optional[float] = None,
         **kwargs
     ):
         """
-        Set trainee parameters for auto optimization.
+        Set trainee parameters for auto analysis.
 
         Parameters
         ----------
         trainee_id : str
-            The ID of the Trainee to set auto optimization parameters for.
-        auto_optimize_enabled : bool, default False
-            When True, the :func:`train` method will trigger an optimize when
-            it's time for the model to be optimized again.
-        optimize_threshold : int, optional
+            The ID of the Trainee to set auto analysis parameters for.
+        auto_analyze_enabled : bool, default False
+            When True, the :func:`train` method will trigger an analyze when
+            it's time for the model to be analyzed again.
+        analyze_threshold : int, optional
             The threshold for the number of cases at which the model should be
-            re-optimized.
-        auto_optimize_limit_size : int, optional
-            The size of of the model at which to stop doing auto-optimization.
+            re-analyzed.
+        auto_analyze_limit_size : int, optional
+            The size of of the model at which to stop doing auto-analysis.
             Value of 0 means no limit.
-        optimize_growth_factor : float, optional
-            The factor by which to increase the optimize threshold every time
+        analyze_growth_factor : float, optional
+            The factor by which to increase the analyze threshold every time
             the model grows to the current threshold size.
         kwargs : dict, optional
-            Parameters specific for optimize() may be passed in via kwargs, and
-            will be cached and used during future auto-optimizations.
+            Parameters specific for analyze() may be passed in via kwargs, and
+            will be cached and used during future auto-analysiss.
         """
         self._auto_resolve_trainee(trainee_id)
 
+        deprecated_params = {
+            'auto_optimize_enabled': 'auto_analyze_enabled',
+            'optimize_threshold': 'analyze_threshold',
+            'optimize_growth_factor': 'analyze_growth_factor',
+            'auto_optimize_limit_size': 'auto_analyze_limit_size',
+        }
+        analyze_deprecated_params = {
+            'bypass_hyperparameter_optimization': 'bypass_hyperparameter_analysis',
+            'num_optimization_samples': 'num_analysis_samples',
+            'optimization_sub_model_size': 'analysis_sub_model_size',
+            'optimize_level': 'analyze_level',
+            'dwe_values': 'dt_values'
+        }
+
+        # explicitly update parameters if old names are provided
+        if kwargs:
+            for old_param, new_param in deprecated_params.items():
+                if old_param in kwargs:
+                    if old_param == 'auto_optimize_enabled':
+                        auto_analyze_enabled = kwargs[old_param]
+                    elif old_param == 'optimize_threshold':
+                        analyze_threshold = kwargs[old_param]
+                    elif old_param == 'optimize_growth_factor':
+                        analyze_growth_factor = kwargs[old_param]
+                    elif old_param == 'auto_optimize_limit_size':
+                        auto_analyze_limit_size = kwargs[old_param]
+
+                    del kwargs[old_param]
+                    warnings.warn(
+                        f'The `{old_param}` parameter has been renamed to '
+                        f'`{new_param}`, please use the new parameter '
+                        'instead.', UserWarning)
+
+            # replace any old kwarg param with new param and remove old param
+            for old_param, new_param in analyze_deprecated_params.items():
+                if old_param in kwargs:
+                    kwargs[new_param] = kwargs[old_param]
+                    del kwargs[old_param]
+                    warnings.warn(
+                        f'The `{old_param}` parameter has been renamed to '
+                        f'`{new_param}`, please use the new parameter '
+                        'instead.', UserWarning)
+
         if 'targeted_model' in kwargs:
             targeted_model = kwargs['targeted_model']
             if targeted_model not in ['single_targeted', 'omni_targeted', 'targetless']:
                 raise ValueError(
                     f'Invalid value "{targeted_model}" for targeted_model. '
                     'Valid values include single_targeted, omni_targeted, '
                     'and targetless.')
 
         # Collect valid parameters
         parameters = {}
         for k in dict(kwargs).keys():
-            if k in client_models.SetAutoOptimizeRequest.attribute_map:
+            if k in client_models.SetAutoAnalyzeRequest.attribute_map:
                 v = kwargs.pop(k)
                 if (
                     v is not None or
-                    k in client_models.SetAutoOptimizeRequest.nullable_attributes
+                    k in client_models.SetAutoAnalyzeRequest.nullable_attributes
                 ):
                     parameters[k] = v
 
         if kwargs:
             warn_params = ', '.join(kwargs)
             warnings.warn(
-                f'The following auto optimize parameter(s) "{warn_params}" '
-                'are not officially supported by optimize and may or may not '
+                f'The following auto analyze parameter(s) "{warn_params}" '
+                'are not officially supported by analyze and may or may not '
                 'have an effect.', UserWarning)
 
-        self.dp.auto_optimize_params(
+        self.dp.auto_analyze_params(
             trainee_id,
-            auto_optimize_enabled,
-            optimize_threshold,
-            optimize_growth_factor,
-            auto_optimize_limit_size,
+            auto_analyze_enabled,
+            analyze_threshold,
+            analyze_growth_factor,
+            auto_analyze_limit_size,
             **parameters,
             **kwargs
         )
         self._auto_persist_trainee(trainee_id)
 
-    def auto_optimize(self, trainee_id: str):
+    def optimize(self, *args, **kwargs):
+        """
+        Optimizes a trainee.
+
+        .. deprecated:: 6.0.0
+            Use :meth:`DiveplaneDirectClient.analyze` instead.
+
+        Parameters
+        ----------
+        trainee_id : str
+            The ID of the Trainee.
+        context_features : iterable of str, optional
+            The context features to optimize for.
+        action_features : iterable of str, optional
+            The action features to optimize for.
+        k_folds : int
+            optional, (default 6) number of cross validation folds to do
+        bypass_hyperparameter_optimization : bool
+            optional, bypasses hyperparameter optimization
+        bypass_calculate_feature_residuals : bool
+            optional, bypasses feature residual calculation
+        bypass_calculate_feature_weights : bool
+            optional, bypasses calculation of feature weights
+        use_deviations : bool
+            optional, uses deviations for LK metric in queries
+        num_samples : int
+            used in calculating feature residuals
+        k_values : list of int
+            optional list used in hyperparameter search
+        p_values : list of float
+            optional list used in hyperparameter search
+        dwe_values : list of float
+            optional list used in hyperparameter search
+        optimize_level : int
+            optional value, if specified, will optimize for the following
+            flows:
+
+                1. predictions/accuracy (hyperparameters)
+                2. data synth (cache: global residuals)
+                3. standard explanations
+                4. full analysis
+        targeted_model : {"omni_targeted", "single_targeted", "targetless"}
+            optional, valid values as follows:
+
+                "single_targeted" = optimize hyperparameters for the
+                    specified action_features
+                "omni_targeted" = optimize hyperparameters for each context
+                    feature as an action feature, ignores action_features
+                    parameter
+                "targetless" = optimize hyperparameters for all context
+                    features as possible action features, ignores
+                    action_features parameter
+        num_optimization_samples : int, optional
+            If the dataset size to too large, optimize on
+            (randomly sampled) subset of data. The
+            `num_optimization_samples` specifies the number of
+            observations to be considered for optimization.
+        optimization_sub_model_size : int or Node, optional
+            Number of samples to use for optimization. The rest
+            will be randomly held-out and not included in calculations.
+        inverse_residuals_as_weights : bool, default is False
+            When True will compute and use inverse of residuals
+            as feature weights
+        use_case_weights : bool, default False
+            When True will scale influence weights by each
+            case's weight_feature weight.
+        weight_feature : str, optional
+            Name of feature whose values to use as case weights.
+            When left unspecified uses the internally managed case weight.
+        kwargs
+            Additional experimental optimize parameters.
+        """
+        warnings.warn(
+            'The method `optimize()` is deprecated and will be'
+            'removed in a future release. Please use `analyze()` '
+            'instead.', DeprecationWarning)
+
+        self.analyze(*args, **kwargs)
+
+    def auto_optimize(self, trainee_id):
         """
         Auto-optimize the trainee model.
 
         Re-uses all parameters from the previous optimize or
         set_auto_optimize_params call. If optimize or set_auto_optimize_params
         has not been previously called, auto_optimize will default to a robust
         and versatile optimization.
 
+        .. deprecated:: 6.0.0
+            Use :meth:`DiveplaneDirectClient.auto_analyze` instead.
+
         Parameters
         ----------
         trainee_id : str
             The ID of the Trainee to auto-optimize.
         """
+        warnings.warn(
+            'The method `auto_optimize()` is deprecated and will be'
+            'removed in a future release. Please use `auto_analyze()` '
+            'instead.', DeprecationWarning)
+
+        return self.auto_analyze(trainee_id)
+
+    def set_auto_optimize_params(self, *args, **kwargs):
+        """
+        Set trainee parameters for auto optimization.
+
+        .. deprecated:: 6.0.0
+            Use :meth:`DiveplaneDirectClient.set_auto_analyze_params` instead.
+
+        Parameters
+        ----------
+        trainee_id : str
+            The ID of the Trainee to set auto optimization parameters for.
+        auto_optimize_enabled : bool, default False
+            When True, the :func:`train` method will trigger an optimize when
+            it's time for the model to be optimized again.
+        optimize_threshold : int, optional
+            The threshold for the number of cases at which the model should be
+            re-optimized.
+        auto_optimize_limit_size : int, optional
+            The size of of the model at which to stop doing auto-optimization.
+            Value of 0 means no limit.
+        optimize_growth_factor : float, optional
+            The factor by which to increase the optimize threshold every time
+            the model grows to the current threshold size.
+        kwargs : dict, optional
+            Parameters specific for optimize() may be passed in via kwargs, and
+            will be cached and used during future auto-optimizations.
+        """
+        warnings.warn(
+            'The method `set_auto_optimize_params()` is deprecated and will be'
+            'removed in a future release. Please use `set_auto_analyze_params()` '
+            'instead.', DeprecationWarning)
+
+        self.set_auto_analyze_params(*args, **kwargs)
+
+    def auto_analyze(self, trainee_id: str):
+        """
+        Auto-analyze the trainee model.
+
+        Re-uses all parameters from the previous analyze or
+        set_auto_analyze_params call. If analyze or set_auto_analyze_params
+        has not been previously called, auto_analyze will default to a robust
+        and versatile analysis.
+
+        Parameters
+        ----------
+        trainee_id : str
+            The ID of the Trainee to auto-analyze.
+        """
         self._auto_resolve_trainee(trainee_id)
         if self.verbose:
-            print(f"Auto-optimizing trainee with id: {trainee_id}")
+            print(f"Auto-analyzing trainee with id: {trainee_id}")
 
-        self.dp.auto_optimize(trainee_id)
+        self.dp.auto_analyze(trainee_id)
         self._auto_persist_trainee(trainee_id)
-        # when debugging output the auto-optimized parameters into the
+        # when debugging output the auto-analyzed parameters into the
         # trace file.
         if self.dp.trace:
             self.dp.get_internal_parameters(trainee_id)
 
     def get_label(self, entity_id: str, label: str) -> object:
         """
         Get a label value from a Trainee.
@@ -5032,168 +5317,196 @@
         self._auto_persist_trainee(trainee_id)
 
         updated_feature_attributes = self.dp.get_feature_attributes(trainee_id)
         # Update trainee in cache
         cached_trainee.features = internals.postprocess_feature_attributes(
             updated_feature_attributes)
 
-    def optimize(
+    def analyze(
         self,
         trainee_id: str,
         context_features: Optional[Iterable[str]] = None,
         action_features: Optional[Iterable[str]] = None,
         *,
         bypass_calculate_feature_residuals: bool = None,
         bypass_calculate_feature_weights: bool = None,
-        bypass_hyperparameter_optimization: bool = None,
-        dwe_values: Optional[List[float]] = None,
+        bypass_hyperparameter_analysis: bool = None,
+        dt_values: Optional[List[float]] = None,
         use_case_weights: bool = None,
         inverse_residuals_as_weights: bool = None,
         k_folds: Optional[int] = None,
         k_values: Optional[List[int]] = None,
-        num_optimization_samples: Optional[int] = None,
+        num_analysis_samples: Optional[int] = None,
         num_samples: Optional[int] = None,
-        optimization_sub_model_size: Optional[int] = None,
-        optimize_level: Optional[int] = None,
+        analysis_sub_model_size: Optional[int] = None,
+        analyze_level: Optional[int] = None,
         p_values: Optional[List[float]] = None,
         targeted_model: Optional[Literal["omni_targeted", "single_targeted", "targetless"]] = None,
         use_deviations: bool = None,
         weight_feature: Optional[str] = None,
         **kwargs
     ):
         """
-        Optimizes a trainee.
+        Analyzes a trainee.
 
         Parameters
         ----------
         trainee_id : str
             The ID of the Trainee.
         context_features : iterable of str, optional
-            The context features to optimize for.
+            The context features to analyze for.
         action_features : iterable of str, optional
-            The action features to optimize for.
+            The action features to analyze for.
         k_folds : int
             optional, (default 6) number of cross validation folds to do
-        bypass_hyperparameter_optimization : bool
-            optional, bypasses hyperparameter optimization
+        bypass_hyperparameter_analysis : bool
+            optional, bypasses hyperparameter analysis
         bypass_calculate_feature_residuals : bool
             optional, bypasses feature residual calculation
         bypass_calculate_feature_weights : bool
             optional, bypasses calculation of feature weights
         use_deviations : bool
             optional, uses deviations for LK metric in queries
         num_samples : int
             used in calculating feature residuals
         k_values : list of int
             optional list used in hyperparameter search
         p_values : list of float
             optional list used in hyperparameter search
-        dwe_values : list of float
+        dt_values : list of float
             optional list used in hyperparameter search
-        optimize_level : int
-            optional value, if specified, will optimize for the following
+        analyze_level : int
+            optional value, if specified, will analyze for the following
             flows:
 
                 1. predictions/accuracy (hyperparameters)
                 2. data synth (cache: global residuals)
                 3. standard explanations
                 4. full analysis
         targeted_model : {"omni_targeted", "single_targeted", "targetless"}
             optional, valid values as follows:
 
-                "single_targeted" = optimize hyperparameters for the
+                "single_targeted" = analyze hyperparameters for the
                     specified action_features
-                "omni_targeted" = optimize hyperparameters for each context
+                "omni_targeted" = analyze hyperparameters for each context
                     feature as an action feature, ignores action_features
                     parameter
-                "targetless" = optimize hyperparameters for all context
+                "targetless" = analyze hyperparameters for all context
                     features as possible action features, ignores
                     action_features parameter
-        num_optimization_samples : int, optional
-            If the dataset size to too large, optimize on
+        num_analysis_samples : int, optional
+            If the dataset size to too large, analyze on
             (randomly sampled) subset of data. The
-            `num_optimization_samples` specifies the number of
-            observations to be considered for optimization.
-        optimization_sub_model_size : int or Node, optional
-            Number of samples to use for optimization. The rest
+            `num_analysis_samples` specifies the number of
+            observations to be considered for analysis.
+        analysis_sub_model_size : int or Node, optional
+            Number of samples to use for analysis. The rest
             will be randomly held-out and not included in calculations.
         inverse_residuals_as_weights : bool, default is False
             When True will compute and use inverse of residuals
             as feature weights
         use_case_weights : bool, default False
             When True will scale influence weights by each
             case's weight_feature weight.
         weight_feature : str, optional
             Name of feature whose values to use as case weights.
             When left unspecified uses the internally managed case weight.
         kwargs
-            Additional experimental optimize parameters.
+            Additional experimental analyze parameters.
         """
         self._auto_resolve_trainee(trainee_id)
         cached_trainee = self.trainee_cache.get(trainee_id)
 
         validate_list_shape(context_features, 1, "context_features", "str")
         validate_list_shape(action_features, 1, "action_features", "str")
         validate_list_shape(p_values, 1, "p_values", "int")
         validate_list_shape(k_values, 1, "k_values", "float")
-        validate_list_shape(dwe_values, 1, "dwe_values", "float")
+        validate_list_shape(dt_values, 1, "dt_values", "float")
 
         if targeted_model not in ['single_targeted', 'omni_targeted', 'targetless', None]:
             raise ValueError(
                 f'Invalid value "{targeted_model}" for targeted_model. '
                 'Valid values include single_targeted, omni_targeted, '
                 'and targetless.')
 
         if action_features is None:
             action_features = cached_trainee.default_action_features
         if context_features is None:
             context_features = cached_trainee.default_context_features
 
-        optimize_params = dict(
+        deprecated_params = {
+            'bypass_hyperparameter_optimization': 'bypass_hyperparameter_analysis',
+            'num_optimization_samples': 'num_analysis_samples',
+            'optimization_sub_model_size': 'analysis_sub_model_size',
+            'optimize_level': 'analyze_level',
+            'dwe_values': 'dt_values'
+        }
+         # explicitly update parameters if old names are provided
+        if kwargs:
+            for old_param, new_param in deprecated_params.items():
+                if old_param in kwargs:
+                    if old_param == 'bypass_hyperparameter_optimization':
+                        bypass_hyperparameter_analysis = kwargs[old_param]
+                    elif old_param == 'num_optimization_samples':
+                        num_analysis_samples = kwargs[old_param]
+                    elif old_param == 'optimization_sub_model_size':
+                        analysis_sub_model_size = kwargs[old_param]
+                    elif old_param == 'optimize_level':
+                        analyze_level = kwargs[old_param]
+                    elif old_param == 'dwe_values':
+                        dt_values = kwargs[old_param]
+
+                    del kwargs[old_param]
+                    warnings.warn(
+                        f'The `{old_param}` parameter has been renamed to '
+                        f'`{new_param}`, please use the new parameter '
+                        'instead.', UserWarning)
+
+        analyze_params = dict(
             action_features=action_features,
             context_features=context_features,
             bypass_calculate_feature_residuals=bypass_calculate_feature_residuals,  # noqa: #E501
             bypass_calculate_feature_weights=bypass_calculate_feature_weights,
-            bypass_hyperparameter_optimization=bypass_hyperparameter_optimization,  # noqa: #E501
-            dwe_values=dwe_values,
+            bypass_hyperparameter_analysis=bypass_hyperparameter_analysis,  # noqa: #E501
+            dt_values=dt_values,
             use_case_weights=use_case_weights,
             inverse_residuals_as_weights=inverse_residuals_as_weights,
             k_folds=k_folds,
             k_values=k_values,
-            num_optimization_samples=num_optimization_samples,
+            num_analysis_samples=num_analysis_samples,
             num_samples=num_samples,
-            optimization_sub_model_size=optimization_sub_model_size,
-            optimize_level=optimize_level,
+            analysis_sub_model_size=analysis_sub_model_size,
+            analyze_level=analyze_level,
             p_values=p_values,
             targeted_model=targeted_model,
             use_deviations=use_deviations,
             weight_feature=weight_feature,
         )
 
         # Filter out non nullable parameters
-        optimize_params = {
-            k: v for k, v in optimize_params.items()
+        analyze_params = {
+            k: v for k, v in analyze_params.items()
             if v is not None or
-            k in client_models.OptimizeRequest.nullable_attributes
+            k in client_models.AnalyzeRequest.nullable_attributes
         }
         # Add experimental options
-        optimize_params.update(kwargs)
+        analyze_params.update(kwargs)
 
         if kwargs:
             warn_params = ', '.join(kwargs)
             warnings.warn(
-                f'The following optimize parameter(s) "{warn_params}" are '
-                'not officially supported by optimize and may or may not '
+                f'The following analyze parameter(s) "{warn_params}" are '
+                'not officially supported by analyze and may or may not '
                 'have an effect.', UserWarning)
 
         if self.verbose:
-            print(f'Optimizing trainee with id: {trainee_id}')
-            print(f'Optimizing trainee with parameters: {optimize_params}')
+            print(f'Analyzing trainee with id: {trainee_id}')
+            print(f'Analyzing trainee with parameters: {analyze_params}')
 
-        self.dp.optimize(trainee_id, **optimize_params)
+        self.dp.analyze(trainee_id, **analyze_params)
         self._auto_persist_trainee(trainee_id)
 
     def evaluate(
         self,
         trainee_id: str,
         features_to_code_map: Dict[str, str],
         *,
```

## diveplane/direct/core.py

```diff
@@ -572,30 +572,30 @@
 
     def get_num_training_cases(self, trainee):
         return self._execute("get_num_training_cases",
                              {
                                  "trainee": trainee
                              })
 
-    def auto_optimize_params(self, trainee,
-                             auto_optimize_enabled=False,
-                             optimize_threshold=None,
-                             optimize_growth_factor=None,
-                             auto_optimize_limit_size=None, **kwargs):
+    def auto_analyze_params(self, trainee,
+                             auto_analyze_enabled=False,
+                             analyze_threshold=None,
+                             analyze_growth_factor=None,
+                             auto_analyze_limit_size=None, **kwargs):
         params = {
             "trainee": trainee,
-            "auto_optimize_enabled": auto_optimize_enabled,
-            "optimize_threshold": optimize_threshold,
-            "optimize_growth_factor": optimize_growth_factor,
-            "auto_optimize_limit_size": auto_optimize_limit_size
+            "auto_analyze_enabled": auto_analyze_enabled,
+            "analyze_threshold": analyze_threshold,
+            "analyze_growth_factor": analyze_growth_factor,
+            "auto_analyze_limit_size": auto_analyze_limit_size
         }
-        return self._execute("set_auto_optimize", {**kwargs, **params})
+        return self._execute("set_auto_analyze_params", {**kwargs, **params})
 
-    def auto_optimize(self, trainee):
-        return self._execute("auto_optimize",
+    def auto_analyze(self, trainee):
+        return self._execute("auto_analyze",
                              {
                                  "trainee": trainee
                              })
 
     def compute_feature_weights(self, trainee, action_feature, context_features,
                                 robust, weight_feature, use_case_weights):
         return self._execute("compute_feature_weights",
@@ -985,21 +985,14 @@
     def get_session_training_indices(self, trainee, session):
         return self._execute("get_session_training_indices",
                              {
                                  "trainee": trainee,
                                  "session": session
                              })
 
-    def set_distance_weight_exponent(self, trainee, distance_weight_exponent):
-        return self._execute("set_distance_weight_exponent",
-                             {
-                                 "trainee": trainee,
-                                 "distance_weight_exponent": distance_weight_exponent
-                             })
-
     def set_internal_parameters(self, trainee, hyperparameter_map):
         params = {**{"trainee": trainee}, **hyperparameter_map}
         return self._execute("set_internal_parameters", params)
 
     def set_feature_attributes(self, trainee, feature_attributes):
         return self._execute("set_feature_attributes",
                              {
@@ -1036,17 +1029,17 @@
                              {
                                  "trainee": trainee_id,
                                  "trainee_filepath": f"{path_to_trainee}/",
                                  "root_filepath": f"{self.diveplane_path}/",
                                  "separate_files": separate_files
                              })
 
-    def optimize(self, trainee, **kwargs):
+    def analyze(self, trainee, **kwargs):
         params = {**kwargs, "trainee": trainee}
-        return self._execute("optimize", params)
+        return self._execute("analyze", params)
 
     def get_feature_residuals(self, trainee_id,
                               action_feature=None,
                               robust=None,
                               robust_hyperparameters=None,
                               weight_feature=None):
         return self._execute("get_feature_residuals",
@@ -1081,24 +1074,32 @@
                                  "action_feature": action_feature,
                                  "weight_feature": weight_feature,
                              })
 
     def get_prediction_stats(
         self, trainee_id, *,
         action_feature=None,
+        condition=None,
+        num_cases=None,
+        num_robust_influence_samples_per_case=None,
+        precision=None,
         robust=None,
         robust_hyperparameters=None,
         stats=None,
-        weight_feature=None
+        weight_feature=None,
     ):
         return self._execute("get_prediction_stats",
                              {
                                  "trainee": trainee_id,
                                  "robust": robust,
                                  "action_feature": action_feature,
+                                 "condition": condition,
+                                 "num_cases": num_cases,
+                                 "num_robust_influence_samples_per_case": num_robust_influence_samples_per_case,
+                                 "precision": precision,
                                  "robust_hyperparameters": robust_hyperparameters,
                                  "stats": stats,
                                  "weight_feature": weight_feature,
                              })
 
     def get_marginal_stats(self, trainee_id, *, weight_feature=None):
         return self._execute("get_marginal_stats",
@@ -1113,14 +1114,17 @@
                            contributions=None,
                            contributions_robust=None,
                            hyperparameter_param_path=None,
                            mda=None,
                            mda_permutation=None,
                            mda_robust=None,
                            mda_robust_permutation=None,
+                           num_robust_influence_samples=None,
+                           num_robust_residual_samples=None,
+                           num_robust_influence_samples_per_case=None,
                            num_samples=None,
                            residuals=None,
                            residuals_robust=None,
                            sample_model_fraction=None,
                            sub_model_size=None,
                            use_case_weights=False,
                            weight_feature=None):
@@ -1135,14 +1139,17 @@
                           "residuals_robust": residuals_robust,
                           "contributions": contributions,
                           "contributions_robust": contributions_robust,
                           "mda": mda,
                           "mda_permutation": mda_permutation,
                           "mda_robust": mda_robust,
                           "mda_robust_permutation": mda_robust_permutation,
+                          "num_robust_influence_samples": num_robust_influence_samples,
+                          "num_robust_residual_samples": num_robust_residual_samples,
+                          "num_robust_influence_samples_per_case": num_robust_influence_samples_per_case,
                           "hyperparameter_param_path": hyperparameter_param_path,
                           "sample_model_fraction": sample_model_fraction,
                           "sub_model_size": sub_model_size,
                           "action_feature": action_feature
                       })
 
     def set_random_seed(self, trainee, seed):
```

## diveplane/reactor/trainee.py

```diff
@@ -495,14 +495,17 @@
         self._created = False
         self._id = None
 
     def unload(self) -> None:
         """
         Unload the trainee.
 
+        .. deprecated:: 1.0.0
+            Use :meth:`Trainee.release_resources` instead.
+
         Returns
         -------
         None
         """
         warnings.warn(
             'The method `unload()` is deprecated and will be removed '
             'in a future release. Please use `release_resources()` '
@@ -586,15 +589,16 @@
         accumulate_weight_feature: Optional[str] = None,
         batch_size: Optional[int] = None,
         derived_features: Optional[Iterable[str]] = None,
         features: Optional[Iterable[str]] = None,
         input_is_substituted: Optional[bool] = False,
         progress_callback: Optional[Callable] = None,
         series: Optional[str] = None,
-        train_weights_only: Optional[bool] = False
+        train_weights_only: Optional[bool] = False,
+        validate: bool = True,
     ) -> None:
         """
         Train one or more cases into the trainee (model).
 
         Parameters
         ----------
         cases : list of list of object or pandas.DataFrame
@@ -652,14 +656,18 @@
             this case are appended to all cases in the series. If cases is the
             same length as the series, the value of each case in cases is
             applied in order to each of the cases in the series.
         train_weights_only:  bool, default False
             When true, and accumulate_weight_feature is provided,
             will accumulate all of the cases' neighbor weights instead of
             training the cases into the model.
+        validate : bool, default True
+            Whether to validate the data against the provided feature
+            attributes. Issues warnings if there are any discrepancies between
+            the data and the features dictionary.
 
         Returns
         -------
         None
         """
         self.client.train(
             trainee_id=self.id,
@@ -668,193 +676,419 @@
             batch_size=batch_size,
             cases=cases,
             derived_features=derived_features,
             features=features,
             input_is_substituted=input_is_substituted,
             progress_callback=progress_callback,
             series=series,
-            train_weights_only=train_weights_only
+            train_weights_only=train_weights_only,
+            validate=validate,
         )
 
-    def auto_optimize(self) -> None:
+    def optimize(self, *args, **kwargs):
         """
-        Auto-optimize the trainee.
+        Optimizes a trainee.
 
-        Re-use all parameters from the previous optimize call, assuming that
-        the user has called 'optimize' before. If not, it will default to a
-        robust and versatile optimization.
+        .. deprecated:: 6.0.0
+            Use :meth:`Trainee.analyze` instead.
 
-        Returns
-        -------
-        None
+        Parameters
+        ----------
+        context_features : iterable of str, optional
+            The context features to optimize for.
+        action_features : iterable of str, optional
+            The action features to optimize for.
+        k_folds : int
+            optional, (default 6) number of cross validation folds to do
+        bypass_hyperparameter_optimization : bool
+            optional, bypasses hyperparameter optimization
+        bypass_calculate_feature_residuals : bool
+            optional, bypasses feature residual calculation
+        bypass_calculate_feature_weights : bool
+            optional, bypasses calculation of feature weights
+        use_deviations : bool
+            optional, uses deviations for LK metric in queries
+        num_samples : int
+            used in calculating feature residuals
+        k_values : list of int
+            optional list used in hyperparameter search
+        p_values : list of float
+            optional list used in hyperparameter search
+        dwe_values : list of float
+            optional list used in hyperparameter search
+        optimize_level : int
+            optional value, if specified, will optimize for the following
+            flows:
+
+                1. predictions/accuracy (hyperparameters)
+                2. data synth (cache: global residuals)
+                3. standard explanations
+                4. full analysis
+        targeted_model : {"omni_targeted", "single_targeted", "targetless"}
+            optional, valid values as follows:
+
+                "single_targeted" = optimize hyperparameters for the
+                    specified action_features
+                "omni_targeted" = optimize hyperparameters for each context
+                    feature as an action feature, ignores action_features
+                    parameter
+                "targetless" = optimize hyperparameters for all context
+                    features as possible action features, ignores
+                    action_features parameter
+        num_optimization_samples : int, optional
+            If the dataset size to too large, optimize on
+            (randomly sampled) subset of data. The
+            `num_optimization_samples` specifies the number of
+            observations to be considered for optimization.
+        optimization_sub_model_size : int or Node, optional
+            Number of samples to use for optimization. The rest
+            will be randomly held-out and not included in calculations.
+        inverse_residuals_as_weights : bool, default is False
+            When True will compute and use inverse of residuals
+            as feature weights
+        use_case_weights : bool, default False
+            When True will scale influence weights by each
+            case's weight_feature weight.
+        weight_feature : str, optional
+            Name of feature whose values to use as case weights.
+            When left unspecified uses the internally managed case weight.
+        kwargs
+            Additional experimental optimize parameters.
         """
-        self.client.auto_optimize(self.id)
+        warnings.warn(
+            'The method `optimize()` is deprecated and will be'
+            'removed in a future release. Please use `analyze()` '
+            'instead.', DeprecationWarning)
 
-    def set_auto_optimize_params(
-        self,
-        auto_optimize_enabled: bool = False,
-        optimize_threshold: Optional[int] = None,
-        *,
-        auto_optimize_limit_size: Optional[int] = None,
-        optimize_growth_factor: Optional[float] = None,
-        **kwargs,
-    ) -> None:
+        self.analyze(*args, **kwargs)
+
+    def auto_optimize(self):
+        """
+        Auto-optimize the trainee model.
+
+        Re-uses all parameters from the previous optimize or
+        set_auto_optimize_params call. If optimize or set_auto_optimize_params
+        has not been previously called, auto_optimize will default to a robust
+        and versatile optimization.
+
+        .. deprecated:: 6.0.0
+            Use :meth:`Trainee.auto_analyze` instead.
+        """
+        warnings.warn(
+            'The method `auto_optimize()` is deprecated and will be'
+            'removed in a future release. Please use `auto_analyze()` '
+            'instead.', DeprecationWarning)
+
+        return self.auto_analyze()
+
+    def set_auto_optimize_params(self, *args, **kwargs):
         """
-        Set parameters for auto optimization.
+        Set trainee parameters for auto optimization.
 
-        Auto-optimization is disabled if this is called without specifying an
-        optimize_threshold.
+        .. deprecated:: 6.0.0
+            Use :meth:`Trainee.set_auto_analyze_params` instead.
 
         Parameters
         ----------
         auto_optimize_enabled : bool, default False
             When True, the :func:`train` method will trigger an optimize when
             it's time for the model to be optimized again.
         optimize_threshold : int, optional
             The threshold for the number of cases at which the model should be
             re-optimized.
         auto_optimize_limit_size : int, optional
-            The size of the model at which to stop doing auto-optimization.
+            The size of of the model at which to stop doing auto-optimization.
             Value of 0 means no limit.
         optimize_growth_factor : float, optional
-            The factor by which to increase the optimization threshold every
+            The factor by which to increase the optimize threshold every time
+            the model grows to the current threshold size.
+        kwargs : dict, optional
+            Parameters specific for optimize() may be passed in via kwargs, and
+            will be cached and used during future auto-optimizations.
+        """
+        warnings.warn(
+            'The method `set_auto_optimize_params()` is deprecated and will be'
+            'removed in a future release. Please use `set_auto_analyze_params()` '
+            'instead.', DeprecationWarning)
+
+        self.set_auto_analyze_params(*args, **kwargs)
+
+    def auto_analyze(self) -> None:
+        """
+        Auto-analyze the trainee.
+
+        Re-use all parameters from the previous analyze call, assuming that
+        the user has called 'analyze' before. If not, it will default to a
+        robust and versatile analysis.
+
+        Returns
+        -------
+        None
+        """
+        self.client.auto_analyze(self.id)
+
+    def set_auto_analyze_params(
+        self,
+        auto_analyze_enabled: bool = False,
+        analyze_threshold: Optional[int] = None,
+        *,
+        auto_analyze_limit_size: Optional[int] = None,
+        analyze_growth_factor: Optional[float] = None,
+        **kwargs,
+    ) -> None:
+        """
+        Set parameters for auto analysis.
+
+        Auto-analysis is disabled if this is called without specifying an
+        analyze_threshold.
+
+        Parameters
+        ----------
+        auto_analyze_enabled : bool, default False
+            When True, the :func:`train` method will trigger an analyze when
+            it's time for the model to be analyzed again.
+        analyze_threshold : int, optional
+            The threshold for the number of cases at which the model should be
+            re-analyzed.
+        auto_analyze_limit_size : int, optional
+            The size of the model at which to stop doing auto-analysis.
+            Value of 0 means no limit.
+        analyze_growth_factor : float, optional
+            The factor by which to increase the analysis threshold every
             time the model grows to the current threshold size.
         kwargs : dict, optional
-            See parameters in :func:`optimize`.
+            See parameters in :func:`analyze`.
 
         Returns
         -------
         None
         """
-        self.client.set_auto_optimize_params(
+        self.client.set_auto_analyze_params(
             trainee_id=self.id,
-            auto_optimize_enabled=auto_optimize_enabled,
-            auto_optimize_limit_size=auto_optimize_limit_size,
-            optimize_growth_factor=optimize_growth_factor,
-            optimize_threshold=optimize_threshold,
+            auto_analyze_enabled=auto_analyze_enabled,
+            auto_analyze_limit_size=auto_analyze_limit_size,
+            analyze_growth_factor=analyze_growth_factor,
+            analyze_threshold=analyze_threshold,
             **kwargs,
         )
 
-    def optimize(
+    def analyze(
         self,
         context_features: Optional[Iterable[str]] = None,
         action_features: Optional[Iterable[str]] = None,
         *,
         bypass_calculate_feature_residuals: Optional[bool] = None,
         bypass_calculate_feature_weights: Optional[bool] = None,
-        bypass_hyperparameter_optimization: Optional[bool] = None,
-        dwe_values: Optional[List[float]] = None,
+        bypass_hyperparameter_analysis: Optional[bool] = None,
+        dt_values: Optional[List[float]] = None,
         inverse_residuals_as_weights: Optional[bool] = None,
         k_folds: Optional[int] = None,
         k_values: Optional[List[int]] = None,
-        num_optimization_samples: Optional[int] = None,
+        num_analysis_samples: Optional[int] = None,
         num_samples: Optional[int] = None,
-        optimization_sub_model_size: Optional[int] = None,
-        optimize_level: Optional[int] = None,
+        analysis_sub_model_size: Optional[int] = None,
+        analyze_level: Optional[int] = None,
         p_values: Optional[List[float]] = None,
         targeted_model: Optional[str] = None,
         use_case_weights: Optional[bool] = None,
         use_deviations: Optional[bool] = None,
         weight_feature: Optional[str] = None,
         **kwargs
     ) -> None:
         """
-        Optimizes the trainee.
+        Analyzes the trainee.
 
         Parameters
         ----------
         context_features : list of str, optional
-            The context features to optimize for.
+            The context features to analyze for.
         action_features : list of str, optional
-            The action features to optimize for.
+            The action features to analyze for.
         bypass_calculate_feature_residuals : bool, default False
             When True, bypasses calculation of feature residuals.
         bypass_calculate_feature_weights : bool, default False
             When True, bypasses calculation of feature weights.
-        bypass_hyperparameter_optimization : bool, default False
-            When True, bypasses hyperparameter optimization.
-        dwe_values : list of float, optional
-            The dwe value hyperparameters to optimize with.
+        bypass_hyperparameter_analysis : bool, default False
+            When True, bypasses hyperparameter analysis.
+        dt_values : list of float, optional
+            The dt value hyperparameters to analyze with.
         inverse_residuals_as_weights : bool, default is False
             When True, will compute and use inverse of residuals as feature
             weights.
         k_folds : int, optional
             The number of cross validation folds to do. A value of 1 does
             hold-one-out instead of k-fold.
         k_values : list of int, optional
-            The k value hyperparameters to optimize with.
-        num_optimization_samples : int, optional
+            The k value hyperparameters to analyze with.
+        num_analysis_samples : int, optional
             Specifies the number of observations to be considered for
-            optimization.
+            analysis.
         num_samples : int, optional
             Number of samples used in calculating feature residuals.
-        optimization_sub_model_size : int, optional
-            Number of samples to use for optimization. The rest will be
+        analysis_sub_model_size : int, optional
+            Number of samples to use for analysis. The rest will be
             randomly held-out and not included in calculations.
-        optimize_level : int, optional
-            If specified, will optimize for the following flows:
+        analyze_level : int, optional
+            If specified, will analyze for the following flows:
 
                 1. Predictions/accuracy (hyperparameters)
                 2. Data synth (cache: global residuals)
                 3. Standard explanations
                 4. Full analysis
 
         p_values : list of float, optional
-            The p value hyperparameters to optimize with.
+            The p value hyperparameters to analyze with.
         targeted_model : str or None
             Type of hyperparameter targeting.
             Valid options include:
 
-                - **single_targeted**: Optimize hyperparameters for the
+                - **single_targeted**: Analyze hyperparameters for the
                   specified action_features.
-                - **omni_targeted**: Optimize hyperparameters for each context
+                - **omni_targeted**: Analyze hyperparameters for each context
                   feature as an action feature, ignores action_features
                   parameter.
-                - **targetless**: Optimize hyperparameters for all context
+                - **targetless**: Analyze hyperparameters for all context
                   features as possible action features, ignores
                   action_features parameter.
 
         use_case_weights : bool, default False
             (Optional) When True will scale influence weights by each
             case's weight_feature weight.
         use_deviations : bool, default False
             When True, uses deviations for LK metric in queries.
         weight_feature : str, optional
             Name of feature whose values to use as case weights.
             When left unspecified uses the internally managed case weight.
         kwargs
-            Additional experimental optimize parameters.
+            Additional experimental analyze parameters.
 
         Returns
         -------
         None
         """
-        self.client.optimize(
+        self.client.analyze(
             trainee_id=self.id,
             action_features=action_features,
             context_features=context_features,
             bypass_calculate_feature_residuals=bypass_calculate_feature_residuals,  # noqa: E501
             bypass_calculate_feature_weights=bypass_calculate_feature_weights,
-            bypass_hyperparameter_optimization=bypass_hyperparameter_optimization,  # noqa: E501
-            dwe_values=dwe_values,
+            bypass_hyperparameter_analysis=bypass_hyperparameter_analysis,  # noqa: E501
+            dt_values=dt_values,
             use_case_weights=use_case_weights,
             inverse_residuals_as_weights=inverse_residuals_as_weights,
             k_folds=k_folds,
             k_values=k_values,
-            num_optimization_samples=num_optimization_samples,
+            num_analysis_samples=num_analysis_samples,
             num_samples=num_samples,
-            optimization_sub_model_size=optimization_sub_model_size,
-            optimize_level=optimize_level,
+            analysis_sub_model_size=analysis_sub_model_size,
+            analyze_level=analyze_level,
             p_values=p_values,
             targeted_model=targeted_model,
             use_deviations=use_deviations,
             weight_feature=weight_feature,
             **kwargs
         )
 
+    def predict(
+        self,
+        contexts: Optional[Union[List[List[object]], "DataFrame"]] = None,
+        *,
+        action_features: Optional[Iterable[str]] = None,
+        allow_nulls: Optional[bool] = False,
+        case_indices: Optional[CaseIndices] = None,
+        context_features: Optional[Iterable[str]] = None,
+        derived_action_features: Optional[Iterable[str]] = None,
+        derived_context_features: Optional[Iterable[str]] = None,
+        leave_case_out: Optional[bool] = None,
+        suppress_warning: Optional[bool] = False,
+        use_case_weights: Optional[bool] = False,
+        weight_feature: Optional[str] = None,
+    ) -> list:
+        """
+        Wrapper around :func:`react`.
+
+        Performs a discriminative react to predict the action feature values based on the
+        given contexts. Returns only the predicted action values.
+
+        .. seealso::
+            :func:`react`
+
+        Parameters
+        ----------
+        contexts : list of list of object or pandas.DataFrame, optional
+            (Optional) The context values to react to. If context values are not specified,
+            then `case_indices` must be specified.
+        action_features : list of str, optional
+            (Optional) Feature names to treat as action features during react. If no
+            `action_features` is specified, the Trainee `default_action_features`
+            is used.
+        allow_nulls : bool, default False, optional
+            See parameter ``allow_nulls`` in :func:`react`.
+        case_indices : Iterable of Sequence[str, int], default None, optional
+            (Optional) Iterable of Sequences, of session id and index, where index
+            is the original 0-based index of the case as it was trained into
+            the session. If this case does not exist, discriminative react
+            outputs null.
+        context_features : list of str, optional
+            (Optional) Feature names to treat as context features during react. If no
+            `context_features` is specified, then the Trainee's `default_action_features`
+            are used. If the Trainee has no `default_action_features`, then
+            `context_features` will be all of the `features` excluding the
+            `action_features`.
+        derived_action_features : list of str, optional
+            See parameter ``derived_action_features`` in :func:`react`.
+        derived_context_features : list of str, optional
+            See parameter ``derived_context_features`` in :func:`react`.
+        leave_case_out : bool, default False
+            See parameter ``leave_case_out`` in :func:`react`.
+        suppress_warning : bool, default False
+            See parameter ``suppress_warning`` in :func:`react`.
+        use_case_weights : bool, default False
+            See parameter ``use_case_weights`` in :func:`react`.
+        weight_feature : str, optional
+            See parameter ``weight_feature`` in :func:`react`.
+
+        Returns
+        -------
+        pandas.DataFrame
+            DataFrame consisting of the discriminative predicted results.
+        """
+
+        if action_features is None:
+            if self.default_action_features is None:
+                raise DiveplaneError(
+                    "No action features specified and no default action features are present. "
+                    "Please specify the action feature or add default action features "
+                    "to the Trainee."
+                )
+            else:
+                action_features = self.default_action_features
+
+        if context_features is None:
+            if self.default_context_features is None:
+                context_features = [key for key in self.features.keys() if key not in action_features]
+            else:
+                context_features = self.default_context_features
+
+        results = self.react(
+            action_features=action_features,
+            allow_nulls=allow_nulls,
+            case_indices=case_indices,
+            contexts=contexts,
+            context_features=context_features,
+            derived_action_features=derived_action_features,
+            derived_context_features=derived_context_features,
+            leave_case_out=leave_case_out,
+            suppress_warning=suppress_warning,
+            use_case_weights=use_case_weights,
+            weight_feature=weight_feature,
+        )
+
+        return results['action']
+
     def react(
         self,
         contexts: Optional[Union[List[List[object]], "DataFrame"]] = None,
         *,
         action_features: Optional[Iterable[str]] = None,
         actions: Optional[Union[List[List[object]], "DataFrame"]] = None,
         allow_nulls: Optional[bool] = False,
@@ -1049,17 +1283,22 @@
                 similar cases, which will first include the influential cases.
                 Uses only the context features of the reacted case.
             - num_boundary_cases : int, optional
                 Outputs this manually specified number of boundary cases.
             - num_most_similar_cases : int, optional
                 Outputs this manually specified number of most similar cases,
                 which will first include the influential cases.
-            - num_most_similar_case_indices: int, optional
+            - num_most_similar_case_indices : int, optional
                 Outputs this specified number of most similar case indices when
                 'distance_ratio' is also set to True.
+            - num_robust_influence_samples_per_case : int, optional
+                Specifies the number of robust samples to use for each case.
+	            Applicable only for computing robust feature contributions or
+                robust case feature contributions. Defaults to 2000. Higher
+                values will take longer but provide more stable results.
             - observational_errors : bool, optional
                 If True, outputs observational errors for all features as
                 defined in feature attributes.
             - outlying_feature_values : bool, optional
                 If True, outputs the reacted case's context feature values that
                 are outside the min or max of the corresponding feature values
                 of all the cases in the local model area. Uses only the context
@@ -2238,21 +2477,24 @@
         Get cached feature residuals.
 
         All keyword arguments are optional, when not specified will auto-select
         cached residuals for output, when specified will attempt to
         output the cached residuals best matching the requested parameters,
         or None if no cached match is found.
 
+        .. deprecated:: 1.0.0
+            Use :meth:`Trainee.get_prediction_stats` instead.
+
         Parameters
         ----------
         action_feature : str, optional
             When specified, will attempt to return residuals that
             were computed for this specified action_feature.
             Note: ".targetless" is the action feature used during targetless
-            optimization.
+            analysis.
         robust : bool, optional
             When specified, will attempt to return residuals that
             were computed with the specified robust or non-robust type.
         robust_hyperparameters : bool, optional
             When specified, will attempt to return residuals that were
             computed using hyperpparameters with the specified robust or
             non-robust type.
@@ -2273,51 +2515,99 @@
             weight_feature=weight_feature,
         )
 
     def get_prediction_stats(
         self,
         *,
         action_feature: Optional[str] = None,
+        condition: Optional[Dict[str, Any]] = None,
+        num_cases: Optional[int] = None,
+        num_robust_influence_samples_per_case: Optional[int] = None,
+        precision: Optional[str] = None,
         robust: Optional[bool] = None,
         robust_hyperparameters: Optional[bool] = None,
         stats: Optional[Iterable[str]] = None,
         weight_feature: Optional[str] = None,
     ) -> "DataFrame":
         """
-        Get cached feature prediction stats.
+        Get feature prediction stats.
+
+        Gets cached stats when condition is None.
+        If condition is not None, then uses the condition to select cases and
+        computes prediction stats for that set of cases.
 
         All keyword arguments are optional, when not specified will auto-select
         all cached stats for output, when specified will attempt to
         output the cached stats best matching the requested parameters,
         or None if no cached match is found.
 
         Parameters
         ----------
         action_feature : str, optional
             When specified, will attempt to return stats that
             were computed for this specified action_feature.
             Note: ".targetless" is the action feature used during targetless
-            optimization.
+            analysis.
+
+            .. NOTE::
+                If get_prediction_stats is being used with time series analysis,
+                the action feature for which the prediction statistics information
+                is desired must be specified.
+        condition : dict or None, optional
+            A condition map to select which cases to compute prediction stats
+            for.
+
+            .. NOTE::
+                The dictionary keys are the feature name and values are one of:
+
+                    - None
+                    - A value, must match exactly.
+                    - An array of two numeric values, specifying an inclusive
+                      range. Only applicable to continuous and numeric ordinal
+                      features.
+                    - An array of string values, must match any of these values
+                      exactly. Only applicable to nominal and string ordinal
+                      features.
+        num_cases : int, default None
+            The maximum amount of cases to use to calculate prediction stats.
+            If not specified, the limit will be k cases if precision is
+            "similar", or 1000 cases if precision is "exact". Only used if
+            `condition` is not None.
+        num_robust_influence_samples_per_case : int, optional
+            Specifies the number of robust samples to use for each case for
+            robust contribution computations.
+            Defaults to 300 + 2 * (number of features).
+        precision : str, default None
+            The precision to use when selecting cases with the condition.
+            Options are 'exact' or 'similar'. If not specified "exact" will be
+            used. Only used if `condition` is not None.
         robust : bool, optional
             When specified, will attempt to return stats that
             were computed with the specified robust or non-robust type.
         robust_hyperparameters : bool, optional
             When specified, will attempt to return stats that were
             computed using hyperparameters with the specified robust or
             non-robust type.
         stats : list of str, optional
             List of stats to output. When unspecified, returns all.
             Allowed values:
 
-                - accuracy : Accuracy (1 - mean absolute error) value for
-                  nominal features only.
+                - accuracy : The number of correct predictions divided by the
+                  total number of predictions.
+                - confusion_matrix : A map of actual feature value to a map of
+                  predicted feature value to counts.
                 - contribution : Feature contributions to predicted value when
                   each feature is dropped from the model, applies to all
                   features.
-                - mae : Mean absolute error, applies to all features.
+                - mae : Mean absolute error. For continuous features, this is
+                  calculated as the mean of absolute values of the difference
+                  between the actual and predicted values. For nominal features,
+                  this is 1 - the average categorical action probability of each case's
+                  correct classes. Categorical action probabilities are the probabilities
+                  for each class for the action feature.
                 - mda : Mean decrease in accuracy when each feature is dropped
                   from the model, applies to all features.
                 - mda_permutation : Mean decrease in accuracy that used
                   scrambling of feature values instead of dropping each
                   feature, applies to all features.
                 - precision : Precision (positive predictive) value for nominal
                   features only.
@@ -2341,14 +2631,18 @@
         return self.client.get_prediction_stats(
             trainee_id=self.id,
             action_feature=action_feature,
             robust=robust,
             robust_hyperparameters=robust_hyperparameters,
             stats=stats,
             weight_feature=weight_feature,
+            condition=condition,
+            precision=precision,
+            num_cases=num_cases,
+            num_robust_influence_samples_per_case=num_robust_influence_samples_per_case,
         )
 
     def get_marginal_stats(
         self, *, weight_feature: Optional[str] = None,
     ) -> "DataFrame":
         """
         Get marginal stats for all features.
@@ -2440,14 +2734,17 @@
         contributions: Optional[bool] = None,
         contributions_robust: Optional[bool] = None,
         hyperparameter_param_path: Optional[Iterable[str]] = None,
         mda: Optional[bool] = None,
         mda_permutation: Optional[bool] = None,
         mda_robust: Optional[bool] = None,
         mda_robust_permutation: Optional[bool] = None,
+        num_robust_influence_samples=None,
+        num_robust_residual_samples=None,
+        num_robust_influence_samples_per_case=None,
         num_samples: Optional[int] = None,
         residuals: Optional[bool] = None,
         residuals_robust: Optional[bool] = None,
         sample_model_fraction: Optional[float] = None,
         sub_model_size: Optional[int] = None,
         use_case_weights: Optional[bool] = False,
         weight_feature: Optional[str] = None,
@@ -2455,20 +2752,21 @@
         """
         Compute and cache specified feature interpretations.
 
         Parameters
         ----------
         action_feature : str, optional
             Name of target feature whose hyperparameters to use
-            for computations.  Default is whatever the model was optimized for,
-            or the mda_action_features for MDA, or ".targetless" if optimized
+            for computations.  Default is whatever the model was analyzed for,
+            or the mda_action_features for MDA, or ".targetless" if analyzed
             for targetless.
         context_features : list of str, optional
             List of features names to use as contexts for
-            computations. Default is all trained features if unspecified.
+            computations. Default is all trained non-unique features if
+            unspecified.
         contributions : bool, optional
             For each context_feature, use the full set of all other
             context_features to compute the mean absolute delta between
             prediction of action_feature with and without the context_feature
             in the model. False removes cached values.
         contributions_robust : bool, optional
             For each context_feature, use the robust (power set/permutation)
@@ -2493,19 +2791,30 @@
             Compute MDA by dropping each feature and using the
             robust (power set/permutations) set of remaining context features
             for each prediction. False removes cached values.
         mda_robust_permutation : bool, optional
             Compute MDA by scrambling each feature and using the
             robust (power set/permutations) set of remaining context features
             for each prediction. False removes cached values.
+        num_robust_influence_samples : int, optional
+            Total sample size of model to use (using sampling with replacement)
+            for robust contribution computation. Defaults to 300.
+        num_robust_residual_samples : int, optional
+            Total sample size of model to use (using sampling with replacement)
+            for robust mda and residual computation.
+            Defaults to 1000 * (1 + log(number of features)).  Note: robust mda
+            will be updated to use num_robust_influence_samples in a future release.
+        num_robust_influence_samples_per_case : int, optional
+            Specifies the number of robust samples to use for each case for
+            robust contribution computations.
+            Defaults to 300 + 2 * (number of features).
         num_samples : int, optional
-            Sample size of model to use (using sampling with replacement).
-            Defaults to 1000 for non-robust, for robust computations scales the
-            1000 up dynamically by (1 + log(number of features)). If specified
-            uses the unmodified value and overrides sample_model_fraction.
+            Total sample size of model to use (using sampling with replacement)
+            for all non-robust computation. Defaults to 1000.
+            If specified overrides sample_model_fraction.```
         residuals : bool, optional
             For each context_feature, use the full
             set of all other context_features to predict the feature.
             False removes cached values.
         residuals_robust : bool, optional
             For each context_feature, use the robust (power
             set/permutations) set of all other context_features to predict the
@@ -2536,14 +2845,17 @@
             contributions=contributions,
             contributions_robust=contributions_robust,
             hyperparameter_param_path=hyperparameter_param_path,
             mda=mda,
             mda_permutation=mda_permutation,
             mda_robust=mda_robust,
             mda_robust_permutation=mda_robust_permutation,
+            num_robust_influence_samples=num_robust_influence_samples,
+            num_robust_residual_samples=num_robust_residual_samples,
+            num_robust_influence_samples_per_case=num_robust_influence_samples_per_case,
             num_samples=num_samples,
             residuals=residuals,
             residuals_robust=residuals_robust,
             sample_model_fraction=sample_model_fraction,
             sub_model_size=sub_model_size,
             use_case_weights=use_case_weights,
             weight_feature=weight_feature,
@@ -2561,14 +2873,17 @@
         Get cached feature Mean Decrease In Accuracy (MDA).
 
         All keyword arguments are optional, when not specified will auto-select
         cached MDA for output, when specified will attempt to
         output the cached MDA best matching the requested parameters,
         or None if no cached match is found.
 
+        .. deprecated:: 1.0.0
+            Use :meth:`Trainee.get_prediction_stats` instead.
+
         Parameters
         ----------
         action_feature : str
             Will attempt to return MDA that was
             computed for this specified action_feature.
         permutation : bool, optional
             When False, will attempt to return MDA that was computed
@@ -2605,14 +2920,17 @@
         Get cached feature contributions.
 
         All keyword arguments are optional, when not specified will auto-select
         cached contributions for output, when specified will attempt to
         output the cached contributions best matching the requested parameters,
         or None if no cached match is found.
 
+        .. deprecated:: 1.0.0
+            Use :meth:`Trainee.get_prediction_stats` instead.
+
         Parameters
         ----------
         action_feature : str
             Will attempt to return contributions that were
             computed for this specified action_feature.
         robust : bool, optional
             When specified, will attempt to return contributions that were
@@ -2637,16 +2955,16 @@
         """
         Get the workflow attributes used by the trainee.
 
         Returns
         -------
         dict
             A dict including the trainee's hyperparameter_map,
-            optimize_threshold, optimize_growth_factor and
-            auto_optimize_limit_size.
+            analyze_threshold, analyze_growth_factor and
+            auto_analyze_limit_size.
         """
         return self.client.get_params(self.id)
 
     def set_params(self, params: Dict[str, Any]) -> None:
         """
         Set the workflow attributes for the trainee.
 
@@ -2660,23 +2978,23 @@
             Example::
 
                 {
                     "hyperparameter_map": {
                         ".targetless": {
                             "robust": {
                                 ".none": {
-                                    "dwe": -1, "p": .1, "k": 8
+                                    "dt": -1, "p": .1, "k": 8
                                 }
                             }
                         }
                     },
-                    "auto_optimize_enabled": False,
-                    "optimize_threshold": 100,
-                    "optimize_growth_factor": 7.389,
-                    "auto_optimize_limit_size": 100000
+                    "auto_analyze_enabled": False,
+                    "analyze_threshold": 100,
+                    "analyze_growth_factor": 7.389,
+                    "auto_analyze_limit_size": 100000
                 }
         """
         self.client.set_params(self.id, params=params)
 
     @property
     def client(self) -> Union[AbstractDiveplaneClient, DiveplanePandasClientMixin]:
         """
```

## diveplane/reactor/tests/test_reactor.py

```diff
@@ -1,7 +1,8 @@
+from pandas.testing import assert_frame_equal
 import pytest
 
 from diveplane.reactor import Trainee
 
 
 class TestReactor:
     @pytest.fixture(autouse=True)
@@ -96,7 +97,22 @@
         c1 = trainee.get_cases()
 
         sessions = trainee.get_sessions()
         session = sessions[0]
         c2 = trainee.get_cases(session=session['id'])
 
         assert c1.equals(c2)
+
+    def test_predict(self, trainee):
+        """
+        Test that predict returns the same results as react.
+        """
+
+        action_features = ['target']
+        context_features = [k for k in trainee.features.keys() if k not in action_features]
+
+        test_data = [[5.5, 3.6, 1.6, 0.2], [5.2, 3.2, 1.2, 0.2]]
+
+        prediction = trainee.predict(test_data, action_features=action_features, context_features=context_features)
+        react = trainee.react(test_data, action_features=action_features, context_features=context_features)
+
+        assert_frame_equal(prediction, react['action'])
```

## diveplane/scikit/scikit.py

```diff
@@ -356,28 +356,28 @@
                 continue
             elif deep and hasattr(value, 'get_params'):
                 deep_items = value.get_params().items()
                 out.update((key + '__' + k, val) for k, val in deep_items)
                 out[key] = value
         return out
 
-    def fit(self, X, y, optimize=True) -> "DiveplaneEstimator":
+    def fit(self, X, y, analyze=True) -> "DiveplaneEstimator":
         """
         Fit a model with Diveplane.
 
         Parameters
         -----------
         X : numpy.ndarray, shape (n_samples, n_features)
             Data
         y : numpy.ndarray, shape (n_samples,)
             Target. Will be cast to X's dtype if necessary
-        optimize : bool, default=True
-            A flag to not optimize the trainee by default
+        analyze : bool, default=True
+            A flag to not analyze the trainee by default
 
-                - A user may plan to call optimize themselves after fit() to specify parameters
+                - A user may plan to call analyze themselves after fit() to specify parameters
 
         Returns
         -------
         DiveplaneEstimator
             self
         """
         if not isinstance(X, np.ndarray):
@@ -408,18 +408,18 @@
 
             self.trainee_params = self._get_trainee_params()
 
         self.persistence = self.trainee.persistence
 
         self._train(X, y)
 
-        if optimize:
+        if analyze:
             if self.verbose:
-                print('Optimizing trainee')
-            self.optimize()
+                print('Analyzing trainee')
+            self.analyze()
 
         return self
 
     def partial_fit(self, X, y):
         """
         Add data to an existing Diveplane model.
 
@@ -996,45 +996,45 @@
             self.feature_names.append(str(fname))
 
     def _store_target_names(self):
         self.target_names = []
         for tname in self.targets.keys():
             self.target_names.append(str(tname))
 
-    def optimize(self, seed=None, **kwargs):
+    def analyze(self, seed=None, **kwargs):
         """
-        Optimize a trainee.
+        Analyze a trainee.
 
         Parameters
         ----------
         seed : int, optional
             A random seed.
         **kwargs
-            Refer to docstring in diveplane.client.optimize method for complete
+            Refer to docstring in diveplane.client.analyze method for complete
             reference of all parameters
         """
         if not kwargs:
             kwargs = dict()
 
         if seed is not None:
             self.trainee.set_random_seed(seed)
 
         if kwargs.get('targeted_model', "") == 'targetless':
-            # for targetless optimize, override 'action_features'
+            # for targetless analyze, override 'action_features'
             # and 'context_features'
             kwargs['action_features'] = []
             kwargs['context_features'] = self.feature_names + self.target_names
         else:
             if 'action_features' not in kwargs:
                 kwargs['action_features'] = self.target_names
 
             if 'context_features' not in kwargs:
                 kwargs['context_features'] = self.feature_names
 
-        self.trainee.optimize(**kwargs)
+        self.trainee.analyze(**kwargs)
 
     def partial_unfit(self, precision: str, num_cases: int,
                       criteria: Optional[Dict] = None):
         """
         Remove a training case from a trainee.
 
         The training case will be completely purged from the model and the
@@ -1222,35 +1222,35 @@
                                                   verbose=verbose, debug=debug,
                                                   ttl=ttl,
                                                   method=CLASSIFICATION,
                                                   client_params=client_params,
                                                   trainee_params=trainee_params)
         self.classes_ = np.empty((0,), dtype=str)
 
-    def fit(self, X: np.ndarray, y: np.ndarray, optimize: bool = True):
+    def fit(self, X: np.ndarray, y: np.ndarray, analyze: bool = True):
         """
         Fit a model with Diveplane.
 
         Parameters
         -----------
         X : numpy.ndarray, shape (n_samples, n_features)
             Data
         y : numpy.ndarray, shape (n_samples,)
             Target. Will be cast to X's dtype if necessary
-        optimize : bool, default=True
-            (Optional) If trainee should be optimized.
+        analyze : bool, default=True
+            (Optional) If trainee should be analyzed.
 
-                - a user may plan to call optimize themselves after fit() to specify parameters
+                - a user may plan to call analyze themselves after fit() to specify parameters
 
         Returns
         -------
         DiveplaneEstimator
             self
         """
-        DiveplaneEstimator.fit(self, X, y, optimize)
+        DiveplaneEstimator.fit(self, X, y, analyze)
         # To keep fit as an idempotent operation, clear out the classes variable before populating them.
         self.classes_ = np.empty((0,), dtype=str)
         self._populate_classes(y)
 
     def partial_fit(self, X: np.ndarray, y: np.ndarray):
         """
         Adds data to an existing Diveplane model.
```

## diveplane/utilities/installation_verification.py

```diff
@@ -405,18 +405,21 @@
                 "allow_null": False},
         }
     }
     feature_names = list(features.keys())
     context_features = list(feature_names)[:-1]
     action_features = [list(feature_names)[-1]]
 
-    trainee_obj = Trainee(f"generated data ({get_nonce()})", features=features,
-                          default_context_features=context_features,
-                          default_action_features=action_features,
-                          persistence="never")
+    trainee_obj = Trainee(
+        f"installation_verification: generated dataframe ({get_nonce()})",
+        features=features,
+        default_context_features=context_features,
+        default_action_features=action_features,
+        persistence="never"
+    )
     trainee = client.create_trainee(trainee_obj)
     client.set_feature_attributes(trainee.id, features)
     client.acquire_trainee_resources(trainee.id, max_wait_time=0)
     if timeout:
         # Generate 1 case at a time until `timeout` has passed.
         end_time = datetime.now() + timedelta(seconds=timeout)
         cases = {"action": []}
@@ -612,16 +615,18 @@
     """
     try:
         client = registry.client
         if source_df is None:
             source_df, _ = generate_dataframe(client=client)
         features = infer_feature_attributes(source_df)
         feature_names = list(features.keys())
-        trainee_obj = Trainee(f"generated data ({get_nonce()})",
-                              features=features)
+        trainee_obj = Trainee(
+            f"installation_verification: check save ({get_nonce()})",
+            features=features
+        )
         trainee = client.create_trainee(trainee_obj)
         client.train(trainee.id, source_df, features=feature_names)
         client.persist_trainee(trainee.id)
     except Exception:  # noqa: Deliberately broad
         traceback.print_exc(file=registry.logger)
         return (Status.CRITICAL,
                 "Could not save model. Please check file permissions.")
@@ -833,18 +838,21 @@
         features = infer_feature_attributes(source_df)
         X = source_df.drop("class", axis=1)
         y = source_df["class"]
 
         X_train, X_test, y_train, _ = train_test_split(X, y, test_size=0.2)
         action_features = ["class"]
         context_features = X.columns.tolist()
-        trainee = reactor.Trainee(name="reactor check",
-                                  features=features, overwrite_existing=True)
+        trainee = reactor.Trainee(
+            name=(f"installation_verification: "
+                  f"check reactor operations ({get_nonce()})"),
+            features=features, overwrite_existing=True
+        )
         trainee.train(X_train.join(y_train))
-        trainee.optimize()
+        trainee.analyze()
         response = trainee.react(X_test, context_features=context_features,
                                  action_features=action_features)
         results = response['action'][action_features]
         if results.shape[0] != X_test.shape[0]:
             return (Status.ERROR,
                     "Results do not have the same number of samples as the "
                     "input data.")
@@ -895,45 +903,51 @@
             source_df, _ = generate_dataframe(client=registry.client, num_samples=150)
 
         orig_df = source_df.sample(frac=0.5)
         gen_df = source_df[~source_df.index.isin(orig_df.index)]
         features = infer_feature_attributes(orig_df)
 
         dqt = DataQuality(orig_df, gen_df, features=features, verbose=-1)
-        desirability = dqt.run_metrics(distance_ratio_type="avg").overall_desirability
+        desirability = dqt.run_metric("DescriptiveStatistics").desirability
         if desirability < desirability_threshold:
             return (Status.WARNING, f"Desirability did not exceed the threshold of {desirability_threshold:,.1f}.")
     except Exception:
         traceback.print_exc(file=registry.logger)
         return (Status.CRITICAL,
                 "Could not complete operation. Check installation.")
     else:
         return (Status.OK, "")
     finally:
-        try:
-            registry.client.release_trainee_resources(dqt.trainee_orig.id)
-        except Exception:  # noqa: Deliberately broad
-            pass
+        for dataset in [dqt.orig, dqt.gen]:
+            if isinstance(dataset, Trainee):
+                if id := getattr(dataset, "id", None):
+                    try:
+                        registry.client.release_trainee_resources(id)
+                    except Exception:  # noqa: Deliberately broad
+                        pass
 
 
 def _attempt_train_date_feature(result_queue: multiprocessing.Queue):
     """
     Attempt to train a date feature to check for proper time zone support.
 
     Parameters
     ----------
     result_queue : A multiprocessing queue instance
         A queue to put the results.
     """
     client = DiveplaneClient()
     features = {'date': {'type': 'continous', 'date_time_format': '%Y-%m-%d'}}
-    trainee_obj = Trainee("check_tzdata_installed", features=features,
-                          default_context_features='date',
-                          default_action_features='date',
-                          persistence='never')
+    trainee_obj = Trainee(
+        f"installation_verification: check_tzdata_installed ({get_nonce()})",
+        features=features,
+        default_context_features='date',
+        default_action_features='date',
+        persistence='never'
+    )
     trainee = client.create_trainee(trainee_obj)
     client.train(trainee_id=trainee.id, cases=[["2001-01-01"]], features=['date'])
     result_queue.put(client.get_num_training_cases(trainee.id))
     client.delete_trainee(trainee.id)
 
 
 def check_tzdata_installed(*, registry: InstallationCheckRegistry):
```

## diveplane/utilities/feature_attributes/base.py

```diff
@@ -19,15 +19,15 @@
 
 logger = logging.getLogger(__name__)
 
 
 class FeatureAttributesBase(dict):
     """Provides accessor methods for and dict-like access to inferred feature attributes."""
 
-    def __init__(self, feature_attributes, params):
+    def __init__(self, feature_attributes: Dict, params: Dict = {}):
         """Instantiate this FeatureAttributesBase object."""
         self.params = params
         self.update(feature_attributes)
 
     def __copy__(self) -> "FeatureAttributesBase":
         """Return a (deep)copy of this instance of FeatureAttributesBase."""
         cls = self.__class__
@@ -121,160 +121,177 @@
             # Check nominal bounds
             allowed_values = attributes['bounds']['allowed']
             out_of_band_values = set(unique_values) - set(allowed_values)
             if pd.isna(list(out_of_band_values)).all():
                 # Placeholder for behavior when columns contain nans
                 pass
             elif out_of_band_values:
-                errors.append(f'{feature} contains out-of-band values: {out_of_band_values}')
+                errors.append(f"'{feature}' contains out-of-band values: {out_of_band_values}")
         elif attributes.get('date_time_format'):
             # Since this is a datetime feature, convert dates to epoch time for bounds comparison
             try:
                 if min_bound:
-                    min_bound = date_to_epoch(min_bound, time_format=attributes['date_time_format'])
+                    min_bound_epoch = date_to_epoch(min_bound, time_format=attributes['date_time_format'])
                 if max_bound:
-                    max_bound = date_to_epoch(max_bound, time_format=attributes['date_time_format'])
+                    max_bound_epoch = date_to_epoch(max_bound, time_format=attributes['date_time_format'])
                 for value in unique_values:
                     epoch = date_to_epoch(value, time_format=attributes['date_time_format'])
-                    if (max_bound and epoch > max_bound) or (min_bound and epoch < min_bound):
-                        errors.append(f'{feature} has a value outside of bounds (min: {min_bound}, '
-                                      f'max: {max_bound}): {value}')
+                    if (max_bound and epoch > max_bound_epoch) or (min_bound and epoch < min_bound_epoch):
+                        errors.append(f"'{feature}' has a value outside of bounds (min: {min_bound}, "
+                                      f"max: {max_bound}): {value}")
             except ValueError as err:
                 errors.append(f'Could not validate datetime bounds due to the following error: {err}')
         elif min_bound or max_bound:
             # Check int/float bounds
             for value in unique_values:
                 if (max_bound and float(value) > float(max_bound)) or (min_bound and float(value) < float(min_bound)):
-                    errors.append(f'{feature} has a value outside of bounds (min: {min_bound}, '
-                                  f'max: {max_bound}): {value}')
+                    errors.append(f"'{feature}' has a value outside of bounds (min: {min_bound}, "
+                                  f"max: {max_bound}): {value}")
 
         return errors
 
-    def _validate_dtype(self, data: pd.DataFrame, feature: str,
-                        expected_dtype: str, coerced_df: pd.DataFrame,
-                        coerce: bool = False) -> List[str]:
+    def _validate_dtype(self, data: pd.DataFrame, feature: str,  # noqa: C901
+                        expected_dtype: Union[str, pd.CategoricalDtype], coerced_df: pd.DataFrame,
+                        coerce: bool = False, localize_datetimes=True) -> List[str]:
         """Validate the data type of a feature and optionally attempt to coerce."""
         errors = []
         series = coerced_df[feature]
         is_valid = False
 
-        if isinstance(data[feature].dtype, pd.CategoricalDtype):
-            # If the feature is a Categorical dtype, check the dtype of the categories
-            if data[feature].cat.categories.dtype.name == expected_dtype:
+        if isinstance(expected_dtype, pd.CategoricalDtype):
+            # If the feature is a Categorical dtype, try to coerce
+            try:
+                series = series.astype(expected_dtype)
+                if coerce:
+                    coerced_df[feature] = series
+                is_valid = True
+            except Exception: # noqa: Intentionally broad
+                pass
+        elif expected_dtype == 'datetime64':
+            try:
+                series = pd.to_datetime(coerced_df[feature], format=self[feature]['date_time_format'])
+                if coerce:
+                    if localize_datetimes and not pd.api.types.is_datetime64tz_dtype(series):
+                        series = series.dt.tz_localize('UTC', ambiguous='infer', nonexistent='NaT')
+                        coerced_df[feature] = coerced_df[feature].astype(series.dtype)
+                    else:
+                        coerced_df[feature] = series
                 is_valid = True
+            except Exception: # noqa: Intentionally broad
+                pass
         else:
             # Else, compare the dtype directly
             if data[feature].dtype.name == expected_dtype:
                 is_valid = True
             # If the feature can be converted, consider it valid (slightly differing numeric types, etc.)
             else:
                 try:
                     series = series.astype(expected_dtype)
                     if coerce:
                         coerced_df[feature] = series
                     is_valid = True
+                except pd.errors.IntCastingNaNError:
+                    # If this happens, there is a null value, thus a float dtype is OK
+                    if pd.api.types.is_float_dtype(series):
+                        is_valid = True
                 except Exception: # noqa: Intentionally broad
                     pass
 
-        # Don't strictly enforce pandas datetime subtype (datetime64[ns], datetime64[ms], etc.)
-        if not is_valid and expected_dtype == 'datetime64':
-            if pd.api.types.is_datetime64_any_dtype(data[feature]):
-                is_valid = True
-            # If the feature is detected as an object, consider it valid if it could be converted
-            elif data[feature].dtype.name == 'object':
-                try:
-                    series = pd.to_datetime(series)
-                    if coerce:
-                        coerced_df[feature] = series
-                    is_valid = True
-                except Exception: # noqa: Intentionally broad
-                    pass
-
-        # Raise warnings and/or try to coerce if the types do not match
+        # Raise warnings if the types do not match
         if not is_valid:
             if coerce:
-                errors.append(f'Expected dtype {expected_dtype} for feature {feature} '
-                              'but could not coerce.')
+                errors.append(f"Expected dtype '{expected_dtype}' for feature '{feature}' "
+                              "but could not coerce.")
             else:
-                errors.append(f"Feature {feature} should be '{expected_dtype}' dtype, but found "
+                errors.append(f"Feature '{feature}' should be '{expected_dtype}' dtype, but found "
                               f"'{data[feature].dtype}'")
 
         return errors
 
     @staticmethod
     def _allows_null(attributes: Dict) -> bool:
         """Return whether the given attributes indicates the allowance of null values."""
         return 'bounds' in attributes and attributes['bounds'].get('allow_null', False)
 
     def _validate_df(self, data: pd.DataFrame, coerce: bool = False,  # noqa: C901
-                     raise_errors: bool = False, table_name: str = None):
+                     raise_errors: bool = False, table_name: str = None, validate_bounds=True,
+                     allow_missing_features: bool = False, localize_datetimes=True):
         errors = []
         coerced_df = data.copy(deep=True)
         features = self[table_name] if table_name else self
 
         for feature, attributes in features.items():
             if feature not in data.columns:
                 # Column is missing
-                if not feature.startswith('.'):
+                if not feature.startswith('.') and not allow_missing_features:
                     errors.append(f'{feature} is missing from the dataframe')
-                # OK if it's an internal feature
+                # OK if it's an internal feature or is being processed by DQT
                 continue
 
-            # Check ordinal types
-            if attributes['type'] == 'ordinal':
-                if 'bounds' in attributes and 'allowed' in attributes['bounds']:
-                    # Check type (should be object)
-                    errors.extend(self._validate_dtype(data, feature, 'object',
-                                                       coerced_df, coerce=coerce))
-                elif 'bounds' in attributes:
-                    # Check type (should be float)
-                    if attributes.get('decimal_places', 0) > 0 or self._allows_null(attributes):
-                        errors.extend(self._validate_dtype(data, feature, 'float64',
-                                                           coerced_df, coerce=coerce))
-                    # Check type (should be int)
-                    else:
-                        errors.extend(self._validate_dtype(data, feature, 'int64',
-                                                           coerced_df, coerce=coerce))
-
             # Check nominal types
-            elif attributes['type'] == 'nominal':
+            if attributes['type'] == 'nominal':
                 if attributes.get('data_type') == 'number':
-                    # Check type (should be float)
+                    # Check type (float)
                     if attributes.get('decimal_places', 0) > 0 or self._allows_null(attributes):
                         errors.extend(self._validate_dtype(data, feature, 'float64',
                                                            coerced_df, coerce=coerce))
-                    # Check type (should be int)
+                    # Check type (int)
                     else:
                         errors.extend(self._validate_dtype(data, feature, 'int64',
                                                            coerced_df, coerce=coerce))
                 elif attributes.get('data_type') == 'boolean':
-                    # Check type (should be bool)
+                    # Check type (boolean)
                     errors.extend(self._validate_dtype(data, feature, 'bool',
                                                        coerced_df, coerce=coerce))
+                elif attributes.get('bounds') and attributes['bounds'].get('allowed'):
+                    # Check type (categorical)
+                    schema_dtype = pd.CategoricalDtype(attributes['bounds']['allowed'],
+                                                       ordered=True)
+                    errors.extend(self._validate_dtype(data, feature, schema_dtype,
+                                                       coerced_df, coerce=coerce))
                 else:
-                    # Else, should be object
+                    # Else, should be an object
                     errors.extend(self._validate_dtype(data, feature, 'object',
                                                        coerced_df, coerce=coerce))
 
+            # Check ordinal types
+            elif attributes['type'] == 'ordinal':
+                if attributes.get('bounds') and attributes['bounds'].get('allowed'):
+                    # Check type (categorical)
+                    schema_dtype = pd.CategoricalDtype(attributes['bounds']['allowed'],
+                                                       ordered=True)
+                    errors.extend(self._validate_dtype(data, feature, schema_dtype,
+                                                       coerced_df, coerce=coerce))
+                # Check type (float)
+                elif attributes.get('decimal_places', 0) > 0 or self._allows_null(attributes):
+                    errors.extend(self._validate_dtype(data, feature, 'float64',
+                                                       coerced_df, coerce=coerce))
+                # Check type (int)
+                else:
+                    errors.extend(self._validate_dtype(data, feature, 'int64',
+                                                       coerced_df, coerce=coerce))
+
             # Check continuous types
             elif attributes['type'] == 'continuous':
                 if 'date_time_format' in attributes:
-                    # Check type (should be datetime64)
+                    # Check type (datetime)
                     errors.extend(self._validate_dtype(data, feature, 'datetime64',
-                                                       coerced_df, coerce=coerce))
+                                                       coerced_df, coerce=coerce,
+                                                       localize_datetimes=localize_datetimes))
                 elif attributes.get('decimal_places', -1) > 0 or self._allows_null(attributes):
-                    # Check type (should be float64)
+                    # Check type (float)
                     errors.extend(self._validate_dtype(data, feature, 'float64',
                                                        coerced_df, coerce=coerce))
                 elif attributes.get('decimal_places', -1) == 0:
-                    # Check type (should be int64)
+                    # Check type (int)
                     errors.extend(self._validate_dtype(data, feature, 'int64',
                                                        coerced_df, coerce=coerce))
             # Check feature bounds
-            errors.extend(self._validate_bounds(data, feature, attributes))
+            if validate_bounds:
+                errors.extend(self._validate_bounds(data, feature, attributes))
 
         if errors:
             msg = ('Failed to validate DataFrame against feature attributes due to the '
                    'following errors:\n')
             for error in errors:
                 msg = msg + f'{error}\n'
             if raise_errors:
@@ -282,28 +299,37 @@
             else:
                 warnings.warn(msg)
 
         if coerce:
             return coerced_df
 
     @abstractmethod
-    def validate(data: Any, coerce=False, raise_errors=False):
+    def validate(data: Any, coerce=False, raise_errors=False, validate_bounds=True,
+                 allow_missing_features=False, localize_datetimes=True):
         """
         Validate the given data against this FeatureAttributes object.
 
         Check that feature bounds and data types loosely describe the data. Optionally
         attempt to coerce the data into conformity.
         Parameters
         ----------
         data : Any
             The data to validate
-        coerce : bool
-            Whether to attempt to coerce DataFrame columns into correct data types
-        raise_errors : bool
+        coerce : bool (default False)
+            Whether to attempt to coerce DataFrame columns into correct data types. Coerced
+            datetimes will be localized to UTC.
+        raise_errors : bool (default False)
             If True, raises a ValueError if nonconforming columns are found; else issue a warning
+        validate_bounds : bool (default True)
+            Whether to validate the data against the attributes' inferred bounds
+        allow_missing_features : bool (default False)
+            Allows features that are missing from the DataFrame to be ignored
+        localize_datetimes : bool (default True)
+            Whether to localize datetime features to UTC.
+
         Returns
         -------
         None | DataFrame
             None or the coerced DataFrame if 'coerce' is True and there were no errors.
         """
         raise NotImplementedError()
 
@@ -324,31 +350,39 @@
 
         Check that feature bounds and data types loosely describe the data. Optionally
         attempt to coerce the data into conformity.
         Parameters
         ----------
         data : Any
             The data to validate (single table only)
-        coerce : bool
-            Whether to attempt to coerce DataFrame columns into correct data types
-        raise_errors : bool
+        coerce : bool (default False)
+            Whether to attempt to coerce DataFrame columns into correct data types.
+        raise_errors : bool (default False)
             If True, raises a ValueError if nonconforming columns are found; else issue a warning
-        strict_numeric_types : bool
-            Whether to strictly enforce numeric dtypes if what is inferred does not match the
-            feature attributes.
+        validate_bounds : bool (default True)
+            Whether to validate the data against the attributes' inferred bounds
+        allow_missing_features : bool (default False)
+            Allows features that are missing from the DataFrame to be ignored
+        localize_datetimes : bool (default True)
+            Whether to localize datetime features to UTC.
+
         Returns
         -------
         None | DataFrame
             None or the coerced DataFrame if 'coerce' is True and there were no errors.
         """
         raise NotImplementedError("'data' is an unsupported type")
 
     @validate.register
-    def _(self, data: pd.DataFrame, coerce=False, raise_errors=False):
-        return self._validate_df(data, coerce=coerce, raise_errors=raise_errors)
+    def _(self, data: pd.DataFrame, coerce=False, raise_errors=False, validate_bounds=True,
+          allow_missing_features=False, localize_datetimes=True):
+        return self._validate_df(data, coerce=coerce, raise_errors=raise_errors,
+                                 validate_bounds=validate_bounds,
+                                 allow_missing_features=allow_missing_features,
+                                 localize_datetimes=localize_datetimes)
 
 
 class InferFeatureAttributesBase(ABC):
     """
     This is an abstract Feature Attributes inferrer base class.
 
     It is agnostic to the type of data being inspected.
```

## diveplane/utilities/feature_attributes/pandas.py

```diff
@@ -430,15 +430,15 @@
             decimals = max([
                 len((str(np.format_float_positional(r))).split('.')[1])
                 for r in col
             ])
 
             # If decimal place = max precision of the float type, do not
             # specify decimal place.
-            if not (decimals >= 15 and col.dtype == np.float64):
+            if not (decimals >= 15 and pd.api.types.is_float_dtype(col.dtype)):
                 attributes['decimal_places'] = decimals
 
         return attributes
 
     def _infer_datetime_attributes(self, feature_name: str) -> Dict:
         column = self.data[feature_name]
         dt_format = ISO_8601_FORMAT
```

## diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py

```diff
@@ -574,15 +574,15 @@
 
 @pytest.mark.parametrize("ftype, data_type, decimal_places, bounds, date_time_format, expected_dtype", [
     ("continuous", "number", 0, {'allow_null': False}, None, "int64"),
     ("continuous", "number", 1, {'allow_null': False}, None, "float64"),
     ("continuous", "number", 0, {'allow_null': False}, "%Y-%m-%d", "datetime64"),
     ("ordinal", "number", 0, {'allow_null': False}, None, "int64"),
     ("ordinal", "number", 2, {'allow_null': True}, None, "float64"),
-    ("ordinal", "string", 0, {'allowed': ['SBIN'], 'allow_null': False}, None, "object"),
+    ("ordinal", "string", None, {'allowed': ['SBIN'], 'allow_null': False}, None, "object"),
     ("nominal", "number", 0, {'allow_null': False}, None, "int64"),
     ("nominal", "number", 9, {'allow_null': False}, None, "float64"),
     ("nominal", "boolean", 0, {'allow_null': False}, None, "bool"),
 ])
 def test_validate_df_multiple_dtypes(ftype, data_type, decimal_places, bounds, date_time_format,
                                      expected_dtype):
     """Test the validate() method with all possible inferred dtypes."""
@@ -613,8 +613,8 @@
     }
     if not date_time_format:
         del attrs[feature]['date_time_format']
     # validate() should not raise any errors
     coerced_df = attrs.validate(df, raise_errors=True, coerce=True)
     assert coerced_df is not None
     # coerced_df should also contain a coerced DATE column, as it is originally detected as a string
-    assert coerced_df['DATE'].dtype.name == 'datetime64[ns]'
+    assert pd.api.types.is_datetime64_any_dtype(coerced_df['DATE'].dtype)
```

## Comparing `diveplane_reactor_api-5.0.18.dist-info/LICENSE.txt` & `diveplane_reactor_api-6.5.8.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `diveplane_reactor_api-5.0.18.dist-info/METADATA` & `diveplane_reactor_api-6.5.8.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: diveplane-reactor-api
-Version: 5.0.18
+Version: 6.5.8
 Summary: Diveplane Reactor and Scikit Estimator for the interpretable Machine Learning and Artificial Intelligence API Diveplane.
 Home-page: https://www.diveplane.com
 Author: Diveplane Corporation
 Author-email: support@diveplane.com
 License: Diveplane Corporation Free Software License
 Project-URL: Documentation, https://docs.community.diveplane.com/
 Keywords: machine learning,artificial intelligence
@@ -31,22 +31,22 @@
 Requires-Dist: deprecation (<3,>=2.1.0)
 Requires-Dist: Faker (>=4.1.1)
 Requires-Dist: humanize (>=4.0.0)
 Requires-Dist: mmh3
 Requires-Dist: numpy
 Requires-Dist: pandas
 Requires-Dist: pyjwt (<3,>=2.6.0)
-Requires-Dist: pyyaml (<7.0,>=5.4.1)
+Requires-Dist: pyyaml (<7.0,>=6.0.1)
 Requires-Dist: rich (>=12.5.1)
 Requires-Dist: scikit-learn
 Requires-Dist: semantic-version (<3,>=2.8.5)
 Requires-Dist: typing-extensions (<5.0,>=4.1.0)
 Requires-Dist: urllib3 (<=2,>=1.26.2)
-Requires-Dist: diveplane-openapi-client (==23.0.6)
-Requires-Dist: diveplane-amalgam-api (==2.3.9)
+Requires-Dist: diveplane-openapi-client (==24.0.9)
+Requires-Dist: diveplane-amalgam-api (==2.3.12)
 
 # Diveplane Reactor™
 
 Diveplane is an Understandable AI platform which is rooted in instance-based
 machine learning and harnesses a fast spatial query system and information
 theory for performance, accuracy, and auditability. With Diveplane, your data is
 the model. As such, your data is stored in memory and predictions about a new
```

## Comparing `diveplane_reactor_api-5.0.18.dist-info/RECORD` & `diveplane_reactor_api-6.5.8.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,70 +1,70 @@
-diveplane/client/LICENSE-3RD-PARTY.txt,sha256=QEmcu1Zg-r4WfRD4AIwQKN5UeegTiq5f8a1WYk5ytII,171737
-diveplane/client/__init__.py,sha256=4GNjZRT1LoQB3o9MqHazav_aqMEzz8-2hUu2m4eoQoU,2146
-diveplane/client/base.py,sha256=vOihpEjiqXiK9k4h_nu6705WOVPv8_u0eenNIMf3FSs,14410
+diveplane/client/LICENSE-3RD-PARTY.txt,sha256=6fDm0283kElS3eIQiIr4yIRvExebY70FkQNUEsuokzU,171816
+diveplane/client/__init__.py,sha256=1L_Opn7RCA_141611EZV6C_L5zWTfuYSzP4nTJePLDE,2145
+diveplane/client/base.py,sha256=MHHNed67XpH5NkEkYbR6XuiWqQ3zKdkbDYz0_W2ck_U,14668
 diveplane/client/cache.py,sha256=Kmaz31Br2aS9BiDCStmGFxpkiRY71iPwvetzyamON3U,3080
 diveplane/client/client.py,sha256=hhdv48zGkpHO6jN3dvq8KJixE-28tTzi_0fGU8dVCN8,16092
 diveplane/client/configuration.py,sha256=oqPT4a4oBWS-jMMOSqlUL8AiQtSlg7bvBai96JZhBmo,4594
 diveplane/client/exceptions.py,sha256=2rSJ0BGw5wsBZI6iQ4V3G8mXL0ZQeK7ypQpPeKUVSNY,4329
 diveplane/client/protocols.py,sha256=m0X5RHkPyG5F3oqs7WbleEqo7Pv-nrnTuZc0o7_-K1U,958
-diveplane/client/requirements-3.10.txt,sha256=9y_rCjwIIq-Y_kV8WPW3Jrs3XUlYqZDr7hfzp1jo7HI,1900
-diveplane/client/requirements-3.11.txt,sha256=IhGsFvoDh1e2L99yZJeIZAn8Fdt2Smd4zoXH0hlobPg,1900
-diveplane/client/requirements-3.8.txt,sha256=wHrH4_WR2YICIeAVgWvkwC6zQU9WCVt7sHsamQwtpzQ,1919
-diveplane/client/requirements-3.9.txt,sha256=dvT_0eHsUJtM6VtqiVggGSE_3yj4C1-3dnO-1WHpw6M,1898
-diveplane/client/requirements-dev-3.10.txt,sha256=_3mn7zpznzV2SftC7hF5WhlaNa_bKJa66firu3EM0qA,3309
-diveplane/client/requirements-dev-3.11.txt,sha256=NwqkMNIWTq3XlE9-x3ApZyiGa5VnDQw-0sXdkt7Olvs,3060
-diveplane/client/requirements-dev-3.8.txt,sha256=r0rfvYDBCJW468Iv4Q8X4tg21t4R1JbahosocDrmygg,3296
-diveplane/client/requirements-dev-3.9.txt,sha256=mL61kfUnkiQysbKy3_9wiDb3CMk_R9OHFZMTw3yoY7s,3296
+diveplane/client/requirements-3.10.txt,sha256=cqOqEno2D40aJzYzOGC9pX7EYiAPMEZnSzmm723Mg6U,1903
+diveplane/client/requirements-3.11.txt,sha256=6bdOOVDbYR-R72nXrieoU7PjgV9AUhJUSNVDNj4srzk,1903
+diveplane/client/requirements-3.8.txt,sha256=2FWDikqtmksPPDlY6sPe9l-TyiAUKWCbGAY9aj2Fi9Q,1936
+diveplane/client/requirements-3.9.txt,sha256=k0_QQcvZx2hT7TOflGVdOuSdSh_SshBLPdLgTIwAnYU,1901
+diveplane/client/requirements-dev-3.10.txt,sha256=veQ1g6qoqUNZgrSzAIg4WMRsQ6CxdQnQwobj0Gv4k54,3346
+diveplane/client/requirements-dev-3.11.txt,sha256=oPzgNp56qNdR0FdKKRIkzLx7fSNTMupe-UiE2fPgZ18,3097
+diveplane/client/requirements-dev-3.8.txt,sha256=4t8k7ZYUPlSrK9aP33gFyJc-foU6eAF2y8cf-rcISE8,3332
+diveplane/client/requirements-dev-3.9.txt,sha256=fY5xgBHxMwHXP0lHpd4HyzADJUpaRyQKXt_X-ftbpBM,3332
 diveplane/client/requirements-dev.in,sha256=JBgU9DPrMnbzV5xteJ0AQSFH5JIUQF_qlukMZ_Rrzsc,210
-diveplane/client/requirements.in,sha256=uLHJTjnOM9IwWMpFvI1OwMZPDYq-8041fA2ELDVAzmc,371
+diveplane/client/requirements.in,sha256=Tu_0V6wPWYj5lzEEDVwum7M-bzbRIorXC0pcxTcyNOs,372
 diveplane/client/pandas/__init__.py,sha256=ZdaK7e_ovjv158JIde8kVt9XSWGbhhzDtGOLouqCqOo,175
 diveplane/client/pandas/client.py,sha256=Xqoy3hlIueLEdOkjxc5tK1uREz3J7xtk5hpP5BEsaME,10935
 diveplane/client/tests/__init__.py,sha256=LnuLdAce6XZRk49LY3lMNcuOplDB3pe8L2OLAnyYMNk,845
-diveplane/client/tests/test_client.py,sha256=ZmEjl_N8Xbi5P7XbkqbwYQsTHx8h1KU6Juyf6Bj0JS8,50937
+diveplane/client/tests/test_client.py,sha256=2v41Hz8N9SJqlFSfuoc8GxB2HpHaIpszwDyUHyIc2b0,50923
 diveplane/client/tests/test_infer_feature_attributes.py,sha256=JNuxenY4rUxb7ONCZF9Yz0j1sT9aOlMAzARzvukYtyk,13534
 diveplane/client/tests/test_scikit.py,sha256=MtPxO1AFP9l_MYoGKThQuN7_emOY-zpYvwbarr7yl58,8783
 diveplane/direct/__init__.py,sha256=Yv7hQCFhrEXpMud0DTn-XxRggERJGvsoCBv8ywqcsE8,193
 diveplane/direct/_utilities.py,sha256=Bkh5xDDL4SQudN8vhdUXzizXc5uVoGYvG4iMoV8IQgY,4538
-diveplane/direct/client.py,sha256=5e9xPJ1XxqEOKigj--qRYLk96pJK-wgEnHLMWLA_cFs,213772
-diveplane/direct/core.py,sha256=ZpJlOpTSN67A-FjA2mhS5wVGETSiSYM2abFeOno8Zn4,66259
+diveplane/direct/client.py,sha256=07GBuqESXRMgfrbGv758-6XEEKllJX4r8GxPQFsvNz4,228745
+diveplane/direct/core.py,sha256=ewlNVP5w7H4aZvK2AUgN2jWG8sUAlGYXdT25POeJatU,66788
 diveplane/direct/tests/test_standalone.py,sha256=JnScTGnewhw0EdkiMsCt-HYJ0b931Frd9fN5QrfG4Eg,923
 diveplane/reactor/__init__.py,sha256=mio87Im_AHb24labU6xx8w7K_rjHU4hGHxTIraZ4YPY,768
 diveplane/reactor/client.py,sha256=LUNTU571VyLCYpnlh2kNsR06LeRXInctLlRsw2aml1w,1282
 diveplane/reactor/project.py,sha256=EfD-TQZ0vcJDliiqpT2HZXhKi0giC4J3YORwyaHhuoQ,10613
 diveplane/reactor/session.py,sha256=ZZ5NoP_jYdeFo20HXboYVMX7_crP6mdTibHgis9wMAE,9637
-diveplane/reactor/trainee.py,sha256=-sypPpkJ4JyLSs5HoDKGtEQisH5u6ogylwrE3_j-Umc,126345
+diveplane/reactor/trainee.py,sha256=cmS2tDQ9sYbi507x0Rokrpe9XWzrAePZV4xbKicD3iI,141124
 diveplane/reactor/tests/conftest.py,sha256=ZIgUWK98WXhI9wOMrZ7BaQsYElaW8iqAqpQa-25kOBU,302
-diveplane/reactor/tests/test_reactor.py,sha256=eLvrzb07ei9uJUw-XdOvSNZ1wo46R0wkVpxQQc44F8I,3243
+diveplane/reactor/tests/test_reactor.py,sha256=Xa9nKEFp1Zi6urMhbsva0LTO9-Z07pWGac7aUK-AcMo,3891
 diveplane/scikit/__init__.py,sha256=3c8FbYnCRuRBHiv9anNmDR0rHM61RQwKekzarBkuH2w,426
-diveplane/scikit/scikit.py,sha256=P4EoPWltdO3Xg22KhrJuLuqTuV7qUocnc6V17-Y8Ag8,54659
+diveplane/scikit/scikit.py,sha256=dAHVYxVNmfE5J4Kn1Mcqcv-zKyKWAsSOiFd27o-aYbg,54642
 diveplane/utilities/__init__.py,sha256=SEeEY7-IYV03B2XMCnrEf4O8tGKUKZenN3_xgJNGaig,1741
 diveplane/utilities/eula_helper.py,sha256=DqfHstMAX2_eM2GMW_Qd_IEd_12hGXj-et7mCjvlJRY,3655
 diveplane/utilities/features.py,sha256=Wd3vibeFa6ICcQsDNQgfuRChquHN5iLQmMBhO_JyqGo,22403
 diveplane/utilities/guess_feature_attributes.py,sha256=hJhmat3zHg9KeJ0Gt-2mm9iBw54YnqlHTQH8YPSS2QI,698
-diveplane/utilities/installation_verification.py,sha256=ok1br_JBzHzZXA9eHpuOF2lVMJ2nOabOJCDrQxXLLGI,39658
+diveplane/utilities/installation_verification.py,sha256=wSDoOL31Kd5364rtetq_yERIc4GvjJYSO7fT-S1cdfQ,39939
 diveplane/utilities/internals.py,sha256=z3MQGBuzbtTQzDfaLJNLiptvUEqyf6d705KX2kXnu-Q,25873
 diveplane/utilities/json_wrapper.py,sha256=rGv60a185rJRNLvk0vYPfWchHAoXBPLYz7fZuZV8uIE,2126
 diveplane/utilities/locale.py,sha256=EREn4KKN9RxAnT8ysgi8qiiv9-4P7kXE1UKIOKWoj0w,2424
 diveplane/utilities/monitors.py,sha256=lVszyiU8sVflcJ-UXlbqZW3YBrNzDSkjYSbg6mT16Jo,4697
 diveplane/utilities/posix.py,sha256=94LaB9gwtVJKC8LBdoE8E7HudE0BIHRmHf-my7jzwFU,2341
 diveplane/utilities/random.py,sha256=5rTYgJXwgcCTtGmOcy7la7MyGl1npwHYdK-qgNa1sx0,380
 diveplane/utilities/utilities.py,sha256=Swyr8SH2rWbrX94Bb0jOCrKDJZcmRBi-PQ-f4PK7I30,40637
 diveplane/utilities/feature_attributes/__init__.py,sha256=sVfvYYLhM7zqilIg94vEYmCHLrgf6r_YYnDmU-k5dMg,194
-diveplane/utilities/feature_attributes/base.py,sha256=qV7S2YAt5Umk6Sk8uYKriPyEMQheD-Tl3UpWO-lUVuM,39499
+diveplane/utilities/feature_attributes/base.py,sha256=rsVNA3iXW3TdgXs90Wl5-sRG_gkDMRBmTa0SR9tUBiY,41791
 diveplane/utilities/feature_attributes/infer_feature_attributes.py,sha256=0-wQ-MhIPvfkVxxz5PXWd-I9TGKPPR96li1wAUgVDI8,16407
-diveplane/utilities/feature_attributes/pandas.py,sha256=fYzbh2JecKBbZQX2Plh0c7B6wcHeyMelnkGDZ97_g_U,23164
+diveplane/utilities/feature_attributes/pandas.py,sha256=kTNnwPzHRiWLpTcSeNTPQAmkQEJnlEt1v6pPW9UsXrM,23179
 diveplane/utilities/feature_attributes/protocols.py,sha256=bZFdT0AeQiKXsoPGTQVQQ7fs4wle1wghZox-EVTHhYY,6359
 diveplane/utilities/feature_attributes/relational.py,sha256=Fthe9UUSd53NcK9PIgl2Nl491s1VF3REMhoz49YND8E,37406
 diveplane/utilities/feature_attributes/time_series.py,sha256=JBSAV7p1LIks0z07iv88GV2zqdu9P-Ohu_Y9MUZD2-U,31809
-diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py,sha256=WcpdSo_foGDWR4AmOhfRrjQq9KCDp9-2SnUotKLFYtA,24901
+diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py,sha256=U2-8ZKaCc7kCZkwiLbRRLez19ykuzlO5vy9tLKfY1aM,24917
 diveplane/utilities/feature_attributes/tests/test_infer_time_series_attributes.py,sha256=VuvQJrRveKKUbaWkqWJ5N3Yp1q495gMrwtHvYvpPEAs,6442
 diveplane/utilities/tests/__init__.py,sha256=PDWNKsI7BxQHj3t6j9JzGsIe2PtMNV97QNjmHLuGseQ,1321
 diveplane/utilities/tests/conftest.py,sha256=UPuIMzLSn76ug_bh8uRzVtZAw1HCB_fGDS-nmuhjnZE,51
 diveplane/utilities/tests/test_features.py,sha256=MFdnGZd5MUJEw6GL-vS7LYaN5GTRj0DcI1qLL-Z-dSk,13611
 diveplane/utilities/tests/test_internals.py,sha256=E2Qy5L-00E2kzhjz9sk8zolXebom_dVHiFsAMRWhDhM,8189
 diveplane/utilities/tests/test_utilities.py,sha256=9lWVm3_iR7YZh3OAXT0Hjb6tnj-2X7l4uYDnISmGANM,11237
-diveplane_reactor_api-5.0.18.dist-info/LICENSE.txt,sha256=4UTGY81GVApDEB2E0QKXiQuyEUVTWXJvrR46vpD9oNM,12104
-diveplane_reactor_api-5.0.18.dist-info/METADATA,sha256=9XbSptNS4gGrHQGfYH18ENiF3Dygj1uP4EJiTgP2jbI,3192
-diveplane_reactor_api-5.0.18.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-diveplane_reactor_api-5.0.18.dist-info/entry_points.txt,sha256=qY25dCLu2kebBfJ5CmFkRoaNlc_nEYI3LW_D2RgXPM0,157
-diveplane_reactor_api-5.0.18.dist-info/top_level.txt,sha256=tZqkTmi8MrypFXe_oDtXVHM7ePHFgma5zJaXO5OpwfA,10
-diveplane_reactor_api-5.0.18.dist-info/RECORD,,
+diveplane_reactor_api-6.5.8.dist-info/LICENSE.txt,sha256=4UTGY81GVApDEB2E0QKXiQuyEUVTWXJvrR46vpD9oNM,12104
+diveplane_reactor_api-6.5.8.dist-info/METADATA,sha256=KgbyDWnKh89tjkID6UYLkc_sqYFPZhWU9MSIIS3DKyU,3192
+diveplane_reactor_api-6.5.8.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+diveplane_reactor_api-6.5.8.dist-info/entry_points.txt,sha256=qY25dCLu2kebBfJ5CmFkRoaNlc_nEYI3LW_D2RgXPM0,157
+diveplane_reactor_api-6.5.8.dist-info/top_level.txt,sha256=tZqkTmi8MrypFXe_oDtXVHM7ePHFgma5zJaXO5OpwfA,10
+diveplane_reactor_api-6.5.8.dist-info/RECORD,,
```

