# Comparing `tmp/mfhpo_simulator-1.2.6-py3-none-any.whl.zip` & `tmp/mfhpo_simulator-1.2.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,21 +1,21 @@
-Zip file size: 35324 bytes, number of entries: 19
--rw-rw-r--  2.0 unx      560 b- defN 23-Jul-25 08:31 benchmark_simulator/__init__.py
--rw-rw-r--  2.0 unx     7761 b- defN 23-Jul-19 11:36 benchmark_simulator/_constants.py
--rw-rw-r--  2.0 unx    12622 b- defN 23-Jul-19 07:25 benchmark_simulator/_secure_proc.py
+Zip file size: 37256 bytes, number of entries: 19
+-rw-rw-r--  2.0 unx      560 b- defN 23-Aug-01 14:25 benchmark_simulator/__init__.py
+-rw-rw-r--  2.0 unx     8275 b- defN 23-Aug-01 13:19 benchmark_simulator/_constants.py
+-rw-rw-r--  2.0 unx    16238 b- defN 23-Aug-01 13:41 benchmark_simulator/_secure_proc.py
 -rw-rw-r--  2.0 unx     5116 b- defN 23-Jul-06 23:36 benchmark_simulator/_utils.py
--rw-rw-r--  2.0 unx    28910 b- defN 23-Jul-19 11:36 benchmark_simulator/simulator.py
+-rw-rw-r--  2.0 unx    30591 b- defN 23-Aug-01 13:19 benchmark_simulator/simulator.py
 -rw-rw-r--  2.0 unx     2582 b- defN 23-Jul-06 18:41 benchmark_simulator/_simulator/_base_wrapper.py
--rw-rw-r--  2.0 unx     3335 b- defN 23-Jul-19 11:36 benchmark_simulator/_simulator/_utils.py
--rw-rw-r--  2.0 unx    13196 b- defN 23-Jul-19 11:36 benchmark_simulator/_simulator/_worker.py
+-rw-rw-r--  2.0 unx     3362 b- defN 23-Aug-01 13:19 benchmark_simulator/_simulator/_utils.py
+-rw-rw-r--  2.0 unx    15295 b- defN 23-Aug-01 13:36 benchmark_simulator/_simulator/_worker.py
 -rw-rw-r--  2.0 unx     3767 b- defN 23-Jul-18 21:06 benchmark_simulator/_simulator/_worker_manager.py
--rw-rw-r--  2.0 unx     9134 b- defN 23-Jul-18 21:22 benchmark_simulator/_simulator/_worker_manager_for_ask_and_tell.py
+-rw-rw-r--  2.0 unx    10175 b- defN 23-Aug-01 13:19 benchmark_simulator/_simulator/_worker_manager_for_ask_and_tell.py
 -rw-rw-r--  2.0 unx     2181 b- defN 23-Jul-06 18:55 benchmark_simulator/_trackers/_config_tracker.py
 -rw-rw-r--  2.0 unx     5012 b- defN 23-Jul-06 23:14 benchmark_simulator/_trackers/_state_tracker.py
 -rw-rw-r--  2.0 unx      284 b- defN 23-Jul-10 08:56 benchmark_simulator/utils/__init__.py
--rw-rw-r--  2.0 unx     8990 b- defN 23-Jul-11 10:13 benchmark_simulator/utils/_performance_over_time.py
--rw-rw-r--  2.0 unx    10760 b- defN 23-Jul-25 08:32 mfhpo_simulator-1.2.6.dist-info/LICENSE
--rw-rw-r--  2.0 unx      307 b- defN 23-Jul-25 08:32 mfhpo_simulator-1.2.6.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jul-25 08:32 mfhpo_simulator-1.2.6.dist-info/WHEEL
--rw-rw-r--  2.0 unx      107 b- defN 23-Jul-25 08:32 mfhpo_simulator-1.2.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1812 b- defN 23-Jul-25 08:32 mfhpo_simulator-1.2.6.dist-info/RECORD
-19 files, 116528 bytes uncompressed, 32288 bytes compressed:  72.3%
+-rw-rw-r--  2.0 unx     9963 b- defN 23-Jul-25 19:56 benchmark_simulator/utils/_performance_over_time.py
+-rw-rw-r--  2.0 unx    10760 b- defN 23-Aug-01 14:26 mfhpo_simulator-1.2.7.dist-info/LICENSE
+-rw-rw-r--  2.0 unx      307 b- defN 23-Aug-01 14:26 mfhpo_simulator-1.2.7.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Aug-01 14:26 mfhpo_simulator-1.2.7.dist-info/WHEEL
+-rw-rw-r--  2.0 unx      107 b- defN 23-Aug-01 14:26 mfhpo_simulator-1.2.7.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     1813 b- defN 23-Aug-01 14:26 mfhpo_simulator-1.2.7.dist-info/RECORD
+19 files, 126480 bytes uncompressed, 34220 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -36,23 +36,23 @@
 
 Filename: benchmark_simulator/utils/__init__.py
 Comment: 
 
 Filename: benchmark_simulator/utils/_performance_over_time.py
 Comment: 
 
-Filename: mfhpo_simulator-1.2.6.dist-info/LICENSE
+Filename: mfhpo_simulator-1.2.7.dist-info/LICENSE
 Comment: 
 
-Filename: mfhpo_simulator-1.2.6.dist-info/METADATA
+Filename: mfhpo_simulator-1.2.7.dist-info/METADATA
 Comment: 
 
-Filename: mfhpo_simulator-1.2.6.dist-info/WHEEL
+Filename: mfhpo_simulator-1.2.7.dist-info/WHEEL
 Comment: 
 
-Filename: mfhpo_simulator-1.2.6.dist-info/top_level.txt
+Filename: mfhpo_simulator-1.2.7.dist-info/top_level.txt
 Comment: 
 
-Filename: mfhpo_simulator-1.2.6.dist-info/RECORD
+Filename: mfhpo_simulator-1.2.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## benchmark_simulator/__init__.py

```diff
@@ -1,12 +1,12 @@
 from benchmark_simulator._constants import AbstractAskTellOptimizer, ObjectiveFuncType
 from benchmark_simulator.simulator import ObjectiveFuncWrapper, get_multiple_wrappers
 
 
-__version__ = "1.2.6"
+__version__ = "1.2.7"
 __copyright__ = "Copyright (C) 2023 Shuhei Watanabe"
 __licence__ = "Apache-2.0 License"
 __author__ = "Shuhei Watanabe"
 __author_email__ = "shuhei.watanabe.utokyo@gmail.com"
 __url__ = "https://github.com/nabenabe0928/mfhpo-simulator"
```

## benchmark_simulator/_constants.py

```diff
@@ -7,20 +7,22 @@
 from typing import Any, Final, Protocol
 
 import numpy as np
 
 
 DIR_NAME: Final[str] = "mfhpo-simulator-info/"
 INF: Final[float] = float(1 << 30)
+NEGLIGIBLE_SEC: Final[float] = 1e-12
 
 
 @dataclass(frozen=True)
 class _SampledTimeDictType:
     before_sample: float
     after_sample: float
+    worker_index: int
 
 
 @dataclass(frozen=True)
 class _StateType:
     runtime: float = 0.0
     cumtime: float = 0.0
     fidel: int = 0
@@ -32,14 +34,15 @@
     proc_alloc: str
     result: str
     state_cache: str
     worker_cumtime: str
     timestamp: str
     sampled_time: str
     config_tracker: str
+    sample_waiting: str
 
     def __len__(self) -> int:
         return len(self.__dict__)
 
 
 @dataclass(frozen=True)
 class _ResultData:
@@ -56,14 +59,15 @@
     proc_alloc: str = "proc_alloc.json"
     result: str = "results.json"
     state_cache: str = "state_cache.json"
     worker_cumtime: str = "simulated_cumtime.json"
     timestamp: str = "timestamp.json"
     sampled_time: str = "sampled_time.json"
     config_tracker: str = "config_tracker.json"
+    sample_waiting: str = "sample_waiting.json"
 
 
 @dataclass(frozen=True)
 class _TimeValue:
     terminated: float = float(1 << 40)
     crashed: float = float(1 << 41)
 
@@ -82,25 +86,32 @@
     seed: int | None
     continual_max_fidel: int | None
     max_waiting_time: float
     check_interval_time: float
     store_config: bool
     allow_parallel_sampling: bool
     config_tracking: bool
+    expensive_sampler: bool
 
     def validate(self) -> None:
         if self.n_actual_evals_in_opt < self.n_workers + self.n_evals:
             threshold = self.n_workers + self.n_evals
             # In fact, n_workers + n_evals - 1 is the real minimum threshold.
             raise ValueError(
                 "Cannot guarantee that optimziers will not hang. "
                 f"Use n_actual_evals_in_opt >= {threshold} (= n_evals + n_workers) at least. "
                 "Note that our package cannot change your optimizer setting, so "
                 "make sure that you changed your optimizer setting, but not only `n_actual_evals_in_opt`."
             )
+        if self.allow_parallel_sampling and self.expensive_sampler:
+            raise ValueError(
+                "expensive_sampler and allow_parallel_sampling cannot be True simultaneously.\n"
+                "Note that allow_parallel_sampling=True correctly handles expensive samplers"
+                " if sampling happens in parallel."
+            )
 
 
 @dataclass(frozen=True)
 class _WorkerVars:
     continual_eval: bool
     worker_id: str
     worker_index: int
```

## benchmark_simulator/_secure_proc.py

```diff
@@ -96,14 +96,23 @@
     with lock.edit(path) as f:
         existing_configs = json.load(f)
         existing_configs[config_id_str] = config
         f.seek(0)
         json.dump(existing_configs, f, indent=4)
 
 
+def _record_sample_waiting(path: str, worker_id: str, sample_start: float, lock: _SecureLock) -> None:
+    with lock.edit(path) as f:
+        record = json.load(f)
+        # Initially, every worker waits for a sample.
+        record[worker_id] = sample_start
+        f.seek(0)
+        json.dump(record, f, indent=4)
+
+
 def _cache_state(
     path: str, config_hash: int, new_state: _StateType, lock: _SecureLock, update_index: int | None = None
 ) -> None:
     with lock.edit(path) as f:
         config_hash_str = str(config_hash)
         cache = json.load(f)
         _new_state = [new_state.runtime, new_state.cumtime, new_state.fidel, new_state.seed]
@@ -144,14 +153,24 @@
     if len(data) == 0:
         # It ensures nothing will change in the main proc.
         return dict(before_sample=np.array([-np.inf]), after_sample=np.array([-np.inf]))
     else:
         return data
 
 
+def _fetch_sample_waiting(path: str | None, lock: _SecureLock) -> dict[str, float] | None:
+    if path is None:
+        return None
+
+    with lock.read(path) as f:
+        sample_waiting = json.load(f)
+
+    return sample_waiting
+
+
 def _fetch_cumtimes(path: str, lock: _SecureLock) -> dict[str, float]:
     with lock.read(path) as f:
         cumtimes = json.load(f)
 
     return cumtimes
 
 
@@ -234,14 +253,18 @@
     _record_timestamp(path=path, worker_id=worker_id, prev_timestamp=time.time(), lock=lock)
 
 
 def _start_worker_timer(path: str, worker_id: str, lock: _SecureLock) -> None:
     _record_cumtime(path=path, worker_id=worker_id, cumtime=0.0, lock=lock)
 
 
+def _start_sample_waiting(path: str, worker_id: str, lock: _SecureLock) -> None:
+    _record_sample_waiting(path=path, worker_id=worker_id, sample_start=time.time(), lock=lock)
+
+
 def _finish_worker_timer(path: str, worker_id: str, lock: _SecureLock) -> None:
     _record_cumtime(path=path, worker_id=worker_id, cumtime=_TIME_VALUES.terminated, lock=lock)
 
 
 def _kill_worker_timer(path: str, worker_id: str, lock: _SecureLock) -> None:
     _record_cumtime(path=path, worker_id=worker_id, cumtime=_TIME_VALUES.crashed, lock=lock)
 
@@ -312,32 +335,101 @@
     # The worker with minimum cumlative time may be able to evaluate several HPs during the wait,
     # but it does not matter because the timeout happens due to too long wait.
     time.sleep(1.0)
     _kill_worker_timer_with_min_cumtime(path=path, lock=lock)
     _raise_unexpected_timeout_error(max_waiting_time=max_waiting_time)
 
 
+def _update_min_cumtimes(
+    old_min_cumtime_waiting: float,
+    sampling_duration: float,
+    new_cumtimes: dict[str, float],
+    new_sample_waiting: dict[str, float] | None,
+) -> tuple[float, float]:
+    min_cumtime_confirmed = min(ct for ct in new_cumtimes.values())
+    if new_sample_waiting is None:
+        return min_cumtime_confirmed, -_TIME_VALUES.crashed
+
+    min_cumtime_waiting = min(
+        (new_cumtimes[wid] for wid in new_cumtimes if new_sample_waiting[wid] > 0.0),
+        default=min_cumtime_confirmed,
+    )
+    return min_cumtime_confirmed, max(old_min_cumtime_waiting + sampling_duration, min_cumtime_waiting)
+
+
+def _check_long_waiting(
+    path: str,
+    worker_id: str,
+    lock: _SecureLock,
+    curtime: float,
+    start: float,
+    warning_interval: int,
+    max_waiting_time: float,
+) -> None:
+    if int(curtime - start + 1) % warning_interval == 0:
+        msg = (
+            "Workers might be hanging. Please consider setting `max_waiting_time` (< np.inf).\n"
+            "Note that if samplers or the objective function need long time (> 10 seconds), or "
+            "n_workers is large, please ignore this warning."
+        )
+        warnings.warn(msg)
+
+    if curtime - start > max_waiting_time:
+        _terminate_with_unexpected_timeout(path=path, worker_id=worker_id, max_waiting_time=max_waiting_time, lock=lock)
+
+
+def _get_initial_min_cumtimes(
+    cumtimes: dict[str, float], sample_waiting: dict[str, float] | None, start: float
+) -> tuple[float, float]:
+    min_cumtime_confirmed = min(ct for ct in cumtimes.values())
+    min_cumtime_waiting = (
+        min(
+            (cumtimes[wid] + start - sample_waiting[wid] for wid in cumtimes if sample_waiting[wid] > 0.0),
+            default=min_cumtime_confirmed,
+        )
+        if sample_waiting is not None
+        else -_TIME_VALUES.crashed
+    )
+    return min_cumtime_confirmed, min_cumtime_waiting
+
+
 def _wait_until_next(
     path: str,
     worker_id: str,
     lock: _SecureLock,
     waiting_time: float,
     warning_interval: int = 10,
     max_waiting_time: float = np.inf,
+    sample_waiting_path: str | None = None,
 ) -> None:
-    start = time.time()
-    waiting_time *= 1 + np.random.random()
-    while not _is_min_cumtime(path, worker_id=worker_id, lock=lock):
+    long_time_kwargs = dict(
+        path=path, worker_id=worker_id, lock=lock, warning_interval=warning_interval, max_waiting_time=max_waiting_time
+    )
+    cumtimes, start, cur_sampling_duration = _fetch_cumtimes(path, lock=lock), time.time(), 0.0
+    proc_cumtime, sample_start, waiting_time = cumtimes[worker_id], start, waiting_time * (1 + np.random.random())
+    sample_waiting = _fetch_sample_waiting(sample_waiting_path, lock=lock)
+    min_cumtime_confirmed = min(ct for ct in cumtimes.values())
+    min_cumtime_confirmed, min_cumtime_waiting = _get_initial_min_cumtimes(
+        cumtimes=cumtimes, sample_waiting=sample_waiting, start=start
+    )
+
+    while min_cumtime_confirmed != proc_cumtime:
+        if min_cumtime_waiting + cur_sampling_duration >= proc_cumtime:
+            break
+
         time.sleep(waiting_time)
         curtime = time.time()
-        if int(curtime - start + 1) % warning_interval == 0:
-            msg = (
-                "Workers might be hanging. Please consider setting `max_waiting_time` (< np.inf).\n"
-                "Note that if samplers or the objective function need long time (> 10 seconds), or "
-                "n_workers is large, please ignore this warning."
-            )
-            warnings.warn(msg)
+        _check_long_waiting(curtime=curtime, start=start, **long_time_kwargs)  # type: ignore[arg-type]
 
-        if curtime - start > max_waiting_time:
-            _terminate_with_unexpected_timeout(
-                path=path, worker_id=worker_id, max_waiting_time=max_waiting_time, lock=lock
+        new_cumtimes = _fetch_cumtimes(path, lock=lock)
+        new_sample_waiting = _fetch_sample_waiting(sample_waiting_path, lock=lock)
+        if new_cumtimes == cumtimes and sample_waiting == new_sample_waiting:
+            cur_sampling_duration = 0.0 if sample_waiting_path is None else curtime - sample_start
+        else:
+            min_cumtime_confirmed, min_cumtime_waiting = _update_min_cumtimes(
+                old_min_cumtime_waiting=min_cumtime_waiting,
+                new_cumtimes=new_cumtimes,
+                sampling_duration=cur_sampling_duration,
+                new_sample_waiting=new_sample_waiting,
             )
+            cumtimes, sample_waiting = new_cumtimes, new_sample_waiting
+            cur_sampling_duration, sample_start = 0.0, curtime
```

## benchmark_simulator/simulator.py

```diff
@@ -32,15 +32,15 @@
 
 2. mfhpo-simulator-info/*/results.json
     * obj1_name -- list[obj1 at the n-th evaluation]
     * obj2_name -- list[obj2 at the n-th evaluation]
     :
     * objM_name -- list[objM at the n-th evaluation]
     * cumtime -- list[cumtime up to the n-th evaluation]
-    * index -- list[the index of the worker of the n-th evaluation]
+    * worker_index -- list[the index of the worker of the n-th evaluation]
 This file is necessary for post-hoc analysis.
 
 3. mfhpo-simulator-info/*/state_cache.json
     * config1 -- list[tuple[runtime, cumtime, fidel, seed]]
     * config2 -- list[tuple[runtime, cumtime, fidel, seed]]
     :
     * configN -- list[tuple[runtime, cumtime, fidel, seed]]
@@ -64,16 +64,24 @@
     :
     * workerN_id -- prev_timestampN
 This file tells the last checkpoint timestamp of each worker (prev_timestamp).
 
 6. mfhpo-simulator-info/*/sampled_time.json
     * before_sample -- [time1 before sample, time2 before sample, ...]
     * after_sample -- [time1 after sample, time2 after sample, ...]
+    * worker_index -- [worker_index of the 1st eval, worker_index of the 2nd eval, ...]
 This file is used to consider the sampling time.
 after_sample is the latest cumtime immediately after the last sample and before_sample is before the last sample.
+
+7. mfhpo-simulator-info/*/sample_waiting.json
+    * worker1_id -- timestamp when the latest sample started (use 0.0 when it is not waiting for sampling)
+    * worker2_id -- timestamp when the latest sample started (use 0.0 when it is not waiting for sampling)
+    :
+    * workerN_id -- timestamp when the latest sample started (use 0.0 when it is not waiting for sampling)
+This file is used only if expensive_sampler=True.
 """
 from __future__ import annotations
 
 import json
 import os
 from datetime import datetime
 from typing import Any
@@ -99,14 +107,15 @@
     continual_max_fidel: int | None = None,
     max_waiting_time: float = np.inf,
     check_interval_time: float = 1e-4,
     store_config: bool = False,
     allow_parallel_sampling: bool = False,
     config_tracking: bool = True,
     max_total_eval_time: float = np.inf,
+    expensive_sampler: bool = False,
 ) -> list[ObjectiveFuncWrapper]:
     """Return multiple wrapper instances.
 
     Args:
         obj_func (ObjectiveFuncType):
             A callable object that serves as the objective function.
             Args:
@@ -172,14 +181,20 @@
             It slows the simulation down when n_evals is large (> 3000),
             but it is recommended to avoid unexpected bugs that could happen.
         max_total_eval_time (float):
             The maximum total evaluation time for the optimization.
             For example, if max_total_eval_time=3600, the simulation evaluates until the simulated cumulative time
             reaches 3600 seconds.
             It is useful to combine with a large n_evals and n_actual_evals_in_opt.
+        expensive_sampler (bool):
+            Whether the optimizer used by users is expensive or not for a function evaluation.
+            For example, if a function evaluation costs 1 hour and a sample takes several minutes,
+            we consider it expensive.
+            This argument may matter slightly for expensive samplers, but in most cases, this argument does not matter.
+            When using expensive_sampler=True, this may slightly slow down a simulation.
 
     Returns:
         wrappers (list[ObjectiveFuncWrapper]):
             A list of wrappers.
             It contains n_workers of worker wrappers.
     """
     curtime = datetime.now().strftime("%Y-%m-%d-%H:%M:%S.%f")
@@ -203,14 +218,15 @@
         continual_max_fidel=continual_max_fidel,
         max_waiting_time=max_waiting_time,
         check_interval_time=check_interval_time,
         store_config=store_config,
         allow_parallel_sampling=allow_parallel_sampling,
         config_tracking=config_tracking,
         max_total_eval_time=max_total_eval_time,
+        expensive_sampler=expensive_sampler,
         _async_instantiations=False,
     )
     return [ObjectiveFuncWrapper(**wrapper_kwargs, worker_index=i) for i in range(n_workers)]  # type: ignore[arg-type]
 
 
 class ObjectiveFuncWrapper:
     """Objective function wrapper API for users.
@@ -267,14 +283,15 @@
         max_waiting_time: float = np.inf,
         check_interval_time: float = 1e-4,
         store_config: bool = False,
         allow_parallel_sampling: bool = False,
         config_tracking: bool = True,
         worker_index: int | None = None,
         max_total_eval_time: float = np.inf,
+        expensive_sampler: bool = False,
         careful_init: bool = False,
         _async_instantiations: bool = True,
     ):
         """The initialization of a wrapper class.
 
         Both ObjectiveFuncWorker and CentralWorkerManager have the same interface and the same set of control params.
 
@@ -364,14 +381,21 @@
                 probably users would like to use this option.
                 The worker indices must be unique across the parallel workers and must be in [0, n_workers - 1].
             max_total_eval_time (float):
                 The maximum total evaluation time for the optimization.
                 For example, if max_total_eval_time=3600, the simulation evaluates until the simulated cumulative time
                 reaches 3600 seconds.
                 It is useful to combine with a large n_evals and n_actual_evals_in_opt.
+            expensive_sampler (bool):
+                Whether the optimizer used by users is expensive or not for a function evaluation.
+                For example, if a function evaluation costs 1 hour and a sample takes several minutes,
+                we consider it expensive.
+                This argument may matter slightly for expensive samplers, but in most cases,
+                this argument does not matter.
+                When using expensive_sampler=True, this may slightly slow down a simulation.
             careful_init (bool):
                 Whether doing initialization very carefully or not in the default setup (and only for the default).
                 If True, we try to match the initialization order using sleep.
                 It is not necessary for normal usage, but if users expect perfect reproducibility, users want to use it.
             _async_instantiations (bool):
                 Whether each worker is instantiated asynchrously.
                 In other words, whether to wait all workers' instantiations or not.
@@ -391,14 +415,15 @@
             continual_max_fidel=continual_max_fidel,
             max_waiting_time=max_waiting_time,
             check_interval_time=check_interval_time,
             store_config=store_config,
             allow_parallel_sampling=allow_parallel_sampling,
             config_tracking=config_tracking,
             max_total_eval_time=max_total_eval_time,
+            expensive_sampler=expensive_sampler,
         )
 
         self._main_wrapper: _AskTellWorkerManager | _CentralWorkerManager | _ObjectiveFuncWorker
         self._validate(
             save_dir_name=save_dir_name,
             ask_and_tell=ask_and_tell,
             launch_multiple_wrappers_from_user_side=launch_multiple_wrappers_from_user_side,
```

## benchmark_simulator/_simulator/_utils.py

```diff
@@ -3,15 +3,15 @@
 from benchmark_simulator._constants import AbstractAskTellOptimizer
 
 
 def _raise_optimizer_init_error() -> None:
     msg = [
         "The initialization of the optimizer must be cheaper than one objective evuation.",
         "In principle, n_workers is too large for the objective to simulate correctly."
-        "Please set a smaller n_workers or use a cheaper initialization.",
+        "Please set expensive_sampler=True or a smaller n_workers, or use a cheaper initialization.",
     ]
     raise TimeoutError("\n".join(msg))
 
 
 def _validate_opt_class(opt: AbstractAskTellOptimizer) -> None:
     if not hasattr(opt, "ask") or not hasattr(opt, "tell"):
         example_url = "https://github.com/nabenabe0928/mfhpo-simulator/blob/main/examples/ask_and_tell/"
```

## benchmark_simulator/_simulator/_worker.py

```diff
@@ -2,30 +2,33 @@
 
 import os
 import time
 from typing import Any
 
 from benchmark_simulator._constants import (
     INF,
+    NEGLIGIBLE_SEC,
     _SampledTimeDictType,
     _TIME_VALUES,
     _WorkerVars,
     _WrapperVars,
 )
 from benchmark_simulator._secure_proc import (
     _fetch_cumtimes,
     _fetch_sampled_time,
     _fetch_timestamps,
     _finish_worker_timer,
     _init_simulator,
     _is_simulator_terminated,
     _record_cumtime,
     _record_result,
+    _record_sample_waiting,
     _record_sampled_time,
     _record_timestamp,
+    _start_sample_waiting,
     _start_timestamp,
     _start_worker_timer,
     _wait_all_workers,
     _wait_until_next,
 )
 from benchmark_simulator._simulator._base_wrapper import _BaseWrapperInterface
 from benchmark_simulator._simulator._utils import (
@@ -72,14 +75,16 @@
         self._state_tracker = _StateTracker(
             path=self._paths.state_cache, lock=self._lock, continual_max_fidel=self._wrapper_vars.continual_max_fidel
         )
         self._config_tracker = _ConfigIDTracker(path=self._paths.config_tracker, lock=self._lock)
         _init_simulator(dir_name=self.dir_name, worker_index=self._temp_worker_index)
         self._wait_till_init_simulator_finish()
         _start_worker_timer(path=self._paths.worker_cumtime, worker_id=worker_id, lock=self._lock)
+        if self._wrapper_vars.expensive_sampler:
+            _start_sample_waiting(path=self._paths.sample_waiting, worker_id=worker_id, lock=self._lock)
 
     def _wait_till_init_simulator_finish(self) -> None:
         n_files = len(self._paths)
         n_repeats = 10000
         n_workers = self._wrapper_vars.n_workers
         while len(os.listdir(self.dir_name)) < n_files:  # pragma: no cover
             # In fact, below is covered by test, but not detected by pytest-cov
@@ -121,54 +126,94 @@
         )
         self._init_timestamp()
 
         # These variables change over time and must be either loaded from file system or updated.
         self._cumtime = 0.0
         self._terminated = False
         self._crashed = False
-        self._used_config: dict[str, Any] = {}
+        self._data_to_store: dict[str, Any] = {}
 
-    def _validate(self) -> None:
+    def _validate_call(self, fidels: dict[str, int | float] | None) -> None:
         if self._crashed:
             raise InterruptedError(
                 "The simulation is interrupted due to deadlock or the dead of at least one of the workers.\n"
                 "This error could be avoided by increasing `max_waiting_time` (however, np.inf is discouraged).\n"
             )
+        _validate_fidels(
+            fidels=fidels,
+            fidel_keys=self._fidel_keys,
+            use_fidel=self._worker_vars.use_fidel,
+            continual_eval=self._worker_vars.continual_eval,
+        )
 
-    def _wait_other_workers(self) -> None:
+    def _wait_until_next(self) -> None:
         """
         Wait until the worker's cumulative runtime becomes the smallest.
         The smallest cumulative runtime implies that the order in the record will not disturbed
         even if the worker reports its results now.
+        Note that `expensive_sampler=True` considers sampling waiting time as well.
         """
         _wait_until_next(
             path=self._paths.worker_cumtime,
             worker_id=self._worker_vars.worker_id,
             max_waiting_time=self._wrapper_vars.max_waiting_time,
             waiting_time=self._wrapper_vars.check_interval_time,
             lock=self._lock,
+            sample_waiting_path=self._paths.sample_waiting if self._wrapper_vars.expensive_sampler else None,
         )
+
+    def _record_timestamp(self) -> None:
         _record_timestamp(
             path=self._paths.timestamp,
             worker_id=self._worker_vars.worker_id,
             prev_timestamp=time.time(),
             lock=self._lock,
         )
 
+    def _record_cumtime(self) -> None:
+        _record_cumtime(
+            path=self._paths.worker_cumtime,
+            worker_id=self._worker_vars.worker_id,
+            cumtime=self._cumtime,
+            lock=self._lock,
+        )
+
+    def _record_sample_waiting(self, sample_start: float) -> None:
+        if self._wrapper_vars.expensive_sampler:
+            _record_sample_waiting(
+                path=self._paths.sample_waiting,
+                worker_id=self._worker_vars.worker_id,
+                sample_start=sample_start,
+                lock=self._lock,
+            )
+
+    def _record_result(self) -> None:
+        fixed = bool(not self._wrapper_vars.store_config)
+        _record_result(self._paths.result, results=self._data_to_store, fixed=fixed, lock=self._lock)
+        self._data_to_store = {}  # Make it empty
+
+    def _is_simulator_terminated(self) -> bool:
+        return _is_simulator_terminated(
+            self._paths.result,
+            max_evals=self._wrapper_vars.n_evals,
+            max_total_eval_time=self._wrapper_vars.max_total_eval_time,
+            lock=self._lock,
+        )
+
     def _query_obj_func(
         self,
         eval_config: dict[str, Any],
         fidels: dict[str, int | float] | None,
         seed: int | None,
         prev_fidel: int | None = None,
         **data_to_scatter: Any,
     ) -> dict[str, float]:
         if self._wrapper_vars.store_config:
-            self._used_config = eval_config.copy()
-            self._used_config.update(
+            self._data_to_store.update(
+                eval_config,
                 **(fidels if fidels is not None else {}),
                 **({"prev_fidel": prev_fidel} if prev_fidel is not None else {}),
                 seed=seed,
             )
 
         return self._wrapper_vars.obj_func(eval_config=eval_config, fidels=fidels, seed=seed, **data_to_scatter)
 
@@ -222,92 +267,92 @@
 
         # Otherwise, we try the continual evaluation
         fidel = _validate_fidels_continual(fidels)
         return self._proc_output_from_existing_state(
             eval_config=eval_config, fidel=fidel, config_id=config_id, **data_to_scatter
         )
 
-    def _post_proc(self, results: dict[str, float]) -> None:
+    def _post_proc(self) -> None:
+        # Not waiting for sample now.
+        # NOTE: must be _record_sample_waiting --> _record_cumtime due to the update in the other workers.
+        # More specifically, if large cumtime is taken as min_cumtime before the record happens, the run will fail.
+        self._record_sample_waiting(sample_start=-1)
         # First, record the simulated cumulative runtime after calling the objective
-        _record_cumtime(
-            path=self._paths.worker_cumtime,
-            worker_id=self._worker_vars.worker_id,
-            cumtime=self._cumtime,
-            lock=self._lock,
-        )
-        # Wait till the cumulative runtime becomes the smallest
-        self._wait_other_workers()
-
-        row = dict(
-            cumtime=self._cumtime,
-            worker_index=self._worker_vars.worker_index,
-            **{k: results[k] for k in self._obj_keys},
-            **self._used_config,
-        )
+        self._record_cumtime()
+        # Wait till the cumulative runtime (+ sampling waiting time) becomes the smallest
+        self._wait_until_next()
+        # Start to wait for another sample.
+        self._record_sample_waiting(sample_start=time.time())
         # Record the results to the main database when the cumulative runtime is the smallest
-        _record_result(
-            self._paths.result, results=row, fixed=bool(not self._wrapper_vars.store_config), lock=self._lock
-        )
-        self._used_config = {}  # Make it empty
-        if _is_simulator_terminated(
-            self._paths.result,
-            max_evals=self._wrapper_vars.n_evals,
-            max_total_eval_time=self._wrapper_vars.max_total_eval_time,
-            lock=self._lock,
-        ):
+        self._record_result()
+        # Record the timestamp when this worker is freed up
+        self._record_timestamp()
+
+        if self._is_simulator_terminated():
             self._finish()
 
+    def _determine_cumtime(self, sampling_time: float) -> None:
+        cumtimes = _fetch_cumtimes(self._paths.worker_cumtime, lock=self._lock)
+        cumtime = cumtimes[self._worker_vars.worker_id]
+
+        if self._wrapper_vars.expensive_sampler:
+            # In this case, sampling time already includes the idling time for the other workers.
+            self._cumtime = cumtime + sampling_time
+        else:
+            sampled_time = _fetch_sampled_time(path=self._paths.sampled_time, lock=self._lock)
+            # Consider the sampling time overlap
+            is_init_sample = bool(cumtime < NEGLIGIBLE_SEC)
+            self._cumtime = (
+                max(cumtime, np.max(sampled_time["after_sample"][sampled_time["before_sample"] <= cumtime]))
+                if not self._wrapper_vars.allow_parallel_sampling and not is_init_sample
+                else cumtime
+            ) + sampling_time
+            if is_init_sample and any(NEGLIGIBLE_SEC < ct < self._cumtime for ct in cumtimes.values()):
+                _raise_optimizer_init_error()
+
     def _load_timestamps(self) -> None:
         worker_id = self._worker_vars.worker_id
         sampling_time = max(0.0, time.time() - _fetch_timestamps(self._paths.timestamp, lock=self._lock)[worker_id])
-        sampled_time = _fetch_sampled_time(path=self._paths.sampled_time, lock=self._lock)
-        cumtimes = _fetch_cumtimes(self._paths.worker_cumtime, lock=self._lock)
-        cumtime = cumtimes[worker_id]
-        is_init_sample = bool(cumtime < 1e-12)
-        # Consider the sampling time overlap
-        self._cumtime = (
-            max(cumtime, np.max(sampled_time["after_sample"][sampled_time["before_sample"] <= cumtime]))
-            if not self._wrapper_vars.allow_parallel_sampling and not is_init_sample
-            else cumtime
-        ) + sampling_time
-        if cumtime < 1e-12 and any(1e-12 < ct < self._cumtime for ct in cumtimes.values()):
-            _raise_optimizer_init_error()
 
-        new_sampled_time = _SampledTimeDictType(before_sample=self._cumtime - sampling_time, after_sample=self._cumtime)
+        self._determine_cumtime(sampling_time=sampling_time)
+        new_sampled_time = _SampledTimeDictType(
+            before_sample=self._cumtime - sampling_time,
+            after_sample=self._cumtime,
+            worker_index=self._worker_vars.worker_index,
+        )
         _record_sampled_time(path=self._paths.sampled_time, sampled_time=new_sampled_time, lock=self._lock)
 
         self._terminated = self._cumtime >= min(self._wrapper_vars.max_total_eval_time, _TIME_VALUES.terminated - 1e-5)
         self._crashed = self._cumtime >= _TIME_VALUES.crashed - 1e-5
 
     def __call__(
         self,
         eval_config: dict[str, Any],
         *,
         fidels: dict[str, int | float] | None = None,
         config_id: int | None = None,
         **data_to_scatter: Any,
     ) -> dict[str, float]:
         self._load_timestamps()
-        self._validate()
-        _validate_fidels(
-            fidels=fidels,
-            fidel_keys=self._fidel_keys,
-            use_fidel=self._worker_vars.use_fidel,
-            continual_eval=self._worker_vars.continual_eval,
-        )
+        self._validate_call(fidels=fidels)
         config_tracking = config_id is not None and self._wrapper_vars.config_tracking
         if config_tracking:  # validate the config_id to ensure the user implementation is correct
             assert config_id is not None  # mypy redefinition
             self._config_tracker.validate(config=eval_config, config_id=config_id)
 
         if self._terminated:
             return {**{k: INF for k in self._obj_keys}, self.runtime_key: INF}
 
         results = self._proc_output(eval_config=eval_config, fidels=fidels, config_id=config_id, **data_to_scatter)
-        self._post_proc(results)
+        self._data_to_store.update(
+            cumtime=self._cumtime,
+            worker_index=self._worker_vars.worker_index,
+            **{k: results[k] for k in self._obj_keys},
+        )
+        self._post_proc()
         return results
 
     def _finish(self) -> None:
         """The method to close the worker instance.
         This method must be called before we finish the optimization.
         If not called, optimization modules are likely to hang.
         """
```

## benchmark_simulator/_simulator/_worker_manager_for_ask_and_tell.py

```diff
@@ -1,14 +1,20 @@
 from __future__ import annotations
 
 import os
 import time
 from typing import Any
 
-from benchmark_simulator._constants import AbstractAskTellOptimizer, _ResultData, _StateType, _WorkerVars
+from benchmark_simulator._constants import (
+    AbstractAskTellOptimizer,
+    NEGLIGIBLE_SEC,
+    _ResultData,
+    _StateType,
+    _WorkerVars,
+)
 from benchmark_simulator._simulator._base_wrapper import _BaseWrapperInterface
 from benchmark_simulator._simulator._utils import (
     _raise_optimizer_init_error,
     _validate_fidel_args,
     _validate_fidels,
     _validate_fidels_continual,
     _validate_opt_class,
@@ -37,17 +43,18 @@
         self._state_tracker = _AskTellStateTracker(continual_max_fidel=self._wrapper_vars.continual_max_fidel)
 
         self._wrapper_vars.validate()
         _validate_fidel_args(continual_eval=self._worker_vars.continual_eval, fidel_keys=self._fidel_keys)
 
         self._timenow = 0.0
         self._cumtimes: np.ndarray = np.zeros(self._wrapper_vars.n_workers, dtype=np.float64)
+        self._worker_indices = np.arange(self._wrapper_vars.n_workers)
         self._pending_results: list[_ResultData | None] = [None] * self._wrapper_vars.n_workers
         self._seen_config_keys: list[str] = []
-        self._sampled_time: dict[str, list[float]] = {"before_sample": [], "after_sample": []}
+        self._sampled_time: dict[str, list[float]] = {"before_sample": [], "after_sample": [], "worker_index": []}
         self._results: dict[str, list[Any]] = {"worker_index": [], "cumtime": []}
         self._results.update({k: [] for k in self._obj_keys})
         if self._wrapper_vars.store_config:
             self._results.update({k: [] for k in self._fidel_keys + ["seed"]})
             if self._wrapper_vars.continual_max_fidel is not None:
                 self._results["prev_fidel"] = []
 
@@ -148,64 +155,79 @@
     def _ask_with_timer(
         self,
         opt: AbstractAskTellOptimizer,
         worker_id: int,
     ) -> tuple[dict[str, Any], dict[str, int | float] | None, int | None]:
         start = time.time()
         eval_config, fidels, config_id = opt.ask()
+        sampling_time = time.time() - start
         config_tracking = config_id is not None and self._wrapper_vars.config_tracking
         if config_tracking:  # validate the config_id to ensure the user implementation is correct
             assert config_id is not None  # mypy redefinition
             self._config_tracker.validate(config=eval_config, config_id=config_id)
 
-        sampling_time = time.time() - start
-        is_first_sample = bool(self._cumtimes[worker_id] < 1e-12)
+        is_first_sample = bool(self._cumtimes[worker_id] < NEGLIGIBLE_SEC)
         if self._wrapper_vars.allow_parallel_sampling:
             self._cumtimes[worker_id] = self._cumtimes[worker_id] + sampling_time
         else:
             self._timenow = max(self._timenow, self._cumtimes[worker_id]) + sampling_time
             self._cumtimes[worker_id] = self._timenow
 
+        self._sampled_time["worker_index"].append(worker_id)
         self._sampled_time["before_sample"].append(self._cumtimes[worker_id] - sampling_time)
         self._sampled_time["after_sample"].append(self._cumtimes[worker_id])
 
-        if is_first_sample and not np.isclose(
-            self._cumtimes[worker_id], np.min(self._cumtimes[self._cumtimes > 1e-12])
+        if (
+            not self._wrapper_vars.expensive_sampler
+            and is_first_sample
+            and self._cumtimes[worker_id] != np.min(self._cumtimes[self._cumtimes > NEGLIGIBLE_SEC])
         ):
             _raise_optimizer_init_error()
 
         return eval_config, fidels, config_id
 
     def _tell_pending_result(self, opt: AbstractAskTellOptimizer, worker_id: int) -> None:
-        result_data = self._pending_results[worker_id]
-        if result_data is None:
-            return
-
-        self._record_result_data(result_data=result_data, worker_id=worker_id)
-        opt.tell(
-            eval_config=result_data.eval_config,
-            results=result_data.results,
-            fidels=result_data.fidels,
-            config_id=result_data.config_id,
-        )
-        self._pending_results[worker_id] = None
+        free_worker_idxs = np.array([worker_id], dtype=np.int32)
+        if self._wrapper_vars.expensive_sampler:
+            before_eval = self._sampled_time["after_sample"][-1]
+            free_worker_idxs = np.union1d(self._worker_indices[self._cumtimes <= before_eval], free_worker_idxs)
+
+        for _worker_id in free_worker_idxs.astype(np.int32):
+            result_data = self._pending_results[_worker_id]
+            if result_data is None:
+                continue
+
+            self._record_result_data(result_data=result_data, worker_id=_worker_id)
+            opt.tell(
+                eval_config=result_data.eval_config,
+                results=result_data.results,
+                fidels=result_data.fidels,
+                config_id=result_data.config_id,
+            )
+            self._pending_results[_worker_id] = None
 
     def _save_results(self) -> None:
+        cumtime = np.array(self._results["cumtime"])
+        order = np.argsort(cumtime) if self._wrapper_vars.expensive_sampler else np.arange(cumtime.size)
         with open(self._paths.result, mode="w") as f:
-            json.dump({k: np.asarray(v).tolist() for k, v in self._results.items()}, f, indent=4)
+            json.dump({k: np.asarray(v)[order].tolist() for k, v in self._results.items()}, f, indent=4)
 
         with open(self._paths.sampled_time, mode="w") as f:
             json.dump({k: np.asarray(v).tolist() for k, v in self._sampled_time.items()}, f, indent=4)
 
     def simulate(self, opt: AbstractAskTellOptimizer) -> None:
         _validate_opt_class(opt)
         worker_id = 0
-        for _ in range(self._wrapper_vars.n_evals + self._wrapper_vars.n_workers - 1):
+        for i in range(self._wrapper_vars.n_evals + self._wrapper_vars.n_workers - 1):
             eval_config, fidels, config_id = self._ask_with_timer(opt=opt, worker_id=worker_id)
             self._proc_obj_func(eval_config=eval_config, worker_id=worker_id, fidels=fidels, config_id=config_id)
             worker_id = np.argmin(self._cumtimes)
-            self._tell_pending_result(opt=opt, worker_id=worker_id)
+
+            if i + 1 >= self._wrapper_vars.n_workers:
+                # This `if` is needed for the compatibility with the other modes.
+                # It ensures that all workers filled out first.
+                self._tell_pending_result(opt=opt, worker_id=worker_id)
 
             if self._cumtimes[worker_id] > self._wrapper_vars.max_total_eval_time:  # exceed time limit
                 break
 
         self._save_results()
```

## benchmark_simulator/utils/_performance_over_time.py

```diff
@@ -107,14 +107,30 @@
         # cumtime[i - 1] < time_step <= cumtime[i]
         indices = np.searchsorted(cumtime, time_steps, side="left")
         return_perf_vals.append((2 * minimize - 1) * loss[indices])
 
     return time_steps, np.asarray(return_perf_vals)
 
 
+def _sort_optimizer_overhead(
+    optimizer_overhead: np.ndarray,
+    saved_worker_indices: np.ndarray,
+    correct_worker_indices: np.ndarray,
+) -> np.ndarray:
+    max_worker_index = np.max(correct_worker_indices)
+    sorted_optimizer_overhead = np.zeros_like(correct_worker_indices, dtype=np.float64)
+    indices = np.arange(optimizer_overhead.size)
+    for idx in range(max_worker_index + 1):
+        flag = correct_worker_indices == idx
+        corresponding_indices = indices[saved_worker_indices == idx]
+        sorted_optimizer_overhead[flag] = optimizer_overhead[corresponding_indices][: np.sum(flag)]
+
+    return sorted_optimizer_overhead
+
+
 def get_performance_over_time_from_paths(
     paths: list[str],
     obj_key: str,
     step: int = 100,
     minimize: bool = True,
     log: bool = True,
     consider_optimizer_overhead: bool = True,
@@ -154,26 +170,32 @@
                 The cumulative best performance metric value up to the corresponding time point.
                 The shape is (step, ).
     """
     cumtimes, perf_vals = [], []
     optimizer_overheads: list[np.ndarray] | None = None if consider_optimizer_overhead else []
     for path in paths:
         n_evals = 0
+        correct_worker_indices: np.ndarray
         with open(os.path.join(path, "results.json"), mode="r") as f:
             data = json.load(f)
             cumtimes.append(np.asarray(data["cumtime"]))
             n_evals = cumtimes[-1].size
             perf_vals.append(np.asarray(data[obj_key]))
+            correct_worker_indices = np.asarray(data["worker_index"])
 
         if optimizer_overheads is None:
             continue
 
         with open(os.path.join(path, "sampled_time.json"), mode="r") as f:
             data = json.load(f)
-            optimizer_overhead = np.array(data["after_sample"]) - np.array(data["before_sample"])
+            optimizer_overhead = _sort_optimizer_overhead(
+                optimizer_overhead=np.array(data["after_sample"]) - np.array(data["before_sample"]),
+                correct_worker_indices=correct_worker_indices,
+                saved_worker_indices=np.asarray(data["worker_index"]),
+            )
             optimizer_overheads.append(optimizer_overhead[:n_evals])
 
     time_steps, return_perf_vals = get_performance_over_time(
         cumtimes=cumtimes,
         perf_vals=perf_vals,
         optimizer_overheads=optimizer_overheads,
         minimize=minimize,
```

## Comparing `mfhpo_simulator-1.2.6.dist-info/LICENSE` & `mfhpo_simulator-1.2.7.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mfhpo_simulator-1.2.6.dist-info/RECORD` & `mfhpo_simulator-1.2.7.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-benchmark_simulator/__init__.py,sha256=A80Q8uXYuFqZkRmxJD12ULnPDjkiH3rtAteDZmclVgg,560
-benchmark_simulator/_constants.py,sha256=j23rkYb3d9d2Mrqc8fs_nOk8GYWgC3Mw_JVa0pZxU-c,7761
-benchmark_simulator/_secure_proc.py,sha256=NfvcOGg8-SUW8FNaPTn6ShKayTjOfwTbZOQmCibEToM,12622
+benchmark_simulator/__init__.py,sha256=WotbIOje4UKe4lyspQVwoDTSIi11Z-Lb-RzSHs_BCLs,560
+benchmark_simulator/_constants.py,sha256=3FIaZuUjJjc36Lb6wCnVbMAadhO3vFlkpqg_Au_MK9Q,8275
+benchmark_simulator/_secure_proc.py,sha256=4stXKjvQqaGCr8JTqq8g8SFx6vpssibRpm-xzgMjm5c,16238
 benchmark_simulator/_utils.py,sha256=ZA8l51dZMxpjhMZCUExO-Mg8r5ilJWdvR2P3RFipzk4,5116
-benchmark_simulator/simulator.py,sha256=xdBLgBwmG3ZDNCdyjGZPvO6IJSD4u1Mwo_VqR3UpdKc,28910
+benchmark_simulator/simulator.py,sha256=1DU1tvisQH_gqGwWbT5tF2rxEdHsiyoJG67CD3-CfZw,30591
 benchmark_simulator/_simulator/_base_wrapper.py,sha256=HvK4Iq1vjCuD1HEcphY-hl4Wo_vo1d-N_FA9PFkYukA,2582
-benchmark_simulator/_simulator/_utils.py,sha256=5vqU_iT-2TdTwyAXSscJsqEuSOp-SMKGMk1K_qVmy1g,3335
-benchmark_simulator/_simulator/_worker.py,sha256=y0hOLlAcSlS8yNIF3TUIiYvkbgTygvAV2Xzi0v8il9w,13196
+benchmark_simulator/_simulator/_utils.py,sha256=_E65XZddiFb6sMpZkZCzfOyoZVpADwtDUd_2TQ0Dudg,3362
+benchmark_simulator/_simulator/_worker.py,sha256=kPuVUkyUxJWrU3JszC5h_fiQ9J3tcavKNFpfQw0yOgc,15295
 benchmark_simulator/_simulator/_worker_manager.py,sha256=wwdsMDeuVLjdFKqbZ3sJ7C5Ep8G2fFDT0jyYw-RkGGc,3767
-benchmark_simulator/_simulator/_worker_manager_for_ask_and_tell.py,sha256=UxuQI4--_uExrhweE3i4I5_z-cVz-mPOcJb_1RX4G3c,9134
+benchmark_simulator/_simulator/_worker_manager_for_ask_and_tell.py,sha256=WWgP7qV_XOKSGgaKBNFeAtM2oPTW__z1qY5x7er88VM,10175
 benchmark_simulator/_trackers/_config_tracker.py,sha256=-tKF_13VtOnAmCm0uzENpUg_ePyd7o_PvG9mkxzXjzQ,2181
 benchmark_simulator/_trackers/_state_tracker.py,sha256=EyeOuoFPf7c8yUgmhrDe54d0wc0Jowxo-6Vs7aUBZ5I,5012
 benchmark_simulator/utils/__init__.py,sha256=PWSU9sL4fLNRLkwm3mElZ7D_wLCWDdyCZa39RqcTYoQ,284
-benchmark_simulator/utils/_performance_over_time.py,sha256=gGUMahKBBZbL0deYYnvF1qzyJ-MH2dRf39IigWGGnek,8990
-mfhpo_simulator-1.2.6.dist-info/LICENSE,sha256=auQsw_aAlfeXmKRQ_tmNBjUD2-wJojtRJPslgGyGqK4,10760
-mfhpo_simulator-1.2.6.dist-info/METADATA,sha256=nJ233eTJiAvdGb1SJnig0nxLV70jlZiGdhqjJNSYt6A,307
-mfhpo_simulator-1.2.6.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-mfhpo_simulator-1.2.6.dist-info/top_level.txt,sha256=uJovuRlKQlk3_JQI_pIvLjq9bJ7D1LdHBYxgRvtIzFk,107
-mfhpo_simulator-1.2.6.dist-info/RECORD,,
+benchmark_simulator/utils/_performance_over_time.py,sha256=FrSdxUuD5R2lBkqxfqlvBySY5tE8WUVPuxmm7kbMLkY,9963
+mfhpo_simulator-1.2.7.dist-info/LICENSE,sha256=auQsw_aAlfeXmKRQ_tmNBjUD2-wJojtRJPslgGyGqK4,10760
+mfhpo_simulator-1.2.7.dist-info/METADATA,sha256=m-jFK3_jwYfCdECCUXF40h-TUaZ8Gk2U6aufLvCMhoQ,307
+mfhpo_simulator-1.2.7.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+mfhpo_simulator-1.2.7.dist-info/top_level.txt,sha256=uJovuRlKQlk3_JQI_pIvLjq9bJ7D1LdHBYxgRvtIzFk,107
+mfhpo_simulator-1.2.7.dist-info/RECORD,,
```

