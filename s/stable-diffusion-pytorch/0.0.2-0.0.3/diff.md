# Comparing `tmp/stable_diffusion_pytorch-0.0.2-py3-none-any.whl.zip` & `tmp/stable_diffusion_pytorch-0.0.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,20 @@
-Zip file size: 14952 bytes, number of entries: 18
--rw-r--r--  2.0 unx      220 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/__init__.py
--rw-r--r--  2.0 unx     2548 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/attention.py
--rw-r--r--  2.0 unx     1798 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/clip.py
--rw-r--r--  2.0 unx     2768 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/decoder.py
--rw-r--r--  2.0 unx     7363 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/diffusion.py
--rw-r--r--  2.0 unx     1553 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/encoder.py
--rw-r--r--  2.0 unx     2592 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/model_loader.py
--rw-r--r--  2.0 unx    10468 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/pipeline.py
--rw-r--r--  2.0 unx     3036 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/tokenizer.py
--rw-r--r--  2.0 unx     1450 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/util.py
--rw-r--r--  2.0 unx      119 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/samplers/__init__.py
--rw-r--r--  2.0 unx     1598 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/samplers/k_euler.py
--rw-r--r--  2.0 unx     1939 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/samplers/k_euler_ancestral.py
--rw-r--r--  2.0 unx     2207 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch/samplers/k_lms.py
--rw-r--r--  2.0 unx      492 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch-0.0.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch-0.0.2.dist-info/WHEEL
--rw-r--r--  2.0 unx       25 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch-0.0.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1704 b- defN 23-Aug-01 09:08 stable_diffusion_pytorch-0.0.2.dist-info/RECORD
-18 files, 41972 bytes uncompressed, 12086 bytes compressed:  71.2%
+Zip file size: 14972 bytes, number of entries: 18
+-rw-r--r--  2.0 unx      220 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/__init__.py
+-rw-r--r--  2.0 unx     2548 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/attention.py
+-rw-r--r--  2.0 unx     1798 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/clip.py
+-rw-r--r--  2.0 unx     2768 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/decoder.py
+-rw-r--r--  2.0 unx     7363 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/diffusion.py
+-rw-r--r--  2.0 unx     1553 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/encoder.py
+-rw-r--r--  2.0 unx     2592 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/model_loader.py
+-rw-r--r--  2.0 unx    10468 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/pipeline.py
+-rw-r--r--  2.0 unx     3083 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/tokenizer.py
+-rw-r--r--  2.0 unx     1450 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/util.py
+-rw-r--r--  2.0 unx      119 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/samplers/__init__.py
+-rw-r--r--  2.0 unx     1598 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/samplers/k_euler.py
+-rw-r--r--  2.0 unx     1939 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/samplers/k_euler_ancestral.py
+-rw-r--r--  2.0 unx     2207 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch/samplers/k_lms.py
+-rw-r--r--  2.0 unx      492 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch-0.0.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch-0.0.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx       25 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch-0.0.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1704 b- defN 23-Aug-01 09:19 stable_diffusion_pytorch-0.0.3.dist-info/RECORD
+18 files, 42019 bytes uncompressed, 12106 bytes compressed:  71.2%
```

## zipnote {}

```diff
@@ -36,20 +36,20 @@
 
 Filename: stable_diffusion_pytorch/samplers/k_euler_ancestral.py
 Comment: 
 
 Filename: stable_diffusion_pytorch/samplers/k_lms.py
 Comment: 
 
-Filename: stable_diffusion_pytorch-0.0.2.dist-info/METADATA
+Filename: stable_diffusion_pytorch-0.0.3.dist-info/METADATA
 Comment: 
 
-Filename: stable_diffusion_pytorch-0.0.2.dist-info/WHEEL
+Filename: stable_diffusion_pytorch-0.0.3.dist-info/WHEEL
 Comment: 
 
-Filename: stable_diffusion_pytorch-0.0.2.dist-info/top_level.txt
+Filename: stable_diffusion_pytorch-0.0.3.dist-info/top_level.txt
 Comment: 
 
-Filename: stable_diffusion_pytorch-0.0.2.dist-info/RECORD
+Filename: stable_diffusion_pytorch-0.0.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## stable_diffusion_pytorch/tokenizer.py

```diff
@@ -1,7 +1,8 @@
+import os
 import unicodedata
 import functools
 import itertools
 import json
 from typing import List, Tuple
 import regex as re
 from . import util
@@ -22,16 +23,16 @@
 def pairwise(seq):
     a = iter(seq)
     b = iter(seq)
     next(b)
     return zip(a, b)
 
 class Tokenizer:
-    def __init__(self, ):
-        with open(util.get_file_path('vocab.json'), encoding='utf-8') as f:
+    def __init__(self, model_path):
+        with open(util.get_file_path(os.path.join(model_path, 'vocab.json')), encoding='utf-8') as f:
             self.vocab = json.load(f)
 
         with open(util.get_file_path('merges.txt'), encoding='utf-8') as f:
             lines = f.read().split('\n')
             lines = lines[1:-1]
             self.merges = {tuple(bigram.split()): i for i, bigram in enumerate(lines)}
 
@@ -80,8 +81,8 @@
             for word in words:
                 if word == second and new_words and new_words[-1] == first:
                     new_words[-1] = first + second
                 else:
                     new_words.append(word)
             words = new_words
         
-        return tuple(words)
+        return tuple(words)
```

## Comparing `stable_diffusion_pytorch-0.0.2.dist-info/RECORD` & `stable_diffusion_pytorch-0.0.3.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -2,17 +2,17 @@
 stable_diffusion_pytorch/attention.py,sha256=XXYPGEEjS9HyJVAwKtQLiBjYbYUueHj9GifE8mKYN4s,2548
 stable_diffusion_pytorch/clip.py,sha256=Z6VPCxlz2TjFJrA9WW04duMLpFWJx3xB22ikD_fxCpo,1798
 stable_diffusion_pytorch/decoder.py,sha256=y1sYhgr1UJkUrU8jiyfnDBnz50FlEs2Gj3iJNGUolnE,2768
 stable_diffusion_pytorch/diffusion.py,sha256=8Ed-7sIe9T5cu2uPKGoxOiLQVzV-eTbccASBVUicWbQ,7363
 stable_diffusion_pytorch/encoder.py,sha256=TwBMSLQrffH3E2V1rTjihx6HraY-1Fu6BzdEskJonMQ,1553
 stable_diffusion_pytorch/model_loader.py,sha256=hIASqfgJxxXGiOWfyDfC18gIFHrNp0KfEQYj6rEwZMk,2592
 stable_diffusion_pytorch/pipeline.py,sha256=731CrOSXIGtlK8cMuzjht753feGV-PIFNjPIszKdUSA,10468
-stable_diffusion_pytorch/tokenizer.py,sha256=ToaFF-MeUEiExQAXVVV03hJQL02cwk0wlpm2RnVaMdc,3036
+stable_diffusion_pytorch/tokenizer.py,sha256=8HaE_mhCNoZrfTIlXfzH5YR6Iawg4Rxyr7d31khtXrQ,3083
 stable_diffusion_pytorch/util.py,sha256=2Du5RH1ieVO3mZzKpItUilLYBKsa9MBqIXt60tDbbWc,1450
 stable_diffusion_pytorch/samplers/__init__.py,sha256=6HRwIPEhsx84gmQ9NwDow_tSqmTfDIsD68rUWuJ6pnA,119
 stable_diffusion_pytorch/samplers/k_euler.py,sha256=0h2tw9KN50ilb5k9CC1s02HLt0k5cN1dawm9MzpgCO8,1598
 stable_diffusion_pytorch/samplers/k_euler_ancestral.py,sha256=MDrftTo0DA-PFEo8XmRXFo-xedIaMVTMmwowqeaUzi8,1939
 stable_diffusion_pytorch/samplers/k_lms.py,sha256=HW-A2o5ERd9vwexOYdKS-zaWGwELy84xQa4eiPb5kus,2207
-stable_diffusion_pytorch-0.0.2.dist-info/METADATA,sha256=pHFu-f94De2HwrSjGxrRJmIwRyarUPZbCmcMuGDD0aM,492
-stable_diffusion_pytorch-0.0.2.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
-stable_diffusion_pytorch-0.0.2.dist-info/top_level.txt,sha256=EZhsoh3z5lkw5KoQ4g9zKUj0vQXB_TCewpRa8YbOWdE,25
-stable_diffusion_pytorch-0.0.2.dist-info/RECORD,,
+stable_diffusion_pytorch-0.0.3.dist-info/METADATA,sha256=fUvobYqweiOwcZdbbtgXhxf70yvL1mk_4EmvFQhX_Uc,492
+stable_diffusion_pytorch-0.0.3.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+stable_diffusion_pytorch-0.0.3.dist-info/top_level.txt,sha256=EZhsoh3z5lkw5KoQ4g9zKUj0vQXB_TCewpRa8YbOWdE,25
+stable_diffusion_pytorch-0.0.3.dist-info/RECORD,,
```

