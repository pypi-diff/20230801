# Comparing `tmp/fw_file-2.4.1-py3-none-any.whl.zip` & `tmp/fw_file-3.0.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,38 +1,38 @@
-Zip file size: 358400 bytes, number of entries: 36
--rw-r--r--  2.0 unx      229 b- defN 80-Jan-01 00:00 fw_file/__init__.py
--rw-r--r--  2.0 unx     8377 b- defN 80-Jan-01 00:00 fw_file/base.py
--rw-r--r--  2.0 unx     3184 b- defN 80-Jan-01 00:00 fw_file/bruker.py
+Zip file size: 357168 bytes, number of entries: 36
+-rw-r--r--  2.0 unx      121 b- defN 80-Jan-01 00:00 fw_file/__init__.py
+-rw-r--r--  2.0 unx     6917 b- defN 80-Jan-01 00:00 fw_file/base.py
+-rw-r--r--  2.0 unx     3179 b- defN 80-Jan-01 00:00 fw_file/bruker.py
 -rw-r--r--  2.0 unx     1515 b- defN 80-Jan-01 00:00 fw_file/dicom/__init__.py
 -rw-r--r--  2.0 unx     6011 b- defN 80-Jan-01 00:00 fw_file/dicom/config.py
 -rw-r--r--  2.0 unx    44285 b- defN 80-Jan-01 00:00 fw_file/dicom/dcmdict.py
--rw-r--r--  2.0 unx    17633 b- defN 80-Jan-01 00:00 fw_file/dicom/dicom.py
+-rw-r--r--  2.0 unx    17412 b- defN 80-Jan-01 00:00 fw_file/dicom/dicom.py
 -rw-r--r--  2.0 unx    18949 b- defN 80-Jan-01 00:00 fw_file/dicom/fixers.py
--rw-r--r--  2.0 unx    18042 b- defN 80-Jan-01 00:00 fw_file/dicom/reader.py
--rw-r--r--  2.0 unx    27658 b- defN 80-Jan-01 00:00 fw_file/dicom/series.py
+-rw-r--r--  2.0 unx    17915 b- defN 80-Jan-01 00:00 fw_file/dicom/reader.py
+-rw-r--r--  2.0 unx    24712 b- defN 80-Jan-01 00:00 fw_file/dicom/series.py
 -rw-r--r--  2.0 unx   421033 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023b/json/dict_info.json
 -rw-r--r--  2.0 unx   188832 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023b/json/iod_info.json
 -rw-r--r--  2.0 unx   528709 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023b/json/module_info.json
 -rw-r--r--  2.0 unx    31221 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023b/json/uid_info.json
 -rw-r--r--  2.0 unx        5 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023b/json/version
 -rw-r--r--  2.0 unx   422965 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023c/json/dict_info.json
 -rw-r--r--  2.0 unx   191779 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023c/json/iod_info.json
 -rw-r--r--  2.0 unx   531128 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023c/json/module_info.json
 -rw-r--r--  2.0 unx    31351 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023c/json/uid_info.json
 -rw-r--r--  2.0 unx        5 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/2023c/json/version
 -rw-r--r--  2.0 unx      353 b- defN 80-Jan-01 00:00 fw_file/dicom/standard/editions.json
--rw-r--r--  2.0 unx     9047 b- defN 80-Jan-01 00:00 fw_file/dicom/utils.py
+-rw-r--r--  2.0 unx     8995 b- defN 80-Jan-01 00:00 fw_file/dicom/utils.py
 -rw-r--r--  2.0 unx     6880 b- defN 80-Jan-01 00:00 fw_file/dicom/validation.py
 -rw-r--r--  2.0 unx     3103 b- defN 80-Jan-01 00:00 fw_file/exif.py
--rw-r--r--  2.0 unx    12581 b- defN 80-Jan-01 00:00 fw_file/ge.py
--rw-r--r--  2.0 unx      778 b- defN 80-Jan-01 00:00 fw_file/jpg.py
--rw-r--r--  2.0 unx     2186 b- defN 80-Jan-01 00:00 fw_file/json.py
--rw-r--r--  2.0 unx     1440 b- defN 80-Jan-01 00:00 fw_file/nifti.py
--rw-r--r--  2.0 unx     5265 b- defN 80-Jan-01 00:00 fw_file/philips.py
--rw-r--r--  2.0 unx     8385 b- defN 80-Jan-01 00:00 fw_file/png.py
+-rw-r--r--  2.0 unx    12596 b- defN 80-Jan-01 00:00 fw_file/ge.py
+-rw-r--r--  2.0 unx      955 b- defN 80-Jan-01 00:00 fw_file/jpg.py
+-rw-r--r--  2.0 unx     2369 b- defN 80-Jan-01 00:00 fw_file/json.py
+-rw-r--r--  2.0 unx     1619 b- defN 80-Jan-01 00:00 fw_file/nifti.py
+-rw-r--r--  2.0 unx     5240 b- defN 80-Jan-01 00:00 fw_file/philips.py
+-rw-r--r--  2.0 unx     8557 b- defN 80-Jan-01 00:00 fw_file/png.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 fw_file/py.typed
--rw-r--r--  2.0 unx    10521 b- defN 80-Jan-01 00:00 fw_file/siemens.py
--rw-r--r--  2.0 unx     1078 b- defN 80-Jan-01 00:00 fw_file-2.4.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     9569 b- defN 80-Jan-01 00:00 fw_file-2.4.1.dist-info/METADATA
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 fw_file-2.4.1.dist-info/WHEEL
-?rw-r--r--  2.0 unx     3031 b- defN 16-Jan-01 00:00 fw_file-2.4.1.dist-info/RECORD
-36 files, 2567216 bytes uncompressed, 353590 bytes compressed:  86.2%
+-rw-r--r--  2.0 unx    10385 b- defN 80-Jan-01 00:00 fw_file/siemens.py
+-rw-r--r--  2.0 unx     1078 b- defN 80-Jan-01 00:00 fw_file-3.0.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     8951 b- defN 80-Jan-01 00:00 fw_file-3.0.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 fw_file-3.0.0.dist-info/WHEEL
+?rw-r--r--  2.0 unx     3031 b- defN 16-Jan-01 00:00 fw_file-3.0.0.dist-info/RECORD
+36 files, 2562244 bytes uncompressed, 352358 bytes compressed:  86.3%
```

## zipnote {}

```diff
@@ -90,20 +90,20 @@
 
 Filename: fw_file/py.typed
 Comment: 
 
 Filename: fw_file/siemens.py
 Comment: 
 
-Filename: fw_file-2.4.1.dist-info/LICENSE
+Filename: fw_file-3.0.0.dist-info/LICENSE
 Comment: 
 
-Filename: fw_file-2.4.1.dist-info/METADATA
+Filename: fw_file-3.0.0.dist-info/METADATA
 Comment: 
 
-Filename: fw_file-2.4.1.dist-info/WHEEL
+Filename: fw_file-3.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: fw_file-2.4.1.dist-info/RECORD
+Filename: fw_file-3.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fw_file/__init__.py

```diff
@@ -1,8 +1,5 @@
 """fw_file package metadata."""
-try:
-    from importlib.metadata import version
-except ImportError:  # pragma: no cover
-    from importlib_metadata import version  # type: ignore
+from importlib.metadata import version
 
 __version__ = version(__name__)
 NAME = "fw_file"
```

## fw_file/base.py

```diff
@@ -1,38 +1,27 @@
 """Abstract file class and common helpers."""
 import functools
 import typing as t
 from collections.abc import MutableMapping
-from pathlib import Path
 
 import yaml
-from fw_meta import MetaData, MetaExtractor
+from fw_meta import MetaData
 from fw_utils import AnyFile, AnyPath, BinFile, open_any
-from memoization import cached
 
 __all__ = [
     "AnyFile",
     "AnyPath",
     "AttrMixin",
     "FieldsMixin",
     "File",
     "ReadOnly",
-    "filter_meta",
     "parse_yaml_value",
 ]
 
 
-cached_extractor = cached(MetaExtractor)
-
-
-def filter_meta(meta: dict) -> dict:
-    """Return meta dict with keys cast to str and None/"" values filtered."""
-    return {str(k): v for k, v in sorted(meta.items()) if v not in {None, ""}}
-
-
 @functools.lru_cache(maxsize=4096)
 def parse_yaml_value(value: str):
     """Return field value parsed with YAML syntax."""
     try:
         return yaml.safe_load(value)
     except Exception:
         return value
@@ -134,52 +123,25 @@
     def __init__(self, file: AnyFile) -> None:
         """Read and parse a data-file - subclasses are expected to add parsing."""
         with open_any(file) as rfile:
             if not rfile.read(1):
                 raise ValueError(f"Zero-byte file: {rfile}")
         # NOTE using object.__setattr__ to side-step AttrMixin
         object.__setattr__(self, "_file", rfile.localpath or rfile)
-        object.__setattr__(self, "_meta", {})
         object.__setattr__(self, "localpath", rfile.localpath)
         object.__setattr__(self, "filepath", rfile.metapath)
 
     @property
     def file(self) -> BinFile:
         """Return the underlying file opened for reading as a BinFile."""
         return open_any(self._file, mode="rb")
 
-    def get_default_meta(self) -> t.Dict[str, t.Any]:
+    def get_meta(self) -> MetaData:
         """Return the default Flywheel metadata extracted from the file."""
-        return {}
-
-    def get_meta(
-        self,
-        cache: bool = True,
-        extractor: t.Optional[MetaExtractor] = None,
-        **kwargs,
-    ) -> MetaData:
-        """Return the customized Flywheel metadata extracted from the file.
-
-        Args:
-            cache (optional): Toggle for disabling caching. Set to False to
-                return a fresh extract instead of a previously cached result.
-            extractor (optional): Pre-configured MetaExtractor instance to
-                extract Flywheel metadata with.
-            **kwargs: Keyword arguments to initialize a new MetaExtractor with
-                when a pre-configured instance is not provided.
-        """
-        extractor = extractor or cached_extractor(**kwargs)
-        key = id(extractor)
-        if not cache or key not in self._meta:
-            meta = extractor.extract(self)
-            meta.setdefault("file.type", self.__class__.__name__.lower())
-            if self.localpath:
-                meta.setdefault("file.name", Path(self.localpath).name)
-            self._meta[key] = meta
-        return self._meta[key]
+        return MetaData()  # pragma: no cover
 
     @property
     def sort_key(self):
         """Return sort key used for comparing/ordering instances."""
         return self.filepath  # pragma: no cover
 
     def open_dst(self, file: t.Optional[AnyFile] = None) -> BinFile:
```

## fw_file/bruker.py

```diff
@@ -1,15 +1,16 @@
 """Bruker ParaVision file format - read only (subject/acqp/method files)."""
 import re
 import typing as t
 from datetime import datetime
 
+from fw_meta import MetaData
 from fw_utils import BinFile, get_datetime
 
-from .base import AnyFile, FieldsMixin, File, ReadOnly, filter_meta
+from .base import AnyFile, FieldsMixin, File, ReadOnly
 
 ARRAY_RE = re.compile(r"\( \d+(, \d+)* \)")
 VERSION_RE = re.compile(r"(PV|ParaVision) ?(?P<ver>\d+(\.\d+)+)")
 
 
 class ParaVision(ReadOnly, FieldsMixin, File):
     """Bruker ParaVision file class."""
@@ -20,15 +21,15 @@
         Args:
             file (str|Path|file): Filepath (str|Path) or open file to read from.
         """
         super().__init__(file)
         with self.file as rfile:
             object.__setattr__(self, "fields", load_paravision(rfile))
 
-    def get_default_meta(self) -> t.Dict[str, t.Any]:
+    def get_meta(self) -> MetaData:
         """Return the default Flywheel metadata for a ParaVision file."""
         study_name = self.get("subject_study_name", "")
         session_ts = get_timestamp(
             self.get("subject_study_date")  # pv360 and up
             or self.get("subject_abs_date")  # pv7 and down
             or ""
         )
@@ -45,15 +46,15 @@
             "session.uid": self.get("subject_study_instance_uid"),
             "session.label": session_label,
             "session.timestamp": session_ts,
             "acquisition.label": self.get("acq_protocol_name"),
             "acquisition.timestamp": acq_ts,
             "file.type": "ParaVision",
         }
-        return filter_meta(meta)
+        return MetaData(meta)
 
 
 def load_paravision(file: BinFile) -> t.Dict[str, t.Any]:
     """Parse ParaVision parameters file as a dictionary."""
     fields: t.Dict[str, t.Any] = {}
     key = None
     for line_bytes in file:
```

## fw_file/dicom/dicom.py

```diff
@@ -1,28 +1,28 @@
 """DICOM file format."""
 import io
 import re
 import typing as t
-import warnings
 from datetime import timedelta
 from functools import lru_cache
 
+from fw_meta import MetaData
 from fw_utils import AttrDict
 from pydicom.datadict import dictionary_VR, keyword_dict, private_dictionaries
 from pydicom.dataelem import DataElement
 from pydicom.dataset import Dataset
 from pydicom.errors import InvalidDicomError
 from pydicom.filereader import read_partial
 from pydicom.multival import MultiValue
 from pydicom.sequence import Sequence
 from pydicom.tag import BaseTag, Tag
 from pydicom.uid import UID
 from pydicom.valuerep import DA, DT, IS, TM, DSdecimal, DSfloat, PersonName
 
-from ..base import AnyFile, AttrMixin, File, filter_meta
+from ..base import AnyFile, AttrMixin, File
 from . import utils
 from .config import get_config
 from .reader import ReadContext
 
 __all__ = ["DICOM"]
 
 # extend TagType with (str, str)
@@ -39,15 +39,14 @@
         self,
         file: AnyFile,
         force: bool = False,
         defer_size: t.Union[int, str] = None,
         specific_tags: t.List[TagType] = None,
         stop_when: t.Union[TagType, StopWhen] = None,
         decode: bool = False,
-        track: t.Optional[bool] = None,
     ) -> None:
         """Read and parse a DICOM dataset.
 
         Args:
             file: Filepath (str|Path) or open file to read from.
             force: Flag to force reading the file as DICOM
                 even if the header is missing or invalid. Defaults to False.
@@ -56,27 +55,24 @@
                 Defaults to 256 bytes if the file has a local path.
             specific_tags: List of tags to include if set
                 when parsing while skipping all others. Defaults to None.
             stop_when: Tag or callback function to be
                 used as a stop condition when reading. Defaults to None.
             decode: Flag to automatically decode string values
                 immediately after loading the Dataset. Defaults to False.
-            track: Deprecated.
         """
         super().__init__(file)
         if defer_size and not self.localpath:
             raise ValueError("defer_size can only be used with local files")
         # use a default defer size on local files
         if defer_size is None and self.localpath:
             defer_size = 256
         # create a callback if stop_when is provided as a tag (eg. "PixelData")
         if stop_when and not callable(stop_when):
             stop_when = stop_at_tag(stop_when)
-        if track is not None:
-            warnings.warn("track option is deprecated", category=DeprecationWarning)
         with self.file as rfile, ReadContext() as read_context:
             try:
                 dataset = read_partial(
                     rfile,  # type: ignore
                     force=force,
                     defer_size=defer_size,
                     specific_tags=specific_tags,  # type: ignore
@@ -87,20 +83,20 @@
                 raise ValueError(f"Invalid DICOM: {file!r}") from exc
         object.__setattr__(self, "dataset", DSWrap(dataset))
         object.__setattr__(self, "read_context", read_context)  # for convenience
 
         if decode:
             self.decode()
 
-    def get_default_meta(self) -> t.Dict[str, t.Any]:
+    def get_meta(self) -> MetaData:
         """Return the default Flywheel metadata of the DICOM dataset."""
         # TODO consider decorating every util func to warn and return None
         # when encountering an unhandled exception
         firstname, lastname = utils.get_patient_name(self)
-        meta: t.Dict[str, t.Any] = {
+        meta: dict = {
             "subject.label": self.get("PatientID"),
             "subject.firstname": firstname,
             "subject.lastname": lastname,
             "subject.sex": self.get("PatientSex"),
             "session.uid": self.get("StudyInstanceUID"),
             "session.label": utils.get_session_label(self),
             "session.age": utils.get_session_age(self),
@@ -109,15 +105,15 @@
             "session.timestamp": utils.get_session_timestamp(self),
             "acquisition.uid": utils.get_acquisition_uid(self),
             "acquisition.label": utils.get_acquisition_label(self),
             "acquisition.timestamp": utils.get_acquisition_timestamp(self),
             "file.name": utils.get_instance_filename(self),
             "file.type": "dicom",
         }
-        return filter_meta(meta)
+        return MetaData(meta)
 
     @property
     def ge_physio_match(self) -> t.Optional[AttrDict]:
         """Return the metadata needed for matching GE physio filenames to."""
         manufacturer = self.get("Manufacturer") or ""
         if not manufacturer.startswith("GE"):
             return None
```

## fw_file/dicom/reader.py

```diff
@@ -371,22 +371,19 @@
                 # only the original Value from the dicom will be written
                 # back if it can be
                 VR = t.cast(str, elem.VR)
                 # So that validation_mode is set to do as little validation as possible,
                 # it's set to 0 ("IGNORE") here, so that it won't raise if the value
                 # doesn't fit DICOM standards, only if it's unable to be parsed at all.
                 # If a group 2 element gets in the main dataset, the DICOM can't save...
-                if (hasattr(dataset, "file_meta") and elem.tag.group != 0x0002) or (
-                    hasattr(dataset, "file_meta") is False and elem.tag.group == 0x0002
-                ):
-                    out = DataElement(
-                        elem.tag, VR, elem.original.value, validation_mode=0
-                    )
-                    write_data_element(buffer, out)
-                    dataset[out.tag] = out
+                if elem.tag.group == 2 and hasattr(dataset, "file_meta"):
+                    continue  # pragma: no cover
+                out = DataElement(elem.tag, VR, elem.original.value, validation_mode=0)
+                write_data_element(buffer, out)
+                dataset[out.tag] = out
             except Exception as exc:
                 # Writing will fail but we want to warn the user on all DEs that
                 # fall into this category
                 warnings.warn(
                     (
                         "Could not write original element. Original element "
                         "is required but invalid and cannot be written.\n\n"
```

## fw_file/dicom/series.py

```diff
@@ -1,93 +1,81 @@
 """DICOM Series management."""
 import functools
 import io
 import logging
-import re
 import shutil
 import tempfile
 import typing as t
 import warnings
 import zipfile
 from collections import OrderedDict
 from dataclasses import dataclass
 from pathlib import Path
 
-from fw_meta import MetaData as Meta
-from fw_meta import MetaExtractor
+from fw_meta import MetaData
 from fw_utils import AnyFile, AnyPath, BinFile, fileglob, get_datetime, open_any
 from natsort import natsorted
 
-from ..base import cached_extractor
 from .config import get_config
 from .dicom import DICOM, TagType
-
-try:
-    from fw_storage import StorageError
-except ImportError:  # pragma: no cover
-
-    class StorageError(Exception):  # type: ignore
-        """Storage error stub."""
-
+from .utils import get_instance_filename
 
 __all__ = ["DICOMCollection", "DICOMSeries", "build_dicom_tree"]
 
 log = logging.getLogger(__name__)
 
 AnyDICOM = t.Union[AnyFile, DICOM]
 TIMESTAMP_MAX_DELTA = 5 * 60 * 60
+TIMESTAMP_FIELDS = [
+    "session.timestamp",
+    "session.label",
+    "acquisition.timestamp",
+    "acquisition.label",
+]
 
 
 def get_instance_sort_key(dcm: DICOM) -> str:
     """Return instance sort key string for a DICOM."""
-    return "/".join(dcm.sort_key)
-
-
-def get_instance_name(dcm: DICOM) -> str:
-    """Return instance filename for a DICOM."""
-    meta = dcm.get_meta()
-    if meta.get("file.name"):
-        return meta["file.name"]
-    raise ValueError("Cannot determine DICOM instance filename")
+    return "-".join(dcm.sort_key)
 
 
 def get_series_name(dcm: DICOM) -> t.Optional[str]:
     """Return series filename for a DICOM."""
     meta = dcm.get_meta()
     return meta.get("acquisition.label") or meta.get("acquisition.uid")
 
 
-def validate_series_meta(series_meta: Meta, dcm_meta: Meta) -> Meta:
-    """Build and validate DICOM series metadata instance by instance.
+def merge_series_meta(series_meta: MetaData, dataset_meta: MetaData) -> MetaData:
+    """Merge DICOM instance metadata into the series metadata.
 
     Given a (potentially empty) series meta dict and a DICOM's meta dict
      * apply every key from the DICOM to the series if it's not set yet
      * raise on any key that is present in both but has a different value
     """
-    meta = series_meta or Meta()
-    if not meta:
-        meta.update(dcm_meta)
-        meta.pop("file.name")
-    for key, value in dcm_meta.items():
-        # ignore instance file names
+    for key in dataset_meta:
+        # skip file.name which is different on each dataset instance
         if key == "file.name":
             continue
-        # enforce equality on extracted metadata fields
-        if meta[key] != value:
-            # allow some delta across timestamp values (and time-based labels)
-            if key.endswith(".timestamp") and meta[key] and value:
-                ts1, ts2 = get_datetime(meta[key]), get_datetime(value)
+        # for keys which weren't set previously, simply use the dataset's value
+        series_meta.setdefault(key, dataset_meta[key])
+        # enforce equality on all other extracted metadata fields
+        v1, v2 = series_meta.get(key), dataset_meta.get(key)
+        if v1 == v2:
+            continue
+        # allow some delta across timestamps (and any timestamp-based labels)
+        if key in TIMESTAMP_FIELDS and v1 and v2:
+            try:
+                ts1, ts2 = get_datetime(v1), get_datetime(v2)
+            except ValueError:  # pragma: no cover
+                pass
+            else:
                 if abs((ts1 - ts2).total_seconds()) <= TIMESTAMP_MAX_DELTA:
                     continue
-            if key in {"session.label", "acquisition.label"}:
-                ts_re = re.compile(r"\d\d\d\d-\d\d-\d\d.+")
-                if ts_re.match(meta[key]) and ts_re.match(value):
-                    continue
-            raise ValueError(f"Metadata conflict on {key}: {meta[key]} != {value}")
-    return meta
+        raise ValueError(f"Metadata conflict on {key}: {v1} != {v2}")
+    return series_meta
 
 
 def warn_on_cache_limit(method):
     """Decorator to warn after a method call if the instance cache is full.
 
     The decorator is intended for the convenience collection/series methods like
     get() and set(), which do not scale well to large series / number of slices.
@@ -121,36 +109,32 @@
     """DICOMCollection represents a list of instances."""
 
     def __init__(
         self,
         *files: AnyDICOM,
         defer_parse: bool = True,
         write_cache_drop: bool = True,
-        instance_name_fn: t.Callable[[DICOM], str] = get_instance_name,
         filter_fn: t.Optional[t.Callable[[DICOM], bool]] = None,
         **dcm_kw,
     ) -> None:
         """Initialize DICOMCollection.
 
         Args:
             *files (str|Path|file|DICOM): DICOMs to load into the collection.
             defer_parse (bool): Set to False to parse DICOM filepaths during
                 collection initialization instead of when being accessed later.
             write_cache_drop (bool): Set to False to skip writing DICOMs back to
                 disk automatically when they are dropped from the instance cache.
-            instance_name_fn (callable): Function to generate instance filenames
-                with when saving to a directory/ZIP. Default: get_instance_name
             filter_fn (callable): Function to filter out dicoms.  Return True to
                 add the dicom to the collection, False to not add.
             **dcm_kw: Keyword arguments to pass to DICOM when reading files.
         """
         super().__init__()
         self.defer_parse = defer_parse
         self.instance_cache = InstanceCache(dcm_kw, write_cache_drop=write_cache_drop)
-        self.instance_name_fn = instance_name_fn
         self.dirpath: t.Optional[Path] = None  # from_dir()
         self.is_tmp = False  # from_zip()
         self.dcm_kw = dcm_kw
         for file in files:
             self.append(file)
         if filter_fn:
             self.filter(filter_fn)
@@ -377,15 +361,15 @@
             on_progress (t.Callable[[DICOM], None]): Function to call
                 when a single DICOM file is processed. Default: None.
         """
         if isinstance(dirpath, str):
             dirpath = Path(dirpath)
         dirpath = dirpath.resolve()
         for file in self:
-            filepath = dirpath / self.instance_name_fn(file)
+            filepath = dirpath / get_instance_filename(file)
             filepath.parent.mkdir(parents=True, exist_ok=True)
             file.save(filepath, write_like_original=write_like_original)
             if on_progress:
                 on_progress(file)
 
     def to_zip(
         self,
@@ -405,15 +389,15 @@
             on_progress (t.Callable[[DICOM], None]): Function to call
                 when a single DICOM file is processed. Default: None.
             **zip_kw: Additional keyword arguments to pass to zipfile.ZipFile.
         """
         zip_kw.setdefault("compression", zipfile.ZIP_DEFLATED)
         with zipfile.ZipFile(archive, mode="w", **zip_kw) as zfile:
             for file in self:
-                arcname = self.instance_name_fn(file)
+                arcname = get_instance_filename(file)
                 content = io.BytesIO()
                 file.save(content, write_like_original=write_like_original)
                 zfile.writestr(arcname, content.getvalue())
                 if on_progress:
                     on_progress(file)
             if isinstance(comment, str):
                 zfile.comment = comment.encode("utf8")
@@ -443,99 +427,67 @@
     """DICOMSeries represents a list of instances from the same series."""
 
     def __init__(  # noqa: PLR0913
         self,
         *files: AnyDICOM,
         defer_parse: bool = True,
         write_cache_drop: bool = True,
-        instance_name_fn: t.Callable[[DICOM], str] = get_instance_name,
         filter_fn: t.Optional[t.Callable[[DICOM], bool]] = None,
-        series_name_fn: t.Callable[[DICOM], t.Optional[str]] = get_series_name,
-        validate_meta_fn: t.Callable[[Meta, Meta], Meta] = validate_series_meta,
         **kwargs,
     ) -> None:
         """Initialize DICOMSeries.
 
         Args:
             *files (str|Path|file|DICOM): DICOMs to load into the series.
             defer_parse (bool): Set to False to parse DICOM filepaths during
                 series initialization instead of when being accessed later.
             write_cache_drop (bool): Set to False to skip writing DICOMs back to
                 disk automatically when they are dropped from the instance cache.
-            instance_name_fn (callable): Function to generate instance filenames
-                with when saving to a directory/ZIP. Default: get_instance_name
             filter_fn (callable): Function to filter out dicoms.  Return True to
                 add the dicom to the collection, False to not add.
-            series_name_fn (callable): Function to generate the series filename
-                with when getting meta or saving to a ZIP. Default: get_series_name
-            validate_meta_fn (callable): Function to build and validate series
-                meta with, instance by instance. Default: validate_series_meta
             **kwargs: Keyword arguments to use when reading files.
         """
         super().__init__(
             *files,
             defer_parse=defer_parse,
             write_cache_drop=write_cache_drop,
-            instance_name_fn=instance_name_fn,
             filter_fn=filter_fn,
             **kwargs,
         )
-        self.series_name_fn = series_name_fn
-        self.validate_meta_fn = validate_meta_fn
-        self._meta: t.Dict[int, Meta] = {}
-
-    def get_meta(
-        self,
-        cache: bool = True,
-        extractor: t.Optional[MetaExtractor] = None,
-        **kwargs,
-    ) -> Meta:
-        """Return the customized Flywheel metadata extracted from the series.
 
-        Args:
-            cache (optional): Toggle for disabling caching. Set to False to
-                return a fresh extract instead of a previously cached result.
-            extractor (optional): Pre-configured MetaExtractor instance to
-                extract Flywheel metadata with.
-            **kwargs: Keyword arguments to initialize a new MetaExtractor with
-                when a pre-configured instance is not provided.
-        """
-        extractor = extractor or cached_extractor(**kwargs)
-        key = id(extractor)
-        if not cache or key not in self._meta:
-            if not self:
-                raise ValueError("No DICOMs to extract series meta from")
-            meta = Meta()
-            for file in self:
-                file_meta = file.get_meta(cache=cache, extractor=extractor)
-                meta = self.validate_meta_fn(meta, file_meta)
-            self._meta[key] = meta
-        return self._meta[key]
+    def get_meta(self) -> MetaData:
+        """Return the default Flywheel metadata of the DICOM series."""
+        if not self:
+            raise ValueError("No DICOMs to extract series meta from")
+        meta = MetaData()
+        for file in self:
+            merge_series_meta(meta, file.get_meta())
+        return meta
 
     def to_upload(
         self,
         dirpath: t.Optional[AnyPath] = None,
         on_progress: t.Optional[t.Callable[[DICOM], None]] = None,
         **kwargs,
-    ) -> t.Tuple[Path, Meta]:
+    ) -> t.Tuple[Path, MetaData]:
         """Return (filepath, metadata) tuple for the series for FW upload.
 
         Prepare (pack/save) the series as a single file in the given directory
         (defaults to the current working dir), and extract metadata for upload.
 
         If there are multiple DICOM files in the series, create a ZIP archive.
         Zipping keeps classic DICOM series together that consist of as many
         files as image slices, simplifying transfers and storage.
 
         If the series only contains one file, the single file is not zipped.
         Enhanced DICOMs allow encoding all instances in a single file, thus
         zipping is not necessary for keeping cohesion.
         """
         meta = self.get_meta(**kwargs)
-        series_name = self.series_name_fn(self[0])
+        series_name = get_series_name(self[0])
         if not dirpath:
             dirpath = Path.cwd()
         elif isinstance(dirpath, str):
             dirpath = Path(dirpath)
         if len(self) > 1:  # classic or otherwise multifile - pack
             meta["file.zip_member_count"] = len(self)
             filepath = dirpath / f"{series_name}.dicom.zip"
@@ -579,48 +531,44 @@
             super().__delitem__(old_path)
 
     def is_full(self) -> bool:
         """Return whether the cache is full."""
         return len(self) >= get_config().instance_cache_size
 
 
-DICOMTree = t.Dict[str, t.Dict[str, t.Dict[str, AnyPath]]]
+StudyUID = str
+SeriesUID = str
+InstanceUID = str
+DICOMTree = t.Dict[StudyUID, t.Dict[SeriesUID, t.Dict[InstanceUID, AnyPath]]]
 
 
 @dataclass
 class DICOMError:
     """DICOM error class capturing any issues from build_dicom_tree."""
 
-    __slots__ = ("file", "message")
+    __slots__ = "file", "message"
 
     file: str
     message: str
 
 
 def build_dicom_tree(  # noqa PLR0915
     files: t.Iterable[AnyPath],
     open_fn: t.Optional[t.Callable] = None,
-    validate_meta_fn: t.Optional[t.Callable] = None,
-    natural_sort: bool = True,
     **dcm_kw,
 ) -> t.Tuple[DICOMTree, t.List[DICOMError]]:
     """Build DICOM hierarchy from DICOM files.
 
     Args:
         files (iterable[str|Path]): Files to place in the DICOM tree.
         open_fn (callable, optional): Function to open the files with.
             Default: BinFile (expecting files on the local filesystem).
-        validate_meta_fn (callable, optional): Function to build and validate
-            series meta with instance by instance. Default: validate_series_meta
-        natural_sort (bool, optional): Toggle to disable natural sorting on the
-            UIDs in the returned DICOM tree. Default: True.
         **dcm_kw: Keyword arguments to use when reading files.
     """
     open_file = open_fn or BinFile
-    validate_meta = validate_meta_fn or validate_series_meta
     dcm_kw.setdefault("force", True)
     dcm_kw.setdefault("defer_size", 512)
     dcm_kw.setdefault("stop_when", "PixelData")
     tree: DICOMTree = {}
     series_to_study: t.Dict[str, str] = {}
     series_meta: t.Dict[str, t.Any] = {}
     instance_to_series: t.Dict[str, str] = {}
@@ -652,35 +600,28 @@
             c_series = instance_to_series[instance]
             c_study = series_to_study[c_series]
             c_file = tree[c_study][c_series][instance]
             raise ValueError(
                 f"Instance {study}/{series}/{instance} conflicts with "
                 f"instance {c_study}/{c_series}/{instance} in file {c_file}"
             )
-        series_meta.setdefault(series, {})
-        series_meta[series] = validate_meta(series_meta[series], meta)
+        series_meta.setdefault(series, MetaData())
+        merge_series_meta(series_meta[series], meta)
         tree.setdefault(study, {})
         tree[study].setdefault(series, {})
         tree[study][series][instance] = file
 
     for file in files:
         try:
             process_dcm(file)
-        except (
-            FileNotFoundError,
-            PermissionError,
-            StorageError,
-            ValueError,
-        ) as exc:
+        except Exception as exc:
             errors.append(DICOMError(str(file), str(exc)))
 
-    if natural_sort:
-        sorted_tree: DICOMTree = {}
-        for study in natsorted(tree, key=str.lower):
-            sorted_tree[study] = {}
-            for series in natsorted(tree[study], key=str.lower):
-                sorted_tree[study][series] = {}
-                for inst in natsorted(tree[study][series], key=str.lower):
-                    sorted_tree[study][series][inst] = tree[study][series][inst]
-        tree = sorted_tree
+    sorted_tree: DICOMTree = {}
+    for study in natsorted(tree, key=str.lower):
+        sorted_tree[study] = {}
+        for series in natsorted(tree[study], key=str.lower):
+            sorted_tree[study][series] = {}
+            for inst in natsorted(tree[study][series], key=str.lower):
+                sorted_tree[study][series][inst] = tree[study][series][inst]
 
-    return tree, errors
+    return sorted_tree, errors
```

## fw_file/dicom/utils.py

```diff
@@ -184,22 +184,20 @@
     return int(age_in_seconds)
 
 
 def get_session_label(dcm: t.Mapping[str, t.Any]) -> t.Optional[str]:
     """Return session label.
 
     1. StudyDescription
-    2. Session timestamp
+    2. Session timestamp (YYYY-mm-ddTHH-MM-SS)
     3. StudyInstanceUID
     """
     label = dcm.get("StudyDescription")
-    if not label:
-        ts = get_session_timestamp(dcm)
-        if ts:
-            label = ts.strftime("%Y-%m-%dT%H:%M:%S")
+    if not label and (ts := get_session_timestamp(dcm)):
+        label = ts.strftime("%Y-%m-%dT%H-%M-%S")
     return label or dcm.get("StudyInstanceUID")
 
 
 def get_session_timestamp(dcm: t.Mapping[str, t.Any]) -> t.Optional[datetime]:
     """Return session timestamp.
 
     1. StudyDate + Time
@@ -221,28 +219,24 @@
     # TODO GE: separate UID per AcquisitionNumber
     return dcm.get("SeriesInstanceUID")
 
 
 def get_acquisition_label(dcm: t.Mapping[str, t.Any]) -> t.Optional[str]:
     """Return acquisition label.
 
-    Add "SeriesNumber - " prefix if set in case of SeriesDescription
-    and ProtocolName.
-
-    1. SeriesDescription
-    2. ProtocolName
-    3. Acquisition timestamp
-    4. SeriesInstanceUID
+    1. [SeriesNumber - ]SeriesDescription
+    2. [SeriesNumber - ]ProtocolName
+    3. [SeriesNumber - ]Acquisition timestamp (YYYY-mm-ddTHH-MM-SS)
+    4. [SeriesNumber - ]SeriesInstanceUID
     """
     label = dcm.get("SeriesDescription") or dcm.get("ProtocolName")
-    if not label:
-        ts = get_acquisition_timestamp(dcm)
-        return ts.strftime("%Y-%m-%dT%H:%M:%S") if ts else dcm.get("SeriesInstanceUID")
-    series_number = dcm.get("SeriesNumber")
-    if series_number:
+    if not label and (ts := get_acquisition_timestamp(dcm)):
+        label = ts.strftime("%Y-%m-%dT%H-%M-%S")
+    label = label or dcm.get("SeriesInstanceUID")
+    if label and (series_number := dcm.get("SeriesNumber")):
         label = f"{series_number} - {label}"
     return label
 
 
 def get_acquisition_timestamp(dcm: t.Mapping[str, t.Any]) -> t.Optional[datetime]:
     """Return acquisition timestamp.
 
@@ -256,14 +250,12 @@
     return (
         get_timestamp(dcm, "Acquisition")
         or get_timestamp(dcm, "Series")
         or get_timestamp(dcm, "Study")
     )
 
 
-def get_instance_filename(dcm: t.Mapping[str, t.Any]) -> t.Optional[str]:
+def get_instance_filename(dcm: t.Mapping[str, t.Any]) -> str:
     """Return recommended DICOM instance filename."""
     instance_uid = dcm.get("SOPInstanceUID")
-    if not instance_uid:
-        return None
     modality = dcm.get("Modality") or "NA"
     return f"{instance_uid}.{modality}.dcm"
```

## fw_file/ge.py

```diff
@@ -1,15 +1,16 @@
 """GE MR RAW / PFile (PNNNNN.7) file format."""
 import struct
 import typing as t
 from datetime import datetime, timedelta
 
+from fw_meta import MetaData
 from fw_utils import AttrDict
 
-from .base import AnyFile, FieldsMixin, File, filter_meta
+from .base import AnyFile, FieldsMixin, File
 from .dicom.utils import parse_patient_name
 
 # num of bytes to read into memory (must cover all parsed offsets)
 HEADER_SIZE = 256 << 10  # 256 KB
 
 # possible logo (~ magic bytes) variants
 PFILE_LOGOS = {"GE_MED_NMR", "INVALIDNMR"}
@@ -41,29 +42,30 @@
             # validate pfile logo / magic bytes
             if fields["logo"] not in PFILE_LOGOS:
                 raise ValueError(f"Invalid PFile logo: {fields['logo']!r}")
         object.__setattr__(self, "header", header)
         object.__setattr__(self, "version", version)
         object.__setattr__(self, "fields", fields)
 
-    def get_default_meta(self) -> t.Dict[str, t.Any]:
+    def get_meta(self) -> MetaData:
         """Return the default Flywheel metadata of the PFile."""
         firstname, lastname = parse_patient_name(self.PatientName or "")
         timestamp = self.get_acquisition_timestamp()
-        meta: t.Dict[str, t.Any] = {
+        meta: dict = {
             "subject.label": self.PatientID,
             "subject.firstname": firstname,
             "subject.lastname": lastname,
             "session.uid": self.StudyInstanceUID,
             "session.timestamp": timestamp,
             "acquisition.uid": f"{self.SeriesInstanceUID}_{self.AcquisitionNumber}",
             "acquisition.label": self.SeriesDescription,
             "acquisition.timestamp": timestamp,
+            "file.type": "pfile",
         }
-        return filter_meta(meta)
+        return MetaData(meta)
 
     @property
     def ge_physio_match(self) -> AttrDict:
         """Return the metadata needed for matching GE physio filenames to."""
         psn = self.PulseSequenceName
         start = self.get_acquisition_timestamp()
         duration = self.AcquisitionDuration
```

## fw_file/jpg.py

```diff
@@ -1,10 +1,11 @@
 """JPG file format."""
 import typing as t
 
+from fw_meta import MetaData
 from fw_utils import AnyFile
 from PIL import Image
 
 from .base import FieldsMixin, File
 from .exif import EXIF
 
 
@@ -14,12 +15,16 @@
     def __init__(self, file: AnyFile) -> None:
         """Load and parse JPG files."""
         super().__init__(file)
         img = Image.open(self.file)
         object.__setattr__(self, "fields", EXIF.from_bytes(img.info.get("exif", b"")))
         object.__setattr__(self, "img", img)
 
+    def get_meta(self) -> MetaData:
+        """Return the default Flywheel metadata of the JPG."""
+        return MetaData({"file.type": "image"})
+
     def save(self, file: t.Optional[AnyFile] = None) -> None:
         """Save (potentially modified) data file."""
         with self.open_dst(file) as wfile:
             exif = t.cast(EXIF, self.fields).to_bytes()
             self.img.save(wfile, exif=exif, format="JPEG")
```

## fw_file/json.py

```diff
@@ -1,13 +1,14 @@
 """json file class."""
 import codecs
 import json
 import typing as t
 
 from dotty_dict import Dotty, dotty
+from fw_meta import MetaData
 from fw_utils import AnyFile, AnyPath
 
 from .base import FieldsMixin, File
 
 
 class JSON(FieldsMixin, File):
     """Json file format."""
@@ -21,14 +22,18 @@
         super().__init__(file)
         with self.file as rfile:
             reader = codecs.getreader("utf-8")
             fields = dotty(json.load(reader(rfile)))
         object.__setattr__(self, "fields", fields)
         object.__setattr__(self, "removed", set())
 
+    def get_meta(self) -> MetaData:
+        """Return the default Flywheel metadata of the JSON."""
+        return MetaData({"file.type": "source code"})
+
     def save(self, file: t.Optional[AnyFile] = None) -> None:
         """Saves the current contents to a json file."""
         fields_out = self.to_dict()
         with self.open_dst(file) as wfile:
             out = json.dumps(fields_out)
             wfile.write(out.encode())
 
@@ -60,9 +65,8 @@
                 keys += [f"{k}.{sk}" for sk in subkeys]
         return keys
 
     def get_all_keys(self) -> list:
         """Return all keys (even nested) from the object, sorted by depth."""
         keys = self._get_all_keys(self.fields)
         keys = sorted(keys, key=lambda x: x.count("."), reverse=True)
-
         return keys
```

## fw_file/nifti.py

```diff
@@ -1,9 +1,10 @@
 """NIfTI-1 and NIfTI-2 (.nii.gz) file format."""
 import nibabel
+from fw_meta import MetaData
 
 from .base import AnyPath, File
 
 
 class Nifti(File):
     """NIfTI-1 and NIfTI-2 (.nii.gz) file format."""
 
@@ -16,14 +17,18 @@
         super().__init__(file)
         try:
             object.__setattr__(self, "nifti", nibabel.load(self.localpath))
             object.__setattr__(self, "getattr_proxy", self.nifti)
         except nibabel.filebasedimages.ImageFileError as exc:
             raise ValueError(f"Invalid NIfTI file: {file}") from exc
 
+    def get_meta(self) -> MetaData:
+        """Return the default Flywheel metadata of the Nifti."""
+        return MetaData({"file.type": "nifti"})
+
     def save(self, file: AnyPath = None) -> None:  # type: ignore
         """Save nifti image."""
         nibabel.save(self.nifti, file)
 
     def __getitem__(self, key: str):
         """Get header value by name."""
         return self.nifti.header[key]
```

## fw_file/philips.py

```diff
@@ -1,17 +1,18 @@
 """Philips MR / PARREC header (.par) file format."""
 import re
 import typing as t
 from datetime import datetime
 from decimal import Decimal, InvalidOperation
 
 import dateutil.parser as dt_parser
+from fw_meta import MetaData
 from fw_utils import BinFile
 
-from .base import AnyFile, FieldsMixin, File, filter_meta
+from .base import AnyFile, FieldsMixin, File
 
 PAR_FIELD_RE = r"^\.\s+(?P<name>\w[^:]+).*?:\s*(?P<value>.*)$"
 PAR_KEY_RE = r"^(?P<key>[-\.\w\s/]+).*$"
 
 
 class PARFile(FieldsMixin, File):
     """Philips MR / PARREC header (.par) file format."""
@@ -22,24 +23,24 @@
         Args:
             file (str|Path|file): Filepath (str|Path) or open file to read from.
         """
         super().__init__(file)
         with self.file as rfile:
             object.__setattr__(self, "fields", load_par(rfile))
 
-    def get_default_meta(self) -> t.Dict[str, t.Any]:
+    def get_meta(self) -> MetaData:
         """Return the default Flywheel metadata."""
-        default: t.Dict[str, t.Any] = {
+        meta: dict = {
             "subject.label": self.get("patient_name"),
             "session.label": self.get("examination_name"),
             "acquisition.label": get_acquisition_label(self),
             "acquisition.timestamp": get_acquisition_timestamp(self),
             "file.type": "parrec",
         }
-        return filter_meta(default)
+        return MetaData(meta)
 
     @staticmethod
     def canonize_key(key: str) -> str:
         """Return canonized string form for a given field name."""
         return canonize_key(key)
 
     def save(self, file: AnyFile = None) -> None:
```

## fw_file/png.py

```diff
@@ -3,33 +3,38 @@
 import struct
 import typing as t
 import zlib
 from collections import Counter
 from datetime import datetime
 
 import png
+from fw_meta import MetaData
 from fw_utils import AnyFile, BinFile
 from PIL import Image, PngImagePlugin
 
 from .base import FieldsMixin, File
 from .exif import EXIF
 
 
 class PNG(FieldsMixin, File):
-    """PNG data-file class."""
+    """PNG file class."""
 
     def __init__(self, file: AnyFile) -> None:
         """Load and parse PNG files."""
         super().__init__(file)
         with self.file as rfile:
             fields = load_png(rfile)
         img = Image.open(self.file)
         object.__setattr__(self, "fields", fields)
         object.__setattr__(self, "img", img)
 
+    def get_meta(self) -> MetaData:
+        """Return the default Flywheel metadata of the PNG."""
+        return MetaData({"file.type": "image"})
+
     @staticmethod
     def canonize_key(key):
         """Return canonized string form for a given field name."""
         return canonize_key(key)
 
     def save(self, file: t.Optional[AnyFile] = None) -> None:
         """Save (potentially modified) data file."""
```

## fw_file/siemens.py

```diff
@@ -5,17 +5,18 @@
 - .rda - MR Spectroscopy
 """
 import io
 import re
 import struct
 import typing as t
 
+from fw_meta import MetaData
 from fw_utils import BinFile
 
-from .base import AnyFile, FieldsMixin, File, ReadOnly, filter_meta, parse_yaml_value
+from .base import AnyFile, FieldsMixin, File, ReadOnly, parse_yaml_value
 from .dicom import DICOM, TagType, utils
 
 #######
 # DAT #
 #######
 
 DAT_HEADER = b"<XProtocol>"
@@ -39,34 +40,34 @@
 
     def __init__(self, file: AnyFile) -> None:
         """Load and parse Siemens MR RAW (.dat) file."""
         super().__init__(file)
         with self.file as rfile:
             object.__setattr__(self, "fields", load_dat(rfile))
 
-    def get_default_meta(self) -> t.Dict[str, t.Any]:
+    def get_meta(self) -> MetaData:
         """Return default Flywheel metadata extraction."""
         firstname, lastname = utils.get_patient_name(self)
         timestamp = self.get("PrepareTimestamp")
-        defaults: t.Dict[str, t.Any] = {
+        meta: dict = {
             "subject.label": self.get("PatientID"),
             "subject.firstname": firstname,
             "subject.lastname": lastname,
             "subject.sex": DAT_SEX_MAP.get(int(self.get("PatientSex") or 3)),
             "session.uid": self.get("StudyInstanceUID"),
             "session.label": utils.get_session_label(self),
             "session.age": utils.get_session_age(self),
             "session.weight": self.get("PatientWeight"),
             "session.timestamp": utils.get_session_timestamp(self) or timestamp,
             "acquisition.uid": utils.get_acquisition_uid(self),
             "acquisition.label": utils.get_acquisition_label(self),
             "acquisition.timestamp": utils.get_acquisition_timestamp(self) or timestamp,
             "file.type": "raw/siemens",  # TODO add core filetype
         }
-        return filter_meta(defaults)
+        return MetaData(meta)
 
 
 def load_dat(file: BinFile) -> t.Dict[str, t.Any]:
     """Return fields parsed from a Siemens MR RAW (.dat) file."""
     first_line = file.readline()
     if not first_line.strip().endswith(DAT_HEADER):
         raise ValueError(f"Invalid DAT: cannot find header start: {DAT_HEADER!r}")
@@ -150,19 +151,19 @@
         """Return dataelement iterator."""
         return iter(self.dcm)
 
     def __len__(self) -> int:
         """Return the number of elements in the dataset."""
         return len(self.dcm)
 
-    def get_default_meta(self) -> t.Dict[str, t.Any]:
+    def get_meta(self) -> MetaData:
         """Return the default Flywheel metadata extracted from the PTD."""
-        meta = self.dcm.get_default_meta()
-        meta["file.type"] = "ptd"  # TODO cross-check with core's filetypes
-        meta["file.name"] = meta["file.name"].replace(".dcm", ".ptd")
+        meta = self.dcm.get_meta()
+        meta["file.type"] = "ptd"  # TODO add core filetype?
+        meta.pop("file.name", None)
         return meta
 
     def save(self, file: AnyFile = None) -> None:
         """Save file."""
         buff = io.BytesIO()
         with self.file as rfile:
             buff.write(rfile.read(self.dcm_start))
@@ -208,31 +209,31 @@
             file (str|Path|file): Filepath (str|Path) or open file to read from.
         """
         super().__init__(file)
         with self.file as rfile:
             object.__setattr__(self, "fields", load_rda(rfile))
             object.__setattr__(self, "offset", rfile.tell())
 
-    def get_default_meta(self) -> t.Dict[str, t.Any]:
+    def get_meta(self) -> MetaData:
         """Return the default Flywheel metadata for the RDA file."""
         firstname, lastname = utils.get_patient_name(self)
-        meta: t.Dict[str, t.Any] = {
+        meta: dict = {
             "subject.label": self.get("PatientID"),
             "subject.firstname": firstname,
             "subject.lastname": lastname,
             "subject.sex": self.get("PatientSex"),
             "session.label": utils.get_session_label(self),
             "session.age": utils.get_session_age(self),
             "session.weight": self.get("PatientWeight"),
             "session.timestamp": utils.get_session_timestamp(self),
             "acquisition.label": utils.get_acquisition_label(self),
             "acquisition.timestamp": utils.get_acquisition_timestamp(self),
-            "file.type": "spectroscopy",  # TODO add core filetype
+            "file.type": "spectroscopy",  # TODO add core filetype?
         }
-        return filter_meta(meta)
+        return MetaData(meta)
 
     def save(self, file: t.Optional[AnyFile] = None) -> None:
         """Save RDA file."""
         bytesio = io.BytesIO()
         bytesio.write(dump_rda(self.fields))
         with self.file as rfile:
             rfile.seek(self.offset)
```

## Comparing `fw_file-2.4.1.dist-info/LICENSE` & `fw_file-3.0.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `fw_file-2.4.1.dist-info/METADATA` & `fw_file-3.0.0.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: fw-file
-Version: 2.4.1
+Version: 3.0.0
 Summary: Unified data-file interface
 Home-page: https://gitlab.com/flywheel-io/tools/lib/fw-file
 License: MIT
 Keywords: Flywheel,parse,medical,file,metadata,extract,DICOM,RAW,MR,CT,PET,NIfTI,JPG,JPEG,PNG,Bruker,ParaVision,GE,PFile,Philips,PARREC,Siemens,PTD
 Author: Flywheel
 Author-email: support@flywheel.io
 Requires-Python: >=3.8,<4.0
@@ -17,20 +17,18 @@
 Classifier: Topic :: Scientific/Engineering :: Medical Science Apps.
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Provides-Extra: all
 Provides-Extra: jpg
 Provides-Extra: json
 Provides-Extra: nifti
 Provides-Extra: png
-Requires-Dist: dicom-validator (>=0.3.5,<0.4.0)
+Requires-Dist: dicom-validator (>=0.3.5,<1)
 Requires-Dist: dotty-dict (>=1.3.1,<2.0.0) ; extra == "all" or extra == "json"
-Requires-Dist: fw-meta (>=3.1,<4.0)
-Requires-Dist: fw-utils (>=3,<5)
-Requires-Dist: importlib-metadata (>=4.8.2,<5.0.0) ; python_version < "3.8"
-Requires-Dist: memoization (>=0,<1)
+Requires-Dist: fw-meta (>=4.0.0)
+Requires-Dist: fw-utils (>=4.3.4)
 Requires-Dist: natsort (>=8.0.0,<9.0.0)
 Requires-Dist: nibabel (>=4.0.2,<5.0.0) ; extra == "all" or extra == "nifti"
 Requires-Dist: piexif (>=1.1.3,<2.0.0) ; extra == "all" or extra == "jpg" or extra == "png"
 Requires-Dist: pillow (>=9,<10) ; extra == "all" or extra == "jpg" or extra == "png"
 Requires-Dist: pydantic (>=1.7.3,<2.0.0)
 Requires-Dist: pydicom (>=2.3.0,<3.0.0)
 Requires-Dist: pypng (>=0,<1) ; extra == "all" or extra == "png"
@@ -118,28 +116,25 @@
 ```python
 dcm.PatientAge = "065Y"
 del dcm["PatientAge"]
 ```
 
 ### Metadata
 
-Flywheel metadata can be extracted using the `get_meta()` method. To customize
-fields - eg. to parse group/project info from a routing string - init files with
-a [`MetaExtractor`](https://gitlab.com/flywheel-io/tools/lib/fw-meta) instance:
+Flywheel metadata can be extracted using the `get_meta()` method:
 
 ```python
 from fw_file.dicom import DICOM
 dcm = DICOM("dataset.dcm")
-dcm.get_meta(mappings={"StudyComments": "[fw://]{group}[/{project}]"}) == {
 dcm.get_meta() == {
-    "group._id": "neuro",  # parsed from StudyComments="fw://neuro/Amnesia"
-    "project.label": "Amnesia",
     "subject.label": "PatientID",
     "session.label": "StudyDescription",
+    "session.uid": "1.2.3",  # StudyInstanceUID
     "acquisition.label": "SeriesDescription",
+    "acquisition.uid": "4.5.6",  # SeriesInstanceUID
     # and much, much more...
 }
 ```
 
 ### Saving
 
 ```python
@@ -260,22 +255,14 @@
 1 (WARN), additional options are 2 (RAISE) and 0 (IGNORE).
 
 ```bash
 FW_DCM_READING_VALIDATION_MODE=1
 FW_DCM_WRITING_VALIDATION_MODE=1
 ```
 
-To track any changes like `VR` inferences on (raw) data elements DICOMs can be
-instantiated with `track=True`:
-
-```python
-dcm = DICOM("dataset.dcm", decode=True, track=True)
-dcm.tracker.data_elements[0].events == ["Replace VR: UN -> CS"]
-```
-
 ## Development
 
 Install the project using `poetry` and enable `pre-commit`:
 
 ```bash
 poetry install --extras "all"
 pre-commit install
```

## Comparing `fw_file-2.4.1.dist-info/RECORD` & `fw_file-3.0.0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-fw_file/__init__.py,sha256=T3Tnv5R3uXDGZ-lp03D4noosiBfs9Bu6GzvDUUltgTg,229
-fw_file/base.py,sha256=skak6Ex5bfSPwzN48F5dsJvfEz-vcg-Kn84SkfhhHzI,8377
-fw_file/bruker.py,sha256=yM_xq-cdgJ8zPKjhHBBQ8YrJRvj1U3JHmzMj8MR3oUA,3184
+fw_file/__init__.py,sha256=HcTPX2KDhfQIXqqiLSUwjZCu3cdLhoxFMalisKK76Kg,121
+fw_file/base.py,sha256=2ou7pZ2NQeXAZcgO52Uo2-5n18yJDvRU_ca50kdoCb4,6917
+fw_file/bruker.py,sha256=mGs4TYGVcEz3cSQid83UF4gg7AN738gc0rOOdTt8Axw,3179
 fw_file/dicom/__init__.py,sha256=Dh7WgsG2PoelxuqtM5-_5C0dbAi_CG_TW35_N5aRBxY,1515
 fw_file/dicom/config.py,sha256=LDFuDZGl0Uk0RnIH9WJLR1RdwNZ-bXb-rtFjm5oziDc,6011
 fw_file/dicom/dcmdict.py,sha256=y_NpdECSc-klhPnf69qpYbjwy_TsI2ikXwLckZIDVcA,44285
-fw_file/dicom/dicom.py,sha256=XavmLF1TWEQh4Py6_7v9WNpkF8UJU1iyzC23isVA5Yk,17633
+fw_file/dicom/dicom.py,sha256=NLLwYOpSjhLSFtPja4B7vv_lfjsyDAGwXHr5Ag3Lsvs,17412
 fw_file/dicom/fixers.py,sha256=SAr-XspzAgblkBMTNuexNQyqoy5QE7qz70hLmdXzjwU,18949
-fw_file/dicom/reader.py,sha256=NjypbIYiYvWBLIN3HKVZakkyuVb9MQOVlSfd7oGsBWQ,18042
-fw_file/dicom/series.py,sha256=Fe6_OwInAp8ZU1INnkvRP7yf0BdIbuBNx32jCngY0hI,27658
+fw_file/dicom/reader.py,sha256=i5KWmmT8nhxNTCdc3xW1yJZwDWNYkgeV9Rb-u6IMwkE,17915
+fw_file/dicom/series.py,sha256=m0jfMoMMLcOqLbu6TRy4kPp_4xv5vfRvK0wEqEqyPZ8,24712
 fw_file/dicom/standard/2023b/json/dict_info.json,sha256=G57E1IYRQssb6NVgA6hRA-qgrTAk-b_JgP7QUBjym1k,421033
 fw_file/dicom/standard/2023b/json/iod_info.json,sha256=kOR8UWIR7Gyut7WVGHmEXTJymjF8XYty6lqJWsoTwMs,188832
 fw_file/dicom/standard/2023b/json/module_info.json,sha256=juOJ1KQVVhL0J24fzGDl2joL6MtsBf223_PpkSvfwS8,528709
 fw_file/dicom/standard/2023b/json/uid_info.json,sha256=tkQbqwXGQO6_WsPxbNwf4eHQ8tdANM_PisWW8zqc864,31221
 fw_file/dicom/standard/2023b/json/version,sha256=uPt4p7NTFCveH-UkqLcBd1htCl0u6YAhWEHipHmPbBs,5
 fw_file/dicom/standard/2023c/json/dict_info.json,sha256=D9Ddu8tEEBaEGt3deC-UBirNO37E0Z7LCKvCz2Tux88,422965
 fw_file/dicom/standard/2023c/json/iod_info.json,sha256=W2Umw1578VfNoWNYI5PqtjyQGVgUMUYt1J4SqljLR-U,191779
 fw_file/dicom/standard/2023c/json/module_info.json,sha256=yDCIM3bOeC0F25P1H22HCNi4A0B07zz71ex_Neb3mqQ,531128
 fw_file/dicom/standard/2023c/json/uid_info.json,sha256=L-OpmphiqsvV1wTnPJJ1zPWrUaDruhn-7Fy1nOOhecw,31351
 fw_file/dicom/standard/2023c/json/version,sha256=chs29ukegsplmSIlFfRBz4srybz6fhCYDk5F04KGouQ,5
 fw_file/dicom/standard/editions.json,sha256=114U8pY1673mbEAP4J1FXuc_tgBjr4xKRyGgHfbp5J4,353
-fw_file/dicom/utils.py,sha256=ntJ5f94TJ1AkHku3lw_KtYzzyErZcyl1gGj1X1fAxAA,9047
+fw_file/dicom/utils.py,sha256=1lQYATzlNzoh1oDjoU0hzmFXR7hDIgKg4EA5NM2R-u4,8995
 fw_file/dicom/validation.py,sha256=kIa7jZ0gvV8vaLYSXocFjKI6H73zd_Lkai4x7C9x8wM,6880
 fw_file/exif.py,sha256=kPm3V9pByRzKMtMHRBljgAYA7g3Mk-kIKFLAD-bJMCA,3103
-fw_file/ge.py,sha256=jpd5_MTR5rf3lnX082pCSFYgbAXwDlO41-Xyp8QbrX4,12581
-fw_file/jpg.py,sha256=3qLSgZztivZ3so1CYD-CBC-x59kahOZO5Y5PIbVDX10,778
-fw_file/json.py,sha256=ufQ2YnNsyWocsB1yrTFEncHCKmIr7OeiwEUqOm3n3VY,2186
-fw_file/nifti.py,sha256=hKjKGhUUTrqIv6QsPe2qWJqteLbZPHZ52yWPf788iOQ,1440
-fw_file/philips.py,sha256=E9gyia6ZpyTg2RYktYFFDBNaI-plmYSVunhhWgfR_GA,5265
-fw_file/png.py,sha256=dqpSGSPg7Ox_B4JOFcSD2Uy5y0fXhM5O2tZgI-aZ4kU,8385
+fw_file/ge.py,sha256=S8qt5Rb4N2ml7OaFeOiygO_C-HLw21tqFAmjRFdLaVU,12596
+fw_file/jpg.py,sha256=bE8IUW1TwbpyVWeCsNXKRffu5efXY00_NVlavyiKvWw,955
+fw_file/json.py,sha256=a2596E4bMEuICBimfolR1PHws3zY84j0dhCZUTi6X14,2369
+fw_file/nifti.py,sha256=0mgppj8iICizshcV2RDFKcK8nlzB_xN9kDDUhoTnr-U,1619
+fw_file/philips.py,sha256=7IWfRFCp2Y9L8NND2DVhldmRdFh8076_716OqmwLnT0,5240
+fw_file/png.py,sha256=_WlCzGZwGgnZVTcRjCQxnIMYkshNYHz_DWSC6dyIs-M,8557
 fw_file/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-fw_file/siemens.py,sha256=IYUQkiEJs9_25hb50NgdoHM8cw-l5iATlI1V1EdQwpc,10521
-fw_file-2.4.1.dist-info/LICENSE,sha256=3QLwoJgDkLWRKwcyYzPn3vLqTHhbdltfjFYeBOZSlDY,1078
-fw_file-2.4.1.dist-info/METADATA,sha256=a7a0sIltKvcshmmj9rjTXT7M2ykezbAdqpU1mtmFLjs,9569
-fw_file-2.4.1.dist-info/WHEEL,sha256=Zb28QaM1gQi8f4VCBhsUklF61CTlNYfs9YAZn-TOGFk,88
-fw_file-2.4.1.dist-info/RECORD,,
+fw_file/siemens.py,sha256=OpgHqifsWjdPZcGpXe4mn92afLdEIyJG_dvC3DUb0Ws,10385
+fw_file-3.0.0.dist-info/LICENSE,sha256=3QLwoJgDkLWRKwcyYzPn3vLqTHhbdltfjFYeBOZSlDY,1078
+fw_file-3.0.0.dist-info/METADATA,sha256=vPT60zD_Au3h0m3aQAsmKbhGaVoC45w8IId5JI3dgNc,8951
+fw_file-3.0.0.dist-info/WHEEL,sha256=Zb28QaM1gQi8f4VCBhsUklF61CTlNYfs9YAZn-TOGFk,88
+fw_file-3.0.0.dist-info/RECORD,,
```

