# Comparing `tmp/flax-0.7.0.tar.gz` & `tmp/flax-0.7.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flax-0.7.0.tar", last modified: Sat Jul  8 01:21:39 2023, max compression
+gzip compressed data, was "flax-0.7.1.tar", last modified: Tue Aug  1 01:59:13 2023, max compression
```

## Comparing `flax-0.7.0.tar` & `flax-0.7.1.tar`

### file list

```diff
@@ -1,420 +1,421 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.768545 flax-0.7.0/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.712545 flax-0.7.0/.github/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.712545 flax-0.7.0/.github/ISSUE_TEMPLATE/
--rw-r--r--   0 runner    (1001) docker     (123)      792 2023-07-08 01:21:24.000000 flax-0.7.0/.github/ISSUE_TEMPLATE/bug_report.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.712545 flax-0.7.0/.github/analytics/
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/README.md
--rw-r--r--   0 runner    (1001) docker     (123)    13763 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/get_repo_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     1677 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/issue_activity_since_date.gql
--rw-r--r--   0 runner    (1001) docker     (123)     2016 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/pr_data_query.gql
--rw-r--r--   0 runner    (1001) docker     (123)       34 2023-07-08 01:21:24.000000 flax-0.7.0/.github/analytics/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-07-08 01:21:24.000000 flax-0.7.0/.github/pull_request_template.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.712545 flax-0.7.0/.github/workflows/
--rw-r--r--   0 runner    (1001) docker     (123)     6177 2023-07-08 01:21:24.000000 flax-0.7.0/.github/workflows/build.yml
--rw-r--r--   0 runner    (1001) docker     (123)      834 2023-07-08 01:21:24.000000 flax-0.7.0/.github/workflows/pythonpublish.yml
--rw-r--r--   0 runner    (1001) docker     (123)      138 2023-07-08 01:21:24.000000 flax-0.7.0/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)      786 2023-07-08 01:21:24.000000 flax-0.7.0/.pre-commit-config.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      560 2023-07-08 01:21:24.000000 flax-0.7.0/.readthedocs.yml
--rw-r--r--   0 runner    (1001) docker     (123)      293 2023-07-08 01:21:24.000000 flax-0.7.0/AUTHORS
--rw-r--r--   0 runner    (1001) docker     (123)    19306 2023-07-08 01:21:24.000000 flax-0.7.0/CHANGELOG.md
--rw-r--r--   0 runner    (1001) docker     (123)    11309 2023-07-08 01:21:24.000000 flax-0.7.0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     8602 2023-07-08 01:21:39.768545 flax-0.7.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     7930 2023-07-08 01:21:24.000000 flax-0.7.0/README.md
--rw-r--r--   0 runner    (1001) docker     (123)      110 2023-07-08 01:21:24.000000 flax-0.7.0/contributing.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/dev/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/dev/.devcontainer/
--rw-r--r--   0 runner    (1001) docker     (123)     2643 2023-07-08 01:21:24.000000 flax-0.7.0/dev/.devcontainer/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (123)     1806 2023-07-08 01:21:24.000000 flax-0.7.0/dev/.devcontainer/devcontainer.json
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-08 01:21:24.000000 flax-0.7.0/dev/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     4515 2023-07-08 01:21:24.000000 flax-0.7.0/dev/update_requirements.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/docs/
--rw-r--r--   0 runner    (1001) docker     (123)       18 2023-07-08 01:21:24.000000 flax-0.7.0/docs/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)      634 2023-07-08 01:21:24.000000 flax-0.7.0/docs/Makefile
--rw-r--r--   0 runner    (1001) docker     (123)     5359 2023-07-08 01:21:24.000000 flax-0.7.0/docs/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/docs/_ext/
--rw-r--r--   0 runner    (1001) docker     (123)     4251 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_ext/codediff.py
--rw-r--r--   0 runner    (1001) docker     (123)     3389 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_ext/codediff_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2253 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_ext/flax_module.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.708545 flax-0.7.0/docs/_static/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/docs/_static/css/
--rw-r--r--   0 runner    (1001) docker     (123)      309 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_static/css/flax_theme.css
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.708545 flax-0.7.0/docs/_templates/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.716545 flax-0.7.0/docs/_templates/autosummary/
--rw-r--r--   0 runner    (1001) docker     (123)      463 2023-07-08 01:21:24.000000 flax-0.7.0/docs/_templates/autosummary/flax_module.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.720545 flax-0.7.0/docs/api_reference/
--rw-r--r--   0 runner    (1001) docker     (123)      130 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.config.rst
--rw-r--r--   0 runner    (1001) docker     (123)      321 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.core.frozen_dict.rst
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.errors.rst
--rw-r--r--   0 runner    (1001) docker     (123)      365 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.jax_utils.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.720545 flax-0.7.0/docs/api_reference/flax.linen/
--rw-r--r--   0 runner    (1001) docker     (123)     1143 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/activation_functions.rst
--rw-r--r--   0 runner    (1001) docker     (123)      192 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/decorators.rst
--rw-r--r--   0 runner    (1001) docker     (123)      350 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)      232 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/init_apply.rst
--rw-r--r--   0 runner    (1001) docker     (123)     1099 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/initializers.rst
--rw-r--r--   0 runner    (1001) docker     (123)      162 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/inspection.rst
--rw-r--r--   0 runner    (1001) docker     (123)     2220 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/layers.rst
--rw-r--r--   0 runner    (1001) docker     (123)      278 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/module.rst
--rw-r--r--   0 runner    (1001) docker     (123)      295 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/profiling.rst
--rw-r--r--   0 runner    (1001) docker     (123)      931 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/spmd.rst
--rw-r--r--   0 runner    (1001) docker     (123)      566 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/transformations.rst
--rw-r--r--   0 runner    (1001) docker     (123)      213 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.linen/variable.rst
--rw-r--r--   0 runner    (1001) docker     (123)      480 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.serialization.rst
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.struct.rst
--rw-r--r--   0 runner    (1001) docker     (123)      275 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.traceback_util.rst
--rw-r--r--   0 runner    (1001) docker     (123)     1212 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.training.rst
--rw-r--r--   0 runner    (1001) docker     (123)      798 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/flax.traverse_util.rst
--rw-r--r--   0 runner    (1001) docker     (123)      250 2023-07-08 01:21:24.000000 flax-0.7.0/docs/api_reference/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)     5080 2023-07-08 01:21:24.000000 flax-0.7.0/docs/conf.py
--rw-r--r--   0 runner    (1001) docker     (123)     7338 2023-07-08 01:21:24.000000 flax-0.7.0/docs/conf_sphinx_patch.py
--rw-r--r--   0 runner    (1001) docker     (123)    11920 2023-07-08 01:21:24.000000 flax-0.7.0/docs/contributing.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.720545 flax-0.7.0/docs/developer_notes/
--rw-r--r--   0 runner    (1001) docker     (123)      153 2023-07-08 01:21:24.000000 flax-0.7.0/docs/developer_notes/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    18097 2023-07-08 01:21:24.000000 flax-0.7.0/docs/developer_notes/lift.md
--rw-r--r--   0 runner    (1001) docker     (123)    22018 2023-07-08 01:21:24.000000 flax-0.7.0/docs/developer_notes/module_lifecycle.rst
--rw-r--r--   0 runner    (1001) docker     (123)      184 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples.rst
--rw-r--r--   0 runner    (1001) docker     (123)     4921 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples_community_examples.rst
--rw-r--r--   0 runner    (1001) docker     (123)     4422 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples_core_examples.rst
--rw-r--r--   0 runner    (1001) docker     (123)    22579 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples_google_research_examples.rst
--rw-r--r--   0 runner    (1001) docker     (123)     2028 2023-07-08 01:21:24.000000 flax-0.7.0/docs/examples_repositories_that_use_flax.rst
--rw-r--r--   0 runner    (1001) docker     (123)    20991 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flax.png
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.720545 flax-0.7.0/docs/flip/
--rw-r--r--   0 runner    (1001) docker     (123)      648 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/0000-template.md
--rw-r--r--   0 runner    (1001) docker     (123)    17256 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/1009-optimizer-api.md
--rw-r--r--   0 runner    (1001) docker     (123)     8189 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/1777-default-dtype.md
--rw-r--r--   0 runner    (1001) docker     (123)    10424 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/2434-general-metadata.md
--rw-r--r--   0 runner    (1001) docker     (123)     4099 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/2974-kw-only-dataclasses.md
--rw-r--r--   0 runner    (1001) docker     (123)     1404 2023-07-08 01:21:24.000000 flax-0.7.0/docs/flip/README.md
--rw-r--r--   0 runner    (1001) docker     (123)   102681 2023-07-08 01:21:24.000000 flax-0.7.0/docs/getting_started.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    16394 2023-07-08 01:21:24.000000 flax-0.7.0/docs/getting_started.md
--rw-r--r--   0 runner    (1001) docker     (123)     6661 2023-07-08 01:21:24.000000 flax-0.7.0/docs/glossary.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.728545 flax-0.7.0/docs/guides/
--rw-r--r--   0 runner    (1001) docker     (123)     4087 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/arguments.md
--rw-r--r--   0 runner    (1001) docker     (123)     9149 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/batch_norm.rst
--rw-r--r--   0 runner    (1001) docker     (123)     9329 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/convert_pytorch_to_flax.rst
--rw-r--r--   0 runner    (1001) docker     (123)    11050 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/dropout.rst
--rw-r--r--   0 runner    (1001) docker     (123)    10488 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/ensembling.rst
--rw-r--r--   0 runner    (1001) docker     (123)    12735 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/extracting_intermediates.rst
--rw-r--r--   0 runner    (1001) docker     (123)    40409 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/flax_basics.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    22504 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/flax_basics.md
--rw-r--r--   0 runner    (1001) docker     (123)    63884 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/flax_on_pjit.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    24655 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/flax_on_pjit.md
--rw-r--r--   0 runner    (1001) docker     (123)     7442 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/full_eval.rst
--rw-r--r--   0 runner    (1001) docker     (123)    20294 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/haiku_migration_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)      265 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)      225 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_converting_and_upgrading.rst
--rw-r--r--   0 runner    (1001) docker     (123)       80 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_data_preprocessing.rst
--rw-r--r--   0 runner    (1001) docker     (123)      201 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_flax_fundamentals.rst
--rw-r--r--   0 runner    (1001) docker     (123)      110 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_model_inspection.rst
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_parallel_training.rst
--rw-r--r--   0 runner    (1001) docker     (123)      152 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/index_training_techniques.rst
--rw-r--r--   0 runner    (1001) docker     (123)    38261 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/jax_for_the_impatient.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    21301 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/jax_for_the_impatient.md
--rw-r--r--   0 runner    (1001) docker     (123)    17271 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/linen_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     8175 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/lr_schedule.rst
--rw-r--r--   0 runner    (1001) docker     (123)     7142 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/model_surgery.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     4372 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/model_surgery.md
--rw-r--r--   0 runner    (1001) docker     (123)    10621 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/optax_update_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     9077 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/orbax_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     3984 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/regular_dict_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     6129 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/rnncell_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     3125 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/setup_or_nncompact.rst
--rw-r--r--   0 runner    (1001) docker     (123)     6076 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/state_params.rst
--rw-r--r--   0 runner    (1001) docker     (123)    11487 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/transfer_learning.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     8006 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/transfer_learning.md
--rw-r--r--   0 runner    (1001) docker     (123)    53165 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/use_checkpointing.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    26620 2023-07-08 01:21:24.000000 flax-0.7.0/docs/guides/use_checkpointing.md
--rw-r--r--   0 runner    (1001) docker     (123)     8273 2023-07-08 01:21:24.000000 flax-0.7.0/docs/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)     8090 2023-07-08 01:21:24.000000 flax-0.7.0/docs/mission.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.728545 flax-0.7.0/docs/notebooks/
--rw-r--r--   0 runner    (1001) docker     (123)     7391 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/flax_sharp_bits.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     5941 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/flax_sharp_bits.md
--rw-r--r--   0 runner    (1001) docker     (123)    18559 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/full_eval.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     9794 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/full_eval.md
--rw-r--r--   0 runner    (1001) docker     (123)      358 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    40759 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/linen_intro.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    21840 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/linen_intro.md
--rw-r--r--   0 runner    (1001) docker     (123)    21338 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/optax_update_guide.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)    11407 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/optax_update_guide.md
--rw-r--r--   0 runner    (1001) docker     (123)    13947 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/orbax_upgrade_guide.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     8587 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/orbax_upgrade_guide.md
--rw-r--r--   0 runner    (1001) docker     (123)     9512 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/state_params.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     6687 2023-07-08 01:21:24.000000 flax-0.7.0/docs/notebooks/state_params.md
--rw-r--r--   0 runner    (1001) docker     (123)     2510 2023-07-08 01:21:24.000000 flax-0.7.0/docs/overview.md
--rw-r--r--   0 runner    (1001) docker     (123)     7604 2023-07-08 01:21:24.000000 flax-0.7.0/docs/philosophy.md
--rw-r--r--   0 runner    (1001) docker     (123)      557 2023-07-08 01:21:24.000000 flax-0.7.0/docs/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.728545 flax-0.7.0/examples/
--rw-r--r--   0 runner    (1001) docker     (123)      743 2023-07-08 01:21:24.000000 flax-0.7.0/examples/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.728545 flax-0.7.0/examples/cloud/
--rw-r--r--   0 runner    (1001) docker     (123)     4635 2023-07-08 01:21:24.000000 flax-0.7.0/examples/cloud/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     8965 2023-07-08 01:21:24.000000 flax-0.7.0/examples/cloud/launch_gce.py
--rw-r--r--   0 runner    (1001) docker     (123)     1650 2023-07-08 01:21:24.000000 flax-0.7.0/examples/cloud/startup_script.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.732545 flax-0.7.0/examples/imagenet/
--rw-r--r--   0 runner    (1001) docker     (123)     9387 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.732545 flax-0.7.0/examples/imagenet/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     2192 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     1305 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/fake_data_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     1670 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/tpu.py
--rw-r--r--   0 runner    (1001) docker     (123)     1055 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/v100_x8.py
--rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/configs/v100_x8_mixed_precision.py
--rw-r--r--   0 runner    (1001) docker     (123)   294627 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/imagenet.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     3360 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/imagenet_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     2235 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/imagenet_fake_data_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     8204 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2175 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/main.py
--rw-r--r--   0 runner    (1001) docker     (123)     4529 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     1971 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/models_test.py
--rw-r--r--   0 runner    (1001) docker     (123)      341 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)    12701 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     3061 2023-07-08 01:21:24.000000 flax-0.7.0/examples/imagenet/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.732545 flax-0.7.0/examples/linen_design_test/
--rw-r--r--   0 runner    (1001) docker     (123)     6470 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/attention_simple.py
--rw-r--r--   0 runner    (1001) docker     (123)     3214 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1283 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/dense.py
--rw-r--r--   0 runner    (1001) docker     (123)     1346 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/linear_regression.py
--rw-r--r--   0 runner    (1001) docker     (123)     2385 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/mlp_explicit.py
--rw-r--r--   0 runner    (1001) docker     (123)     1996 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/mlp_inline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1926 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/mlp_lazy.py
--rw-r--r--   0 runner    (1001) docker     (123)     1540 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/tied_autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2270 2023-07-08 01:21:24.000000 flax-0.7.0/examples/linen_design_test/weight_std.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/lm1b/
--rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/lm1b/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     3443 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)    12698 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3209 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2240 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/main.py
--rw-r--r--   0 runner    (1001) docker     (123)    12495 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/models.py
--rw-r--r--   0 runner    (1001) docker     (123)      343 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     4969 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/temperature_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/temperature_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5618 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    20239 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     1994 2023-07-08 01:21:24.000000 flax-0.7.0/examples/lm1b/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/mnist/
--rw-r--r--   0 runner    (1001) docker     (123)     1741 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/mnist/configs/
--rw-r--r--   0 runner    (1001) docker     (123)      912 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/main.py
--rw-r--r--   0 runner    (1001) docker     (123)    99011 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/mnist.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     2366 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/mnist_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)      298 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     5301 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     2226 2023-07-08 01:21:24.000000 flax-0.7.0/examples/mnist/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.736545 flax-0.7.0/examples/nlp_seq/
--rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     8010 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3868 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6643 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/models.py
--rw-r--r--   0 runner    (1001) docker     (123)       60 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)    14128 2023-07-08 01:21:24.000000 flax-0.7.0/examples/nlp_seq/train.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.740545 flax-0.7.0/examples/ogbg_molpcba/
--rw-r--r--   0 runner    (1001) docker     (123)     4486 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.740545 flax-0.7.0/examples/ogbg_molpcba/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     1551 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/configs/default_graph_net.py
--rw-r--r--   0 runner    (1001) docker     (123)     1946 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/configs/hparam_sweep.py
--rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/configs/test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8256 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2554 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2248 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/main.py
--rw-r--r--   0 runner    (1001) docker     (123)     6891 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     5223 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/models_test.py
--rw-r--r--   0 runner    (1001) docker     (123)  1111228 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/ogbg_molpcba.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     4841 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)      329 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)    13830 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/train.py
--rw-r--r--   0 runner    (1001) docker     (123)    12218 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ogbg_molpcba/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.740545 flax-0.7.0/examples/ppo/
--rw-r--r--   0 runner    (1001) docker     (123)     2501 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     2593 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/agent.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.740545 flax-0.7.0/examples/ppo/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     1954 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     2451 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/env_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2252 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    13175 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/ppo_lib.py
--rw-r--r--   0 runner    (1001) docker     (123)     5281 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/ppo_lib_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/ppo_main.py
--rw-r--r--   0 runner    (1001) docker     (123)      147 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     8715 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/seed_rl_atari_preprocessing.py
--rw-r--r--   0 runner    (1001) docker     (123)     1895 2023-07-08 01:21:24.000000 flax-0.7.0/examples/ppo/test_episodes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/seq2seq/
--rw-r--r--   0 runner    (1001) docker     (123)      913 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     5558 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     4361 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/models.py
--rw-r--r--   0 runner    (1001) docker     (123)       65 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)    25401 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/seq2seq.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     7095 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     3269 2023-07-08 01:21:24.000000 flax-0.7.0/examples/seq2seq/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/sst2/
--rw-r--r--   0 runner    (1001) docker     (123)     1893 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/README.md
--rwxr-xr-x   0 runner    (1001) docker     (123)     2031 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/build_vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/sst2/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/configs/default.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    10057 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3528 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2168 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/main.py
--rw-r--r--   0 runner    (1001) docker     (123)    14299 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     3558 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/models_test.py
--rw-r--r--   0 runner    (1001) docker     (123)      156 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     9117 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/sst2.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     9402 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     2127 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/train_test.py
--rw-r--r--   0 runner    (1001) docker     (123)   117898 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/vocab.txt
--rwxr-xr-x   0 runner    (1001) docker     (123)     4421 2023-07-08 01:21:24.000000 flax-0.7.0/examples/sst2/vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/vae/
--rw-r--r--   0 runner    (1001) docker     (123)      593 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     1458 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1697 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/main.py
--rw-r--r--   0 runner    (1001) docker     (123)     1775 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     2152 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/reconstruction.png
--rw-r--r--   0 runner    (1001) docker     (123)      113 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.744545 flax-0.7.0/examples/vae/results/
--rw-r--r--   0 runner    (1001) docker     (123)        6 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/results/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)    43139 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/sample.png
--rw-r--r--   0 runner    (1001) docker     (123)     4574 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     3583 2023-07-08 01:21:24.000000 flax-0.7.0/examples/vae/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.748545 flax-0.7.0/examples/wmt/
--rw-r--r--   0 runner    (1001) docker     (123)     6106 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     7394 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/bleu.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.748545 flax-0.7.0/examples/wmt/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     3482 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (123)    15118 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/decode.py
--rw-r--r--   0 runner    (1001) docker     (123)    13174 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3172 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2216 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/main.py
--rw-r--r--   0 runner    (1001) docker     (123)    19064 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/models.py
--rw-r--r--   0 runner    (1001) docker     (123)      398 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     5619 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    23635 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     1995 2023-07-08 01:21:24.000000 flax-0.7.0/examples/wmt/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.748545 flax-0.7.0/flax/
--rw-r--r--   0 runner    (1001) docker     (123)      908 2023-07-08 01:21:24.000000 flax-0.7.0/flax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4062 2023-07-08 01:21:24.000000 flax-0.7.0/flax/configurations.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.752545 flax-0.7.0/flax/core/
--rw-r--r--   0 runner    (1001) docker     (123)     1382 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5336 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/axes_scan.py
--rw-r--r--   0 runner    (1001) docker     (123)    10412 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/flax_functional_engine.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     9418 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/frozen_dict.py
--rw-r--r--   0 runner    (1001) docker     (123)    56616 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/lift.py
--rw-r--r--   0 runner    (1001) docker     (123)    11617 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/meta.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.752545 flax-0.7.0/flax/core/nn/
--rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18626 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)    12538 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)     7164 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     1503 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/nn/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (123)     2539 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/partial_eval.py
--rw-r--r--   0 runner    (1001) docker     (123)    35812 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/scope.py
--rw-r--r--   0 runner    (1001) docker     (123)     1053 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/tracers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-07-08 01:21:24.000000 flax-0.7.0/flax/core/variables.py
--rw-r--r--   0 runner    (1001) docker     (123)    29794 2023-07-08 01:21:24.000000 flax-0.7.0/flax/errors.py
--rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-07-08 01:21:24.000000 flax-0.7.0/flax/ids.py
--rw-r--r--   0 runner    (1001) docker     (123)     5316 2023-07-08 01:21:24.000000 flax-0.7.0/flax/io.py
--rw-r--r--   0 runner    (1001) docker     (123)    11483 2023-07-08 01:21:24.000000 flax-0.7.0/flax/jax_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/linen/
--rw-r--r--   0 runner    (1001) docker     (123)     2201 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     4014 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2559 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/activation.py
--rw-r--r--   0 runner    (1001) docker     (123)    19211 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     3236 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/combinators.py
--rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/dtypes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/linen/experimental/
--rw-r--r--   0 runner    (1001) docker     (123)    11542 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/experimental/layers_with_named_axes.py
--rw-r--r--   0 runner    (1001) docker     (123)     2619 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/initializers.py
--rw-r--r--   0 runner    (1001) docker     (123)     7528 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/kw_only_dataclasses.py
--rw-r--r--   0 runner    (1001) docker     (123)    32979 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)    87710 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/module.py
--rw-r--r--   0 runner    (1001) docker     (123)    21102 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)    19763 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/partitioning.py
--rw-r--r--   0 runner    (1001) docker     (123)     5480 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/pooling.py
--rw-r--r--   0 runner    (1001) docker     (123)    38078 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/recurrent.py
--rw-r--r--   0 runner    (1001) docker     (123)    11368 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/spmd.py
--rw-r--r--   0 runner    (1001) docker     (123)     3028 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (123)    19162 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/summary.py
--rw-r--r--   0 runner    (1001) docker     (123)    59683 2023-07-08 01:21:24.000000 flax-0.7.0/flax/linen/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)      582 2023-07-08 01:21:24.000000 flax-0.7.0/flax/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7727 2023-07-08 01:21:24.000000 flax-0.7.0/flax/metrics/tensorboard.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/oss/
--rw-r--r--   0 runner    (1001) docker     (123)      443 2023-07-08 01:21:24.000000 flax-0.7.0/flax/oss/ .git-blame-ignore-revs
--rw-r--r--   0 runner    (1001) docker     (123)       58 2023-07-08 01:21:24.000000 flax-0.7.0/flax/py.typed
--rw-r--r--   0 runner    (1001) docker     (123)    14472 2023-07-08 01:21:24.000000 flax-0.7.0/flax/serialization.py
--rw-r--r--   0 runner    (1001) docker     (123)     7332 2023-07-08 01:21:24.000000 flax-0.7.0/flax/struct.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.756545 flax-0.7.0/flax/testing/
--rw-r--r--   0 runner    (1001) docker     (123)      647 2023-07-08 01:21:24.000000 flax-0.7.0/flax/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9472 2023-07-08 01:21:24.000000 flax-0.7.0/flax/testing/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-07-08 01:21:24.000000 flax-0.7.0/flax/traceback_util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.760545 flax-0.7.0/flax/training/
--rw-r--r--   0 runner    (1001) docker     (123)      613 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    43475 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (123)     3681 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/common_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     5843 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/dynamic_scale.py
--rw-r--r--   0 runner    (1001) docker     (123)     2718 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (123)     7499 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/lr_schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)     2722 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/orbax_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2971 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/prefetch_iterator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3322 2023-07-08 01:21:24.000000 flax-0.7.0/flax/training/train_state.py
--rw-r--r--   0 runner    (1001) docker     (123)    13392 2023-07-08 01:21:24.000000 flax-0.7.0/flax/traverse_util.py
--rw-r--r--   0 runner    (1001) docker     (123)      651 2023-07-08 01:21:24.000000 flax-0.7.0/flax/version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.752545 flax-0.7.0/flax.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     8602 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    11022 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      406 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        5 2023-07-08 01:21:39.000000 flax-0.7.0/flax.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.760545 flax-0.7.0/images/
--rw-r--r--   0 runner    (1001) docker     (123)    80407 2023-07-08 01:21:24.000000 flax-0.7.0/images/flax_logo.png
--rw-r--r--   0 runner    (1001) docker     (123)     3862 2023-07-08 01:21:24.000000 flax-0.7.0/images/flax_logo.svg
--rw-r--r--   0 runner    (1001) docker     (123)    15137 2023-07-08 01:21:24.000000 flax-0.7.0/images/flax_logo_250px.png
--rw-r--r--   0 runner    (1001) docker     (123)    29095 2023-07-08 01:21:24.000000 flax-0.7.0/images/flax_logo_500px.png
--rw-r--r--   0 runner    (1001) docker     (123)    14116 2023-07-08 01:21:24.000000 flax-0.7.0/pylintrc
--rw-r--r--   0 runner    (1001) docker     (123)     4607 2023-07-08 01:21:24.000000 flax-0.7.0/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-07-08 01:21:39.768545 flax-0.7.0/setup.cfg
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.760545 flax-0.7.0/tests/
--rw-r--r--   0 runner    (1001) docker     (123)    17587 2023-07-08 01:21:24.000000 flax-0.7.0/tests/checkpoints_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-07-08 01:21:24.000000 flax-0.7.0/tests/colab_tpu_jax_version.ipynb
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.760545 flax-0.7.0/tests/core/
--rw-r--r--   0 runner    (1001) docker     (123)     5087 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/core_frozen_dict_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     8005 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/core_lift_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6803 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/core_meta_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     9408 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/core_scope_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.764545 flax-0.7.0/tests/core/design/
--rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4539 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_auto_encoder_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2815 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_big_resnets_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_custom_vjp_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4457 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_dense_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2630 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_flow_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4694 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_resnet_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2541 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_scan_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2172 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_tied_autoencoder_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2694 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_vmap_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2448 2023-07-08 01:21:24.000000 flax-0.7.0/tests/core/design/core_weight_std_test.py
--rw-r--r--   0 runner    (1001) docker     (123)      887 2023-07-08 01:21:24.000000 flax-0.7.0/tests/download_dataset_metadata.sh
--rw-r--r--   0 runner    (1001) docker     (123)     3260 2023-07-08 01:21:24.000000 flax-0.7.0/tests/early_stopping_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-07-08 01:21:24.000000 flax-0.7.0/tests/import_test.ipynb
--rw-r--r--   0 runner    (1001) docker     (123)     8234 2023-07-08 01:21:24.000000 flax-0.7.0/tests/io_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     3473 2023-07-08 01:21:24.000000 flax-0.7.0/tests/jax_utils_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-08 01:21:39.764545 flax-0.7.0/tests/linen/
--rw-r--r--   0 runner    (1001) docker     (123)     1992 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/initializers_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4355 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/kw_only_dataclasses_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_activation_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     7748 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     5370 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_combinators_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     1590 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_dtypes_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    35451 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_linear_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6355 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_meta_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    65434 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_module_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    16404 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_recurrent_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    16481 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    54451 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/linen_transforms_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    17800 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/partitioning_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    18567 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/summary_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2403 2023-07-08 01:21:24.000000 flax-0.7.0/tests/linen/toplevel_test.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3960 2023-07-08 01:21:24.000000 flax-0.7.0/tests/run_all_tests.sh
--rw-r--r--   0 runner    (1001) docker     (123)    15319 2023-07-08 01:21:24.000000 flax-0.7.0/tests/serialization_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     2371 2023-07-08 01:21:24.000000 flax-0.7.0/tests/struct_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    14163 2023-07-08 01:21:24.000000 flax-0.7.0/tests/tensorboard_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     6027 2023-07-08 01:21:24.000000 flax-0.7.0/tests/traceback_util_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    10482 2023-07-08 01:21:24.000000 flax-0.7.0/tests/traverse_util_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.620033 flax-0.7.1/
+-rw-r--r--   0 runner    (1001) docker     (123)       54 2023-08-01 01:58:55.000000 flax-0.7.1/.git-blame-ignore-revs
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.556033 flax-0.7.1/.github/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.556033 flax-0.7.1/.github/ISSUE_TEMPLATE/
+-rw-r--r--   0 runner    (1001) docker     (123)      792 2023-08-01 01:58:55.000000 flax-0.7.1/.github/ISSUE_TEMPLATE/bug_report.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.556033 flax-0.7.1/.github/analytics/
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-08-01 01:58:55.000000 flax-0.7.1/.github/analytics/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)    13893 2023-08-01 01:58:55.000000 flax-0.7.1/.github/analytics/get_repo_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1677 2023-08-01 01:58:55.000000 flax-0.7.1/.github/analytics/issue_activity_since_date.gql
+-rw-r--r--   0 runner    (1001) docker     (123)     2016 2023-08-01 01:58:55.000000 flax-0.7.1/.github/analytics/pr_data_query.gql
+-rw-r--r--   0 runner    (1001) docker     (123)       34 2023-08-01 01:58:55.000000 flax-0.7.1/.github/analytics/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-08-01 01:58:55.000000 flax-0.7.1/.github/pull_request_template.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.556033 flax-0.7.1/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (123)     6177 2023-08-01 01:58:55.000000 flax-0.7.1/.github/workflows/build.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      834 2023-08-01 01:58:55.000000 flax-0.7.1/.github/workflows/pythonpublish.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      138 2023-08-01 01:58:55.000000 flax-0.7.1/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)      865 2023-08-01 01:58:55.000000 flax-0.7.1/.pre-commit-config.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      560 2023-08-01 01:58:55.000000 flax-0.7.1/.readthedocs.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      293 2023-08-01 01:58:55.000000 flax-0.7.1/AUTHORS
+-rw-r--r--   0 runner    (1001) docker     (123)    20706 2023-08-01 01:58:55.000000 flax-0.7.1/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (123)    11309 2023-08-01 01:58:55.000000 flax-0.7.1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     8602 2023-08-01 01:59:13.620033 flax-0.7.1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     7930 2023-08-01 01:58:55.000000 flax-0.7.1/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      110 2023-08-01 01:58:55.000000 flax-0.7.1/contributing.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.556033 flax-0.7.1/dev/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.556033 flax-0.7.1/dev/.devcontainer/
+-rw-r--r--   0 runner    (1001) docker     (123)     2643 2023-08-01 01:58:55.000000 flax-0.7.1/dev/.devcontainer/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (123)     1806 2023-08-01 01:58:55.000000 flax-0.7.1/dev/.devcontainer/devcontainer.json
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-08-01 01:58:55.000000 flax-0.7.1/dev/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     4646 2023-08-01 01:58:55.000000 flax-0.7.1/dev/update_requirements.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.560033 flax-0.7.1/docs/
+-rw-r--r--   0 runner    (1001) docker     (123)       18 2023-08-01 01:58:55.000000 flax-0.7.1/docs/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)      634 2023-08-01 01:58:55.000000 flax-0.7.1/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (123)     5359 2023-08-01 01:58:55.000000 flax-0.7.1/docs/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.564033 flax-0.7.1/docs/_ext/
+-rw-r--r--   0 runner    (1001) docker     (123)     4308 2023-08-01 01:58:55.000000 flax-0.7.1/docs/_ext/codediff.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3400 2023-08-01 01:58:55.000000 flax-0.7.1/docs/_ext/codediff_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2321 2023-08-01 01:58:55.000000 flax-0.7.1/docs/_ext/flax_module.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.552033 flax-0.7.1/docs/_static/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.564033 flax-0.7.1/docs/_static/css/
+-rw-r--r--   0 runner    (1001) docker     (123)      309 2023-08-01 01:58:55.000000 flax-0.7.1/docs/_static/css/flax_theme.css
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.552033 flax-0.7.1/docs/_templates/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.564033 flax-0.7.1/docs/_templates/autosummary/
+-rw-r--r--   0 runner    (1001) docker     (123)      463 2023-08-01 01:58:55.000000 flax-0.7.1/docs/_templates/autosummary/flax_module.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.564033 flax-0.7.1/docs/api_reference/
+-rw-r--r--   0 runner    (1001) docker     (123)      130 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.config.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      321 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.core.frozen_dict.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.errors.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      365 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.jax_utils.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.568033 flax-0.7.1/docs/api_reference/flax.linen/
+-rw-r--r--   0 runner    (1001) docker     (123)     1143 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/activation_functions.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      192 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/decorators.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      350 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      232 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/init_apply.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     1099 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/initializers.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      162 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/inspection.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2220 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/layers.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      278 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/module.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      295 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/profiling.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      931 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/spmd.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      566 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/transformations.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      213 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.linen/variable.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      480 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.serialization.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.struct.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      275 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.traceback_util.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     1212 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.training.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      798 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/flax.traverse_util.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      250 2023-08-01 01:58:55.000000 flax-0.7.1/docs/api_reference/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     5084 2023-08-01 01:58:55.000000 flax-0.7.1/docs/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6684 2023-08-01 01:58:55.000000 flax-0.7.1/docs/conf_sphinx_patch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11920 2023-08-01 01:58:55.000000 flax-0.7.1/docs/contributing.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.568033 flax-0.7.1/docs/developer_notes/
+-rw-r--r--   0 runner    (1001) docker     (123)      153 2023-08-01 01:58:55.000000 flax-0.7.1/docs/developer_notes/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    18097 2023-08-01 01:58:55.000000 flax-0.7.1/docs/developer_notes/lift.md
+-rw-r--r--   0 runner    (1001) docker     (123)    22018 2023-08-01 01:58:55.000000 flax-0.7.1/docs/developer_notes/module_lifecycle.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      184 2023-08-01 01:58:55.000000 flax-0.7.1/docs/examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4921 2023-08-01 01:58:55.000000 flax-0.7.1/docs/examples_community_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4422 2023-08-01 01:58:55.000000 flax-0.7.1/docs/examples_core_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    22579 2023-08-01 01:58:55.000000 flax-0.7.1/docs/examples_google_research_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2028 2023-08-01 01:58:55.000000 flax-0.7.1/docs/examples_repositories_that_use_flax.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    20991 2023-08-01 01:58:55.000000 flax-0.7.1/docs/flax.png
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.568033 flax-0.7.1/docs/flip/
+-rw-r--r--   0 runner    (1001) docker     (123)      648 2023-08-01 01:58:55.000000 flax-0.7.1/docs/flip/0000-template.md
+-rw-r--r--   0 runner    (1001) docker     (123)    17256 2023-08-01 01:58:55.000000 flax-0.7.1/docs/flip/1009-optimizer-api.md
+-rw-r--r--   0 runner    (1001) docker     (123)     8189 2023-08-01 01:58:55.000000 flax-0.7.1/docs/flip/1777-default-dtype.md
+-rw-r--r--   0 runner    (1001) docker     (123)    10424 2023-08-01 01:58:55.000000 flax-0.7.1/docs/flip/2434-general-metadata.md
+-rw-r--r--   0 runner    (1001) docker     (123)     4099 2023-08-01 01:58:55.000000 flax-0.7.1/docs/flip/2974-kw-only-dataclasses.md
+-rw-r--r--   0 runner    (1001) docker     (123)     1404 2023-08-01 01:58:55.000000 flax-0.7.1/docs/flip/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)   102681 2023-08-01 01:58:55.000000 flax-0.7.1/docs/getting_started.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    16394 2023-08-01 01:58:55.000000 flax-0.7.1/docs/getting_started.md
+-rw-r--r--   0 runner    (1001) docker     (123)     6661 2023-08-01 01:58:55.000000 flax-0.7.1/docs/glossary.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.576033 flax-0.7.1/docs/guides/
+-rw-r--r--   0 runner    (1001) docker     (123)     4087 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/arguments.md
+-rw-r--r--   0 runner    (1001) docker     (123)     9149 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/batch_norm.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     9329 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/convert_pytorch_to_flax.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11050 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/dropout.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    10488 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/ensembling.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    12735 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/extracting_intermediates.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    39480 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/flax_basics.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    21887 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/flax_basics.md
+-rw-r--r--   0 runner    (1001) docker     (123)    63884 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/flax_on_pjit.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    24655 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/flax_on_pjit.md
+-rw-r--r--   0 runner    (1001) docker     (123)     7442 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/full_eval.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    24512 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/haiku_migration_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      265 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      255 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/index_converting_and_upgrading.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       80 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/index_data_preprocessing.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      201 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/index_flax_fundamentals.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      110 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/index_model_inspection.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/index_parallel_training.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      152 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/index_training_techniques.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    38261 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/jax_for_the_impatient.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    21301 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/jax_for_the_impatient.md
+-rw-r--r--   0 runner    (1001) docker     (123)    17271 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/linen_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8175 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/lr_schedule.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     7142 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/model_surgery.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     4372 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/model_surgery.md
+-rw-r--r--   0 runner    (1001) docker     (123)    10621 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/optax_update_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     9077 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/orbax_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4002 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/regular_dict_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6129 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/rnncell_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     3125 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/setup_or_nncompact.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6076 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/state_params.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11497 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/transfer_learning.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     8016 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/transfer_learning.md
+-rw-r--r--   0 runner    (1001) docker     (123)    52447 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/use_checkpointing.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    26556 2023-08-01 01:58:55.000000 flax-0.7.1/docs/guides/use_checkpointing.md
+-rw-r--r--   0 runner    (1001) docker     (123)     8273 2023-08-01 01:58:55.000000 flax-0.7.1/docs/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8090 2023-08-01 01:58:55.000000 flax-0.7.1/docs/mission.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.576033 flax-0.7.1/docs/notebooks/
+-rw-r--r--   0 runner    (1001) docker     (123)     7391 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/flax_sharp_bits.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     5941 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/flax_sharp_bits.md
+-rw-r--r--   0 runner    (1001) docker     (123)    18559 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/full_eval.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     9794 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/full_eval.md
+-rw-r--r--   0 runner    (1001) docker     (123)      358 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    40823 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/linen_intro.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    21904 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/linen_intro.md
+-rw-r--r--   0 runner    (1001) docker     (123)    21338 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/optax_update_guide.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)    11407 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/optax_update_guide.md
+-rw-r--r--   0 runner    (1001) docker     (123)    13947 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/orbax_upgrade_guide.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     8587 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/orbax_upgrade_guide.md
+-rw-r--r--   0 runner    (1001) docker     (123)     9555 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/state_params.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     6721 2023-08-01 01:58:55.000000 flax-0.7.1/docs/notebooks/state_params.md
+-rw-r--r--   0 runner    (1001) docker     (123)     2510 2023-08-01 01:58:55.000000 flax-0.7.1/docs/overview.md
+-rw-r--r--   0 runner    (1001) docker     (123)     7604 2023-08-01 01:58:55.000000 flax-0.7.1/docs/philosophy.md
+-rw-r--r--   0 runner    (1001) docker     (123)      566 2023-08-01 01:58:55.000000 flax-0.7.1/docs/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.576033 flax-0.7.1/examples/
+-rw-r--r--   0 runner    (1001) docker     (123)      743 2023-08-01 01:58:55.000000 flax-0.7.1/examples/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.576033 flax-0.7.1/examples/cloud/
+-rw-r--r--   0 runner    (1001) docker     (123)     4635 2023-08-01 01:58:55.000000 flax-0.7.1/examples/cloud/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     9222 2023-08-01 01:58:55.000000 flax-0.7.1/examples/cloud/launch_gce.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1650 2023-08-01 01:58:55.000000 flax-0.7.1/examples/cloud/startup_script.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.580033 flax-0.7.1/examples/imagenet/
+-rw-r--r--   0 runner    (1001) docker     (123)     9387 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.580033 flax-0.7.1/examples/imagenet/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     2192 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1305 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/configs/fake_data_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1670 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/configs/tpu.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1055 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/configs/v100_x8.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/configs/v100_x8_mixed_precision.py
+-rw-r--r--   0 runner    (1001) docker     (123)   294627 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/imagenet.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     3334 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/imagenet_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2235 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/imagenet_fake_data_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8124 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2125 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4342 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1943 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)      341 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    12883 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3065 2023-08-01 01:58:55.000000 flax-0.7.1/examples/imagenet/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.580033 flax-0.7.1/examples/linen_design_test/
+-rw-r--r--   0 runner    (1001) docker     (123)     6341 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/attention_simple.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3217 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/dense.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1350 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/linear_regression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2434 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/mlp_explicit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1993 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/mlp_inline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1927 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/mlp_lazy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1540 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/tied_autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2271 2023-08-01 01:58:55.000000 flax-0.7.1/examples/linen_design_test/weight_std.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.584033 flax-0.7.1/examples/lm1b/
+-rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.584033 flax-0.7.1/examples/lm1b/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     3443 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12499 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3355 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2190 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12043 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)      343 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     4843 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/temperature_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1453 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/temperature_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5313 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19898 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1994 2023-08-01 01:58:55.000000 flax-0.7.1/examples/lm1b/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.584033 flax-0.7.1/examples/mnist/
+-rw-r--r--   0 runner    (1001) docker     (123)     1741 2023-08-01 01:58:55.000000 flax-0.7.1/examples/mnist/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.584033 flax-0.7.1/examples/mnist/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)      912 2023-08-01 01:58:55.000000 flax-0.7.1/examples/mnist/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2121 2023-08-01 01:58:55.000000 flax-0.7.1/examples/mnist/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    99011 2023-08-01 01:58:55.000000 flax-0.7.1/examples/mnist/mnist.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     2366 2023-08-01 01:58:55.000000 flax-0.7.1/examples/mnist/mnist_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)      298 2023-08-01 01:58:55.000000 flax-0.7.1/examples/mnist/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     5235 2023-08-01 01:58:55.000000 flax-0.7.1/examples/mnist/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2266 2023-08-01 01:58:55.000000 flax-0.7.1/examples/mnist/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.588033 flax-0.7.1/examples/nlp_seq/
+-rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-08-01 01:58:55.000000 flax-0.7.1/examples/nlp_seq/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     7884 2023-08-01 01:58:55.000000 flax-0.7.1/examples/nlp_seq/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4073 2023-08-01 01:58:55.000000 flax-0.7.1/examples/nlp_seq/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6449 2023-08-01 01:58:55.000000 flax-0.7.1/examples/nlp_seq/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)       60 2023-08-01 01:58:55.000000 flax-0.7.1/examples/nlp_seq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    14104 2023-08-01 01:58:55.000000 flax-0.7.1/examples/nlp_seq/train.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.588033 flax-0.7.1/examples/ogbg_molpcba/
+-rw-r--r--   0 runner    (1001) docker     (123)     4486 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.592033 flax-0.7.1/examples/ogbg_molpcba/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1551 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/configs/default_graph_net.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1946 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/configs/hparam_sweep.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/configs/test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8133 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2198 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7068 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5187 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)  1111228 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/ogbg_molpcba.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     4769 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)      329 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    13701 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12435 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ogbg_molpcba/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.592033 flax-0.7.1/examples/ppo/
+-rw-r--r--   0 runner    (1001) docker     (123)     2501 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     2607 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/agent.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.592033 flax-0.7.1/examples/ppo/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     1955 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2460 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/env_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2346 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13188 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/ppo_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5294 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/ppo_lib_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1529 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/ppo_main.py
+-rw-r--r--   0 runner    (1001) docker     (123)      192 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     8794 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/seed_rl_atari_preprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1897 2023-08-01 01:58:55.000000 flax-0.7.1/examples/ppo/test_episodes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.592033 flax-0.7.1/examples/seq2seq/
+-rw-r--r--   0 runner    (1001) docker     (123)      913 2023-08-01 01:58:55.000000 flax-0.7.1/examples/seq2seq/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     5538 2023-08-01 01:58:55.000000 flax-0.7.1/examples/seq2seq/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4446 2023-08-01 01:58:55.000000 flax-0.7.1/examples/seq2seq/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)       65 2023-08-01 01:58:55.000000 flax-0.7.1/examples/seq2seq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    25401 2023-08-01 01:58:55.000000 flax-0.7.1/examples/seq2seq/seq2seq.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     6830 2023-08-01 01:58:55.000000 flax-0.7.1/examples/seq2seq/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3213 2023-08-01 01:58:55.000000 flax-0.7.1/examples/seq2seq/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.596033 flax-0.7.1/examples/sst2/
+-rw-r--r--   0 runner    (1001) docker     (123)     1893 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/README.md
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2028 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/build_vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.596033 flax-0.7.1/examples/sst2/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/configs/default.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9834 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3522 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2118 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14407 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3571 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)      156 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     9117 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/sst2.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     9339 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2127 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/train_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)   117898 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/vocab.txt
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4407 2023-08-01 01:58:55.000000 flax-0.7.1/examples/sst2/vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.596033 flax-0.7.1/examples/vae/
+-rw-r--r--   0 runner    (1001) docker     (123)      593 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     1458 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1598 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1777 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2152 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/reconstruction.png
+-rw-r--r--   0 runner    (1001) docker     (123)      113 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.596033 flax-0.7.1/examples/vae/results/
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/results/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)    43139 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/sample.png
+-rw-r--r--   0 runner    (1001) docker     (123)     4512 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3580 2023-08-01 01:58:55.000000 flax-0.7.1/examples/vae/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.600033 flax-0.7.1/examples/wmt/
+-rw-r--r--   0 runner    (1001) docker     (123)     6106 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     7281 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/bleu.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.600033 flax-0.7.1/examples/wmt/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     3482 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14745 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/decode.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12908 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3318 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18559 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)      398 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     5314 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23417 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1995 2023-08-01 01:58:55.000000 flax-0.7.1/examples/wmt/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.600033 flax-0.7.1/flax/
+-rw-r--r--   0 runner    (1001) docker     (123)      929 2023-08-01 01:58:55.000000 flax-0.7.1/flax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4094 2023-08-01 01:58:55.000000 flax-0.7.1/flax/configurations.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.604033 flax-0.7.1/flax/core/
+-rw-r--r--   0 runner    (1001) docker     (123)     1439 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5406 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/axes_scan.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10412 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/flax_functional_engine.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     9452 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/frozen_dict.py
+-rw-r--r--   0 runner    (1001) docker     (123)    55971 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/lift.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11590 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/meta.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.604033 flax-0.7.1/flax/core/nn/
+-rw-r--r--   0 runner    (1001) docker     (123)     1795 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18314 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/nn/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12204 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/nn/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6974 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/nn/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1503 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/nn/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2540 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/partial_eval.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35668 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/scope.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1053 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/tracers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-08-01 01:58:55.000000 flax-0.7.1/flax/core/variables.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29167 2023-08-01 01:58:55.000000 flax-0.7.1/flax/errors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1752 2023-08-01 01:58:55.000000 flax-0.7.1/flax/ids.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5317 2023-08-01 01:58:55.000000 flax-0.7.1/flax/io.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11498 2023-08-01 01:58:55.000000 flax-0.7.1/flax/jax_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.608033 flax-0.7.1/flax/linen/
+-rw-r--r--   0 runner    (1001) docker     (123)     2201 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     4180 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2558 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/activation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18548 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3225 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/combinators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3872 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/dtypes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.608033 flax-0.7.1/flax/linen/experimental/
+-rw-r--r--   0 runner    (1001) docker     (123)    11490 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/experimental/layers_with_named_axes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2622 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/initializers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7553 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/kw_only_dataclasses.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33001 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)    88743 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/module.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21679 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19344 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/partitioning.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5509 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38023 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/recurrent.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11459 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/spmd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3041 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20309 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/summary.py
+-rw-r--r--   0 runner    (1001) docker     (123)    59542 2023-08-01 01:58:55.000000 flax-0.7.1/flax/linen/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.608033 flax-0.7.1/flax/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)     1163 2023-08-01 01:58:55.000000 flax-0.7.1/flax/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7804 2023-08-01 01:58:55.000000 flax-0.7.1/flax/metrics/tensorboard.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.608033 flax-0.7.1/flax/oss/
+-rw-r--r--   0 runner    (1001) docker     (123)      443 2023-08-01 01:58:55.000000 flax-0.7.1/flax/oss/ .git-blame-ignore-revs
+-rw-r--r--   0 runner    (1001) docker     (123)       58 2023-08-01 01:58:55.000000 flax-0.7.1/flax/py.typed
+-rw-r--r--   0 runner    (1001) docker     (123)    14282 2023-08-01 01:58:55.000000 flax-0.7.1/flax/serialization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7351 2023-08-01 01:58:55.000000 flax-0.7.1/flax/struct.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.608033 flax-0.7.1/flax/testing/
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-08-01 01:58:55.000000 flax-0.7.1/flax/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9452 2023-08-01 01:58:55.000000 flax-0.7.1/flax/testing/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-08-01 01:58:55.000000 flax-0.7.1/flax/traceback_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.612033 flax-0.7.1/flax/training/
+-rw-r--r--   0 runner    (1001) docker     (123)      613 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43203 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3692 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/common_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5835 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/dynamic_scale.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2662 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/early_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7429 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/lr_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3714 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/orbax_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2984 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/prefetch_iterator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3314 2023-08-01 01:58:55.000000 flax-0.7.1/flax/training/train_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13437 2023-08-01 01:58:55.000000 flax-0.7.1/flax/traverse_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      650 2023-08-01 01:58:55.000000 flax-0.7.1/flax/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.604033 flax-0.7.1/flax.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     8602 2023-08-01 01:59:13.000000 flax-0.7.1/flax.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    11045 2023-08-01 01:59:13.000000 flax-0.7.1/flax.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-08-01 01:59:13.000000 flax-0.7.1/flax.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      443 2023-08-01 01:59:13.000000 flax-0.7.1/flax.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        5 2023-08-01 01:59:13.000000 flax-0.7.1/flax.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.612033 flax-0.7.1/images/
+-rw-r--r--   0 runner    (1001) docker     (123)    80407 2023-08-01 01:58:55.000000 flax-0.7.1/images/flax_logo.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3862 2023-08-01 01:58:55.000000 flax-0.7.1/images/flax_logo.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    15137 2023-08-01 01:58:55.000000 flax-0.7.1/images/flax_logo_250px.png
+-rw-r--r--   0 runner    (1001) docker     (123)    29095 2023-08-01 01:58:55.000000 flax-0.7.1/images/flax_logo_500px.png
+-rw-r--r--   0 runner    (1001) docker     (123)    14116 2023-08-01 01:58:55.000000 flax-0.7.1/pylintrc
+-rw-r--r--   0 runner    (1001) docker     (123)     4759 2023-08-01 01:58:55.000000 flax-0.7.1/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-08-01 01:59:13.620033 flax-0.7.1/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.616033 flax-0.7.1/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)    18003 2023-08-01 01:58:55.000000 flax-0.7.1/tests/checkpoints_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-08-01 01:58:55.000000 flax-0.7.1/tests/colab_tpu_jax_version.ipynb
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.616033 flax-0.7.1/tests/core/
+-rw-r--r--   0 runner    (1001) docker     (123)     5139 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/core_frozen_dict_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8282 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/core_lift_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6996 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/core_meta_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9600 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/core_scope_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.616033 flax-0.7.1/tests/core/design/
+-rw-r--r--   0 runner    (1001) docker     (123)     4931 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4786 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_auto_encoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2928 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_big_resnets_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2512 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_custom_vjp_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4661 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_dense_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2660 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_flow_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4893 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_resnet_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2615 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_scan_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2231 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_tied_autoencoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2673 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_vmap_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2542 2023-08-01 01:58:55.000000 flax-0.7.1/tests/core/design/core_weight_std_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)      887 2023-08-01 01:58:55.000000 flax-0.7.1/tests/download_dataset_metadata.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     3081 2023-08-01 01:58:55.000000 flax-0.7.1/tests/early_stopping_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-08-01 01:58:55.000000 flax-0.7.1/tests/import_test.ipynb
+-rw-r--r--   0 runner    (1001) docker     (123)     8409 2023-08-01 01:58:55.000000 flax-0.7.1/tests/io_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3484 2023-08-01 01:58:55.000000 flax-0.7.1/tests/jax_utils_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 01:59:13.620033 flax-0.7.1/tests/linen/
+-rw-r--r--   0 runner    (1001) docker     (123)     2065 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/initializers_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4357 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/kw_only_dataclasses_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_activation_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7669 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5257 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_combinators_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1597 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_dtypes_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34278 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_linear_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6442 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_meta_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    65084 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_module_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17219 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_recurrent_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16693 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    55533 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/linen_transforms_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18349 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/partitioning_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18519 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/summary_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2406 2023-08-01 01:58:55.000000 flax-0.7.1/tests/linen/toplevel_test.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3960 2023-08-01 01:58:55.000000 flax-0.7.1/tests/run_all_tests.sh
+-rw-r--r--   0 runner    (1001) docker     (123)    16054 2023-08-01 01:58:55.000000 flax-0.7.1/tests/serialization_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2377 2023-08-01 01:58:55.000000 flax-0.7.1/tests/struct_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14581 2023-08-01 01:58:55.000000 flax-0.7.1/tests/tensorboard_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6044 2023-08-01 01:58:55.000000 flax-0.7.1/tests/traceback_util_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10811 2023-08-01 01:58:55.000000 flax-0.7.1/tests/traverse_util_test.py
```

### Comparing `flax-0.7.0/.github/ISSUE_TEMPLATE/bug_report.md` & `flax-0.7.1/.github/ISSUE_TEMPLATE/bug_report.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/.github/analytics/README.md` & `flax-0.7.1/.github/analytics/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/.github/analytics/get_repo_metrics.py` & `flax-0.7.1/.github/analytics/get_repo_metrics.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,25 +21,26 @@
 
 import pandas as pd
 import requests
 import matplotlib.pyplot as plt
 import matplotlib.dates as mdates
 
 
-token = os.environ["GITHUB_TOKEN"]
-endpoint = r"https://api.github.com/graphql"
-headers = {"Authorization": f"bearer {token}"}
+token = os.environ['GITHUB_TOKEN']
+endpoint = r'https://api.github.com/graphql'
+headers = {'Authorization': f'bearer {token}'}
 
-#------------------------------------------------------------------------------
+# ------------------------------------------------------------------------------
 # GraphQL
-#------------------------------------------------------------------------------
+# ------------------------------------------------------------------------------
 # NOTE: This GraphQL logic was ported and adapted from this script:
 # https://github.com/scientific-python/devstats-data/blob/4c022961abc4ca6061f8719d9c3387e98734b90c/query.py
 # It contains style differences from Google's style guide.
 
+
 def load_query_from_file(fname, repo_owner, repo_name) -> str:
   with open(fname) as fh:
     query = fh.read()
     # Set target repo from template
     query = query.replace('_REPO_OWNER_', repo_owner)
     query = query.replace('_REPO_NAME_', repo_name)
   return query
@@ -71,44 +72,46 @@
   Notes
   -----
   This is intended mostly for internal use within `get_all_responses`.
   """
   # TODO: Expand this, either by parsing the query type from the query
   # directly or manually adding more query_types to the set
   if query_type not in {'issues', 'pullRequests'}:
-      raise ValueError(
-          'Only \'issues\' and \'pullRequests\' queries are currently supported'
-      )
+    raise ValueError(
+        "Only 'issues' and 'pullRequests' queries are currently supported"
+    )
   # TODO: Generalize this
   # WARNING: The cursor injection depends on the specific structure of the
   # query, this is the main reason why query types are limited to issues/PRs
   if cursor is not None:
     cursor_insertion_key = query_type + '('
     cursor_ind = query.find(cursor_insertion_key) + len(cursor_insertion_key)
     query = query[:cursor_ind] + f'after:"{cursor}", ' + query[cursor_ind:]
   # Build request payload
-  payload = {'query' : query}
+  payload = {'query': query}
   response = requests.post(endpoint, json=payload, headers=headers)
   return json.loads(response.content)
 
+
 def get_all_responses(query, query_type):
-  "Helper function to bypass GitHub GraphQL API node limit."
+  'Helper function to bypass GitHub GraphQL API node limit.'
   # Get data from a single response
   initial_data = send_query(query, query_type)
   data, last_cursor, total_count = parse_single_query(initial_data, query_type)
   print(f'Retrieving {len(data)} out of {total_count} values...')
   # Continue requesting data (with pagination) until all are acquired
   while len(data) < total_count:
     rdata = send_query(query, query_type, cursor=last_cursor)
     pdata, last_cursor, _ = parse_single_query(rdata, query_type)
     data.extend(pdata)
     print(f'Retrieving {len(data)} out of {total_count} values...')
   print('Done.')
   return data
 
+
 def parse_single_query(data, query_type):
   """
   Parses the data returned by `send_query`
 
   .. warning::
 
     Like `send_query`, the logic here depends on the specific structure
@@ -156,27 +159,30 @@
     self.repo_owner = repo_owner
     self.repo_name = repo_name
     self.raw_data = None
     self.load_query()
 
   def load_query(self):
     self.query = load_query_from_file(
-      self.query_fname, self.repo_owner, self.repo_name
+        self.query_fname, self.repo_owner, self.repo_name
     )
 
   def get(self):
     self.raw_data = get_all_responses(self.query, self.query_type)
 
-#------------------------------------------------------------------------------
+
+# ------------------------------------------------------------------------------
 # metrics helpers
-#------------------------------------------------------------------------------
+# ------------------------------------------------------------------------------
+
 
 def _to_datetime(date_str: str) -> datetime:
   return datetime.fromisoformat(date_str.replace('Z', ''))
 
+
 def _get_issues_features(issues):
   for issue in issues:
     issue = issue['node']
 
     created_at = _to_datetime(issue['createdAt'])
     time_labeled_or_converted = None
     time_issue_closed = None
@@ -187,89 +193,92 @@
       if event['__typename'] in {'LabeledEvent', 'ConvertedToDiscussionEvent'}:
         time_labeled_or_converted = _to_datetime(event['createdAt'])
 
       if event['__typename'] == 'ClosedEvent':
         time_issue_closed = _to_datetime(event['createdAt'])
 
     yield {
-      'created_at': created_at,
-      'time_labeled_or_converted': time_labeled_or_converted,
-      'time_issue_closed': time_issue_closed,
-      'issue_closed': issue['state'] == 'CLOSED',
+        'created_at': created_at,
+        'time_labeled_or_converted': time_labeled_or_converted,
+        'time_issue_closed': time_issue_closed,
+        'issue_closed': issue['state'] == 'CLOSED',
     }
 
+
 def _get_pr_features(prs):
   for pr in prs:
     pr = pr['node']
 
     created_at = _to_datetime(pr['createdAt'])
     ready_for_review_at = _to_datetime(pr['createdAt'])
     time_labeled_or_assigned = None
     time_merged_or_closed = None
     time_review = None
 
-    if pr["reviews"]["nodes"]:
-      review = pr["reviews"]["nodes"][0]
-      time_review = _to_datetime(review["createdAt"])
+    if pr['reviews']['nodes']:
+      review = pr['reviews']['nodes'][0]
+      time_review = _to_datetime(review['createdAt'])
 
     for event in pr['timelineItems']['edges']:
       event = event['node']
 
       if (
-        time_labeled_or_assigned is None
-        and event['__typename'] == 'LabeledEvent'
-        and 'cla:' not in event['label']['name']
+          time_labeled_or_assigned is None
+          and event['__typename'] == 'LabeledEvent'
+          and 'cla:' not in event['label']['name']
       ):
         time_labeled_or_assigned = _to_datetime(event['createdAt'])
 
       if (
-        time_labeled_or_assigned is None
-        and event['__typename'] == 'AssignedEvent'
+          time_labeled_or_assigned is None
+          and event['__typename'] == 'AssignedEvent'
       ):
         time_labeled_or_assigned = _to_datetime(event['createdAt'])
 
       if event['__typename'] in {'ClosedEvent', 'MergedEvent'}:
         time_merged_or_closed = _to_datetime(event['createdAt'])
 
       if event['__typename'] == 'ReadyForReviewEvent':
         ready_for_review_at = _to_datetime(event['createdAt'])
 
     yield {
-      'created_at': created_at,
-      'ready_for_review_at': ready_for_review_at,
-      'time_labeled_or_assigned': time_labeled_or_assigned,
-      'time_merged_or_closed': time_merged_or_closed,
-      'time_review': time_review,
-      'pr_closed': pr['state'] != 'OPEN',
+        'created_at': created_at,
+        'ready_for_review_at': ready_for_review_at,
+        'time_labeled_or_assigned': time_labeled_or_assigned,
+        'time_merged_or_closed': time_merged_or_closed,
+        'time_review': time_review,
+        'pr_closed': pr['state'] != 'OPEN',
     }
 
+
 def _start_of_month(date: datetime) -> datetime:
   return date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
 
+
 def _shift_n_months(date: datetime, n: int) -> datetime:
   month = ((date.month + n - 1) % 12) + 1
 
   # shift to next year if necessary
   if date.month > month:
     date = date.replace(year=date.year + 1)
 
   date = date.replace(month=month)
 
   return date
 
 
 def _rolling_window(
-  df: pd.DataFrame,
-  f: Callable[[pd.DataFrame], pd.Series],
-  window_size: int = 6,
-  step: int = 1,
+    df: pd.DataFrame,
+    f: Callable[[pd.DataFrame], pd.Series],
+    window_size: int = 6,
+    step: int = 1,
 ) -> pd.DataFrame:
   # start of month of the first issue
   start: datetime = df.iloc[0]['created_at'].replace(
-    day=1, hour=0, minute=0, second=0, microsecond=0
+      day=1, hour=0, minute=0, second=0, microsecond=0
   )
   end = _shift_n_months(start, window_size)
 
   last_month = _start_of_month(df.iloc[-1]['created_at'])
   last_month = _shift_n_months(last_month, 1)
 
   rows: List[pd.Series] = []
@@ -282,64 +291,78 @@
     end = _shift_n_months(end, step)
 
   df = pd.DataFrame(rows)
   df = df[['period_start', 'period_end'] + list(df.columns[:-2])]
 
   return df
 
+
 def _process_prs(df: pd.DataFrame) -> pd.Series:
   return pd.Series({
-    'pr_response_time': df['pr_response_time'].dt.days.mean(),
-    'pr_resolution_time': df['pr_resolution_time'].dt.days.mean(),
+      'pr_response_time': df['pr_response_time'].dt.days.mean(),
+      'pr_resolution_time': df['pr_resolution_time'].dt.days.mean(),
   })
 
+
 def _process_issues(df: pd.DataFrame) -> pd.Series:
   return pd.Series({
-    'issue_response_time': df['issue_response_time'].dt.days.mean(),
-    'issue_resolution_time': df['issue_resolution_time'].dt.days.mean(),
+      'issue_response_time': df['issue_response_time'].dt.days.mean(),
+      'issue_resolution_time': df['issue_resolution_time'].dt.days.mean(),
   })
 
-#-----------------------------------------------------------------------------
+
+# -----------------------------------------------------------------------------
 # main
-#-----------------------------------------------------------------------------
+# -----------------------------------------------------------------------------
 FLAGS = flags.FLAGS
 flags.DEFINE_string('repo_owner', 'google', 'User name or organization')
 flags.DEFINE_string('repo_name', 'flax', 'Name of the repository')
 
+
 def main(_):
   repo_owner: str = FLAGS.repo_owner
   repo_name: str = FLAGS.repo_name
 
   # Download issue data
   issues = GithubGrabber(
-    '.github/analytics/issue_activity_since_date.gql',
-    'issues',
-    repo_owner=repo_owner,
-    repo_name=repo_name,
+      '.github/analytics/issue_activity_since_date.gql',
+      'issues',
+      repo_owner=repo_owner,
+      repo_name=repo_name,
   )
   issues.get()
 
-  df_issues = df_issues0 = pd.DataFrame(list(_get_issues_features(issues.raw_data)))
-  df_issues['issue_response_time'] = df_issues['time_labeled_or_converted'] - df_issues['created_at']
-  df_issues['issue_resolution_time'] = df_issues['time_issue_closed'] - df_issues['created_at']
+  df_issues = df_issues0 = pd.DataFrame(
+      list(_get_issues_features(issues.raw_data))
+  )
+  df_issues['issue_response_time'] = (
+      df_issues['time_labeled_or_converted'] - df_issues['created_at']
+  )
+  df_issues['issue_resolution_time'] = (
+      df_issues['time_issue_closed'] - df_issues['created_at']
+  )
 
   df_issues = _rolling_window(df_issues, _process_issues)
 
   prs = GithubGrabber(
-    '.github/analytics/pr_data_query.gql',
-    'pullRequests',
-    repo_owner=repo_owner,
-    repo_name=repo_name,
+      '.github/analytics/pr_data_query.gql',
+      'pullRequests',
+      repo_owner=repo_owner,
+      repo_name=repo_name,
   )
   prs.get()
 
   df_prs = df_prs0 = pd.DataFrame(list(_get_pr_features(prs.raw_data)))
-  time_response = df_prs[['time_labeled_or_assigned', 'time_review']].min(axis=1)
+  time_response = df_prs[['time_labeled_or_assigned', 'time_review']].min(
+      axis=1
+  )
   df_prs['pr_response_time'] = time_response - df_prs['ready_for_review_at']
-  df_prs['pr_resolution_time'] = df_prs['time_merged_or_closed'] - df_prs['ready_for_review_at']
+  df_prs['pr_resolution_time'] = (
+      df_prs['time_merged_or_closed'] - df_prs['ready_for_review_at']
+  )
 
   df_prs = _rolling_window(df_prs, _process_prs)
 
   # get cummulative issues
   df_issues0 = df_issues0.copy()
   df_issues0['number_of_issues'] = 1
   df_issues0['number_of_issues'] = df_issues0['number_of_issues'].cumsum()
@@ -363,15 +386,14 @@
   plt.plot(df_prs0['created_at'], df_prs0['number_of_prs'])
   plt.xlabel('Date')
   plt.ylabel('Number of PRs')
   plt.title('Number of PRs')
   plt.gca().xaxis.set_major_locator(plt.MaxNLocator(5))
   plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
 
-
   # plot for isssue_response_time
   plt.figure()
   plt.plot(df_issues['period_end'], df_issues['issue_response_time'])
   plt.xlabel('Date')
   plt.ylabel('Issue Response Time (days)')
   plt.title('Issue Response Time')
   plt.gca().xaxis.set_major_locator(plt.MaxNLocator(5))
@@ -407,9 +429,10 @@
   plt.gca().xaxis.set_major_locator(plt.MaxNLocator(5))
   plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
   plt.ylim(0)
 
   # show plots
   plt.show()
 
+
 if __name__ == '__main__':
   app.run(main)
```

### Comparing `flax-0.7.0/.github/analytics/issue_activity_since_date.gql` & `flax-0.7.1/.github/analytics/issue_activity_since_date.gql`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/.github/analytics/pr_data_query.gql` & `flax-0.7.1/.github/analytics/pr_data_query.gql`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/.github/pull_request_template.md` & `flax-0.7.1/.github/pull_request_template.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/.github/workflows/build.yml` & `flax-0.7.1/.github/workflows/build.yml`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/.github/workflows/pythonpublish.yml` & `flax-0.7.1/.github/workflows/pythonpublish.yml`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/.pre-commit-config.yaml` & `flax-0.7.1/.pre-commit-config.yaml`

 * *Files 19% similar despite different names*

```diff
@@ -9,14 +9,18 @@
 
 repos:
 - repo: https://github.com/mwouts/jupytext
   rev: v1.13.8
   hooks:
   - id: jupytext
     args: [--sync]
+- repo: https://github.com/google/pyink
+  rev: 23.5.0
+  hooks:
+    - id: pyink
 - repo: https://github.com/pre-commit/pre-commit-hooks
   rev: v4.3.0
   hooks:
   - id: check-toml
   - id: trailing-whitespace
     exclude: ^docs/.*\.md$
 - repo: https://github.com/kynan/nbstripout
```

### Comparing `flax-0.7.0/.readthedocs.yml` & `flax-0.7.1/.readthedocs.yml`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/CHANGELOG.md` & `flax-0.7.1/CHANGELOG.md`

 * *Files 3% similar despite different names*

```diff
@@ -19,14 +19,43 @@
 -
 -
 -
 -
 -
 -
 
+0.7.1
+-----
+Breaking changes:
+- Migrating Flax from returning FrozenDicts to returning regular dicts. More details can be found in this [announcement](https://github.com/google/flax/discussions/3191)
+
+New features:
+- Use pyink
+- added dict migration guide to index
+- add scan over layers section
+- Expose options to customize rich.Table
+- add support for initializing carry variables in scan
+- Let Flax-Orbax to not port the shape of `target` arrays when they port the `target` shardings.
+
+Bug fixes:
+- Use import `orbax.checkpoint` which is a better import pattern.
+- Use import `orbax.checkpoint as ocp` to avoid the verbosity of using 'orbax.checkpoint` every time.
+- [linen] Add alternative, more numerically stable, variance calculation to `LayerNorm`.
+- [linen] Minor cleanup to normalization code.
+- Fix norm calculation bug for 0-rank arrays.
+- [JAX] Remove references to jax.config.jax_array.
+- [linen] Use `stack` instead of `concatenate` in `compute_stats`, to handle scalar stats case.
+- [linen] More minor cleanup in normalization `compute_stats`.
+- Fix warnings from atari gym.
+- Refactor TypeHandler to operate over batches of values, rather than individual ones. This allows more flexibility for implementations that may operate more efficiently on batches.
+- Fix carry slice logic
+- make flax_basics guide use utility fns
+- Fix checkpointing guide error at head
+- Improve scan docs
+
 0.7.0
 -----
 - RNNCellBase refactor.
 
 0.6.11
 -----
 - Set Orbax-as-backend to be the default checkpointing method.
```

### Comparing `flax-0.7.0/LICENSE` & `flax-0.7.1/LICENSE`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/PKG-INFO` & `flax-0.7.1/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flax
-Version: 0.7.0
+Version: 0.7.1
 Summary: Flax: A neural network library for JAX designed for flexibility
 Author-email: Flax team <flax-dev@google.com>
 Project-URL: homepage, https://github.com/google/flax
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
@@ -211,15 +211,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.7.0},
+  version = {0.7.1},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.7.0/README.md` & `flax-0.7.1/README.md`

 * *Files 0% similar despite different names*

```diff
@@ -193,15 +193,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.7.0},
+  version = {0.7.1},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.7.0/dev/.devcontainer/Dockerfile` & `flax-0.7.1/dev/.devcontainer/Dockerfile`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/dev/.devcontainer/devcontainer.json` & `flax-0.7.1/dev/.devcontainer/devcontainer.json`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/dev/README.md` & `flax-0.7.1/dev/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/dev/update_requirements.py` & `flax-0.7.1/dev/update_requirements.py`

 * *Files 4% similar despite different names*

```diff
@@ -35,14 +35,16 @@
 The actual pypi-version list can be read copied from the "Install Dependencies"
 step from the latest Github build action:
 https://github.com/google/flax/actions/workflows/build.yml
 
 Alternatively, the list can also be provided from the local environment with:
 
 python dev --versions="$(pip freeze | sed s/==/-/g) flax-0.3.6"
+
+This will use the currently installed versions of all packages.
 """
 
 import pathlib
 import re
 
 from absl import app
 from absl import flags
@@ -54,39 +56,46 @@
     'versions',
     None,
     'space-separated list of pkg-name-1.0.0 pairs. can be obtained from local '
     'environment with '
     '`--version="$(pip freeze | sed s/==/-/g) flax-0.3.6"` '
     '(note the flax version "override") '
     'or from the "install dependencies" step in the github build action '
-    'https://github.com/google/flax/actions/workflows/build.yml')
+    'https://github.com/google/flax/actions/workflows/build.yml',
+)
 flags.mark_flag_as_required('versions')
 flags.DEFINE_bool('verbose', False, 'enables verbose output.')
 flags.DEFINE_list('ignore', ['jax'], 'packages not to add to requirements.')
 
 
 import_re = re.compile(r'(?:from|import)\s+(\w+)')
 # maps `import cv2` to `pip install opencv-python`
 pkg_map = {
-  'absl': 'absl-py',
-  'atari_py': 'atari-py',
-  'cv2': 'opencv-python',
-  'ml_collections': 'ml-collections',
-  'PIL': 'Pillow',
-  'tensorflow_datasets': 'tensorflow-datasets',
-  'tensorflow_text': 'tensorflow-text',
+    'absl': 'absl-py',
+    'atari_py': 'atari-py',
+    'cv2': 'opencv-python',
+    'ml_collections': 'ml-collections',
+    'PIL': 'Pillow',
+    'tensorflow_datasets': 'tensorflow-datasets',
+    'tensorflow_text': 'tensorflow-text',
 }
-standard_libs = set('codecs collections dataclasses datetime enum functools math multiprocessing itertools os pathlib random re sys tempfile time typing unicodedata warnings'.split(' '))
+standard_libs = set(
+    'codecs collections dataclasses datetime enum functools math'
+    ' multiprocessing itertools os pathlib random re sys tempfile time typing'
+    ' unicodedata warnings'.split(' ')
+)
 
 
 def main(argv):
   del argv
 
   versions = {
-      pkg_version[:pkg_version.rindex('-')]: pkg_version[pkg_version.rindex('-') + 1:]
+      pkg_version[: pkg_version.rindex('-')]: pkg_version[
+          pkg_version.rindex('-') + 1 :
+      ]
       for pkg_version in FLAGS.versions.replace('\n', ' ').split(' ')
       if '-' in pkg_version
   }
   if FLAGS.verbose:
     print('parsed versions:', sorted(versions.items()))
   ignore = set(FLAGS.ignore)
 
@@ -113,15 +122,16 @@
           pkgs.add(m.group(1))
 
     pkgs -= local_pkgs_and_modules | standard_libs
 
     print(f'{requirements} -', end=' ')
     with requirements.open('w') as f:
       for pkg in sorted(pkgs, key=str.casefold):
-        if pkg in ignore: continue
+        if pkg in ignore:
+          continue
         pkg = pkg_map.get(pkg, pkg)
         print(f'{pkg}-{versions[pkg]}', end=' ')
         f.write(f'{pkg}=={versions[pkg]}\n')
       print()
 
 
 if __name__ == '__main__':
```

### Comparing `flax-0.7.0/docs/Makefile` & `flax-0.7.1/docs/Makefile`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/README.md` & `flax-0.7.1/docs/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/_ext/codediff.py` & `flax-0.7.1/docs/_ext/codediff.py`

 * *Files 5% similar despite different names*

```diff
@@ -34,30 +34,40 @@
 from docutils.statemachine import ViewList
 
 import sphinx
 from sphinx.util.docutils import SphinxDirective
 
 MISSING = object()
 
+
 class CodeDiffParser:
 
   def parse(
-    self, lines, title_left='Base', title_right='Diff', code_sep='---', sync=MISSING):
+      self,
+      lines,
+      title_left='Base',
+      title_right='Diff',
+      code_sep='---',
+      sync=MISSING,
+  ):
     sync = sync is not MISSING
 
     if code_sep not in lines:
-      raise ValueError('Code separator not found! Code snippets should be '
-                       f'separated by {code_sep}.')
+      raise ValueError(
+          'Code separator not found! Code snippets should be '
+          f'separated by {code_sep}.'
+      )
     idx = lines.index(code_sep)
-    code_left = self._code_block(lines[0: idx])
-    test_code = lines[idx+1:]
+    code_left = self._code_block(lines[0:idx])
+    test_code = lines[idx + 1 :]
     code_right = self._code_block(test_code)
 
     output = self._tabs(
-      (title_left, code_left), (title_right, code_right), sync=sync)
+        (title_left, code_left), (title_right, code_right), sync=sync
+    )
 
     return output, test_code
 
   def _code_block(self, lines):
     """Creates a codeblock."""
     # Remove right trailing whitespace so we can detect the comments.
     lines = [x.rstrip() for x in lines]
@@ -84,26 +94,28 @@
         output += [f'    :sync: {key}']
 
       output += ['    ']
       output += ['    ' + line for line in content]
 
     return output
 
+
 class CodeDiffDirective(SphinxDirective):
   has_content = True
   option_spec = {
       'title_left': directives.unchanged,
       'title_right': directives.unchanged,
       'code_sep': directives.unchanged,
       'sync': directives.flag,
   }
 
   def run(self):
     table_code, test_code = CodeDiffParser().parse(
-        list(self.content), **self.options)
+        list(self.content), **self.options
+    )
 
     # Create a test node as a comment node so it won't show up in the docs.
     # We add attribute "testnodetype" so it is be picked up by the doctest
     # builder. This functionality is not officially documented but can be found
     # in the source code:
     # https://github.com/sphinx-doc/sphinx/blob/3.x/sphinx/ext/doctest.py
     # (search for 'testnodetype').
```

### Comparing `flax-0.7.0/docs/_ext/codediff_test.py` & `flax-0.7.1/docs/_ext/codediff_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,53 +18,53 @@
 
 from codediff import CodeDiffParser
 
 
 class CodeDiffTest(absltest.TestCase):
 
   def test_parse(self):
-
-    input_text = r'''@jax.jit #!
+    input_text = r"""@jax.jit #!
 def get_initial_params(key):   #!
   init_val = jnp.ones((1, 28, 28, 1), jnp.float32)
   initial_params = CNN().init(key, init_val)['params']
   extra_line
   return initial_params
 ---
 @jax.pmap #!
 def get_initial_params(key):
   init_val = jnp.ones((1, 28, 28, 1), jnp.float32)
   initial_params = CNN().init(key, init_val)['params']
-  return initial_params'''
+  return initial_params"""
 
-    expected_table = r'''+----------------------------------------------------------+----------------------------------------------------------+
+    expected_table = r"""+----------------------------------------------------------+----------------------------------------------------------+
 | Single device                                            | Ensembling on multiple devices                           |
 +----------------------------------------------------------+----------------------------------------------------------+
 | .. code-block:: python                                   | .. code-block:: python                                   |
 |   :emphasize-lines: 1,2                                  |   :emphasize-lines: 1                                    |
 |                                                          |                                                          |
 |   @jax.jit                                               |   @jax.pmap                                              |
 |   def get_initial_params(key):                           |   def get_initial_params(key):                           |
 |     init_val = jnp.ones((1, 28, 28, 1), jnp.float32)     |     init_val = jnp.ones((1, 28, 28, 1), jnp.float32)     |
 |     initial_params = CNN().init(key, init_val)['params'] |     initial_params = CNN().init(key, init_val)['params'] |
 |     extra_line                                           |     return initial_params                                |
 |     return initial_params                                |                                                          |
-+----------------------------------------------------------+----------------------------------------------------------+'''
++----------------------------------------------------------+----------------------------------------------------------+"""
 
-    expected_testcode = r'''@jax.pmap #!
+    expected_testcode = r"""@jax.pmap #!
 def get_initial_params(key):
   init_val = jnp.ones((1, 28, 28, 1), jnp.float32)
   initial_params = CNN().init(key, init_val)['params']
-  return initial_params'''
+  return initial_params"""
 
     title_left = 'Single device'
     title_right = 'Ensembling on multiple devices'
 
     actual_table, actual_testcode = CodeDiffParser().parse(
-      lines=input_text.split('\n'),
-      title_left=title_left,
-      title_right=title_right)
+        lines=input_text.split('\n'),
+        title_left=title_left,
+        title_right=title_right,
+    )
     actual_table = '\n'.join(actual_table)
     actual_testcode = '\n'.join(actual_testcode)
 
     self.assertEqual(expected_table, actual_table)
     self.assertEqual(expected_testcode, actual_testcode)
```

### Comparing `flax-0.7.0/docs/_ext/flax_module.py` & `flax-0.7.1/docs/_ext/flax_module.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,15 +15,14 @@
 """Sphinx directive for visualizing Flax modules.
 
 Use directive as follows:
 
 .. flax_module::
   :module: flax.linen
   :class: Dense
-
 """
 
 from docutils import nodes
 from docutils.parsers.rst import directives
 from docutils.statemachine import ViewList
 
 import sphinx
@@ -33,32 +32,43 @@
 import importlib
 
 
 def render_module(modname: str, qualname: str, app):
   parent = importlib.import_module(modname)
   obj = getattr(parent, qualname)
   template = ag.AutosummaryRenderer(app)
-  template_name = "flax_module"
+  template_name = 'flax_module'
   imported_members = False
   recursive = False
   context = {}
   return generate_autosummary_content(
-    qualname, obj, parent, template, template_name, imported_members,
-    app, recursive, context, modname, qualname)
+      qualname,
+      obj,
+      parent,
+      template,
+      template_name,
+      imported_members,
+      app,
+      recursive,
+      context,
+      modname,
+      qualname,
+  )
+
 
 class FlaxModuleDirective(SphinxDirective):
   has_content = True
   option_spec = {
-    'module': directives.unchanged,
-    'class': directives.unchanged,
+      'module': directives.unchanged,
+      'class': directives.unchanged,
   }
 
   def run(self):
     module_template = render_module(
-      self.options['module'], self.options['class'], self.env.app
+        self.options['module'], self.options['class'], self.env.app
     )
     module_template = module_template.splitlines()
 
     # Create a container for the rendered nodes
     container_node = nodes.container()
     self.content = ViewList(module_template, self.content.parent)
     self.state.nested_parse(self.content, self.content_offset, container_node)
```

### Comparing `flax-0.7.0/docs/api_reference/flax.linen/activation_functions.rst` & `flax-0.7.1/docs/api_reference/flax.linen/activation_functions.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/api_reference/flax.linen/initializers.rst` & `flax-0.7.1/docs/api_reference/flax.linen/initializers.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/api_reference/flax.linen/layers.rst` & `flax-0.7.1/docs/api_reference/flax.linen/layers.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/api_reference/flax.linen/spmd.rst` & `flax-0.7.1/docs/api_reference/flax.linen/spmd.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/api_reference/flax.linen/transformations.rst` & `flax-0.7.1/docs/api_reference/flax.linen/transformations.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/api_reference/flax.training.rst` & `flax-0.7.1/docs/api_reference/flax.training.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/api_reference/flax.traverse_util.rst` & `flax-0.7.1/docs/api_reference/flax.traverse_util.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/conf.py` & `flax-0.7.1/docs/conf.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,14 +27,15 @@
 #
 # import os
 # import sys
 # sys.path.insert(0, os.path.abspath('.'))
 
 import os
 import sys
+
 sys.path.insert(0, os.path.abspath('..'))
 # Include local extension.
 sys.path.append(os.path.abspath('./_ext'))
 
 # patch sphinx
 import docs.conf_sphinx_patch
 # -- Project information -----------------------------------------------------
@@ -106,17 +107,19 @@
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named 'default.css' will overwrite the builtin 'default.css'.
 html_static_path = ['_static']
 
 html_theme_options = {
     'repository_url': 'https://github.com/google/flax',
-    'use_repository_button': True,     # add a 'link to repository' button
-    'use_issues_button': False,        # add an 'Open an Issue' button
-    'path_to_docs': 'docs',            # used to compute the path to launch notebooks in colab
+    'use_repository_button': True,  # add a 'link to repository' button
+    'use_issues_button': False,  # add an 'Open an Issue' button
+    'path_to_docs': (
+        'docs'
+    ),  # used to compute the path to launch notebooks in colab
     'launch_buttons': {
         'colab_url': 'https://colab.research.google.com/',
     },
     'prev_next_buttons_location': None,
     'show_navbar_depth': 1,
 }
 
@@ -125,16 +128,16 @@
 # nb_execution_mode = 'off'
 # Notebook cell execution timeout; defaults to 30.
 nb_execution_timeout = 100
 # List of patterns, relative to source directory, that match notebook
 # files that will not be executed.
 myst_enable_extensions = ['dollarmath']
 nb_execution_excludepatterns = [
-  'getting_started.ipynb', # <-- times out
-  'optax_update_guide.ipynb', # <-- requires flax<=0.5.3
+    'getting_started.ipynb',  # <-- times out
+    'optax_update_guide.ipynb',  # <-- requires flax<=0.5.3
 ]
 # raise exceptions on execution so CI can catch errors
 nb_execution_allow_errors = False
 nb_execution_raise_on_error = True
 
 # -- Extension configuration -------------------------------------------------
```

### Comparing `flax-0.7.0/docs/conf_sphinx_patch.py` & `flax-0.7.1/docs/conf_sphinx_patch.py`

 * *Files 12% similar despite different names*

```diff
@@ -23,151 +23,177 @@
 #
 # We should consider sending a PR to sphinx so we can get rid of this.
 # Original source: https://github.com/sphinx-doc/sphinx/blob/0aedcc9a916daa92d477226da67d33ce1831822e/sphinx/ext/autosummary/generate.py#L211-L351
 from typing import Any, Dict, List, Set, Tuple
 import sphinx.ext.autosummary.generate as ag
 import sphinx.ext.autodoc
 
-def generate_autosummary_content(name: str, obj: Any, parent: Any,
-                                 template: ag.AutosummaryRenderer, template_name: str,
-                                 imported_members: bool, app: Any,
-                                 recursive: bool, context: Dict,
-                                 modname: str = None, qualname: str = None) -> str:
-    doc = ag.get_documenter(app, obj, parent)
-
-    def skip_member(obj: Any, name: str, objtype: str) -> bool:
-        try:
-            return app.emit_firstresult('autodoc-skip-member', objtype, name,
-                                        obj, False, {})
-        except Exception as exc:
-            ag.logger.warning(__('autosummary: failed to determine %r to be documented, '
-                              'the following exception was raised:\n%s'),
-                           name, exc, type='autosummary')
-            return False
-
-    def get_class_members(obj: Any) -> Dict[str, Any]:
-        members = sphinx.ext.autodoc.get_class_members(obj, [qualname], ag.safe_getattr)
-        return {name: member.object for name, member in members.items()}
-
-    def get_module_members(obj: Any) -> Dict[str, Any]:
-        members = {}
-        for name in ag.members_of(obj, app.config):
-            try:
-                members[name] = ag.safe_getattr(obj, name)
-            except AttributeError:
-                continue
-        return members
-
-    def get_all_members(obj: Any) -> Dict[str, Any]:
-        if doc.objtype == "module":
-            return get_module_members(obj)
-        elif doc.objtype == "class":
-            return get_class_members(obj)
-        return {}
-
-    def get_members(obj: Any, types: Set[str], include_public: List[str] = [],
-                    imported: bool = True) -> Tuple[List[str], List[str]]:
-        items: List[str] = []
-        public: List[str] = []
-
-        all_members = get_all_members(obj)
-        for name, value in all_members.items():
-            documenter = ag.get_documenter(app, value, obj)
-            if documenter.objtype in types:
-                # skip imported members if expected
-                if imported or getattr(value, '__module__', None) == obj.__name__:
-                    skipped = skip_member(value, name, documenter.objtype)
-                    if skipped is True:
-                        pass
-                    elif skipped is False:
-                        # show the member forcedly
-                        items.append(name)
-                        public.append(name)
-                    else:
-                        items.append(name)
-                        if name in include_public or not name.startswith('_'):
-                            # considers member as public
-                            public.append(name)
-        return public, items
-
-    def get_module_attrs(members: Any) -> Tuple[List[str], List[str]]:
-        """Find module attributes with docstrings."""
-        attrs, public = [], []
-        try:
-            analyzer = ag.ModuleAnalyzer.for_module(name)
-            attr_docs = analyzer.find_attr_docs()
-            for namespace, attr_name in attr_docs:
-                if namespace == '' and attr_name in members:
-                    attrs.append(attr_name)
-                    if not attr_name.startswith('_'):
-                        public.append(attr_name)
-        except ag.PycodeError:
-            pass    # give up if ModuleAnalyzer fails to parse code
-        return public, attrs
-
-    def get_modules(obj: Any) -> Tuple[List[str], List[str]]:
-        items: List[str] = []
-        for _, modname, _ispkg in ag.pkgutil.iter_modules(obj.__path__):
-            fullname = name + '.' + modname
-            try:
-                module = ag.import_module(fullname)
-                if module and hasattr(module, '__sphinx_mock__'):
-                    continue
-            except ImportError:
-                pass
-
-            items.append(fullname)
-        public = [x for x in items if not x.split('.')[-1].startswith('_')]
-        return public, items
 
-    ns: Dict[str, Any] = {}
-    ns.update(context)
+def generate_autosummary_content(
+    name: str,
+    obj: Any,
+    parent: Any,
+    template: ag.AutosummaryRenderer,
+    template_name: str,
+    imported_members: bool,
+    app: Any,
+    recursive: bool,
+    context: Dict,
+    modname: str = None,
+    qualname: str = None,
+) -> str:
+  doc = ag.get_documenter(app, obj, parent)
+
+  def skip_member(obj: Any, name: str, objtype: str) -> bool:
+    try:
+      return app.emit_firstresult(
+          'autodoc-skip-member', objtype, name, obj, False, {}
+      )
+    except Exception as exc:
+      ag.logger.warning(
+          __(
+              'autosummary: failed to determine %r to be documented, '
+              'the following exception was raised:\n%s'
+          ),
+          name,
+          exc,
+          type='autosummary',
+      )
+      return False
+
+  def get_class_members(obj: Any) -> Dict[str, Any]:
+    members = sphinx.ext.autodoc.get_class_members(
+        obj, [qualname], ag.safe_getattr
+    )
+    return {name: member.object for name, member in members.items()}
+
+  def get_module_members(obj: Any) -> Dict[str, Any]:
+    members = {}
+    for name in ag.members_of(obj, app.config):
+      try:
+        members[name] = ag.safe_getattr(obj, name)
+      except AttributeError:
+        continue
+    return members
 
+  def get_all_members(obj: Any) -> Dict[str, Any]:
     if doc.objtype == 'module':
-        scanner = ag.ModuleScanner(app, obj)
-        ns['members'] = scanner.scan(imported_members)
-        ns['functions'], ns['all_functions'] = \
-            get_members(obj, {'function'}, imported=imported_members)
-        ns['classes'], ns['all_classes'] = \
-            get_members(obj, {'class'}, imported=imported_members)
-        ns['exceptions'], ns['all_exceptions'] = \
-            get_members(obj, {'exception'}, imported=imported_members)
-        ns['attributes'], ns['all_attributes'] = \
-            get_module_attrs(ns['members'])
-        ispackage = hasattr(obj, '__path__')
-        if ispackage and recursive:
-            ns['modules'], ns['all_modules'] = get_modules(obj)
+      return get_module_members(obj)
     elif doc.objtype == 'class':
-        ns['members'] = dir(obj)
-        ns['inherited_members'] = \
-            set(dir(obj)) - set(obj.__dict__.keys())
-        ns['methods'], ns['all_methods'] = \
-            get_members(obj, {'method'}, ['__init__'])
-        ns['attributes'], ns['all_attributes'] = \
-            get_members(obj, {'attribute', 'property'})
-        ns['annotations'] = list(getattr(obj, '__annotations__', {}).keys())
-
-    if modname is None or qualname is None:
-        modname, qualname = ag.split_full_qualified_name(name)
-
-    if doc.objtype in ('method', 'attribute', 'property'):
-        ns['class'] = qualname.rsplit(".", 1)[0]
-
-    if doc.objtype in ('class',):
-        shortname = qualname
-    else:
-        shortname = qualname.rsplit(".", 1)[-1]
-
-    ns['fullname'] = name
-    ns['module'] = modname
-    ns['objname'] = qualname
-    ns['name'] = shortname
-
-    ns['objtype'] = doc.objtype
-    ns['underline'] = len(name) * '='
-
-    if template_name:
-        return template.render(template_name, ns)
-    else:
-        return template.render(doc.objtype, ns)
+      return get_class_members(obj)
+    return {}
+
+  def get_members(
+      obj: Any,
+      types: Set[str],
+      include_public: List[str] = [],
+      imported: bool = True,
+  ) -> Tuple[List[str], List[str]]:
+    items: List[str] = []
+    public: List[str] = []
+
+    all_members = get_all_members(obj)
+    for name, value in all_members.items():
+      documenter = ag.get_documenter(app, value, obj)
+      if documenter.objtype in types:
+        # skip imported members if expected
+        if imported or getattr(value, '__module__', None) == obj.__name__:
+          skipped = skip_member(value, name, documenter.objtype)
+          if skipped is True:
+            pass
+          elif skipped is False:
+            # show the member forcedly
+            items.append(name)
+            public.append(name)
+          else:
+            items.append(name)
+            if name in include_public or not name.startswith('_'):
+              # considers member as public
+              public.append(name)
+    return public, items
+
+  def get_module_attrs(members: Any) -> Tuple[List[str], List[str]]:
+    """Find module attributes with docstrings."""
+    attrs, public = [], []
+    try:
+      analyzer = ag.ModuleAnalyzer.for_module(name)
+      attr_docs = analyzer.find_attr_docs()
+      for namespace, attr_name in attr_docs:
+        if namespace == '' and attr_name in members:
+          attrs.append(attr_name)
+          if not attr_name.startswith('_'):
+            public.append(attr_name)
+    except ag.PycodeError:
+      pass  # give up if ModuleAnalyzer fails to parse code
+    return public, attrs
+
+  def get_modules(obj: Any) -> Tuple[List[str], List[str]]:
+    items: List[str] = []
+    for _, modname, _ispkg in ag.pkgutil.iter_modules(obj.__path__):
+      fullname = name + '.' + modname
+      try:
+        module = ag.import_module(fullname)
+        if module and hasattr(module, '__sphinx_mock__'):
+          continue
+      except ImportError:
+        pass
+
+      items.append(fullname)
+    public = [x for x in items if not x.split('.')[-1].startswith('_')]
+    return public, items
+
+  ns: Dict[str, Any] = {}
+  ns.update(context)
+
+  if doc.objtype == 'module':
+    scanner = ag.ModuleScanner(app, obj)
+    ns['members'] = scanner.scan(imported_members)
+    ns['functions'], ns['all_functions'] = get_members(
+        obj, {'function'}, imported=imported_members
+    )
+    ns['classes'], ns['all_classes'] = get_members(
+        obj, {'class'}, imported=imported_members
+    )
+    ns['exceptions'], ns['all_exceptions'] = get_members(
+        obj, {'exception'}, imported=imported_members
+    )
+    ns['attributes'], ns['all_attributes'] = get_module_attrs(ns['members'])
+    ispackage = hasattr(obj, '__path__')
+    if ispackage and recursive:
+      ns['modules'], ns['all_modules'] = get_modules(obj)
+  elif doc.objtype == 'class':
+    ns['members'] = dir(obj)
+    ns['inherited_members'] = set(dir(obj)) - set(obj.__dict__.keys())
+    ns['methods'], ns['all_methods'] = get_members(
+        obj, {'method'}, ['__init__']
+    )
+    ns['attributes'], ns['all_attributes'] = get_members(
+        obj, {'attribute', 'property'}
+    )
+    ns['annotations'] = list(getattr(obj, '__annotations__', {}).keys())
+
+  if modname is None or qualname is None:
+    modname, qualname = ag.split_full_qualified_name(name)
+
+  if doc.objtype in ('method', 'attribute', 'property'):
+    ns['class'] = qualname.rsplit('.', 1)[0]
+
+  if doc.objtype in ('class',):
+    shortname = qualname
+  else:
+    shortname = qualname.rsplit('.', 1)[-1]
+
+  ns['fullname'] = name
+  ns['module'] = modname
+  ns['objname'] = qualname
+  ns['name'] = shortname
+
+  ns['objtype'] = doc.objtype
+  ns['underline'] = len(name) * '='
+
+  if template_name:
+    return template.render(template_name, ns)
+  else:
+    return template.render(doc.objtype, ns)
+
 
 ag.generate_autosummary_content = generate_autosummary_content
```

### Comparing `flax-0.7.0/docs/contributing.md` & `flax-0.7.1/docs/contributing.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/developer_notes/lift.md` & `flax-0.7.1/docs/developer_notes/lift.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/developer_notes/module_lifecycle.rst` & `flax-0.7.1/docs/developer_notes/module_lifecycle.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/examples_community_examples.rst` & `flax-0.7.1/docs/examples_community_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/examples_core_examples.rst` & `flax-0.7.1/docs/examples_core_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/examples_google_research_examples.rst` & `flax-0.7.1/docs/examples_google_research_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/examples_repositories_that_use_flax.rst` & `flax-0.7.1/docs/examples_repositories_that_use_flax.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/flax.png` & `flax-0.7.1/docs/flax.png`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/flip/0000-template.md` & `flax-0.7.1/docs/flip/0000-template.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/flip/1009-optimizer-api.md` & `flax-0.7.1/docs/flip/1009-optimizer-api.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/flip/1777-default-dtype.md` & `flax-0.7.1/docs/flip/1777-default-dtype.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/flip/2434-general-metadata.md` & `flax-0.7.1/docs/flip/2434-general-metadata.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/flip/2974-kw-only-dataclasses.md` & `flax-0.7.1/docs/flip/2974-kw-only-dataclasses.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/flip/README.md` & `flax-0.7.1/docs/flip/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/getting_started.ipynb` & `flax-0.7.1/docs/getting_started.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/getting_started.md` & `flax-0.7.1/docs/getting_started.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/glossary.rst` & `flax-0.7.1/docs/glossary.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/arguments.md` & `flax-0.7.1/docs/guides/arguments.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/batch_norm.rst` & `flax-0.7.1/docs/guides/batch_norm.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/convert_pytorch_to_flax.rst` & `flax-0.7.1/docs/guides/convert_pytorch_to_flax.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/dropout.rst` & `flax-0.7.1/docs/guides/dropout.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/ensembling.rst` & `flax-0.7.1/docs/guides/ensembling.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/extracting_intermediates.rst` & `flax-0.7.1/docs/guides/extracting_intermediates.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/flax_basics.ipynb` & `flax-0.7.1/docs/guides/flax_basics.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9316194227853585%*

 * *Differences: {"'cells'": "{3: {'source': {insert: [(3, 'import flax\\n')], delete: [3]}}, 12: {'source': "*

 * *            '{insert: [(11, "true_params = flax.core.freeze({\'params\': {\'bias\': b, \'kernel\': '*

 * *            'W}})\\n")], delete: [11]}}, 26: {\'source\': {insert: [(24, "print(\'initialized '*

 * *            "parameter shapes:\\\\n', jax.tree_util.tree_map(jnp.shape, "*

 * *            'flax.core.unfreeze(params)))\\n")], delete: [24]}}, 30: {\'source\': {insert: [(21, '*

 * *            '"print(\'initialized parameter shap […]*

```diff
@@ -65,15 +65,15 @@
                 "id": "kN6bZDaReZO2"
             },
             "outputs": [],
             "source": [
                 "import jax\n",
                 "from typing import Any, Callable, Sequence\n",
                 "from jax import lax, random, numpy as jnp\n",
-                "from flax.core import freeze, unfreeze\n",
+                "import flax\n",
                 "from flax import linen as nn"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {
                 "id": "pCCwAbOLiscA"
@@ -168,48 +168,14 @@
                 "* Initialization functions are called to generate the initial set of parameters that the model will use. Those are functions that take as arguments `(PRNG Key, shape, dtype)` and return an Array of shape `shape`.\n",
                 "* The init function returns the initialized set of parameters (you can also get the output of the forward pass on the dummy input with the same syntax by using the `init_with_output` method instead of `init`."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {
-                "id": "3yL9mKk7naJn"
-            },
-            "source": [
-                "The output shows that the parameters are stored in a `FrozenDict` instance, which helps deal with the functional nature of JAX by preventing any mutation of the underlying dict and making the user aware of it. Read more about it in the [`flax.core.frozen_dict.FrozenDict` API docs](https://flax.readthedocs.io/en/latest/api_reference/flax.core.frozen_dict.html#flax.core.frozen_dict.FrozenDict).\n",
-                "\n",
-                "As a consequence, the following doesn't work:"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 5,
-            "metadata": {
-                "id": "HtOFWeiynaJo",
-                "outputId": "689b4230-2a3d-4823-d103-2858e6debc4d"
-            },
-            "outputs": [
-                {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "Error:  FrozenDict is immutable.\n"
-                    ]
-                }
-            ],
-            "source": [
-                "try:\n",
-                "    params['new_key'] = jnp.ones((2,2))\n",
-                "except ValueError as e:\n",
-                "    print(\"Error: \", e)"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {
                 "id": "M1qo9M3_naJo"
             },
             "source": [
                 "To conduct a forward pass with the model with a given set of parameters (which are never stored with the model), we just use the `apply` method by providing it the parameters to use as well as the input:"
             ]
         },
         {
@@ -276,15 +242,15 @@
                 "\n",
                 "# Generate random ground truth W and b.\n",
                 "key = random.PRNGKey(0)\n",
                 "k1, k2 = random.split(key)\n",
                 "W = random.normal(k1, (x_dim, y_dim))\n",
                 "b = random.normal(k2, (y_dim,))\n",
                 "# Store the parameters in a FrozenDict pytree.\n",
-                "true_params = freeze({'params': {'bias': b, 'kernel': W}})\n",
+                "true_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})\n",
                 "\n",
                 "# Generate samples with additional noise.\n",
                 "key_sample, key_noise = random.split(k1)\n",
                 "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
                 "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
                 "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
             ]
@@ -634,15 +600,15 @@
                 "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
                 "x = random.uniform(key1, (4,4))\n",
                 "\n",
                 "model = ExplicitMLP(features=[3,4,5])\n",
                 "params = model.init(key2, x)\n",
                 "y = model.apply(params, x)\n",
                 "\n",
-                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\n",
                 "print('output:\\n', y)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {
                 "id": "DDITIjXitEZl"
@@ -736,15 +702,15 @@
                 "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
                 "x = random.uniform(key1, (4,4))\n",
                 "\n",
                 "model = SimpleMLP(features=[3,4,5])\n",
                 "params = model.init(key2, x)\n",
                 "y = model.apply(params, x)\n",
                 "\n",
-                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\n",
                 "print('output:\\n', y)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {
                 "id": "es7YHjgexT-L"
@@ -959,16 +925,16 @@
                     ]
                 }
             ],
             "source": [
                 "for val in [1.0, 2.0, 3.0]:\n",
                 "  x = val * jnp.ones((10,5))\n",
                 "  y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n",
-                "  old_state, params = variables.pop('params')\n",
-                "  variables = freeze({'params': params, **updated_state})\n",
+                "  old_state, params = flax.core.pop(variables, 'params')\n",
+                "  variables = flax.core.freeze({'params': params, **updated_state})\n",
                 "  print('updated state:\\n', updated_state) # Shows only the mutable part"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {
                 "id": "GuUSOSKegKIM"
@@ -1024,15 +990,15 @@
                 "  (l, state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n",
                 "  updates, opt_state = tx.update(grads, opt_state)\n",
                 "  params = optax.apply_updates(params, updates)\n",
                 "  return opt_state, params, state\n",
                 "\n",
                 "x = jnp.ones((10,5))\n",
                 "variables = model.init(random.PRNGKey(0), x)\n",
-                "state, params = variables.pop('params')\n",
+                "state, params = flax.core.pop(variables, 'params')\n",
                 "del variables\n",
                 "tx = optax.sgd(learning_rate=0.02)\n",
                 "opt_state = tx.init(params)\n",
                 "\n",
                 "for _ in range(3):\n",
                 "  opt_state, params, state = update_step(tx, model.apply, x, opt_state, params, state)\n",
                 "  print('Updated state: ', state)"
@@ -1061,12 +1027,16 @@
                 "JAX released an experimental converter called [jax2tf](https://github.com/google/jax/tree/main/jax/experimental/jax2tf), which allows converting trained Flax models into Tensorflow's SavedModel format (so it can be used for [TF Hub](https://www.tensorflow.org/hub), [TF.lite](https://www.tensorflow.org/lite), [TF.js](https://www.tensorflow.org/js), or other downstream applications). The repository contains more documentation and has various examples for Flax."
             ]
         }
     ],
     "metadata": {
         "jupytext": {
             "formats": "ipynb,md:myst"
+        },
+        "language_info": {
+            "name": "python",
+            "version": "3.8.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 0
 }
```

### Comparing `flax-0.7.0/docs/guides/flax_basics.md` & `flax-0.7.1/docs/guides/flax_basics.md`

 * *Files 4% similar despite different names*

```diff
@@ -42,15 +42,15 @@
 
 ```{code-cell}
 :id: kN6bZDaReZO2
 
 import jax
 from typing import Any, Callable, Sequence
 from jax import lax, random, numpy as jnp
-from flax.core import freeze, unfreeze
+import flax
 from flax import linen as nn
 ```
 
 +++ {"id": "pCCwAbOLiscA"}
 
 ## Linear regression with Flax
 
@@ -92,30 +92,14 @@
 The result is what we expect: bias and kernel parameters of the correct size. Under the hood:
 
 *   The dummy input data `x` is used to trigger shape inference: we only declared the number of features we wanted in the output of the model, not the size of the input. Flax finds out by itself the correct size of the kernel.
 *   The random PRNG key is used to trigger the initialization functions (those have default values provided by the module here).
 * Initialization functions are called to generate the initial set of parameters that the model will use. Those are functions that take as arguments `(PRNG Key, shape, dtype)` and return an Array of shape `shape`.
 * The init function returns the initialized set of parameters (you can also get the output of the forward pass on the dummy input with the same syntax by using the `init_with_output` method instead of `init`.
 
-+++ {"id": "3yL9mKk7naJn"}
-
-The output shows that the parameters are stored in a `FrozenDict` instance, which helps deal with the functional nature of JAX by preventing any mutation of the underlying dict and making the user aware of it. Read more about it in the [`flax.core.frozen_dict.FrozenDict` API docs](https://flax.readthedocs.io/en/latest/api_reference/flax.core.frozen_dict.html#flax.core.frozen_dict.FrozenDict).
-
-As a consequence, the following doesn't work:
-
-```{code-cell}
-:id: HtOFWeiynaJo
-:outputId: 689b4230-2a3d-4823-d103-2858e6debc4d
-
-try:
-    params['new_key'] = jnp.ones((2,2))
-except ValueError as e:
-    print("Error: ", e)
-```
-
 +++ {"id": "M1qo9M3_naJo"}
 
 To conduct a forward pass with the model with a given set of parameters (which are never stored with the model), we just use the `apply` method by providing it the parameters to use as well as the input:
 
 ```{code-cell}
 :id: J8ietJecWiuK
 :outputId: 7bbe6bb4-94d5-4574-fbb5-aa0fcd1c84ae
@@ -144,15 +128,15 @@
 
 # Generate random ground truth W and b.
 key = random.PRNGKey(0)
 k1, k2 = random.split(key)
 W = random.normal(k1, (x_dim, y_dim))
 b = random.normal(k2, (y_dim,))
 # Store the parameters in a FrozenDict pytree.
-true_params = freeze({'params': {'bias': b, 'kernel': W}})
+true_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})
 
 # Generate samples with additional noise.
 key_sample, key_noise = random.split(k1)
 x_samples = random.normal(key_sample, (n_samples, x_dim))
 y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))
 print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)
 ```
@@ -318,15 +302,15 @@
 key1, key2 = random.split(random.PRNGKey(0), 2)
 x = random.uniform(key1, (4,4))
 
 model = ExplicitMLP(features=[3,4,5])
 params = model.init(key2, x)
 y = model.apply(params, x)
 
-print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))
+print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))
 print('output:\n', y)
 ```
 
 +++ {"id": "DDITIjXitEZl"}
 
 As we can see, a `nn.Module` subclass is made of:
 
@@ -374,15 +358,15 @@
 key1, key2 = random.split(random.PRNGKey(0), 2)
 x = random.uniform(key1, (4,4))
 
 model = SimpleMLP(features=[3,4,5])
 params = model.init(key2, x)
 y = model.apply(params, x)
 
-print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))
+print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))
 print('output:\n', y)
 ```
 
 +++ {"id": "es7YHjgexT-L"}
 
 There are, however, a few differences you should be aware of between the two declaration modes:
 
@@ -488,16 +472,16 @@
 ```{code-cell}
 :id: IbTsCAvZcdBy
 :outputId: 09a8bdd1-eaf8-401a-cf7c-386a7a5aa87b
 
 for val in [1.0, 2.0, 3.0]:
   x = val * jnp.ones((10,5))
   y, updated_state = model.apply(variables, x, mutable=['batch_stats'])
-  old_state, params = variables.pop('params')
-  variables = freeze({'params': params, **updated_state})
+  old_state, params = flax.core.pop(variables, 'params')
+  variables = flax.core.freeze({'params': params, **updated_state})
   print('updated state:\n', updated_state) # Shows only the mutable part
 ```
 
 +++ {"id": "GuUSOSKegKIM"}
 
 From this simplified example, you should be able to derive a full BatchNorm implementation, or any layer involving a state. To finish, let's add an optimizer to see how to play with both parameters updated by an optimizer and state variables.
 
@@ -521,15 +505,15 @@
   (l, state), grads = jax.value_and_grad(loss, has_aux=True)(params)
   updates, opt_state = tx.update(grads, opt_state)
   params = optax.apply_updates(params, updates)
   return opt_state, params, state
 
 x = jnp.ones((10,5))
 variables = model.init(random.PRNGKey(0), x)
-state, params = variables.pop('params')
+state, params = flax.core.pop(variables, 'params')
 del variables
 tx = optax.sgd(learning_rate=0.02)
 opt_state = tx.init(params)
 
 for _ in range(3):
   opt_state, params, state = update_step(tx, model.apply, x, opt_state, params, state)
   print('Updated state: ', state)
```

### Comparing `flax-0.7.0/docs/guides/flax_on_pjit.ipynb` & `flax-0.7.1/docs/guides/flax_on_pjit.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/flax_on_pjit.md` & `flax-0.7.1/docs/guides/flax_on_pjit.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/full_eval.rst` & `flax-0.7.1/docs/guides/full_eval.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/haiku_migration_guide.rst` & `flax-0.7.1/docs/guides/haiku_migration_guide.rst`

 * *Files 12% similar despite different names*

```diff
@@ -709,8 +709,156 @@
   y = model.apply(
     {'params': params},
     x=jax.numpy.ones((3, 12, 32)),
   )
 
 The only notable change with respect to the examples in the previous sections is that
 this time around we used ``hk.without_apply_rng`` in Haiku so we didn't have to
-pass the ``rng`` argument as ``None`` to the ``apply`` method.
+pass the ``rng`` argument as ``None`` to the ``apply`` method.
+
+Scan over layers
+----------------
+One very important application of ``scan`` is apply a sequence of layers iteratively
+over an input, passing the output of each layer as the input to the next layer. This
+is very useful to reduce compilation time for big models. As an example we will create
+a simple ``Block`` Module, and then use it inside an ``MLP`` Module that will apply
+the ``Block`` Module ``num_layers`` times.
+
+In Haiku, we define the ``Block`` Module as usual, and then inside ``MLP`` we will
+use ``hk.experimental.layer_stack`` over a ``stack_block`` function to create a stack
+of ``Block`` Modules. In Flax, the definition of ``Block`` is a little different,
+``__call__`` will accept and return a second dummy input/output that in both cases will
+be ``None``. In ``MLP``, we will use ``nn.scan`` as in the previous example, but
+by setting ``split_rngs={'params': True}`` and ``variable_axes={'params': 0}``
+we are telling ``nn.scan`` create different parameters for each step and slice the
+``params`` collection along the first axis, effectively implementing a stack of
+``Block`` Modules as in Haiku.
+
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  class Block(hk.Module):
+    def __init__(self, features: int, name=None):
+      super().__init__(name=name)
+      self.features = features
+
+    def __call__(self, x, training: bool):
+      x = hk.Linear(self.features)(x)
+      x = hk.dropout(hk.next_rng_key(), 0.5 if training else 0, x)
+      x = jax.nn.relu(x)
+      return x
+
+  class MLP(hk.Module):
+    def __init__(self, features: int, num_layers: int, name=None):
+        super().__init__(name=name)
+        self.features = features
+        self.num_layers = num_layers
+
+    def __call__(self, x, training: bool):
+     @hk.experimental.layer_stack(self.num_layers)
+      def stack_block(x):
+        return Block(self.features)(x, training)
+
+      stack = hk.experimental.layer_stack(self.num_layers)
+      return stack_block(x)
+
+  ---
+
+  class Block(nn.Module):
+    features: int
+    training: bool
+
+    @nn.compact
+    def __call__(self, x, _):
+      x = nn.Dense(self.features)(x)
+      x = nn.Dropout(0.5)(x, deterministic=not self.training)
+      x = jax.nn.relu(x)
+      return x, None
+
+  class MLP(nn.Module):
+    features: int
+    num_layers: int
+
+    @nn.compact
+    def __call__(self, x, training: bool):
+      ScanBlock = nn.scan(
+        Block, variable_axes={'params': 0}, split_rngs={'params': True},
+        length=self.num_layers)
+
+      y, _ = ScanBlock(self.features, training)(x, None)
+      return y
+
+Notice how in Flax we pass ``None`` as the second argument to ``ScanBlock`` and ignore
+its second output. These represent the inputs/outputs per-step but they are ``None``
+because in this case we don't have any.
+
+Initializing each model is the same as in previous examples. In this case,
+we will be specifying that we want to use ``5`` layers each with ``64`` features.
+
+.. codediff::
+  :title_left: Haiku
+  :title_right: Flax
+  :sync:
+
+  def forward(x, training: bool):
+    return MLP(64, num_layers=5)(x, training)
+
+  model = hk.transform(forward)
+
+  sample_x = jax.numpy.ones((1, 64))
+  params = model.init(
+    PRNGKey(0),
+    sample_x, training=False # <== inputs
+  )
+  ...
+
+  ---
+
+  ...
+
+
+  model = MLP(64, num_layers=5)
+
+  sample_x = jax.numpy.ones((1, 64))
+  variables = model.init(
+    PRNGKey(0),
+    sample_x, training=False # <== inputs
+  )
+  params = variables['params']
+
+When using scan over layers the one thing you should notice is that all layers
+are fused into a single layer whose parameters have an extra "layer" dimension on
+the first axis. In this case, the shape of all parameters will start with ``(5, ...)``
+as we are using ``5`` layers.
+
+.. tab-set::
+
+  .. tab-item:: Haiku
+    :sync: Haiku
+
+    .. code-block:: python
+
+      ...
+      {
+          'mlp/__layer_stack_no_per_layer/block/linear': {
+              'b': (5, 64),
+              'w': (5, 64, 64)
+          }
+      }
+      ...
+
+  .. tab-item:: Flax
+    :sync: Flax
+
+    .. code-block:: python
+
+      FrozenDict({
+          ScanBlock_0: {
+              Dense_0: {
+                  bias: (5, 64),
+                  kernel: (5, 64, 64),
+              },
+          },
+      })
```

### Comparing `flax-0.7.0/docs/guides/jax_for_the_impatient.ipynb` & `flax-0.7.1/docs/guides/jax_for_the_impatient.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/jax_for_the_impatient.md` & `flax-0.7.1/docs/guides/jax_for_the_impatient.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/linen_upgrade_guide.rst` & `flax-0.7.1/docs/guides/linen_upgrade_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/lr_schedule.rst` & `flax-0.7.1/docs/guides/lr_schedule.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/model_surgery.ipynb` & `flax-0.7.1/docs/guides/model_surgery.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/model_surgery.md` & `flax-0.7.1/docs/guides/model_surgery.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/optax_update_guide.rst` & `flax-0.7.1/docs/guides/optax_update_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/orbax_upgrade_guide.rst` & `flax-0.7.1/docs/guides/orbax_upgrade_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/regular_dict_upgrade_guide.rst` & `flax-0.7.1/docs/guides/regular_dict_upgrade_guide.rst`

 * *Files 1% similar despite different names*

```diff
@@ -28,15 +28,15 @@
 
   import flax
   import flax.linen as nn
   import jax
   import jax.numpy as jnp
 
   x = jnp.empty((1,3))
-  variables = nn.Dense(5).init(jax.random.PRNGKey(0), x)
+  variables = flax.core.freeze(nn.Dense(5).init(jax.random.PRNGKey(0), x))
 
   other_variables = jnp.array([1, 1, 1, 1, 1], dtype=jnp.float32)
 
 :meth:`copy <flax.core.frozen_dict.copy>`
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 .. codediff::
```

### Comparing `flax-0.7.0/docs/guides/rnncell_upgrade_guide.rst` & `flax-0.7.1/docs/guides/rnncell_upgrade_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/setup_or_nncompact.rst` & `flax-0.7.1/docs/guides/setup_or_nncompact.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/state_params.rst` & `flax-0.7.1/docs/guides/state_params.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/guides/transfer_learning.ipynb` & `flax-0.7.1/docs/guides/transfer_learning.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9993041497975709%*

 * *Differences: {"'cells'": "{13: {'source': {insert: [(0, 'import flax\\n'), (2, 'params = "*

 * *            "flax.core.unfreeze(params)\\n'), (4, 'params = flax.core.freeze(params)')], delete: "*

 * *            "[4, 2, 0]}}, 15: {'source': {insert: [(4, 'param_partitions = "*

 * *            "flax.core.freeze(traverse_util.path_aware_map(\\n'), (10, "*

 * *            "'flax.core.freeze(traverse_util.unflatten_dict(dict(flat[:2] + flat[-2:])))')], "*

 * *            'delete: [10, 4]}}}'}*

```diff
@@ -170,19 +170,19 @@
         },
         {
             "cell_type": "code",
             "execution_count": 9,
             "metadata": {},
             "outputs": [],
             "source": [
-                "from flax.core.frozen_dict import freeze\n",
+                "import flax\n",
                 "\n",
-                "params = params.unfreeze()\n",
+                "params = flax.core.unfreeze(params)\n",
                 "params['backbone'] = vision_model_vars['params']\n",
-                "params = freeze(params)"
+                "params = flax.core.freeze(params)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "**Note:** if the model contains other variable collections such as `batch_stats`, these have to be transfered as well.\n",
@@ -243,21 +243,21 @@
                 }
             ],
             "source": [
                 "from flax import traverse_util\n",
                 "import optax\n",
                 "\n",
                 "partition_optimizers = {'trainable': optax.adam(5e-3), 'frozen': optax.set_to_zero()}\n",
-                "param_partitions = freeze(traverse_util.path_aware_map(\n",
+                "param_partitions = flax.core.freeze(traverse_util.path_aware_map(\n",
                 "  lambda path, v: 'frozen' if 'backbone' in path else 'trainable', params))\n",
                 "tx = optax.multi_transform(partition_optimizers, param_partitions)\n",
                 "\n",
                 "# visualize a subset of the param_partitions structure\n",
                 "flat = list(traverse_util.flatten_dict(param_partitions).items())\n",
-                "freeze(traverse_util.unflatten_dict(dict(flat[:2] + flat[-2:])))"
+                "flax.core.freeze(traverse_util.unflatten_dict(dict(flat[:2] + flat[-2:])))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "To implement [differential learning rates](https://blog.slavv.com/differential-learning-rates-59eff5209a4f), the `optax.set_to_zero` can be replaced with any other optimizer, different optimizers and partitioning schemes can be selected depending on the task. For more information on advanced optimizers, refer to Optax's [Combining Optimizers](https://optax.readthedocs.io/en/latest/api.html#combining-optimizers) documentation.\n",
```

### Comparing `flax-0.7.0/docs/guides/transfer_learning.md` & `flax-0.7.1/docs/guides/transfer_learning.md`

 * *Files 3% similar despite different names*

```diff
@@ -110,19 +110,19 @@
 params = variables['params']
 ```
 
 ## Transfering the parameters
 Since `params` are currently random, the pretrained parameters from `vision_model_vars` have to be transfered to the `params` structure at the appropriate location. This can be done by unfreezing `params`, updating the `backbone` parameters, and freezing the `params` again:
 
 ```{code-cell} ipython3
-from flax.core.frozen_dict import freeze
+import flax
 
-params = params.unfreeze()
+params = flax.core.unfreeze(params)
 params['backbone'] = vision_model_vars['params']
-params = freeze(params)
+params = flax.core.freeze(params)
 ```
 
 **Note:** if the model contains other variable collections such as `batch_stats`, these have to be transfered as well.
 
 ## Optimization
 
 If you need to to train different parts of the model separately, you have three options:
@@ -149,21 +149,21 @@
 - Map parameters to partitions using [`flax.traverse_util.path_aware_map`](https://flax.readthedocs.io/en/latest/api_reference/flax.traverse_util.html#flax.traverse_util.path_aware_map), mark the leaves from the `backbone` as `frozen`, and the rest as `trainable`.
 
 ```{code-cell} ipython3
 from flax import traverse_util
 import optax
 
 partition_optimizers = {'trainable': optax.adam(5e-3), 'frozen': optax.set_to_zero()}
-param_partitions = freeze(traverse_util.path_aware_map(
+param_partitions = flax.core.freeze(traverse_util.path_aware_map(
   lambda path, v: 'frozen' if 'backbone' in path else 'trainable', params))
 tx = optax.multi_transform(partition_optimizers, param_partitions)
 
 # visualize a subset of the param_partitions structure
 flat = list(traverse_util.flatten_dict(param_partitions).items())
-freeze(traverse_util.unflatten_dict(dict(flat[:2] + flat[-2:])))
+flax.core.freeze(traverse_util.unflatten_dict(dict(flat[:2] + flat[-2:])))
 ```
 
 To implement [differential learning rates](https://blog.slavv.com/differential-learning-rates-59eff5209a4f), the `optax.set_to_zero` can be replaced with any other optimizer, different optimizers and partitioning schemes can be selected depending on the task. For more information on advanced optimizers, refer to Optax's [Combining Optimizers](https://optax.readthedocs.io/en/latest/api.html#combining-optimizers) documentation.
 
 ## Creating the `TrainState`
 
 Once the module, params, and optimizer are defined, the `TrainState` can be constructed as usual:
```

### Comparing `flax-0.7.0/docs/guides/use_checkpointing.ipynb` & `flax-0.7.1/docs/guides/use_checkpointing.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9937979978901458%*

 * *Differences: {"'cells'": "{4: {'outputs': []}, 7: {'outputs': {0: {'data': {'text/plain': {insert: [(17, ' }), "*

 * *            'tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, '*

 * *            'update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), '*

 * *            "EmptyState())),\\n')], delete: [17]}}}}}, 20: {'outputs': {0: {'data': {'text/plain': "*

 * *            '{insert: [(0, "{\'config\': {\'dimensions\': array([5, 3]), \'name\': '*

 * *            '\'dense\'},\\n"), […]*

```diff
@@ -52,29 +52,14 @@
             "source": [
                 "## Setup\n",
                 "\n",
                 "Install/upgrade Flax and [Orbax](https://github.com/google/orbax). For JAX installation with GPU/TPU support, visit [this section on GitHub](https://github.com/google/jax#installation)."
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 1,
-            "id": "e80f8743",
-            "metadata": {
-                "tags": [
-                    "skip-execution"
-                ]
-            },
-            "outputs": [],
-            "source": [
-                "# replace with `pip install -U flax` after release 0.6.9.\n",
-                "! pip install -U -qq \"git+https://github.com/google/flax.git@main#egg=flax\""
-            ]
-        },
-        {
             "attachments": {},
             "cell_type": "markdown",
             "id": "-icO30rwmKYj",
             "metadata": {
                 "id": "-icO30rwmKYj"
             },
             "source": [
@@ -97,23 +82,15 @@
         {
             "cell_type": "code",
             "execution_count": 3,
             "id": "SJT9DTxTytjn",
             "metadata": {
                 "id": "SJT9DTxTytjn"
             },
-            "outputs": [
-                {
-                    "name": "stderr",
-                    "output_type": "stream",
-                    "text": [
-                        "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
-                    ]
-                }
-            ],
+            "outputs": [],
             "source": [
                 "from typing import Optional, Any\n",
                 "import shutil\n",
                 "\n",
                 "import numpy as np\n",
                 "import jax\n",
                 "from jax import random, numpy as jnp\n",
@@ -179,15 +156,15 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: Array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: Array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState())),\n",
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState())),\n",
                             " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
                             " 'data': [Array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],      dtype=float32)]}"
                         ]
                     },
                     "execution_count": 5,
                     "metadata": {},
                     "output_type": "execute_result"
@@ -453,25 +430,25 @@
                 "id": "150b20a0",
                 "outputId": "85ffceca-f38d-46b8-e567-d9d38b7885f9"
             },
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "{'model': {'step': 1,\n",
+                            "{'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
+                            " 'data': {'0': array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
+                            "        dtype=float32)},\n",
+                            " 'model': {'opt_state': {'0': None, '1': None},\n",
                             "  'params': {'bias': array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "   'kernel': array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "          [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "          [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "          [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "          [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32)},\n",
-                            "  'opt_state': {'0': {}, '1': {}}},\n",
-                            " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
-                            " 'data': {'0': array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
-                            "        dtype=float32)}}"
+                            "  'step': 1}}"
                         ]
                     },
                     "execution_count": 11,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -528,15 +505,15 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState()))}"
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState()))}"
                         ]
                     },
                     "execution_count": 12,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -587,15 +564,15 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState()))}"
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState()))}"
                         ]
                     },
                     "execution_count": 13,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -625,15 +602,15 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState())),\n",
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState())),\n",
                             " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
                             " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
                             "        dtype=float32)]}"
                         ]
                     },
                     "execution_count": 14,
                     "metadata": {},
@@ -763,15 +740,15 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState()))}"
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState()))}"
                         ]
                     },
                     "execution_count": 16,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -795,15 +772,15 @@
                 "To bypass the error, you need to pass an Orbax [`transform`](https://github.com/google/orbax/blob/main/docs/checkpoint.md#transformations) that teaches Orbax how to conform this checkpoint into the structure of the `custom_target`.\n",
                 "\n",
                 "In this case, pass a default `{}` that lets Orbax use values in the `custom_target` to fill in the blank. This allows you to restore an old checkpoint into a new data structure, the `CustomTrainState`."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 17,
+            "execution_count": 20,
             "id": "a5d14c9f",
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
@@ -830,34 +807,35 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))}"
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))}"
                         ]
                     },
-                    "execution_count": 17,
+                    "execution_count": 20,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "try:\n",
                 "    checkpoint_manager.restore(4, items=custom_target)\n",
                 "except KeyError as e:\n",
                 "    print(f'KeyError when target state has an unmentioned field: {e}')\n",
                 "    print('')\n",
                 "\n",
                 "# Step 4 is an original `TrainState`, without the `batch_stats`\n",
-                "restored = checkpoint_manager.restore(4, items=custom_target, \n",
-                "                                      restore_kwargs={'transforms': {}})\n",
+                "custom_restore_args = orbax_utils.restore_args_from_target(custom_target)\n",
+                "restored = checkpoint_manager.restore(4, items=custom_target,\n",
+                "                                      restore_kwargs={'transforms': {}, 'restore_args': custom_restore_args})\n",
                 "assert type(restored['model']) == CustomTrainState\n",
-                "np.testing.assert_equal(restored['model'].batch_stats, \n",
+                "np.testing.assert_equal(restored['model'].batch_stats,\n",
                 "                        custom_target['model'].batch_stats)\n",
                 "restored"
             ]
         },
         {
             "cell_type": "markdown",
             "id": "74a4b0fd",
@@ -866,15 +844,15 @@
                 "##### With Orbax\n",
                 "\n",
                 "If you have already saved your checkpoints with the Orbax backend, you can use `orbax_transforms` to access this `transforms` argument in the Flax API."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 18,
+            "execution_count": 21,
             "id": "29fd1e33",
             "metadata": {
                 "id": "29fd1e33",
                 "outputId": "cdbb9247-d1eb-4458-aa83-8db0332af7cb"
             },
             "outputs": [
                 {
@@ -893,21 +871,21 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])),\n",
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])),\n",
                             " 'config': None,\n",
                             " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
                             "        dtype=float32)]}"
                         ]
                     },
-                    "execution_count": 18,
+                    "execution_count": 21,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "# Save in the \"Flax-with-Orbax\" backend.\n",
                 "flax.config.update('flax_use_orbax_checkpointing', True)\n",
@@ -931,15 +909,15 @@
                 "Using the legacy `flax.training.checkpoints` API, similar things are doable too, but they are not as flexible as the [Orbax Transformations](https://github.com/google/orbax/blob/main/docs/checkpoint.md#transformations).\n",
                 "\n",
                 "You need to restore the checkpoint to a raw dict with `target=None`, modify the structure accordingly, and then deserialize it back to the original target."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 19,
+            "execution_count": 22,
             "id": "051e7a16",
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "{'model': CustomTrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
@@ -955,21 +933,21 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])),\n",
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])),\n",
                             " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
                             " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
                             "        dtype=float32)]}"
                         ]
                     },
-                    "execution_count": 19,
+                    "execution_count": 22,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "# Save using the legacy Flax `checkpoints` API.\n",
                 "flax.config.update('flax_use_orbax_checkpointing', False)\n",
@@ -1003,15 +981,15 @@
                 "Note: You should use the same `async_checkpointer` to handle all your async saves across your training steps, so that it can make sure that a previous async save is done before the next one begins. This enables bookkeeping, such as `keep` (the number of checkpoints) and `overwrite` to be consistent across steps.\n",
                 "\n",
                 "Whenever you want to explicitly wait until an async save is done, you can call `async_checkpointer.wait_until_finished()`."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 20,
+            "execution_count": 23,
             "id": "85be68a6",
             "metadata": {
                 "id": "85be68a6",
                 "outputId": "aefce94c-8bae-4355-c142-05f2b61c39e2"
             },
             "outputs": [
                 {
@@ -1033,18 +1011,18 @@
                             " )>, params=FrozenDict({\n",
                             "     bias: array([-0.001, -0.001, -0.001], dtype=float32),\n",
                             "     kernel: array([[ 0.26048955, -0.61399287, -0.23458514],\n",
                             "            [ 0.11050402, -0.8765793 ,  0.9800635 ],\n",
                             "            [ 0.36260957,  0.18276349, -0.6856061 ],\n",
                             "            [-0.8519373 , -0.6416717 , -0.4818122 ],\n",
                             "            [-0.6886102 , -0.33987316, -0.05898903]], dtype=float32),\n",
-                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x12ab6d1f0>, update=<function chain.<locals>.update_fn at 0x12abb7ca0>), opt_state=(EmptyState(), EmptyState()))}"
+                            " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x158e7cd30>, update=<function chain.<locals>.update_fn at 0x158eaa670>), opt_state=(EmptyState(), EmptyState()))}"
                         ]
                     },
-                    "execution_count": 20,
+                    "execution_count": 23,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "# `orbax.checkpoint.AsyncCheckpointer` needs some multi-process initialization, because it was\n",
                 "# originally designed for multi-process large model checkpointing.\n",
@@ -1073,15 +1051,15 @@
             },
             "source": [
                 "If you are using Orbax `CheckpointManager`, just pass in the async_checkpointer when initializing it. Then, in practice, call `async_checkpoint_manager.wait_until_finished()` instead."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 21,
+            "execution_count": 24,
             "id": "af33b138",
             "metadata": {},
             "outputs": [],
             "source": [
                 "async_checkpoint_manager = orbax.checkpoint.CheckpointManager(\n",
                 "    'tmp/orbax/managed_async', async_checkpointer, options)\n",
                 "async_checkpoint_manager.wait_until_finished()"
@@ -1101,15 +1079,15 @@
                 "In the [Single Program Multi Data (SPMD)](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) paradigm with JAX `jit`, a large multi-process array can have its data sharded across different devices. (Note that JAX `pjit` and `jit` have been merged into a single unified interface. To learn about compiling and executing JAX functions in multi-host or multi-core environments, refer to [this guide](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) and the [jax.Array migration guide](https://jax.readthedocs.io/en/latest/jax_array_migration.html).) When a multi-process array is serialized, each host dumps its data shards to a single shared storage, such as a Google Cloud bucket.\n",
                 "\n",
                 "Orbax supports saving and loading pytrees with multi-process arrays in the same fashion as single-process pytrees. However, it's recommended to use the asynchronized [`orbax.AsyncCheckpointer`](https://github.com/google/orbax/blob/main/orbax/checkpoint/async_checkpointer.py) to save large multi-process arrays on another thread, so that you can perform computation alongside the saves. With pure Orbax, saving checkpoints in a multi-process context uses the same API as in a single-process context."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 22,
+            "execution_count": 25,
             "id": "ubdUvyMrhD-1",
             "metadata": {
                 "id": "ubdUvyMrhD-1"
             },
             "outputs": [],
             "source": [
                 "from jax.sharding import PartitionSpec, NamedSharding\n",
@@ -1124,15 +1102,15 @@
                 "\n",
                 "# Make it a pytree.\n",
                 "mp_ckpt = {'model': mp_array}"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 23,
+            "execution_count": 26,
             "id": "a669bc05",
             "metadata": {},
             "outputs": [],
             "source": [
                 "async_checkpoint_manager.save(0, mp_ckpt)\n",
                 "async_checkpoint_manager.wait_until_finished()"
             ]
@@ -1149,32 +1127,28 @@
                 "### With Orbax\n",
                 "\n",
                 "Orbax allows you to specify this by passing a pytree of `sharding`s in `restore_args`. If you already have a reference pytree that has all the arrays with the right sharding, you can use `orbax_utils.restore_args_from_target` to transform it into the `restore_args` that Orbax needs."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 24,
+            "execution_count": 27,
             "id": "b8e7daaa",
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "{'model': array([[ 0,  1],\n",
-                            "        [ 2,  3],\n",
-                            "        [ 4,  5],\n",
-                            "        [ 6,  7],\n",
-                            "        [ 8,  9],\n",
-                            "        [10, 11],\n",
-                            "        [12, 13],\n",
-                            "        [14, 15]], dtype=int32)}"
+                            "{'model': Array([[0, 1],\n",
+                            "        [2, 3],\n",
+                            "        [4, 5],\n",
+                            "        [6, 7]], dtype=int32)}"
                         ]
                     },
-                    "execution_count": 24,
+                    "execution_count": 27,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "# The reference doesn't need to be as large as your checkpoint!\n",
                 "# Just make sure it has the `.sharding` you want.\n",
@@ -1199,28 +1173,28 @@
                 "In legacy Flax, to save multi-process arrays, use [`flax.training.checkpoints.save_checkpoint_multiprocess()`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint_multiprocess) in place of `save_checkpoint()` and with the same arguments.\n",
                 "\n",
                 "If your checkpoint is too large, you can specify `timeout_secs` in the manager and give it more time to finish writing."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 25,
+            "execution_count": 28,
             "id": "5d10039b",
             "metadata": {
                 "id": "5d10039b",
                 "outputId": "901bb097-0899-479d-b9ae-61dae79e7057"
             },
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "'tmp/checkpoint_3'"
                         ]
                     },
-                    "execution_count": 25,
+                    "execution_count": 28,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "async_checkpointer = orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler(), timeout_secs=50)\n",
                 "checkpoints.save_checkpoint_multiprocess(ckpt_dir,\n",
@@ -1229,35 +1203,31 @@
                 "                                         overwrite=True,\n",
                 "                                         keep=4,\n",
                 "                                         orbax_checkpointer=async_checkpointer)"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 26,
+            "execution_count": 29,
             "id": "a9f9724c",
             "metadata": {
                 "id": "a9f9724c",
                 "outputId": "393c4a0e-8a8c-4ca6-c609-93c8bab38e75"
             },
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "{'model': array([[ 0,  1],\n",
-                            "        [ 2,  3],\n",
-                            "        [ 4,  5],\n",
-                            "        [ 6,  7],\n",
-                            "        [ 8,  9],\n",
-                            "        [10, 11],\n",
-                            "        [12, 13],\n",
-                            "        [14, 15]], dtype=int32)}"
+                            "{'model': Array([[0, 1],\n",
+                            "        [2, 3],\n",
+                            "        [4, 5],\n",
+                            "        [6, 7]], dtype=int32)}"
                         ]
                     },
-                    "execution_count": 26,
+                    "execution_count": 29,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "mp_restored = checkpoints.restore_checkpoint(ckpt_dir,\n",
                 "                                             target=ref_ckpt,\n",
@@ -1332,13 +1302,13 @@
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.9.15"
+            "version": "3.9.16"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 5
 }
```

### Comparing `flax-0.7.0/docs/guides/use_checkpointing.md` & `flax-0.7.1/docs/guides/use_checkpointing.md`

 * *Files 1% similar despite different names*

```diff
@@ -49,19 +49,14 @@
 
 <!-- #region id="5a2f6aae" -->
 ## Setup
 
 Install/upgrade Flax and [Orbax](https://github.com/google/orbax). For JAX installation with GPU/TPU support, visit [this section on GitHub](https://github.com/google/jax#installation).
 <!-- #endregion -->
 
-```python tags=["skip-execution"]
-# replace with `pip install -U flax` after release 0.6.9.
-! pip install -U -qq "git+https://github.com/google/flax.git@main#egg=flax"
-```
-
 <!-- #region id="-icO30rwmKYj" -->
 Note: Before running `import jax`, create eight fake devices to mimic a [multi-host environment](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html?#aside-hosts-and-devices-in-jax) in this notebook. Note that the order of imports is important here. The `os.environ["XLA_FLAGS"] = '--xla_force_host_platform_device_count=8'` command works only with the CPU backend, which means it won't work with GPU/TPU acceleration on if you're running this notebook in Google Colab. If you are already running the code on multiple devices (for example, in a 4x2 TPU environment), you can skip running the next cell.
 <!-- #endregion -->
 
 ```python id="ArKLnsyGRxGv"
 import os
 os.environ["XLA_FLAGS"] = '--xla_force_host_platform_device_count=8'
@@ -314,18 +309,19 @@
 try:
     checkpoint_manager.restore(4, items=custom_target)
 except KeyError as e:
     print(f'KeyError when target state has an unmentioned field: {e}')
     print('')
 
 # Step 4 is an original `TrainState`, without the `batch_stats`
-restored = checkpoint_manager.restore(4, items=custom_target, 
-                                      restore_kwargs={'transforms': {}})
+custom_restore_args = orbax_utils.restore_args_from_target(custom_target)
+restored = checkpoint_manager.restore(4, items=custom_target,
+                                      restore_kwargs={'transforms': {}, 'restore_args': custom_restore_args})
 assert type(restored['model']) == CustomTrainState
-np.testing.assert_equal(restored['model'].batch_stats, 
+np.testing.assert_equal(restored['model'].batch_stats,
                         custom_target['model'].batch_stats)
 restored
 ```
 
 ##### With Orbax
 
 If you have already saved your checkpoints with the Orbax backend, you can use `orbax_transforms` to access this `transforms` argument in the Flax API.
```

### Comparing `flax-0.7.0/docs/index.rst` & `flax-0.7.1/docs/index.rst`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/mission.md` & `flax-0.7.1/docs/mission.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/flax_sharp_bits.ipynb` & `flax-0.7.1/docs/notebooks/flax_sharp_bits.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/flax_sharp_bits.md` & `flax-0.7.1/docs/notebooks/flax_sharp_bits.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/full_eval.ipynb` & `flax-0.7.1/docs/notebooks/full_eval.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/full_eval.md` & `flax-0.7.1/docs/notebooks/full_eval.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/linen_intro.ipynb` & `flax-0.7.1/docs/notebooks/linen_intro.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9997526925710697%*

 * *Differences: {"'cells'": "{4: {'source': {insert: [(4, 'import flax\\n')], delete: [4]}}, 16: {'source': "*

 * *            '{insert: [(24, "print(\'initialized parameter shapes:\\\\n\', '*

 * *            'jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\\n")], delete: '*

 * *            '[24]}}, 18: {\'source\': {insert: [(22, "print(\'initialized parameter '*

 * *            "shapes:\\\\n', jax.tree_util.tree_map(jnp.shape, "*

 * *            'flax.core.unfreeze(init_variables)))\\n")], delete: [22]}}, 29: {\'source\':  […]*

```diff
@@ -71,15 +71,15 @@
             },
             "outputs": [],
             "source": [
                 "import functools\n",
                 "from typing import Any, Callable, Sequence, Optional\n",
                 "import jax\n",
                 "from jax import lax, random, numpy as jnp\n",
-                "from flax.core import freeze, unfreeze\n",
+                "import flax\n",
                 "from flax import linen as nn"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {
@@ -302,15 +302,15 @@
                 "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
                 "x = random.uniform(key1, (4,4))\n",
                 "\n",
                 "model = ExplicitMLP(features=[3,4,5])\n",
                 "init_variables = model.init(key2, x)\n",
                 "y = model.apply(init_variables, x)\n",
                 "\n",
-                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\n",
                 "print('output:\\n', y)"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {
@@ -366,15 +366,15 @@
                 "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
                 "x = random.uniform(key1, (4,4))\n",
                 "\n",
                 "model = SimpleMLP(features=[3,4,5])\n",
                 "init_variables = model.init(key2, x)\n",
                 "y = model.apply(init_variables, x)\n",
                 "\n",
-                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\n",
                 "print('output:\\n', y)"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {
@@ -739,25 +739,25 @@
                 "\n",
                 "key1, key2, key3, key4 = random.split(random.PRNGKey(0), 4)\n",
                 "x = random.uniform(key1, (3,4,4))\n",
                 "\n",
                 "model = Block(features=3, training=True)\n",
                 "\n",
                 "init_variables = model.init({'params': key2, 'dropout': key3}, x)\n",
-                "_, init_params = init_variables.pop('params')\n",
+                "_, init_params = flax.core.pop(init_variables, 'params')\n",
                 "\n",
                 "# When calling `apply` with mutable kinds, returns a pair of output,\n",
                 "# mutated_variables.\n",
                 "y, mutated_variables = model.apply(\n",
                 "    init_variables, x, rngs={'dropout': key4}, mutable=['batch_stats'])\n",
                 "\n",
                 "# Now we reassemble the full variables from the updates (in a real training\n",
                 "# loop, with the updated params from an optimizer).\n",
-                "updated_variables = freeze(dict(params=init_params,\n",
-                "                                **mutated_variables))\n",
+                "updated_variables = flax.core.freeze(dict(params=init_params,\n",
+                "                                          **mutated_variables))\n",
                 "\n",
                 "print('updated variables:\\n', updated_variables)\n",
                 "print('initialized variable shapes:\\n',\n",
                 "      jax.tree_util.tree_map(jnp.shape, init_variables))\n",
                 "print('output:\\n', y)\n",
                 "\n",
                 "# Let's run these model variables during \"evaluation\":\n",
@@ -838,15 +838,15 @@
                 "key1, key2 = random.split(random.PRNGKey(3), 2)\n",
                 "x = random.uniform(key1, (4,4))\n",
                 "\n",
                 "model = MLP(features=[3,4,5])\n",
                 "init_variables = model.init(key2, x)\n",
                 "y = model.apply(init_variables, x)\n",
                 "\n",
-                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\n",
                 "print('output:\\n', y)"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {
@@ -909,15 +909,15 @@
                 "key1, key2 = random.split(random.PRNGKey(3), 2)\n",
                 "x = random.uniform(key1, (4,4))\n",
                 "\n",
                 "model = RematMLP(features=[3,4,5])\n",
                 "init_variables = model.init(key2, x)\n",
                 "y = model.apply(init_variables, x)\n",
                 "\n",
-                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\n",
                 "print('output:\\n', y)"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {
@@ -1065,15 +1065,15 @@
                 "model = functools.partial(\n",
                 "  MultiHeadDotProductAttention,\n",
                 "  broadcast_dropout=False,\n",
                 "  num_heads=2,\n",
                 "  batch_axes=(0,))\n",
                 "\n",
                 "init_variables = model(train=False).init({'params': key2}, x, x)\n",
-                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\n",
                 "\n",
                 "y = model(train=True).apply(init_variables, x, x, rngs={'dropout': key4})\n",
                 "print('output:\\n', y.shape)"
             ]
         },
         {
             "attachments": {},
@@ -1145,15 +1145,15 @@
                 "\n",
                 "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
                 "xs = random.uniform(key1, (1, 5, 2))\n",
                 "\n",
                 "model = SimpleScan(2)\n",
                 "init_variables = model.init(key2, xs)\n",
                 "\n",
-                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))\n",
+                "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))\n",
                 "\n",
                 "y = model.apply(init_variables, xs)\n",
                 "print('output:\\n', y)"
             ]
         }
     ],
     "metadata": {
```

### Comparing `flax-0.7.0/docs/notebooks/linen_intro.md` & `flax-0.7.1/docs/notebooks/linen_intro.md`

 * *Files 2% similar despite different names*

```diff
@@ -49,15 +49,15 @@
 ```{code-cell}
 :id: Kvx7GmavHZbD
 
 import functools
 from typing import Any, Callable, Sequence, Optional
 import jax
 from jax import lax, random, numpy as jnp
-from flax.core import freeze, unfreeze
+import flax
 from flax import linen as nn
 ```
 
 +++ {"id": "u86fYsrEfYow"}
 
 # Invoking Modules
 
@@ -150,15 +150,15 @@
 key1, key2 = random.split(random.PRNGKey(0), 2)
 x = random.uniform(key1, (4,4))
 
 model = ExplicitMLP(features=[3,4,5])
 init_variables = model.init(key2, x)
 y = model.apply(init_variables, x)
 
-print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))
+print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))
 print('output:\n', y)
 ```
 
 +++ {"id": "slwE6ULqc_t_"}
 
 Here we show the equivalent compact form of the MLP that declares the submodules inline using the `@compact` decorator.
 
@@ -185,15 +185,15 @@
 key1, key2 = random.split(random.PRNGKey(0), 2)
 x = random.uniform(key1, (4,4))
 
 model = SimpleMLP(features=[3,4,5])
 init_variables = model.init(key2, x)
 y = model.apply(init_variables, x)
 
-print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))
+print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))
 print('output:\n', y)
 ```
 
 +++ {"id": "b2OzKXYyjFSf"}
 
 ## Declaring and using variables
 
@@ -353,25 +353,25 @@
 
 key1, key2, key3, key4 = random.split(random.PRNGKey(0), 4)
 x = random.uniform(key1, (3,4,4))
 
 model = Block(features=3, training=True)
 
 init_variables = model.init({'params': key2, 'dropout': key3}, x)
-_, init_params = init_variables.pop('params')
+_, init_params = flax.core.pop(init_variables, 'params')
 
 # When calling `apply` with mutable kinds, returns a pair of output,
 # mutated_variables.
 y, mutated_variables = model.apply(
     init_variables, x, rngs={'dropout': key4}, mutable=['batch_stats'])
 
 # Now we reassemble the full variables from the updates (in a real training
 # loop, with the updated params from an optimizer).
-updated_variables = freeze(dict(params=init_params,
-                                **mutated_variables))
+updated_variables = flax.core.freeze(dict(params=init_params,
+                                          **mutated_variables))
 
 print('updated variables:\n', updated_variables)
 print('initialized variable shapes:\n',
       jax.tree_util.tree_map(jnp.shape, init_variables))
 print('output:\n', y)
 
 # Let's run these model variables during "evaluation":
@@ -415,15 +415,15 @@
 key1, key2 = random.split(random.PRNGKey(3), 2)
 x = random.uniform(key1, (4,4))
 
 model = MLP(features=[3,4,5])
 init_variables = model.init(key2, x)
 y = model.apply(init_variables, x)
 
-print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))
+print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))
 print('output:\n', y)
 ```
 
 +++ {"id": "D1tfTdRjyJYK"}
 
 ## Remat
 
@@ -455,15 +455,15 @@
 key1, key2 = random.split(random.PRNGKey(3), 2)
 x = random.uniform(key1, (4,4))
 
 model = RematMLP(features=[3,4,5])
 init_variables = model.init(key2, x)
 y = model.apply(init_variables, x)
 
-print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))
+print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))
 print('output:\n', y)
 ```
 
 +++ {"id": "l0pJtxVwyCgp"}
 
 ## Vmap
 
@@ -583,15 +583,15 @@
 model = functools.partial(
   MultiHeadDotProductAttention,
   broadcast_dropout=False,
   num_heads=2,
   batch_axes=(0,))
 
 init_variables = model(train=False).init({'params': key2}, x, x)
-print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))
+print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))
 
 y = model(train=True).apply(init_variables, x, x, rngs={'dropout': key4})
 print('output:\n', y.shape)
 ```
 
 +++ {"id": "U-bDSQElvM09"}
 
@@ -631,12 +631,12 @@
 
 key1, key2 = random.split(random.PRNGKey(0), 2)
 xs = random.uniform(key1, (1, 5, 2))
 
 model = SimpleScan(2)
 init_variables = model.init(key2, xs)
 
-print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(init_variables)))
+print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(init_variables)))
 
 y = model.apply(init_variables, xs)
 print('output:\n', y)
 ```
```

### Comparing `flax-0.7.0/docs/notebooks/optax_update_guide.ipynb` & `flax-0.7.1/docs/notebooks/optax_update_guide.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/optax_update_guide.md` & `flax-0.7.1/docs/notebooks/optax_update_guide.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/orbax_upgrade_guide.ipynb` & `flax-0.7.1/docs/notebooks/orbax_upgrade_guide.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/orbax_upgrade_guide.md` & `flax-0.7.1/docs/notebooks/orbax_upgrade_guide.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/notebooks/state_params.ipynb` & `flax-0.7.1/docs/notebooks/state_params.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9996995726495727%*

 * *Differences: {"'cells'": "{3: {'source': {insert: [(7, 'import flax\\n')]}}, 8: {'source': {insert: [(3, "*

 * *            '"state, params = flax.core.pop(variables, \'params\')\\n")], delete: [3]}}, 14: '*

 * *            '{\'source\': {insert: [(3, "state, params = flax.core.pop(variables, '*

 * *            '\'params\')\\n")], delete: [3]}}}'}*

```diff
@@ -51,14 +51,15 @@
                 "from functools import partial\n",
                 "\n",
                 "import jax\n",
                 "from jax import numpy as jnp\n",
                 "from jax import random\n",
                 "import optax\n",
                 "\n",
+                "import flax\n",
                 "from flax import linen as nn\n",
                 "\n",
                 "\n",
                 "#@title\n",
                 "# Initialize random variables\n",
                 "dummy_input = jnp.ones((32, 5))\n",
                 "\n",
@@ -148,15 +149,15 @@
                 "id": "8RUFi57GVktj"
             },
             "outputs": [],
             "source": [
                 "model = BiasAdderWithRunningMean()\n",
                 "variables = model.init(random.PRNGKey(0), dummy_input)\n",
                 "# Split state and params (which are updated by optimizer).\n",
-                "state, params = variables.pop('params')\n",
+                "state, params = flax.core.pop(variables, 'params')\n",
                 "del variables  # Delete variables to avoid wasting resources\n",
                 "tx = optax.sgd(learning_rate=0.02)\n",
                 "opt_state = tx.init(params)\n",
                 "\n",
                 "for epoch_num in range(num_epochs):\n",
                 "  opt_state, params, state = update_step(\n",
                 "      model.apply, dummy_input, opt_state, params, state)"
@@ -267,15 +268,15 @@
                 "id": "OdMTQcMoYUtk"
             },
             "outputs": [],
             "source": [
                 "model = MLP(hidden_size=10, out_size=1)\n",
                 "variables = model.init(random.PRNGKey(0), dummy_input)\n",
                 "# Split state and params (which are updated by optimizer).\n",
-                "state, params = variables.pop('params')\n",
+                "state, params = flax.core.pop(variables, 'params')\n",
                 "del variables  # Delete variables to avoid wasting resources\n",
                 "tx = optax.sgd(learning_rate=0.02)\n",
                 "opt_state = tx.init(params)\n",
                 "\n",
                 "for epoch_num in range(num_epochs):\n",
                 "  opt_state, params, state, loss = update_step(\n",
                 "      model.apply, X, Y, opt_state, params, state)\n",
```

### Comparing `flax-0.7.0/docs/notebooks/state_params.md` & `flax-0.7.1/docs/notebooks/state_params.md`

 * *Files 1% similar despite different names*

```diff
@@ -37,14 +37,15 @@
 from functools import partial
 
 import jax
 from jax import numpy as jnp
 from jax import random
 import optax
 
+import flax
 from flax import linen as nn
 
 
 #@title
 # Initialize random variables
 dummy_input = jnp.ones((32, 5))
 
@@ -109,15 +110,15 @@
 
 ```{code-cell} ipython3
 :id: 8RUFi57GVktj
 
 model = BiasAdderWithRunningMean()
 variables = model.init(random.PRNGKey(0), dummy_input)
 # Split state and params (which are updated by optimizer).
-state, params = variables.pop('params')
+state, params = flax.core.pop(variables, 'params')
 del variables  # Delete variables to avoid wasting resources
 tx = optax.sgd(learning_rate=0.02)
 opt_state = tx.init(params)
 
 for epoch_num in range(num_epochs):
   opt_state, params, state = update_step(
       model.apply, dummy_input, opt_state, params, state)
@@ -198,15 +199,15 @@
 
 ```{code-cell} ipython3
 :id: OdMTQcMoYUtk
 
 model = MLP(hidden_size=10, out_size=1)
 variables = model.init(random.PRNGKey(0), dummy_input)
 # Split state and params (which are updated by optimizer).
-state, params = variables.pop('params')
+state, params = flax.core.pop(variables, 'params')
 del variables  # Delete variables to avoid wasting resources
 tx = optax.sgd(learning_rate=0.02)
 opt_state = tx.init(params)
 
 for epoch_num in range(num_epochs):
   opt_state, params, state, loss = update_step(
       model.apply, X, Y, opt_state, params, state)
```

### Comparing `flax-0.7.0/docs/overview.md` & `flax-0.7.1/docs/overview.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/philosophy.md` & `flax-0.7.1/docs/philosophy.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/docs/requirements.txt` & `flax-0.7.1/docs/requirements.txt`

 * *Files 4% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 jaxlib
 ipykernel
 myst_nb
 recommonmark
 ipython_genutils
 sphinx-design
 jupytext==1.13.8
+dm-haiku
 
 # Need to pin docutils to 0.16 to make bulleted lists appear correctly on
 # ReadTheDocs: https://stackoverflow.com/a/68008428
 docutils==0.16
 
 # The next packages are for notebooks.
 matplotlib
```

### Comparing `flax-0.7.0/examples/README.md` & `flax-0.7.1/examples/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/cloud/README.md` & `flax-0.7.1/examples/cloud/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/cloud/launch_gce.py` & `flax-0.7.1/examples/cloud/launch_gce.py`

 * *Files 22% similar despite different names*

```diff
@@ -25,86 +25,115 @@
 from absl import app
 from absl import flags
 
 # General options.
 flags.DEFINE_bool(
     'dry_run',
     False,
-    help='If set, then the command to launch the GCE instance will only be '
-    'printed to stdout.')
+    help=(
+        'If set, then the command to launch the GCE instance will only be '
+        'printed to stdout.'
+    ),
+)
 flags.DEFINE_bool(
     'connect',
     False,
-    help='Same as --wait, but directly connect to VM once it is ready.')
+    help='Same as --wait, but directly connect to VM once it is ready.',
+)
 flags.DEFINE_bool(
-    'wait', False,
-    help='If set, then the script will wait until VM is ready. If VM_READY_CMD '
-    'is set in environment, then that command will be executed once the VM '
-    'is ready. Useful for sending a notification, e.g. "osascript" (mac).')
+    'wait',
+    False,
+    help=(
+        'If set, then the script will wait until VM is ready. If VM_READY_CMD '
+        'is set in environment, then that command will be executed once the VM '
+        'is ready. Useful for sending a notification, e.g. "osascript" (mac).'
+    ),
+)
 
 # Machine configuration.
 flags.DEFINE_string('project', None, help='Name of the Google Cloud project.')
 flags.DEFINE_string('zone', None, help='Zone in which the VM will be created.')
 flags.DEFINE_string(
     'machine_type',
     None,
-    help='Machine type to use for VM. See "gcloud compute machine-types list".')
+    help='Machine type to use for VM. See "gcloud compute machine-types list".',
+)
 flags.DEFINE_string(
     'accelerator_type',
     '',
-    help='Type of accelerator to use, or empty. '
-    'See "gcloud compute accelerator-types list".'
+    help=(
+        'Type of accelerator to use, or empty. '
+        'See "gcloud compute accelerator-types list".'
+    ),
 )
 flags.DEFINE_integer(
     'shutdown_secs',
     300,
-    help='How long to wait (after successful/failed training) before shutting '
-    'down the VM. Set to 0 to disable.'
+    help=(
+        'How long to wait (after successful/failed training) before shutting '
+        'down the VM. Set to 0 to disable.'
+    ),
 )
 flags.DEFINE_integer(
-    'accelerator_count', 8, help='Number of accelerators to use.')
+    'accelerator_count', 8, help='Number of accelerators to use.'
+)
 
 # GCS configuration.
 flags.DEFINE_string(
     'gcs_workdir_base',
     None,
-    help='GCS base directory for model output. The --workdir argument will be '
-    'constructed from {gcs_workdir_base}/{example}/{name}/{timestamp} .')
+    help=(
+        'GCS base directory for model output. The --workdir argument will be '
+        'constructed from {gcs_workdir_base}/{example}/{name}/{timestamp} .'
+    ),
+)
 flags.DEFINE_string(
     'tfds_data_dir',
     '',
-    help='Optional tfds data directory. This can be useful to prepare datasets '
-    'on GCS and then point the jobs to this preloaded directory. Dataset will '
-    'be downloaded from the web if not specified.')
+    help=(
+        'Optional tfds data directory. This can be useful to prepare datasets'
+        ' on GCS and then point the jobs to this preloaded directory. Dataset'
+        ' will be downloaded from the web if not specified.'
+    ),
+)
 
 # Repo configuration.
 flags.DEFINE_string(
-    'repo', 'https://github.com/google/flax', help='Git repository')
+    'repo', 'https://github.com/google/flax', help='Git repository'
+)
 flags.DEFINE_string('branch', 'main', help='Git repository')
 
 # Example configuration.
 flags.DEFINE_string(
-    'example', None, help='Name of Flax example (e.g. "imagenet").')
+    'example', None, help='Name of Flax example (e.g. "imagenet").'
+)
 flags.DEFINE_string(
     'args',
     '',
-    help='Any additional command line arguments for {example}_main.py, like '
-    'for example --config. Note that --workdir will be provided by the '
-    'script.')
+    help=(
+        'Any additional command line arguments for {example}_main.py, like '
+        'for example --config. Note that --workdir will be provided by the '
+        'script.'
+    ),
+)
 
 # Run configuration.
 flags.DEFINE_string(
     'name',
     None,
-    help='Name of the experiment. Note that the provided name will be '
-    'extended to {example}/{name}/{timestamp}')
+    help=(
+        'Name of the experiment. Note that the provided name will be '
+        'extended to {example}/{name}/{timestamp}'
+    ),
+)
 
 FLAGS = flags.FLAGS
 flags.mark_flags_as_required(
-    ['project', 'zone', 'machine_type', 'gcs_workdir_base', 'example', 'name'])
+    ['project', 'zone', 'machine_type', 'gcs_workdir_base', 'example', 'name']
+)
 
 timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
 
 
 def generate_startup_file(vm_name: str) -> str:
   directory = os.path.dirname(os.path.abspath(__file__))
   startup_script_src = os.path.join(directory, 'startup_script.sh')
@@ -118,33 +147,41 @@
       ('__EXAMPLE__', FLAGS.example),
       ('__TIMESTAMP__', timestamp),
       ('__NAME__', FLAGS.name),
       ('__ARGS__', FLAGS.args),
       ('__GCS_WORKDIR_BASE__', FLAGS.gcs_workdir_base),
       ('__TFDS_DATA_DIR__', FLAGS.tfds_data_dir),
       ('__ACCELERATOR_TYPE__', FLAGS.accelerator_type),
-      ('__SHUTDOWN_SECS__', str(FLAGS.shutdown_secs))
+      ('__SHUTDOWN_SECS__', str(FLAGS.shutdown_secs)),
   ):
     startup_script_content = startup_script_content.replace(from_str, to_str)
   with open(startup_script_dst, 'w', encoding='utf8') as f:
     f.write(startup_script_content)
   return startup_script_dst
 
 
 def launch_gce(*, vm_name: str, startup_script: str):
   # Note : Use `gcloud compute images list --project ml-images` to get a list
   # of available VM images.
   args = [
-      'gcloud', 'compute', 'instances', 'create', vm_name,
-      f'--project={FLAGS.project}', f'--zone={FLAGS.zone}',
+      'gcloud',
+      'compute',
+      'instances',
+      'create',
+      vm_name,
+      f'--project={FLAGS.project}',
+      f'--zone={FLAGS.zone}',
       '--image=c1-deeplearning-tf-2-10-cu113-v20221107-debian-10',
-      '--image-project=ml-images', f'--machine-type={FLAGS.machine_type}',
-      '--scopes=cloud-platform,storage-full', '--boot-disk-size=256GB',
-      '--boot-disk-type=pd-ssd', '--metadata=install-nvidia-driver=True',
-      f'--metadata-from-file=startup-script={startup_script}'
+      '--image-project=ml-images',
+      f'--machine-type={FLAGS.machine_type}',
+      '--scopes=cloud-platform,storage-full',
+      '--boot-disk-size=256GB',
+      '--boot-disk-type=pd-ssd',
+      '--metadata=install-nvidia-driver=True',
+      f'--metadata-from-file=startup-script={startup_script}',
   ]
   if FLAGS.accelerator_type and FLAGS.accelerator_count:
     args.extend([
         '--maintenance-policy=TERMINATE',
         f'--accelerator=type={FLAGS.accelerator_type},count={FLAGS.accelerator_count}',
     ])
 
@@ -161,15 +198,15 @@
   print()
   result = subprocess.run(args)
   if result.returncode:
     raise RuntimeError('Could not create VM!')
 
 
 def print_howto(login_args: Sequence[str]):
-  print(f'''
+  print(f"""
 ###############################################################################
 ###############################################################################
 
 You can start/stop the instace via the web UI:
 https://console.cloud.google.com/compute/instances?project={FLAGS.project}
 
 Once the VM has started, you can login and connect to the training session:
@@ -185,15 +222,15 @@
 
 You can also browse the files at
 
 https://console.cloud.google.com/storage/browser/{FLAGS.gcs_workdir_base.replace('gs://', '')}
 
 ###############################################################################
 ###############################################################################
-''')
+""")
 
 
 def main(_):
   for name in ('repo', 'branch', 'example', 'name', 'gcs_workdir_base'):
     value = getattr(FLAGS, name)
     if re.match(r'[^\w:/_-]', value):
       raise ValueError(f'Invalid flag value: --{name}="{value}"')
@@ -234,15 +271,16 @@
   print_howto(login_args)
 
   if FLAGS.connect or FLAGS.wait:
     login_true_args = login_args[:-1] + ['true']
     while True:
       try:
         result = subprocess.run(
-            login_true_args, timeout=10, stderr=subprocess.PIPE)
+            login_true_args, timeout=10, stderr=subprocess.PIPE
+        )
         if result.returncode == 0:
           break
         stderr = result.stderr.decode('utf8')
         if 'connection refused' in stderr.lower():
           print('(Connection refused - waiting a little longer...)')
           time.sleep(20)
         else:
```

### Comparing `flax-0.7.0/examples/cloud/startup_script.sh` & `flax-0.7.1/examples/cloud/startup_script.sh`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/README.md` & `flax-0.7.1/examples/imagenet/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/configs/default.py` & `flax-0.7.1/examples/imagenet/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/configs/fake_data_benchmark.py` & `flax-0.7.1/examples/imagenet/configs/fake_data_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/configs/tpu.py` & `flax-0.7.1/examples/imagenet/configs/tpu.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/configs/v100_x8.py` & `flax-0.7.1/examples/imagenet/configs/v100_x8.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/configs/v100_x8_mixed_precision.py` & `flax-0.7.1/examples/imagenet/configs/v100_x8_mixed_precision.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/imagenet.ipynb` & `flax-0.7.1/examples/imagenet/imagenet.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/imagenet_benchmark.py` & `flax-0.7.1/examples/imagenet/imagenet_benchmark.py`

 * *Files 3% similar despite different names*

```diff
@@ -34,16 +34,17 @@
 FLAGS = flags.FLAGS
 
 
 class ImagenetBenchmark(Benchmark):
   """Benchmarks for the ImageNet Flax example."""
 
   @flagsaver
-  def _test_8x_v100_half_precision(self, num_epochs: int, min_accuracy,
-                                   max_accuracy):
+  def _test_8x_v100_half_precision(
+      self, num_epochs: int, min_accuracy, max_accuracy
+  ):
     """Utility to benchmark ImageNet on 8xV100 GPUs. Use in your test func."""
     # Prepare and set flags defined in main.py.
     config = config_lib.get_config()
     config.num_epochs = num_epochs
     workdir = self.get_tmp_model_dir()
 
     FLAGS.workdir = workdir
@@ -64,32 +65,35 @@
     # Assertions are deferred until the test finishes, so the metrics are
     # always reported and benchmark success is determined based on *all*
     # assertions.
     self.assertBetween(end_accuracy, min_accuracy, max_accuracy)
 
     # Use the reporting API to report single or multiple metrics/extras.
     self.report_wall_time(benchmark_time)
-    self.report_metrics({'sec_per_epoch': sec_per_epoch,
-                         'accuracy': end_accuracy})
+    self.report_metrics(
+        {'sec_per_epoch': sec_per_epoch, 'accuracy': end_accuracy}
+    )
 
   def test_8x_v100_half_precision_short(self):
     """Run ImageNet on 8x V100 GPUs in half precision for 2 epochs."""
     self._test_8x_v100_half_precision(
-        num_epochs=2, min_accuracy=0.06, max_accuracy=0.09)
+        num_epochs=2, min_accuracy=0.06, max_accuracy=0.09
+    )
     self.report_extras({
         'description': 'Short (2 epochs) 8 x V100 test for ImageNet ResNet50.',
         'model_name': 'resnet50',
         'parameters': 'hp=true,bs=2048,num_epochs=2',
         'implementation': 'linen',
     })
 
   def test_8x_v100_half_precision_full(self):
     """Run ImageNet on 8x V100 GPUs in half precision for full 90 epochs."""
     self._test_8x_v100_half_precision(
-        num_epochs=90, min_accuracy=0.76, max_accuracy=0.77)
+        num_epochs=90, min_accuracy=0.76, max_accuracy=0.77
+    )
     self.report_extras({
         'description': 'Full (90 epochs) 8 x V100 test for ImageNet ResNet50.',
         'model_name': 'resnet50',
         'parameters': 'hp=true,bs=2048,num_epochs=90',
         'implementation': 'linen',
     })
```

### Comparing `flax-0.7.0/examples/imagenet/imagenet_fake_data_benchmark.py` & `flax-0.7.1/examples/imagenet/imagenet_fake_data_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/imagenet/input_pipeline.py` & `flax-0.7.1/examples/imagenet/input_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,34 +8,35 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""ImageNet input pipeline.
-"""
+"""ImageNet input pipeline."""
 
 import jax
 import tensorflow as tf
 import tensorflow_datasets as tfds
 
 
 IMAGE_SIZE = 224
 CROP_PADDING = 32
 MEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]
 STDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]
 
 
-def distorted_bounding_box_crop(image_bytes,
-                                bbox,
-                                min_object_covered=0.1,
-                                aspect_ratio_range=(0.75, 1.33),
-                                area_range=(0.05, 1.0),
-                                max_attempts=100):
+def distorted_bounding_box_crop(
+    image_bytes,
+    bbox,
+    min_object_covered=0.1,
+    aspect_ratio_range=(0.75, 1.33),
+    area_range=(0.05, 1.0),
+    max_attempts=100,
+):
   """Generates cropped_image using one of the bboxes randomly distorted.
 
   See `tf.image.sample_distorted_bounding_box` for more documentation.
 
   Args:
     image_bytes: `Tensor` of binary image data.
     bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`
@@ -59,29 +60,31 @@
   sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(
       shape,
       bounding_boxes=bbox,
       min_object_covered=min_object_covered,
       aspect_ratio_range=aspect_ratio_range,
       area_range=area_range,
       max_attempts=max_attempts,
-      use_image_if_no_bounding_boxes=True)
+      use_image_if_no_bounding_boxes=True,
+  )
   bbox_begin, bbox_size, _ = sample_distorted_bounding_box
 
   # Crop the image to the specified bounding box.
   offset_y, offset_x, _ = tf.unstack(bbox_begin)
   target_height, target_width, _ = tf.unstack(bbox_size)
   crop_window = tf.stack([offset_y, offset_x, target_height, target_width])
   image = tf.io.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)
 
   return image
 
 
 def _resize(image, image_size):
-  return tf.image.resize([image], [image_size, image_size],
-                         method=tf.image.ResizeMethod.BICUBIC)[0]
+  return tf.image.resize(
+      [image], [image_size, image_size], method=tf.image.ResizeMethod.BICUBIC
+  )[0]
 
 
 def _at_least_x_are_equal(a, b, x):
   """At least `x` of `a` and `b` `Tensors` are equal."""
   match = tf.equal(a, b)
   match = tf.cast(match, tf.int32)
   return tf.greater_equal(tf.reduce_sum(match), x)
@@ -90,43 +93,52 @@
 def _decode_and_random_crop(image_bytes, image_size):
   """Make a random crop of image_size."""
   bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
   image = distorted_bounding_box_crop(
       image_bytes,
       bbox,
       min_object_covered=0.1,
-      aspect_ratio_range=(3. / 4, 4. / 3.),
+      aspect_ratio_range=(3.0 / 4, 4.0 / 3.0),
       area_range=(0.08, 1.0),
-      max_attempts=10)
+      max_attempts=10,
+  )
   original_shape = tf.io.extract_jpeg_shape(image_bytes)
   bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)
 
   image = tf.cond(
       bad,
       lambda: _decode_and_center_crop(image_bytes, image_size),
-      lambda: _resize(image, image_size))
+      lambda: _resize(image, image_size),
+  )
 
   return image
 
 
 def _decode_and_center_crop(image_bytes, image_size):
   """Crops to center of image with padding then scales image_size."""
   shape = tf.io.extract_jpeg_shape(image_bytes)
   image_height = shape[0]
   image_width = shape[1]
 
   padded_center_crop_size = tf.cast(
-      ((image_size / (image_size + CROP_PADDING)) *
-       tf.cast(tf.minimum(image_height, image_width), tf.float32)),
-      tf.int32)
+      (
+          (image_size / (image_size + CROP_PADDING))
+          * tf.cast(tf.minimum(image_height, image_width), tf.float32)
+      ),
+      tf.int32,
+  )
 
   offset_height = ((image_height - padded_center_crop_size) + 1) // 2
   offset_width = ((image_width - padded_center_crop_size) + 1) // 2
-  crop_window = tf.stack([offset_height, offset_width,
-                          padded_center_crop_size, padded_center_crop_size])
+  crop_window = tf.stack([
+      offset_height,
+      offset_width,
+      padded_center_crop_size,
+      padded_center_crop_size,
+  ])
   image = tf.io.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)
   image = _resize(image, image_size)
 
   return image
 
 
 def normalize_image(image):
@@ -168,17 +180,24 @@
   image = _decode_and_center_crop(image_bytes, image_size)
   image = tf.reshape(image, [image_size, image_size, 3])
   image = normalize_image(image)
   image = tf.image.convert_image_dtype(image, dtype=dtype)
   return image
 
 
-def create_split(dataset_builder, batch_size, train, dtype=tf.float32,
-                 image_size=IMAGE_SIZE, cache=False, shuffle_buffer_size=2_000,
-                 prefetch=10):
+def create_split(
+    dataset_builder,
+    batch_size,
+    train,
+    dtype=tf.float32,
+    image_size=IMAGE_SIZE,
+    cache=False,
+    shuffle_buffer_size=2_000,
+    prefetch=10,
+):
   """Creates a split from the ImageNet dataset using TensorFlow Datasets.
 
   Args:
     dataset_builder: TFDS dataset builder for ImageNet.
     batch_size: the batch size returned by the data pipeline.
     train: Whether to load the train or evaluation split.
     dtype: data type of the image.
@@ -203,17 +222,20 @@
   def decode_example(example):
     if train:
       image = preprocess_for_train(example['image'], dtype, image_size)
     else:
       image = preprocess_for_eval(example['image'], dtype, image_size)
     return {'image': image, 'label': example['label']}
 
-  ds = dataset_builder.as_dataset(split=split, decoders={
-      'image': tfds.decode.SkipDecoding(),
-  })
+  ds = dataset_builder.as_dataset(
+      split=split,
+      decoders={
+          'image': tfds.decode.SkipDecoding(),
+      },
+  )
   options = tf.data.Options()
   options.experimental_threading.private_threadpool_size = 48
   ds = ds.with_options(options)
 
   if cache:
     ds = ds.cache()
```

### Comparing `flax-0.7.0/examples/imagenet/main.py` & `flax-0.7.1/examples/mnist/main.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,18 +8,18 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Main file for running the ImageNet example.
+"""Main file for running the MNIST example.
 
-This file is intentionally kept short. The majority for logic is in libraries
-that can be easily tested and imported in Colab.
+This file is intentionally kept short. The majority of logic is in libraries
+than can be easily tested and imported in Colab.
 """
 
 from absl import app
 from absl import flags
 from absl import logging
 from clu import platform
 import jax
@@ -32,15 +32,16 @@
 FLAGS = flags.FLAGS
 
 flags.DEFINE_string('workdir', None, 'Directory to store model data.')
 config_flags.DEFINE_config_file(
     'config',
     None,
     'File path to the training hyperparameter configuration.',
-    lock_config=True)
+    lock_config=True,
+)
 
 
 def main(argv):
   if len(argv) > 1:
     raise app.UsageError('Too many command-line arguments.')
 
   # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make
@@ -48,18 +49,21 @@
   tf.config.experimental.set_visible_devices([], 'GPU')
 
   logging.info('JAX process: %d / %d', jax.process_index(), jax.process_count())
   logging.info('JAX local devices: %r', jax.local_devices())
 
   # Add a note so that we can tell which task is which JAX host.
   # (Depending on the platform task 0 is not guaranteed to be host 0)
-  platform.work_unit().set_task_status(f'process_index: {jax.process_index()}, '
-                                       f'process_count: {jax.process_count()}')
-  platform.work_unit().create_artifact(platform.ArtifactType.DIRECTORY,
-                                       FLAGS.workdir, 'workdir')
+  platform.work_unit().set_task_status(
+      f'process_index: {jax.process_index()}, '
+      f'process_count: {jax.process_count()}'
+  )
+  platform.work_unit().create_artifact(
+      platform.ArtifactType.DIRECTORY, FLAGS.workdir, 'workdir'
+  )
 
   train.train_and_evaluate(FLAGS.config, FLAGS.workdir)
 
 
 if __name__ == '__main__':
   flags.mark_flags_as_required(['config', 'workdir'])
   app.run(main)
```

### Comparing `flax-0.7.0/examples/imagenet/models.py` & `flax-0.7.1/examples/imagenet/models.py`

 * *Files 16% similar despite different names*

```diff
@@ -24,39 +24,45 @@
 import jax.numpy as jnp
 
 ModuleDef = Any
 
 
 class ResNetBlock(nn.Module):
   """ResNet block."""
+
   filters: int
   conv: ModuleDef
   norm: ModuleDef
   act: Callable
   strides: Tuple[int, int] = (1, 1)
 
   @nn.compact
-  def __call__(self, x,):
+  def __call__(
+      self,
+      x,
+  ):
     residual = x
     y = self.conv(self.filters, (3, 3), self.strides)(x)
     y = self.norm()(y)
     y = self.act(y)
     y = self.conv(self.filters, (3, 3))(y)
     y = self.norm(scale_init=nn.initializers.zeros_init())(y)
 
     if residual.shape != y.shape:
-      residual = self.conv(self.filters, (1, 1),
-                           self.strides, name='conv_proj')(residual)
+      residual = self.conv(
+          self.filters, (1, 1), self.strides, name='conv_proj'
+      )(residual)
       residual = self.norm(name='norm_proj')(residual)
 
     return self.act(residual + y)
 
 
 class BottleneckResNetBlock(nn.Module):
   """Bottleneck ResNet block."""
+
   filters: int
   conv: ModuleDef
   norm: ModuleDef
   act: Callable
   strides: Tuple[int, int] = (1, 1)
 
   @nn.compact
@@ -68,77 +74,90 @@
     y = self.conv(self.filters, (3, 3), self.strides)(y)
     y = self.norm()(y)
     y = self.act(y)
     y = self.conv(self.filters * 4, (1, 1))(y)
     y = self.norm(scale_init=nn.initializers.zeros_init())(y)
 
     if residual.shape != y.shape:
-      residual = self.conv(self.filters * 4, (1, 1),
-                           self.strides, name='conv_proj')(residual)
+      residual = self.conv(
+          self.filters * 4, (1, 1), self.strides, name='conv_proj'
+      )(residual)
       residual = self.norm(name='norm_proj')(residual)
 
     return self.act(residual + y)
 
 
 class ResNet(nn.Module):
   """ResNetV1."""
+
   stage_sizes: Sequence[int]
   block_cls: ModuleDef
   num_classes: int
   num_filters: int = 64
   dtype: Any = jnp.float32
   act: Callable = nn.relu
   conv: ModuleDef = nn.Conv
 
   @nn.compact
   def __call__(self, x, train: bool = True):
     conv = partial(self.conv, use_bias=False, dtype=self.dtype)
-    norm = partial(nn.BatchNorm,
-                   use_running_average=not train,
-                   momentum=0.9,
-                   epsilon=1e-5,
-                   dtype=self.dtype,
-                   axis_name='batch')
-
-
-    x = conv(self.num_filters, (7, 7), (2, 2),
-             padding=[(3, 3), (3, 3)],
-             name='conv_init')(x)
+    norm = partial(
+        nn.BatchNorm,
+        use_running_average=not train,
+        momentum=0.9,
+        epsilon=1e-5,
+        dtype=self.dtype,
+        axis_name='batch',
+    )
+
+    x = conv(
+        self.num_filters,
+        (7, 7),
+        (2, 2),
+        padding=[(3, 3), (3, 3)],
+        name='conv_init',
+    )(x)
     x = norm(name='bn_init')(x)
     x = nn.relu(x)
     x = nn.max_pool(x, (3, 3), strides=(2, 2), padding='SAME')
     for i, block_size in enumerate(self.stage_sizes):
       for j in range(block_size):
         strides = (2, 2) if i > 0 and j == 0 else (1, 1)
-        x = self.block_cls(self.num_filters * 2 ** i,
-                           strides=strides,
-                           conv=conv,
-                           norm=norm,
-                           act=self.act)(x)
+        x = self.block_cls(
+            self.num_filters * 2**i,
+            strides=strides,
+            conv=conv,
+            norm=norm,
+            act=self.act,
+        )(x)
     x = jnp.mean(x, axis=(1, 2))
     x = nn.Dense(self.num_classes, dtype=self.dtype)(x)
     x = jnp.asarray(x, self.dtype)
     return x
 
 
-ResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],
-                   block_cls=ResNetBlock)
-ResNet34 = partial(ResNet, stage_sizes=[3, 4, 6, 3],
-                   block_cls=ResNetBlock)
-ResNet50 = partial(ResNet, stage_sizes=[3, 4, 6, 3],
-                   block_cls=BottleneckResNetBlock)
-ResNet101 = partial(ResNet, stage_sizes=[3, 4, 23, 3],
-                    block_cls=BottleneckResNetBlock)
-ResNet152 = partial(ResNet, stage_sizes=[3, 8, 36, 3],
-                    block_cls=BottleneckResNetBlock)
-ResNet200 = partial(ResNet, stage_sizes=[3, 24, 36, 3],
-                    block_cls=BottleneckResNetBlock)
-
-
-ResNet18Local = partial(ResNet, stage_sizes=[2, 2, 2, 2],
-                        block_cls=ResNetBlock, conv=nn.ConvLocal)
+ResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2], block_cls=ResNetBlock)
+ResNet34 = partial(ResNet, stage_sizes=[3, 4, 6, 3], block_cls=ResNetBlock)
+ResNet50 = partial(
+    ResNet, stage_sizes=[3, 4, 6, 3], block_cls=BottleneckResNetBlock
+)
+ResNet101 = partial(
+    ResNet, stage_sizes=[3, 4, 23, 3], block_cls=BottleneckResNetBlock
+)
+ResNet152 = partial(
+    ResNet, stage_sizes=[3, 8, 36, 3], block_cls=BottleneckResNetBlock
+)
+ResNet200 = partial(
+    ResNet, stage_sizes=[3, 24, 36, 3], block_cls=BottleneckResNetBlock
+)
+
+
+ResNet18Local = partial(
+    ResNet, stage_sizes=[2, 2, 2, 2], block_cls=ResNetBlock, conv=nn.ConvLocal
+)
 
 
 # Used for testing only.
 _ResNet1 = partial(ResNet, stage_sizes=[1], block_cls=ResNetBlock)
-_ResNet1Local = partial(ResNet, stage_sizes=[1], block_cls=ResNetBlock,
-                        conv=nn.ConvLocal)
+_ResNet1Local = partial(
+    ResNet, stage_sizes=[1], block_cls=ResNetBlock, conv=nn.ConvLocal
+)
```

### Comparing `flax-0.7.0/examples/imagenet/models_test.py` & `flax-0.7.1/examples/imagenet/models_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -29,33 +29,29 @@
 class ResNetV1Test(parameterized.TestCase):
   """Test cases for ResNet v1 model definition."""
 
   def test_resnet_v1_model(self):
     """Tests ResNet V1 model definition and output (variables)."""
     rng = jax.random.PRNGKey(0)
     model_def = models.ResNet50(num_classes=10, dtype=jnp.float32)
-    variables = model_def.init(
-        rng, jnp.ones((8, 224, 224, 3), jnp.float32))
+    variables = model_def.init(rng, jnp.ones((8, 224, 224, 3), jnp.float32))
 
     self.assertLen(variables, 2)
     # Resnet50 model will create parameters for the following layers:
     #   conv + batch_norm = 2
     #   BottleneckResNetBlock in stages: [3, 4, 6, 3] = 16
     #   Followed by a Dense layer = 1
     self.assertLen(variables['params'], 19)
 
-  @parameterized.product(
-      model=(models.ResNet18, models.ResNet18Local)
-  )
+  @parameterized.product(model=(models.ResNet18, models.ResNet18Local))
   def test_resnet_18_v1_model(self, model):
     """Tests ResNet18 V1 model definition and output (variables)."""
     rng = jax.random.PRNGKey(0)
     model_def = model(num_classes=2, dtype=jnp.float32)
-    variables = model_def.init(
-        rng, jnp.ones((1, 64, 64, 3), jnp.float32))
+    variables = model_def.init(rng, jnp.ones((1, 64, 64, 3), jnp.float32))
 
     self.assertLen(variables, 2)
     self.assertLen(variables['params'], 11)
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/examples/imagenet/train.py` & `flax-0.7.1/examples/imagenet/train.py`

 * *Files 6% similar despite different names*

```diff
@@ -57,17 +57,19 @@
   else:
     model_dtype = jnp.float32
   return model_cls(num_classes=NUM_CLASSES, dtype=model_dtype, **kwargs)
 
 
 def initialized(key, image_size, model):
   input_shape = (1, image_size, image_size, 3)
+
   @jax.jit
   def init(*args):
     return model.init(*args)
+
   variables = init({'params': key}, jnp.ones(input_shape, model.dtype))
   return variables['params'], variables['batch_stats']
 
 
 def cross_entropy_loss(logits, labels):
   one_hot_labels = common_utils.onehot(labels, num_classes=NUM_CLASSES)
   xentropy = optax.softmax_cross_entropy(logits=logits, labels=one_hot_labels)
@@ -84,112 +86,135 @@
   metrics = lax.pmean(metrics, axis_name='batch')
   return metrics
 
 
 def create_learning_rate_fn(
     config: ml_collections.ConfigDict,
     base_learning_rate: float,
-    steps_per_epoch: int):
+    steps_per_epoch: int,
+):
   """Create learning rate schedule."""
   warmup_fn = optax.linear_schedule(
-      init_value=0., end_value=base_learning_rate,
-      transition_steps=config.warmup_epochs * steps_per_epoch)
+      init_value=0.0,
+      end_value=base_learning_rate,
+      transition_steps=config.warmup_epochs * steps_per_epoch,
+  )
   cosine_epochs = max(config.num_epochs - config.warmup_epochs, 1)
   cosine_fn = optax.cosine_decay_schedule(
-      init_value=base_learning_rate,
-      decay_steps=cosine_epochs * steps_per_epoch)
+      init_value=base_learning_rate, decay_steps=cosine_epochs * steps_per_epoch
+  )
   schedule_fn = optax.join_schedules(
       schedules=[warmup_fn, cosine_fn],
-      boundaries=[config.warmup_epochs * steps_per_epoch])
+      boundaries=[config.warmup_epochs * steps_per_epoch],
+  )
   return schedule_fn
 
 
 def train_step(state, batch, learning_rate_fn):
   """Perform a single training step."""
+
   def loss_fn(params):
     """loss function used for training."""
     logits, new_model_state = state.apply_fn(
         {'params': params, 'batch_stats': state.batch_stats},
         batch['image'],
-        mutable=['batch_stats'])
+        mutable=['batch_stats'],
+    )
     loss = cross_entropy_loss(logits, batch['label'])
     weight_penalty_params = jax.tree_util.tree_leaves(params)
     weight_decay = 0.0001
-    weight_l2 = sum(jnp.sum(x ** 2)
-                     for x in weight_penalty_params
-                     if x.ndim > 1)
+    weight_l2 = sum(
+        jnp.sum(x**2) for x in weight_penalty_params if x.ndim > 1
+    )
     weight_penalty = weight_decay * 0.5 * weight_l2
     loss = loss + weight_penalty
     return loss, (new_model_state, logits)
 
   step = state.step
   dynamic_scale = state.dynamic_scale
   lr = learning_rate_fn(step)
 
   if dynamic_scale:
     grad_fn = dynamic_scale.value_and_grad(
-        loss_fn, has_aux=True, axis_name='batch')
+        loss_fn, has_aux=True, axis_name='batch'
+    )
     dynamic_scale, is_fin, aux, grads = grad_fn(state.params)
     # dynamic loss takes care of averaging gradients across replicas
   else:
     grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
     aux, grads = grad_fn(state.params)
     # Re-use same axis_name as in the call to `pmap(...train_step...)` below.
     grads = lax.pmean(grads, axis_name='batch')
   new_model_state, logits = aux[1]
   metrics = compute_metrics(logits, batch['label'])
   metrics['learning_rate'] = lr
 
   new_state = state.apply_gradients(
-      grads=grads, batch_stats=new_model_state['batch_stats'])
+      grads=grads, batch_stats=new_model_state['batch_stats']
+  )
   if dynamic_scale:
     # if is_fin == False the gradients contain Inf/NaNs and optimizer state and
     # params should be restored (= skip this step).
     new_state = new_state.replace(
         opt_state=jax.tree_util.tree_map(
             functools.partial(jnp.where, is_fin),
             new_state.opt_state,
-            state.opt_state),
+            state.opt_state,
+        ),
         params=jax.tree_util.tree_map(
-            functools.partial(jnp.where, is_fin),
-            new_state.params,
-            state.params),
-        dynamic_scale=dynamic_scale)
+            functools.partial(jnp.where, is_fin), new_state.params, state.params
+        ),
+        dynamic_scale=dynamic_scale,
+    )
     metrics['scale'] = dynamic_scale.scale
 
   return new_state, metrics
 
 
 def eval_step(state, batch):
   variables = {'params': state.params, 'batch_stats': state.batch_stats}
-  logits = state.apply_fn(
-      variables, batch['image'], train=False, mutable=False)
+  logits = state.apply_fn(variables, batch['image'], train=False, mutable=False)
   return compute_metrics(logits, batch['label'])
 
 
 def prepare_tf_data(xs):
   """Convert a input batch from tf Tensors to numpy arrays."""
   local_device_count = jax.local_device_count()
+
   def _prepare(x):
     # Use _numpy() for zero-copy conversion between TF and NumPy.
     x = x._numpy()  # pylint: disable=protected-access
 
     # reshape (host_batch_size, height, width, 3) to
     # (local_devices, device_batch_size, height, width, 3)
     return x.reshape((local_device_count, -1) + x.shape[1:])
 
   return jax.tree_util.tree_map(_prepare, xs)
 
 
-def create_input_iter(dataset_builder, batch_size, image_size, dtype, train,
-                      cache, shuffle_buffer_size, prefetch):
+def create_input_iter(
+    dataset_builder,
+    batch_size,
+    image_size,
+    dtype,
+    train,
+    cache,
+    shuffle_buffer_size,
+    prefetch,
+):
   ds = input_pipeline.create_split(
-      dataset_builder, batch_size, image_size=image_size, dtype=dtype,
-      train=train, cache=cache, shuffle_buffer_size=shuffle_buffer_size,
-      prefetch=prefetch)
+      dataset_builder,
+      batch_size,
+      image_size=image_size,
+      dtype=dtype,
+      train=train,
+      cache=cache,
+      shuffle_buffer_size=shuffle_buffer_size,
+      prefetch=prefetch,
+  )
   it = map(prepare_tf_data, ds)
   it = jax_utils.prefetch_to_device(it, 2)
   return it
 
 
 class TrainState(train_state.TrainState):
   batch_stats: Any
@@ -215,16 +240,17 @@
 def sync_batch_stats(state):
   """Sync the batch statistics across replicas."""
   # Each device has its own version of the running average batch statistics and
   # we sync them before evaluation.
   return state.replace(batch_stats=cross_replica_mean(state.batch_stats))
 
 
-def create_train_state(rng, config: ml_collections.ConfigDict,
-                       model, image_size, learning_rate_fn):
+def create_train_state(
+    rng, config: ml_collections.ConfigDict, model, image_size, learning_rate_fn
+):
   """Create initial training state."""
   dynamic_scale = None
   platform = jax.local_devices()[0].platform
   if config.half_precision and platform == 'gpu':
     dynamic_scale = dynamic_scale_lib.DynamicScale()
   else:
     dynamic_scale = None
@@ -236,32 +262,35 @@
       nesterov=True,
   )
   state = TrainState.create(
       apply_fn=model.apply,
       params=params,
       tx=tx,
       batch_stats=batch_stats,
-      dynamic_scale=dynamic_scale)
+      dynamic_scale=dynamic_scale,
+  )
   return state
 
 
-def train_and_evaluate(config: ml_collections.ConfigDict,
-                       workdir: str) -> TrainState:
+def train_and_evaluate(
+    config: ml_collections.ConfigDict, workdir: str
+) -> TrainState:
   """Execute model training and evaluation loop.
 
   Args:
     config: Hyperparameter configuration for training and evaluation.
     workdir: Directory where the tensorboard summaries are written to.
 
   Returns:
     Final TrainState.
   """
 
   writer = metric_writers.create_default_writer(
-      logdir=workdir, just_logging=jax.process_index() != 0)
+      logdir=workdir, just_logging=jax.process_index() != 0
+  )
 
   rng = random.PRNGKey(0)
 
   image_size = 224
 
   if config.batch_size % jax.device_count() > 0:
     raise ValueError('Batch size must be divisible by the number of devices')
@@ -275,57 +304,74 @@
     else:
       input_dtype = tf.float16
   else:
     input_dtype = tf.float32
 
   dataset_builder = tfds.builder(config.dataset)
   train_iter = create_input_iter(
-      dataset_builder, local_batch_size, image_size, input_dtype, train=True,
-      cache=config.cache, shuffle_buffer_size=config.shuffle_buffer_size,
-      prefetch=config.prefetch)
+      dataset_builder,
+      local_batch_size,
+      image_size,
+      input_dtype,
+      train=True,
+      cache=config.cache,
+      shuffle_buffer_size=config.shuffle_buffer_size,
+      prefetch=config.prefetch,
+  )
   eval_iter = create_input_iter(
-      dataset_builder, local_batch_size, image_size, input_dtype, train=False,
-      cache=config.cache, shuffle_buffer_size=None, prefetch=config.prefetch)
+      dataset_builder,
+      local_batch_size,
+      image_size,
+      input_dtype,
+      train=False,
+      cache=config.cache,
+      shuffle_buffer_size=None,
+      prefetch=config.prefetch,
+  )
 
   steps_per_epoch = (
       dataset_builder.info.splits['train'].num_examples // config.batch_size
   )
 
   if config.num_train_steps <= 0:
     num_steps = int(steps_per_epoch * config.num_epochs)
   else:
     num_steps = config.num_train_steps
 
   if config.steps_per_eval == -1:
     num_validation_examples = dataset_builder.info.splits[
-        'validation'].num_examples
+        'validation'
+    ].num_examples
     steps_per_eval = num_validation_examples // config.batch_size
   else:
     steps_per_eval = config.steps_per_eval
 
   steps_per_checkpoint = steps_per_epoch * 10
 
-  base_learning_rate = config.learning_rate * config.batch_size / 256.
+  base_learning_rate = config.learning_rate * config.batch_size / 256.0
 
   model_cls = getattr(models, config.model)
   model = create_model(
-      model_cls=model_cls, half_precision=config.half_precision)
+      model_cls=model_cls, half_precision=config.half_precision
+  )
 
   learning_rate_fn = create_learning_rate_fn(
-      config, base_learning_rate, steps_per_epoch)
+      config, base_learning_rate, steps_per_epoch
+  )
 
   state = create_train_state(rng, config, model, image_size, learning_rate_fn)
   state = restore_checkpoint(state, workdir)
   # step_offset > 0 if restarting from checkpoint
   step_offset = int(state.step)
   state = jax_utils.replicate(state)
 
   p_train_step = jax.pmap(
       functools.partial(train_step, learning_rate_fn=learning_rate_fn),
-      axis_name='batch')
+      axis_name='batch',
+  )
   p_eval_step = jax.pmap(eval_step, axis_name='batch')
 
   train_metrics = []
   hooks = []
   if jax.process_index() == 0:
     hooks += [periodic_actions.Profile(num_profile_steps=5, logdir=workdir)]
   train_metrics_last_t = time.time()
@@ -339,18 +385,21 @@
 
     if config.get('log_every_steps'):
       train_metrics.append(metrics)
       if (step + 1) % config.log_every_steps == 0:
         train_metrics = common_utils.get_metrics(train_metrics)
         summary = {
             f'train_{k}': v
-            for k, v in jax.tree_util.tree_map(lambda x: x.mean(), train_metrics).items()
+            for k, v in jax.tree_util.tree_map(
+                lambda x: x.mean(), train_metrics
+            ).items()
         }
         summary['steps_per_second'] = config.log_every_steps / (
-            time.time() - train_metrics_last_t)
+            time.time() - train_metrics_last_t
+        )
         writer.write_scalars(step + 1, summary)
         train_metrics = []
         train_metrics_last_t = time.time()
 
     if (step + 1) % steps_per_epoch == 0:
       epoch = step // steps_per_epoch
       eval_metrics = []
@@ -359,18 +408,23 @@
       state = sync_batch_stats(state)
       for _ in range(steps_per_eval):
         eval_batch = next(eval_iter)
         metrics = p_eval_step(state, eval_batch)
         eval_metrics.append(metrics)
       eval_metrics = common_utils.get_metrics(eval_metrics)
       summary = jax.tree_util.tree_map(lambda x: x.mean(), eval_metrics)
-      logging.info('eval epoch: %d, loss: %.4f, accuracy: %.2f',
-                   epoch, summary['loss'], summary['accuracy'] * 100)
+      logging.info(
+          'eval epoch: %d, loss: %.4f, accuracy: %.2f',
+          epoch,
+          summary['loss'],
+          summary['accuracy'] * 100,
+      )
       writer.write_scalars(
-          step + 1, {f'eval_{key}': val for key, val in summary.items()})
+          step + 1, {f'eval_{key}': val for key, val in summary.items()}
+      )
       writer.flush()
     if (step + 1) % steps_per_checkpoint == 0 or step + 1 == num_steps:
       state = sync_batch_stats(state)
       save_checkpoint(state, workdir)
 
   # Wait until computations are done before exiting
   jax.random.normal(jax.random.PRNGKey(0), ()).block_until_ready()
```

### Comparing `flax-0.7.0/examples/imagenet/train_test.py` & `flax-0.7.1/examples/imagenet/train_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -51,24 +51,24 @@
     self.assertEqual(y.shape, (8, 1000))
 
   def test_create_model_local(self):
     """Tests creating an unshared convolution model.
 
     Uses smaller inputs than `test_create_model` to due to higher compute.
     """
-    model = train.create_model(model_cls=models._ResNet1Local, half_precision=False)  # pylint: disable=protected-access
+    model = train.create_model(
+        model_cls=models._ResNet1Local, half_precision=False
+    )  # pylint: disable=protected-access
     params, batch_stats = train.initialized(random.PRNGKey(0), 64, model)
     variables = {'params': params, 'batch_stats': batch_stats}
     x = random.normal(random.PRNGKey(1), (1, 64, 64, 3))
     y = model.apply(variables, x, train=False)
     self.assertEqual(y.shape, (1, 1000))
 
-  @parameterized.product(
-      model=('_ResNet1', '_ResNet1Local')
-  )
+  @parameterized.product(model=('_ResNet1', '_ResNet1Local'))
   def test_train_and_evaluate(self, model):
     """Tests training and evaluation loop using mocked data."""
     # Create a temporary directory where tensorboard metrics are written.
     workdir = tempfile.mkdtemp()
 
     # Go two directories up to the root of the flax directory.
     flax_root_dir = pathlib.Path(__file__).parents[2]
```

### Comparing `flax-0.7.0/examples/linen_design_test/attention_simple.py` & `flax-0.7.1/examples/linen_design_test/attention_simple.py`

 * *Files 11% similar despite different names*

```diff
@@ -21,54 +21,58 @@
 from flax.linen import Module, compact, vmap
 from flax.linen.linear import PrecisionLike
 import jax
 from jax import lax, numpy as jnp, random
 import numpy as np
 
 
-
 class Dense(Module):
   features: int
   use_bias: bool = True
   kernel_init: Callable = initializers.lecun_normal()
   bias_init: Callable = initializers.zeros_init()
   dtype: Any = jnp.float32
   precision: PrecisionLike = None
 
   @compact
   def __call__(self, inputs):
     inputs = jnp.asarray(inputs, self.dtype)
-    kernel = self.param('kernel', self.kernel_init,
-                        (inputs.shape[-1], self.features))
+    kernel = self.param(
+        'kernel', self.kernel_init, (inputs.shape[-1], self.features)
+    )
     kernel = jnp.asarray(kernel, self.dtype)
-    y = lax.dot_general(inputs, kernel,
-                        (((inputs.ndim - 1,), (0,)), ((), ())),
-                        precision=self.precision)
+    y = lax.dot_general(
+        inputs,
+        kernel,
+        (((inputs.ndim - 1,), (0,)), ((), ())),
+        precision=self.precision,
+    )
     if self.use_bias:
       bias = self.param('bias', self.bias_init, (self.features,))
       bias = jnp.asarray(bias, self.dtype)
       y = y + bias
     return y
 
 
 class SoftmaxAttn(Module):
+
   @compact
   def __call__(self, weights):
     norm_dims = tuple(range(weights.ndim // 2, weights.ndim))
     return jax.nn.softmax(weights, axis=norm_dims)
 
 
 class Dropout(Module):
   rate: float
 
   @compact
   def __call__(self, x, deterministic=False, rng=None):
-    if self.rate == 0.:
+    if self.rate == 0.0:
       return x
-    keep_prob = 1. - self.rate
+    keep_prob = 1.0 - self.rate
 
     if deterministic:
       return x
     else:
       if rng is None:
         rng = self.scope.make_rng('dropout')
       mask = random.bernoulli(rng, p=keep_prob, shape=x.shape)
@@ -91,65 +95,68 @@
 
   @compact
   def __call__(self, query, key, value, bias=None, dtype=jnp.float32):
     assert key.ndim == query.ndim
     assert key.ndim == value.ndim
 
     n = query.ndim
-    attn_weights = lax.dot_general(
-        query, key,
-        (((n-1,), (n - 1,)), ((), ())))
+    attn_weights = lax.dot_general(query, key, (((n - 1,), (n - 1,)), ((), ())))
     if bias is not None:
       attn_weights += bias
     attn_weights = self.attn_module()(attn_weights)
     attn_weights = attn_weights.astype(dtype)
 
     contract_dims = (
         tuple(range(n - 1, attn_weights.ndim)),
-        tuple(range(0, n  - 1)))
-    y = lax.dot_general(
-        attn_weights, value,
-        (contract_dims, ((), ())))
+        tuple(range(0, n - 1)),
+    )
+    y = lax.dot_general(attn_weights, value, (contract_dims, ((), ())))
     return y
 
 
 class DotProductAttention(Module):
   qkv_features: Optional[int] = None
   out_features: Optional[int] = None
   attn_module: Callable = SoftmaxAttn
 
   @compact
   def __call__(self, inputs_q, inputs_kv, bias=None, dtype=jnp.float32):
     qkv_features = self.qkv_features or inputs_q.shape[-1]
     out_features = self.out_features or inputs_q.shape[-1]
 
     QKVDense = functools.partial(
-      Dense, features=qkv_features, use_bias=False, dtype=dtype)
+        Dense, features=qkv_features, use_bias=False, dtype=dtype
+    )
     query = QKVDense(name='query')(inputs_q)
     key = QKVDense(name='key')(inputs_kv)
     value = QKVDense(name='value')(inputs_kv)
 
     y = RawDotProductAttention(attn_module=self.attn_module)(
-      query, key, value, bias=bias, dtype=dtype)
+        query, key, value, bias=bias, dtype=dtype
+    )
     y = Dense(features=out_features, dtype=dtype, name='out')(y)
     return y
 
 
 # Trying out a slightly more compact vmap notation:
 
+
 def concise_vmap(module, in_axes, out_axes, axis_size=None, **var_specs):
-  variable_axes = {k: v[0] for k, v in
-                      var_specs.items() if isinstance(v, Sequence)}
+  variable_axes = {
+      k: v[0] for k, v in var_specs.items() if isinstance(v, Sequence)
+  }
   splits = {k: v[1] for k, v in var_specs.items() if isinstance(v, Sequence)}
-  return vmap(module,
-              in_axes=in_axes,
-              out_axes=out_axes,
-              variable_axes=variable_axes,
-              split_rngs=splits,
-              axis_size=axis_size)
+  return vmap(
+      module,
+      in_axes=in_axes,
+      out_axes=out_axes,
+      variable_axes=variable_axes,
+      split_rngs=splits,
+      axis_size=axis_size,
+  )
 
 
 class MultiHeadDotProductAttention(Module):
   qkv_features: Optional[int] = None
   out_features: Optional[int] = None
   attn_module: Callable = SoftmaxAttn
   batch_axes: Sequence[int] = (0,)
@@ -158,48 +165,56 @@
 
   @compact
   def __call__(self, inputs_q, inputs_kv, bias=None, dtype=jnp.float32):
     qkv_features = self.qkv_features or inputs_q.shape[-1]
     out_features = self.out_features or inputs_q.shape[-1]
 
     # Now, vmap attn.__call__ along heads and spatial dims.
-    Attn = concise_vmap(DotProductAttention,
-                        (None, None, None), -2,
-                        param=(0, True),
-                        dropout=(None, not self.broadcast_dropout),
-                        axis_size=self.num_heads)
+    Attn = concise_vmap(
+        DotProductAttention,
+        (None, None, None),
+        -2,
+        param=(0, True),
+        dropout=(None, not self.broadcast_dropout),
+        axis_size=self.num_heads,
+    )
     for axis in reversed(sorted(self.batch_axes)):
-      Attn = concise_vmap(Attn,
-                          (axis, axis, axis), axis,
-                          param=(None, False),
-                          dropout=(None, not self.broadcast_dropout))
-
-    attn = Attn(attn_module=self.attn_module,
-                qkv_features=qkv_features // self.num_heads,
-                out_features=out_features)
+      Attn = concise_vmap(
+          Attn,
+          (axis, axis, axis),
+          axis,
+          param=(None, False),
+          dropout=(None, not self.broadcast_dropout),
+      )
+
+    attn = Attn(
+        attn_module=self.attn_module,
+        qkv_features=qkv_features // self.num_heads,
+        out_features=out_features,
+    )
 
     # evaluate multi-headed-attention.
     y = attn(inputs_q, inputs_kv, bias)
     return y.mean(axis=-2)
 
 
 # run it.
 
 
 if __name__ == '__main__':
-
   inputs = jnp.ones((8, 97, 256))
   rngs = {'params': random.PRNGKey(0), 'dropout': random.PRNGKey(1)}
   model = MultiHeadDotProductAttention(
       broadcast_dropout=False,
       qkv_features=256,
       out_features=256,
       attn_module=functools.partial(SoftmaxAttnWDropout, rate=0.1),
       num_heads=8,
-      batch_axes=(0,),)
+      batch_axes=(0,),
+  )
 
   y, params = model.init_with_output(rngs, inputs, inputs)
 
   print('input shape: ', inputs.shape)
   print('parameter shapes:')
   pprint(jax.tree_util.tree_map(jnp.shape, unfreeze(params)))
   print('output shape: ', y.shape)
```

### Comparing `flax-0.7.0/examples/linen_design_test/autoencoder.py` & `flax-0.7.1/examples/linen_design_test/autoencoder.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,15 +18,14 @@
 from jax import numpy as jnp, random, lax
 import numpy as np
 
 from flax import linen as nn
 from flax.linen import Module, Dense, compact
 
 
-
 # A concise MLP defined via lazy submodule initialization
 class MLP(Module):
   widths: Iterable
 
   @compact
   def __call__(self, x):
     for width in self.widths[:-1]:
@@ -40,47 +39,49 @@
   encoder_widths: Iterable
   decoder_widths: Iterable
   input_shape: Tuple = None
 
   def setup(self):
     # Submodules attached in `setup` get names via attribute assignment
     self.encoder = MLP(self.encoder_widths)
-    self.decoder = MLP(self.decoder_widths + (jnp.prod(self.input_shape), ))
+    self.decoder = MLP(self.decoder_widths + (jnp.prod(self.input_shape),))
 
   def __call__(self, x):
     return self.decode(self.encode(x))
 
   def encode(self, x):
-    assert x.shape[-len(self.input_shape):] == self.input_shape
+    assert x.shape[-len(self.input_shape) :] == self.input_shape
     return self.encoder(jnp.reshape(x, (x.shape[0], -1)))
 
   def decode(self, z):
     z = self.decoder(z)
     x = nn.sigmoid(z)
     x = jnp.reshape(x, (x.shape[0],) + self.input_shape)
     return x
 
 
 # `ae` is a detached module, which has no variables.
 ae = AutoEncoder(
     encoder_widths=(32, 32, 32),
     decoder_widths=(32, 32, 32),
-    input_shape=(28, 28, 1))
+    input_shape=(28, 28, 1),
+)
 
 
 # `ae.initialized` returns a materialized copy of `ae` by
 # running through an input to create submodules defined lazily.
-params = ae.init(
-    {'params': random.PRNGKey(42)},
-    jnp.ones((1, 28, 28, 1)))
+params = ae.init({"params": random.PRNGKey(42)}, jnp.ones((1, 28, 28, 1)))
 
 
 # Now you can use `ae` as a normal object, calling any methods defined on AutoEncoder
 print("reconstruct", jnp.shape(ae.apply(params, jnp.ones((1, 28, 28, 1)))))
-print("encoder", jnp.shape(ae.apply(params, jnp.ones((1, 28, 28, 1)), method=ae.encode)))
+print(
+    "encoder",
+    jnp.shape(ae.apply(params, jnp.ones((1, 28, 28, 1)), method=ae.encode)),
+)
 
 
 # `ae.variables` is a frozen dict that looks like
 # {'params': {"decoder": {"Dense_0": {"bias": ..., "kernel": ...}, ...}}
 print("var shapes", jax.tree_util.tree_map(jnp.shape, params))
```

### Comparing `flax-0.7.0/examples/linen_design_test/dense.py` & `flax-0.7.1/examples/linen_design_test/dense.py`

 * *Files 19% similar despite different names*

```diff
@@ -23,15 +23,19 @@
   features: int
   kernel_init: Callable = initializers.lecun_normal()
   bias_init: Callable = initializers.zeros_init()
   use_bias: bool = True
 
   @compact
   def __call__(self, inputs):
-    kernel = self.param('kernel', self.kernel_init,
-                        (inputs.shape[-1], self.features))
-    y = lax.dot_general(inputs, kernel,
-                        (((inputs.ndim - 1,), (0,)), ((), ())),)
+    kernel = self.param(
+        'kernel', self.kernel_init, (inputs.shape[-1], self.features)
+    )
+    y = lax.dot_general(
+        inputs,
+        kernel,
+        (((inputs.ndim - 1,), (0,)), ((), ())),
+    )
     if self.use_bias:
       bias = self.param('bias', self.bias_init, (self.features,))
       y = y + bias
     return y
```

### Comparing `flax-0.7.0/examples/linen_design_test/linear_regression.py` & `flax-0.7.1/examples/linen_design_test/linear_regression.py`

 * *Files 6% similar despite different names*

```diff
@@ -19,26 +19,30 @@
 
 
 X = jnp.ones((1, 10))
 Y = jnp.ones((5,))
 
 model = Dense(features=5)
 
+
 @jit
 def predict(params):
-  return model.apply({'params': params}, X)
+  return model.apply({"params": params}, X)
+
 
 @jit
 def loss_fn(params):
   return jnp.mean(jnp.abs(Y - predict(params)))
 
+
 @jit
 def init_params(rng):
-  mlp_variables = model.init({'params': rng}, X)
-  return mlp_variables['params']
+  mlp_variables = model.init({"params": rng}, X)
+  return mlp_variables["params"]
+
 
 # Get initial parameters
 params = init_params(jax.random.PRNGKey(42))
 print("initial params", params)
 
 # Run SGD.
 for i in range(50):
```

### Comparing `flax-0.7.0/examples/linen_design_test/mlp_explicit.py` & `flax-0.7.1/examples/linen_design_test/mlp_explicit.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,31 +27,38 @@
 # via shape inference.
 class DenseExplicit(Dense):
   in_features: Optional[int] = None
 
   def setup(self):
     # We feed a fake batch through the module, which initialized parameters.
     # Assuming we're in a jit, should use no FLOPs -- "just shape inference".
-    self.__call__(jnp.zeros((1, self.in_features, )))
+    self.__call__(
+        jnp.zeros((
+            1,
+            self.in_features,
+        ))
+    )
+
 
 class MLP(Module):
+
   def setup(self):
     self.dense1 = DenseExplicit(in_features=3, features=2)
     self.dense2 = DenseExplicit(in_features=2, features=1)
 
     # explicit instances are materialized immediately at init
     pprint(self.dense2.variables)
     # {'params': {'bias': DeviceArray([0.], dtype=float32),
     #            'kernel': DeviceArray([[ 0.6704609 ],
     #              [-0.90477365]], dtype=float32)}}
 
-
   def __call__(self, x):
     return self.dense2(nn.relu(self.dense1(x)))
 
+
 # Return an initialized instance of MLP by only calling `setup`.
 rngkey = jax.random.PRNGKey(10)
 init_variables = MLP().init({'params': rngkey}, jnp.ones((1, 3)))
 
 pprint(init_variables)
 # {'params': {'dense1': {'bias': DeviceArray([0., 0.], dtype=float32),
 #                       'kernel': DeviceArray([[ 0.18307537, -0.38739476],
```

### Comparing `flax-0.7.0/examples/linen_design_test/mlp_inline.py` & `flax-0.7.1/examples/linen_design_test/mlp_inline.py`

 * *Files 0% similar despite different names*

```diff
@@ -26,18 +26,19 @@
 # In this case, variables are initialized during the first call.
 class MLP(Module):
   sizes: Iterable[int]
 
   @compact
   def __call__(self, x):
     for size in self.sizes[:-1]:
-        x = Dense(size)(x)
-        x = nn.relu(x)
+      x = Dense(size)(x)
+      x = nn.relu(x)
     return Dense(self.sizes[-1])(x)
 
+
 # Return an initialized instance of MLP by calling `__call__` with an input batch,
 # initializing all variables.
 #
 # Variable shapes depend on the input shape passed in.
 rngkey = jax.random.PRNGKey(10)
 model = MLP((2, 1))
 x = jnp.ones((1, 3))
```

### Comparing `flax-0.7.0/examples/linen_design_test/mlp_lazy.py` & `flax-0.7.1/examples/linen_design_test/mlp_lazy.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,25 +20,27 @@
 from pprint import pprint
 from dense import Dense
 
 
 # Here submodules are explicitly defined during init, but still materialized
 # lazily only once a first input is passed through and shapes are known.
 class MLP(Module):
+
   def setup(self):
     self.dense1 = Dense(features=2)
     self.dense2 = Dense(features=1)
 
     # shapes aren't yet known, so variables aren't materialized
     print(self.dense2.variables)
     # FrozenDict({})
 
   def __call__(self, x):
     return self.dense2(nn.relu(self.dense1(x)))
 
+
 # Return an initialized instance of MLP by calling `__call__` with an input batch,
 # initializing all variables.
 #
 # Variable shapes depend on the input shape passed in.
 rngkey = jax.random.PRNGKey(10)
 mlp_variables = MLP().init(rngkey, jnp.zeros((1, 3)))
 
@@ -46,8 +48,7 @@
 # {'params': {'dense1': {'bias': DeviceArray([0., 0.], dtype=float32),
 #                       'kernel': DeviceArray([[ 0.18307537, -0.38739476],
 #              [-0.902451  , -0.5190721 ],
 #              [ 0.51552075,  1.1169153 ]], dtype=float32)},
 #            'dense2': {'bias': DeviceArray([0.], dtype=float32),
 #                       'kernel': DeviceArray([[ 0.6704609 ],
 #              [-0.90477365]], dtype=float32)}}}
-
```

### Comparing `flax-0.7.0/examples/linen_design_test/tied_autoencoder.py` & `flax-0.7.1/examples/linen_design_test/tied_autoencoder.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/linen_design_test/weight_std.py` & `flax-0.7.1/examples/linen_design_test/weight_std.py`

 * *Files 0% similar despite different names*

```diff
@@ -25,14 +25,15 @@
 
 
 def standardize(x, axis, eps=1e-8):
   x = x - jnp.mean(x, axis=axis, keepdims=True)
   x = x / jnp.sqrt(jnp.mean(jnp.square(x), axis=axis, keepdims=True) + eps)
   return x
 
+
 # TODO(avital, levskaya): resurrect this example once interactive api is restored.
 
 # A wrapper that calls through a simple module with standardized parameters.
 #
 # Note that StdWeight is /not/ a module, hence it doesn't add another layer
 # of depth in the variable dict (i.e. this is a "transparent module")
 # @dataclass
```

### Comparing `flax-0.7.0/examples/lm1b/README.md` & `flax-0.7.1/examples/lm1b/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/lm1b/configs/default.py` & `flax-0.7.1/examples/lm1b/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/lm1b/input_pipeline.py` & `flax-0.7.1/examples/lm1b/input_pipeline.py`

 * *Files 3% similar despite different names*

```diff
@@ -37,40 +37,44 @@
   def __call__(self, features: Features) -> Features:
     features['inputs'] = features.pop('text')
     # Unnecessary step used for uniformizing with examples/wmt.
     features['targets'] = features['inputs']
     return features
 
 
-def get_raw_dataset(dataset_builder: tfds.core.DatasetBuilder,
-                    split: str) -> tf.data.Dataset:
+def get_raw_dataset(
+    dataset_builder: tfds.core.DatasetBuilder, split: str
+) -> tf.data.Dataset:
   """Loads a raw text dataset and normalizes feature keys.
 
   Args:
     dataset_builder: TFDS dataset builder that can build `split`.
     split: Split to use. This must be the full split. We shard the split across
       multiple hosts and currently don't support sharding subsplits.
 
   Returns:
     Dataset with source and target language features mapped to 'inputs' and
     'targets'.
   """
   num_examples = dataset_builder.info.splits[split].num_examples
   per_host_split = deterministic_data.get_read_instruction_for_host(
-      split, num_examples, drop_remainder=False)
+      split, num_examples, drop_remainder=False
+  )
   ds = dataset_builder.as_dataset(split=per_host_split, shuffle_files=False)
   ds = ds.map(
-      NormalizeFeatureNamesOp(dataset_builder.info),
-      num_parallel_calls=AUTOTUNE)
+      NormalizeFeatureNamesOp(dataset_builder.info), num_parallel_calls=AUTOTUNE
+  )
   return ds
 
 
-def pack_dataset(dataset: tf.data.Dataset,
-                 key2length: Union[int, Dict[str, int]],
-                 keys: Optional[List[str]] = None) -> tf.data.Dataset:
+def pack_dataset(
+    dataset: tf.data.Dataset,
+    key2length: Union[int, Dict[str, int]],
+    keys: Optional[List[str]] = None,
+) -> tf.data.Dataset:
   """Creates a 'packed' version of a dataset on-the-fly.
 
   Adapted from the mesh-tf implementation.
 
   This is meant to replace the irritation of having to create a separate
   "packed" version of a dataset to train efficiently on TPU.
   Each example in the output dataset represents several examples in the
@@ -107,46 +111,51 @@
     a tf.data.Dataset
   """
   shapes = tf.nest.map_structure(lambda spec: spec.shape, dataset.element_spec)
   if keys is None:
     keys = list(shapes.keys())
   for k in keys:
     if k not in shapes:
-      raise ValueError('Key %s not found in dataset.  Available keys are %s' %
-                       (k, shapes.keys()))
-    if not shapes[k].is_compatible_with(tf.TensorShape([None])): # type: ignore[wrong-arg-types]
+      raise ValueError(
+          'Key %s not found in dataset.  Available keys are %s'
+          % (k, shapes.keys())
+      )
+    if not shapes[k].is_compatible_with(tf.TensorShape([None])):  # type: ignore[wrong-arg-types]
       raise ValueError('Tensors to be packed must be one-dimensional.')
   # make sure that the length dictionary contains all keys as well as the
   # keys suffixed by "_segmentation" and "_position"
   if isinstance(key2length, int):
     key2length = {k: key2length for k in keys}
   for k in keys:
     for suffix in ['_segmentation', '_position']:
       key2length[k + suffix] = key2length[k]
 
   # trim to length
   dataset = dataset.map(
-      lambda x: {k: x[k][:key2length[k]] for k in keys},
-      num_parallel_calls=AUTOTUNE)
+      lambda x: {k: x[k][: key2length[k]] for k in keys},
+      num_parallel_calls=AUTOTUNE,
+  )
   # Setting batch_size=length ensures that the concatenated sequences (if they
   # have length >=1) are sufficient to fill at least one packed example.
   batch_size = max(key2length.values())
   dataset = dataset.padded_batch(
-      batch_size, padded_shapes={k: [-1] for k in keys})
+      batch_size, padded_shapes={k: [-1] for k in keys}
+  )
   dataset = _pack_with_tf_ops(dataset, keys, key2length)
 
   # Set the Tensor shapes correctly since they get lost in the process.
   def my_fn(x):
     return {k: tf.reshape(v, [key2length[k]]) for k, v in x.items()}
 
   return dataset.map(my_fn, num_parallel_calls=AUTOTUNE)
 
 
-def _pack_with_tf_ops(dataset: tf.data.Dataset, keys: List[str],
-                      key2length: Dict[str, int]) -> tf.data.Dataset:
+def _pack_with_tf_ops(
+    dataset: tf.data.Dataset, keys: List[str], key2length: Dict[str, int]
+) -> tf.data.Dataset:
   """Helper-function for packing a dataset which has already been batched.
 
   Helper for pack_dataset()  Uses tf.while_loop.
 
   Args:
     dataset: a dataset containing padded batches of examples.
     keys: a list of strings
@@ -163,15 +172,16 @@
 
   def write_packed_example(partial, outputs):
     new_partial = empty_example.copy()
     new_outputs = {}
     for k in keys_etc:
       new_outputs[k] = outputs[k].write(
           outputs[k].size(),
-          tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))
+          tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]),
+      )
     return new_partial, new_outputs
 
   def map_fn(x):
     """Internal function to flat_map over.
 
     Consumes a batch of input examples and produces a variable number of output
     examples.
@@ -183,17 +193,19 @@
     """
     partial = empty_example.copy()
     i = tf.zeros([], dtype=tf.int32)
     dynamic_batch_size = tf.shape(x[keys[0]])[0]
     outputs = {}
     for k in keys:
       outputs[k] = tf.TensorArray(
-          tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
+          tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]]
+      )
       outputs[k + '_position'] = tf.TensorArray(
-          tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
+          tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]]
+      )
 
     def body_fn(i, partial, outputs):
       """Body function for while_loop.
 
       Args:
         i: integer scalar
         partial: dictionary of Tensor (partially-constructed example)
@@ -202,80 +214,83 @@
       Returns:
         A triple containing the new values of the inputs.
       """
       can_append = True
       one_example = {}
       for k in keys:
         val = tf.cast(x[k][i], tf.int32)
-        val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
+        val = val[: tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
         one_example[k] = val
       for k in keys:
         can_append = tf.logical_and(
             can_append,
             tf.less_equal(
-                tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]))
+                tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]
+            ),
+        )
 
       def false_fn():
         return write_packed_example(partial, outputs)
 
       def true_fn():
         return partial, outputs
 
       partial, outputs = tf.cond(can_append, true_fn, false_fn)
       new_partial = {}
       for k in keys:
-        new_seq = one_example[k][:key2length[k]]
+        new_seq = one_example[k][: key2length[k]]
         new_seq_len = tf.size(new_seq)
         new_partial[k] = tf.concat([partial[k], new_seq], 0)
         new_partial[k + '_position'] = tf.concat(
-            [partial[k + '_position'],
-             tf.range(new_seq_len)], 0)
+            [partial[k + '_position'], tf.range(new_seq_len)], 0
+        )
       partial = new_partial
       return i + 1, partial, outputs
 
     # For loop over all examples in the batch.
     i, partial, outputs = tf.while_loop(
         cond=lambda *_: True,
         body=body_fn,
         loop_vars=(i, partial, outputs),
         shape_invariants=(
             tf.TensorShape([]),
-            {k: tf.TensorShape([None]) for k in keys_etc}, # type: ignore[wrong-arg-types]
-            {k: tf.TensorShape(None) for k in keys_etc}, # type: ignore[wrong-arg-types]
+            {k: tf.TensorShape([None]) for k in keys_etc},  # type: ignore[wrong-arg-types]
+            {k: tf.TensorShape(None) for k in keys_etc},  # type: ignore[wrong-arg-types]
         ),
-        maximum_iterations=dynamic_batch_size)
+        maximum_iterations=dynamic_batch_size,
+    )
     _, outputs = write_packed_example(partial, outputs)
     packed = {k: outputs[k].stack() for k in keys_etc}
     for k in keys:
-      packed[k + '_segmentation'] = (
-          tf.cumsum(
-              tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1) *
-          tf.cast(tf.not_equal(packed[k], 0), tf.int32))
+      packed[k + '_segmentation'] = tf.cumsum(
+          tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1
+      ) * tf.cast(tf.not_equal(packed[k], 0), tf.int32)
     return packed
 
   dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)
   return dataset.unbatch()
 
 
 # -----------------------------------------------------------------------------
 # Main dataset prep routines.
 # -----------------------------------------------------------------------------
-def preprocess_data(dataset,
-                    shuffle: bool,
-                    num_epochs: Optional[int] = 1,
-                    pack_examples: bool = True,
-                    shuffle_buffer_size: int = 1024,
-                    max_length: int = 512,
-                    batch_size: int = 256,
-                    drop_remainder: bool = True,
-                    prefetch_size: int = AUTOTUNE):
+def preprocess_data(
+    dataset,
+    shuffle: bool,
+    num_epochs: Optional[int] = 1,
+    pack_examples: bool = True,
+    shuffle_buffer_size: int = 1024,
+    max_length: int = 512,
+    batch_size: int = 256,
+    drop_remainder: bool = True,
+    prefetch_size: int = AUTOTUNE,
+):
   """Shuffle and batch/pack the given dataset."""
 
   def length_filter(max_len):
-
     def filter_fn(x):
       source, target = x['inputs'], x['targets']
       l = tf.maximum(tf.shape(source)[0], tf.shape(target)[0])
       return tf.less(l, max_len + 1)
 
     return filter_fn
 
@@ -288,34 +303,31 @@
 
   if pack_examples:
     dataset = pack_dataset(dataset, max_length)
     dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)
   else:  # simple (static-shape) padded batching
     dataset = dataset.padded_batch(
         batch_size,
-        padded_shapes={
-            'inputs': max_length,
-            'targets': max_length
-        },
-        padding_values={
-            'inputs': 0,
-            'targets': 0
-        },
-        drop_remainder=drop_remainder)
+        padded_shapes={'inputs': max_length, 'targets': max_length},
+        padding_values={'inputs': 0, 'targets': 0},
+        drop_remainder=drop_remainder,
+    )
 
   if prefetch_size:
     dataset = dataset.prefetch(prefetch_size)
 
   return dataset
 
 
-def get_datasets(config: ml_collections.ConfigDict,
-                 *,
-                 n_devices: int,
-                 vocab_path: Optional[str] = None):
+def get_datasets(
+    config: ml_collections.ConfigDict,
+    *,
+    n_devices: int,
+    vocab_path: Optional[str] = None
+):
   """Load and return dataset of batched examples for use during training."""
   if vocab_path is None:
     vocab_path = os.path.expanduser('~/lm1b_sentencepiece_model')
 
   train_ds_builder = tfds.builder(config.dataset_name)
   train_data = get_raw_dataset(train_ds_builder, 'train')
 
@@ -326,43 +338,49 @@
   eval_data = get_raw_dataset(eval_ds_builder, config.eval_split)
 
   # Tokenize data.
   sp_tokenizer = tokenizer.load_or_train_tokenizer(
       train_data,
       vocab_path=vocab_path,
       vocab_size=config.vocab_size,
-      max_corpus_chars=config.max_corpus_chars)
+      max_corpus_chars=config.max_corpus_chars,
+  )
   train_data = train_data.map(
-      tokenizer.TokenizeOp(sp_tokenizer), num_parallel_calls=AUTOTUNE)
+      tokenizer.TokenizeOp(sp_tokenizer), num_parallel_calls=AUTOTUNE
+  )
   eval_data = eval_data.map(
-      tokenizer.TokenizeOp(sp_tokenizer), num_parallel_calls=AUTOTUNE)
+      tokenizer.TokenizeOp(sp_tokenizer), num_parallel_calls=AUTOTUNE
+  )
 
   batch_size = config.per_device_batch_size * n_devices
   if config.eval_per_device_batch_size > 0:
     eval_batch_size = config.eval_per_device_batch_size * n_devices
   else:
     eval_batch_size = batch_size
 
   train_ds = preprocess_data(
       train_data,
       shuffle=True,
       num_epochs=None,
       pack_examples=True,
       batch_size=batch_size,
-      max_length=config.max_target_length)
+      max_length=config.max_target_length,
+  )
 
   eval_ds = preprocess_data(
       eval_data,
       shuffle=False,
       pack_examples=False,
       batch_size=eval_batch_size,
-      max_length=config.max_eval_target_length)
+      max_length=config.max_eval_target_length,
+  )
 
   predict_ds = preprocess_data(
       eval_data,
       shuffle=False,
       pack_examples=False,
       batch_size=eval_batch_size,
       max_length=config.max_predict_length,
-      drop_remainder=False)
+      drop_remainder=False,
+  )
 
   return train_ds, eval_ds, predict_ds, sp_tokenizer
```

### Comparing `flax-0.7.0/examples/lm1b/input_pipeline_test.py` & `flax-0.7.1/examples/lm1b/input_pipeline_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -49,43 +49,53 @@
 
     # Go two directories up to the root of the flax directory.
     flax_root_dir = pathlib.Path(__file__).parents[2]
     data_dir = str(flax_root_dir) + '/.tfds/metadata'  # pylint: disable=unused-variable
 
     with tfds.testing.mock_data(num_examples=128, data_dir=data_dir):
       train_ds, eval_ds, predict_ds, _ = input_pipeline.get_datasets(
-          n_devices=2, config=config, vocab_path=vocab_path)
+          n_devices=2, config=config, vocab_path=vocab_path
+      )
     return train_ds, eval_ds, predict_ds
 
   def test_train_ds(self):
     expected_shape = [2, _TARGET_LENGTH]  # 2 devices.
     # For training we pack multiple short examples in one example.
     # *_position and *_segmentation indicate the boundaries.
     for batch in self.train_ds.take(3):
-      self.assertEqual({k: v.shape.as_list() for k, v in batch.items()}, {
-          'inputs': expected_shape,
-          'inputs_position': expected_shape,
-          'inputs_segmentation': expected_shape,
-          'targets': expected_shape,
-          'targets_position': expected_shape,
-          'targets_segmentation': expected_shape,
-      })
+      self.assertEqual(
+          {k: v.shape.as_list() for k, v in batch.items()},
+          {
+              'inputs': expected_shape,
+              'inputs_position': expected_shape,
+              'inputs_segmentation': expected_shape,
+              'targets': expected_shape,
+              'targets_position': expected_shape,
+              'targets_segmentation': expected_shape,
+          },
+      )
 
   def test_eval_ds(self):
     expected_shape = [4, _EVAL_TARGET_LENGTH]  # 2 devices.
     for batch in self.eval_ds.take(3):
-      self.assertEqual({k: v.shape.as_list() for k, v in batch.items()}, {
-          'inputs': expected_shape,
-          'targets': expected_shape,
-      })
+      self.assertEqual(
+          {k: v.shape.as_list() for k, v in batch.items()},
+          {
+              'inputs': expected_shape,
+              'targets': expected_shape,
+          },
+      )
 
   def test_predict_ds(self):
     expected_shape = [4, _PREDICT_TARGET_LENGTH]  # 2 devices.
     for batch in self.predict_ds.take(3):
-      self.assertEqual({k: v.shape.as_list() for k, v in batch.items()}, {
-          'inputs': expected_shape,
-          'targets': expected_shape,
-      })
+      self.assertEqual(
+          {k: v.shape.as_list() for k, v in batch.items()},
+          {
+              'inputs': expected_shape,
+              'targets': expected_shape,
+          },
+      )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/examples/lm1b/main.py` & `flax-0.7.1/examples/sst2/main.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,38 +8,39 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Main file for running the Language Modelling example with LM1B.
-
+"""Main file for running the SST2 example.
 This file is intentionally kept short. The majority for logic is in libraries
 that can be easily tested and imported in Colab.
 """
 
 from absl import app
 from absl import flags
 from absl import logging
 from clu import platform
 import jax
 from ml_collections import config_flags
 import tensorflow as tf
 
 import train
 
+
 FLAGS = flags.FLAGS
 
 flags.DEFINE_string('workdir', None, 'Directory to store model data.')
 config_flags.DEFINE_config_file(
     'config',
-    'configs/default.py',
+    None,
     'File path to the training hyperparameter configuration.',
-    lock_config=True)
+    lock_config=True,
+)
 flags.mark_flags_as_required(['config', 'workdir'])
 
 
 def main(argv):
   if len(argv) > 1:
     raise app.UsageError('Too many command-line arguments.')
 
@@ -48,18 +49,20 @@
   tf.config.experimental.set_visible_devices([], 'GPU')
 
   logging.info('JAX process: %d / %d', jax.process_index(), jax.process_count())
   logging.info('JAX local devices: %r', jax.local_devices())
 
   # Add a note so that we can tell which task is which JAX host.
   # (Depending on the platform task 0 is not guaranteed to be host 0)
-  platform.work_unit().set_task_status(f'process_index: {jax.process_index()}, '
-                                       f'process_count: {jax.process_count()}')
-  platform.work_unit().create_artifact(platform.ArtifactType.DIRECTORY,
-                                       FLAGS.workdir, 'workdir')
+  platform.work_unit().set_task_status(
+      f'process_index: {jax.process_index()}, '
+      f'process_count: {jax.process_count()}'
+  )
+  platform.work_unit().create_artifact(
+      platform.ArtifactType.DIRECTORY, FLAGS.workdir, 'workdir'
+  )
 
   train.train_and_evaluate(FLAGS.config, FLAGS.workdir)
 
 
 if __name__ == '__main__':
-  jax.config.config_with_absl()
   app.run(main)
```

### Comparing `flax-0.7.0/examples/lm1b/models.py` & `flax-0.7.1/examples/lm1b/models.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,17 +27,19 @@
 
 from flax import linen as nn
 from flax import struct
 from jax import lax
 import jax.numpy as jnp
 import numpy as np
 
+
 @struct.dataclass
 class TransformerConfig:
   """Global hyperparameters used to minimize obnoxious kwarg plumbing."""
+
   vocab_size: int
   output_vocab_size: int
   share_embeddings: bool = False
   logits_via_embedding: bool = False
   dtype: Any = jnp.float32
   emb_dim: int = 512
   num_heads: int = 8
@@ -55,31 +57,30 @@
 
 
 def shift_right(x, axis=1):
   """Shift the input to the right by padding and slicing on axis."""
   pad_widths = [(0, 0)] * len(x.shape)
   pad_widths[axis] = (1, 0)
   padded = jnp.pad(
-      x, pad_widths, mode='constant', constant_values=x.dtype.type(0))
+      x, pad_widths, mode='constant', constant_values=x.dtype.type(0)
+  )
   return lax.dynamic_slice_in_dim(padded, 0, padded.shape[axis] - 1, axis)
 
 
 def shift_inputs(x, segment_ids=None, axis=1):
   """Shift inputs and replace EOS by 0 for packed inputs."""
   shifted = shift_right(x, axis=axis)
   # For packed targets, the first shifted token of a new sequence is made
   # 0, rather than being the EOS token for the last sequence.
   if segment_ids is not None:
-    shifted *= (segment_ids == shift_right(segment_ids, axis=axis))
+    shifted *= segment_ids == shift_right(segment_ids, axis=axis)
   return shifted
 
 
-def sinusoidal_init(max_len=2048,
-                    min_scale=1.0,
-                    max_scale=10000.0):
+def sinusoidal_init(max_len=2048, min_scale=1.0, max_scale=10000.0):
   """1D Sinusoidal Position Embedding Initializer.
 
   Args:
       max_len: maximum possible length for the input.
       min_scale: float: minimum frequency-scale in sine grating.
       max_scale: float: maximum frequency-scale in sine grating.
 
@@ -91,36 +92,35 @@
     """Sinusoidal init."""
     del key, dtype
     d_feature = shape[-1]
     pe = np.zeros((max_len, d_feature), dtype=np.float32)
     position = np.arange(0, max_len)[:, np.newaxis]
     scale_factor = -np.log(max_scale / min_scale) / (d_feature // 2 - 1)
     div_term = min_scale * np.exp(np.arange(0, d_feature // 2) * scale_factor)
-    pe[:, :d_feature // 2] = np.sin(position * div_term)
-    pe[:, d_feature // 2: 2 * (d_feature // 2)] = np.cos(position * div_term)
+    pe[:, : d_feature // 2] = np.sin(position * div_term)
+    pe[:, d_feature // 2 : 2 * (d_feature // 2)] = np.cos(position * div_term)
     pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]
     return jnp.array(pe)
 
   return init
 
 
 class AddPositionEmbs(nn.Module):
   """Adds (optionally learned) positional embeddings to the inputs.
 
   Args:
     config: TransformerConfig dataclass containing hyperparameters.
     decode: whether to run in single-position autoregressive mode.
   """
+
   config: TransformerConfig
   decode: bool = False
 
   @nn.compact
-  def __call__(self,
-               inputs,
-               inputs_positions=None):
+  def __call__(self, inputs, inputs_positions=None):
     """Applies AddPositionEmbs module.
 
     By default this layer uses a fixed sinusoidal embedding table. If a
     learned position embedding is desired, pass an initializer to
     posemb_init in the configuration.
 
     Args:
@@ -128,40 +128,41 @@
       inputs_positions: input position indices for packed sequences.
 
     Returns:
       output: `(bs, timesteps, in_dim)`
     """
     config = self.config
     # inputs.shape is (batch_size, seq_len, emb_dim)
-    assert inputs.ndim == 3, ('Number of dimensions should be 3,'
-                              ' but it is: %d' % inputs.ndim)
+    assert inputs.ndim == 3, (
+        'Number of dimensions should be 3, but it is: %d' % inputs.ndim
+    )
     length = inputs.shape[1]
     pos_emb_shape = (1, config.max_len, inputs.shape[-1])
     if config.posemb_init is None:
       # Use a fixed (non-learned) sinusoidal position embedding.
-      pos_embedding = sinusoidal_init(max_len=config.max_len)(None,
-                                                              pos_emb_shape,
-                                                              None)
+      pos_embedding = sinusoidal_init(max_len=config.max_len)(
+          None, pos_emb_shape, None
+      )
     else:
-      pos_embedding = self.param('pos_embedding', config.posemb_init,
-                                 pos_emb_shape)
+      pos_embedding = self.param(
+          'pos_embedding', config.posemb_init, pos_emb_shape
+      )
     pe = pos_embedding[:, :length, :]
 
     # We use a cache position index for tracking decoding position.
     if self.decode:
       is_initialized = self.has_variable('cache', 'cache_index')
-      cache_index = self.variable('cache', 'cache_index',
-                                  lambda: jnp.array(0, dtype=jnp.uint32))
+      cache_index = self.variable(
+          'cache', 'cache_index', lambda: jnp.array(0, dtype=jnp.uint32)
+      )
       if is_initialized:
         i = cache_index.value
         cache_index.value = i + 1
         _, _, df = pos_embedding.shape
-        pe = lax.dynamic_slice(pos_embedding,
-                               jnp.array((0, i, 0)),
-                               (1, 1, df))
+        pe = lax.dynamic_slice(pos_embedding, jnp.array((0, i, 0)), (1, 1, df))
     if inputs_positions is None:
       # normal unpacked case:
       return inputs + pe
     else:
       # for packed data we need to use known position indices:
       return inputs + jnp.take(pe[0], inputs_positions, axis=0)
 
@@ -169,56 +170,56 @@
 class MlpBlock(nn.Module):
   """Transformer MLP / feed-forward block.
 
   Args:
     config: TransformerConfig dataclass containing hyperparameters.
     out_dim: optionally specify out dimension.
   """
+
   config: TransformerConfig
   out_dim: Optional[int] = None
 
   @nn.compact
   def __call__(self, inputs):
     """Applies Transformer MlpBlock module."""
     config = self.config
-    actual_out_dim = (inputs.shape[-1] if self.out_dim is None
-                      else self.out_dim)
+    actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim
     x = nn.Dense(
         config.mlp_dim,
         dtype=config.dtype,
         kernel_init=config.kernel_init,
-        bias_init=config.bias_init)(
-            inputs)
+        bias_init=config.bias_init,
+    )(inputs)
     x = nn.relu(x)
     x = nn.Dropout(rate=config.dropout_rate)(
-        x, deterministic=config.deterministic)
+        x, deterministic=config.deterministic
+    )
     output = nn.Dense(
         actual_out_dim,
         dtype=config.dtype,
         kernel_init=config.kernel_init,
-        bias_init=config.bias_init)(
-            x)
+        bias_init=config.bias_init,
+    )(x)
     output = nn.Dropout(rate=config.dropout_rate)(
-        output, deterministic=config.deterministic)
+        output, deterministic=config.deterministic
+    )
     return output
 
 
 class EncoderDecoder1DBlock(nn.Module):
   """Transformer encoder-decoder layer.
 
   Args:
     config: TransformerConfig dataclass containing hyperparameters.
   """
+
   config: TransformerConfig
 
   @nn.compact
-  def __call__(self,
-               inputs,
-               decoder_mask=None,
-               encoder_decoder_mask=None):
+  def __call__(self, inputs, decoder_mask=None, encoder_decoder_mask=None):
     """Applies EncoderDecoder1DBlock module.
 
     Args:
       inputs: input data for decoder
       decoder_mask: decoder self-attention mask.
       encoder_decoder_mask: encoder-decoder attention mask.
 
@@ -236,17 +237,19 @@
         qkv_features=config.qkv_dim,
         kernel_init=config.kernel_init,
         bias_init=config.bias_init,
         use_bias=False,
         broadcast_dropout=False,
         dropout_rate=config.attention_dropout_rate,
         deterministic=config.deterministic,
-        decode=config.decode)(x, decoder_mask)
+        decode=config.decode,
+    )(x, decoder_mask)
     x = nn.Dropout(rate=config.dropout_rate)(
-        x, deterministic=config.deterministic)
+        x, deterministic=config.deterministic
+    )
     x = x + inputs
 
     # MLP block.
     z = nn.LayerNorm(dtype=config.dtype)(x)
     z = MlpBlock(config=config)(z)
 
     return x + z
@@ -255,24 +258,27 @@
 class Decoder(nn.Module):
   """Transformer Model Decoder for sequence to sequence translation.
 
   Args:
     config: TransformerConfig dataclass containing hyperparameters.
     shared_embedding: a shared embedding layer to use.
   """
+
   config: TransformerConfig
   shared_embedding: Any = None
 
   @nn.compact
-  def __call__(self,
-               inputs,
-               inputs_positions=None,
-               inputs_segmentation=None,
-               decoder_mask=None,
-               encoder_decoder_mask=None):
+  def __call__(
+      self,
+      inputs,
+      inputs_positions=None,
+      inputs_segmentation=None,
+      decoder_mask=None,
+      encoder_decoder_mask=None,
+  ):
     """Applies Transformer model on the inputs.
 
     Args:
       encoded: encoded input data from encoder.
       inputs: input data.
       inputs_positions: input subsequence positions for packed examples.
       inputs_segmentation: input segmentation info for packed examples.
@@ -286,69 +292,67 @@
     assert inputs.ndim == 2  # (batch, len)
 
     # Target Embedding
     if self.shared_embedding is None:
       output_embed = nn.Embed(
           num_embeddings=config.output_vocab_size,
           features=config.emb_dim,
-          embedding_init=nn.initializers.normal(stddev=1.0))
+          embedding_init=nn.initializers.normal(stddev=1.0),
+      )
     else:
       output_embed = self.shared_embedding
 
     y = inputs.astype('int32')
     if not config.decode:
       y = shift_inputs(y, segment_ids=inputs_segmentation)
     y = output_embed(y)
     y = AddPositionEmbs(
-        config=config, decode=config.decode, name='posembed_output')(
-            y, inputs_positions=inputs_positions)
+        config=config, decode=config.decode, name='posembed_output'
+    )(y, inputs_positions=inputs_positions)
     y = nn.Dropout(rate=config.dropout_rate)(
-        y, deterministic=config.deterministic)
+        y, deterministic=config.deterministic
+    )
 
     y = y.astype(config.dtype)
 
     # Target-Input Decoder
     for lyr in range(config.num_layers):
       y = EncoderDecoder1DBlock(
-          config=config, name=f'encoderdecoderblock_{lyr}')(
-              y,
-              decoder_mask=decoder_mask,
-              encoder_decoder_mask=encoder_decoder_mask)
+          config=config, name=f'encoderdecoderblock_{lyr}'
+      )(y, decoder_mask=decoder_mask, encoder_decoder_mask=encoder_decoder_mask)
     y = nn.LayerNorm(dtype=config.dtype, name='encoderdecoder_norm')(y)
 
     # Decoded Logits
     if config.logits_via_embedding:
       # Use the transpose of embedding matrix for logit transform.
       logits = output_embed.attend(y.astype(jnp.float32))
       # Correctly normalize pre-softmax logits for this shared case.
       logits = logits / jnp.sqrt(y.shape[-1])
     else:
       logits = nn.Dense(
           config.output_vocab_size,
           dtype=config.dtype,
           kernel_init=config.kernel_init,
           bias_init=config.bias_init,
-          name='logitdense')(
-              y)
+          name='logitdense',
+      )(y)
     return logits
 
 
 class TransformerLM(nn.Module):
   """Transformer pure decoder stack for language modelling.
 
   Args:
     config: TransformerConfig dataclass containing hyperparameters.
   """
+
   config: TransformerConfig
 
   @nn.compact
-  def __call__(self,
-               inputs,
-               inputs_positions=None,
-               inputs_segmentation=None):
+  def __call__(self, inputs, inputs_positions=None, inputs_segmentation=None):
     """Applies TransformerLM on the inputs.
 
     Args:
       inputs: target data.
       inputs_positions: input subsequence positions for packed examples.
       inputs_segmentation: input segmentation info for packed examples.
 
@@ -360,27 +364,30 @@
     # Make padding attention masks.
     if config.decode:
       # for fast autoregressive decoding we use no decoder mask
       decoder_mask = None
     else:
       decoder_mask = nn.combine_masks(
           nn.make_attention_mask(inputs > 0, inputs > 0, dtype=config.dtype),
-          nn.make_causal_mask(inputs, dtype=config.dtype))
+          nn.make_causal_mask(inputs, dtype=config.dtype),
+      )
 
     # Add segmentation block-diagonal attention masks if using segmented data.
     if inputs_segmentation is not None:
       decoder_mask = nn.combine_masks(
           decoder_mask,
           nn.make_attention_mask(
               inputs_segmentation,
               inputs_segmentation,
               jnp.equal,
-              dtype=config.dtype))
-
-    logits = Decoder(
-        config=config, shared_embedding=None, name='decoder')(
-            inputs,
-            inputs_positions=inputs_positions,
-            inputs_segmentation=inputs_segmentation,
-            decoder_mask=decoder_mask,
-            encoder_decoder_mask=None)
+              dtype=config.dtype,
+          ),
+      )
+
+    logits = Decoder(config=config, shared_embedding=None, name='decoder')(
+        inputs,
+        inputs_positions=inputs_positions,
+        inputs_segmentation=inputs_segmentation,
+        decoder_mask=decoder_mask,
+        encoder_decoder_mask=None,
+    )
     return logits.astype(self.config.dtype)
```

### Comparing `flax-0.7.0/examples/lm1b/temperature_sampler.py` & `flax-0.7.1/examples/lm1b/temperature_sampler.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,21 +20,23 @@
 
 
 # Constants
 # The default End-of-Sentence token id is 2 (SentencePiece).
 EOS_ID = 2
 
 
-def temperature_sample(prompt_inputs,
-                       init_cache,
-                       tokens_to_logits,
-                       prng_key,
-                       temperature=1.0,
-                       topk=20,
-                       eos_token=EOS_ID):
+def temperature_sample(
+    prompt_inputs,
+    init_cache,
+    tokens_to_logits,
+    prng_key,
+    temperature=1.0,
+    topk=20,
+    eos_token=EOS_ID,
+):
   """Temperature sampling for language model generation.
 
   Args:
     prompt_inputs: array: [batch_size, max_decode_len] int32 sequence of tokens.
     init_cache: flax attention cache.
     tokens_to_logits: fast autoregressive decoder function taking single token
       slices and cache and returning next-token logits and updated cache.
@@ -68,15 +70,15 @@
   # Sampling loop state is stored in a simple tuple.
   sampling_loop_init_state = (i0, sequences0, init_cache, token0, ended0, rng0)
 
   def sampling_loop_cond_fn(state):
     """Sampling loop termination condition."""
     (i, _, _, _, ended, _) = state
     # Have we reached max decoding length?
-    not_at_end = (i < max_decode_len - 1)
+    not_at_end = i < max_decode_len - 1
     # Have all sampled sequences reached an end marker?
     all_sequences_ended = jnp.all(ended)
     return not_at_end & (~all_sequences_ended)
 
   def sampling_loop_body_fn(state):
     """Sampling loop state update."""
     i, sequences, cache, cur_token, ended, rng = state
@@ -85,35 +87,41 @@
     # Call fast-decoder model on current tokens to get next-position logits.
     logits, new_cache = tokens_to_logits(cur_token, cache)
     # Sample next token from logits.
     # TODO(levskaya): add top-p "nucleus" sampling option.
     if topk:
       # Get top-k logits and their indices, sample within these top-k tokens.
       topk_logits, topk_idxs = lax.top_k(logits, topk)
-      topk_token = jnp.expand_dims(random.categorical(
-          rng1, topk_logits / temperature).astype(jnp.int32), axis=-1)
+      topk_token = jnp.expand_dims(
+          random.categorical(rng1, topk_logits / temperature).astype(jnp.int32),
+          axis=-1,
+      )
       # Return the original indices corresponding to the sampled top-k tokens.
       next_token = jnp.squeeze(
-          jnp.take_along_axis(topk_idxs, topk_token, axis=-1), axis=-1)
+          jnp.take_along_axis(topk_idxs, topk_token, axis=-1), axis=-1
+      )
     else:
-      next_token = random.categorical(
-          rng1, logits / temperature).astype(jnp.int32)
+      next_token = random.categorical(rng1, logits / temperature).astype(
+          jnp.int32
+      )
     # Only use sampled tokens if we're past provided prefix tokens.
-    out_of_prompt = (sequences[:, i+1] == 0)
-    next_token = (next_token * out_of_prompt +
-                  sequences[:, i+1] * ~out_of_prompt)
+    out_of_prompt = sequences[:, i + 1] == 0
+    next_token = (
+        next_token * out_of_prompt + sequences[:, i + 1] * ~out_of_prompt
+    )
     # If end-marker reached for batch item, only emit padding tokens.
-    next_token_or_endpad = (next_token[None] * ~ended)
-    ended |= (next_token_or_endpad == end_marker)
+    next_token_or_endpad = next_token[None] * ~ended
+    ended |= next_token_or_endpad == end_marker
     # Add current sampled tokens to recorded sequences.
     new_sequences = lax.dynamic_update_slice(
-        sequences, next_token_or_endpad, (0, i+1))
-    return (i+1, new_sequences, new_cache, next_token_or_endpad, ended, rng2)
+        sequences, next_token_or_endpad, (0, i + 1)
+    )
+    return (i + 1, new_sequences, new_cache, next_token_or_endpad, ended, rng2)
 
   # Run sampling loop and collect final state.
-  final_state = lax.while_loop(sampling_loop_cond_fn,
-                               sampling_loop_body_fn,
-                               sampling_loop_init_state)
+  final_state = lax.while_loop(
+      sampling_loop_cond_fn, sampling_loop_body_fn, sampling_loop_init_state
+  )
 
   # Pick part of the state corresponding to the sampled sequences.
   final_sequences = final_state[1]
   return final_sequences
```

### Comparing `flax-0.7.0/examples/lm1b/temperature_sampler_test.py` & `flax-0.7.1/examples/lm1b/temperature_sampler_test.py`

 * *Files 9% similar despite different names*

```diff
@@ -20,26 +20,29 @@
 from temperature_sampler import temperature_sample
 
 
 jax.config.update('jax_disable_most_optimizations', True)
 
 
 class TestTemperatureSampler(absltest.TestCase):
-  def test_temperature_sampler(self):
 
+  def test_temperature_sampler(self):
     tokens = jnp.array([[5, 0, 0, 0]], dtype=jnp.int32)
     cache = None
     key = jax.random.PRNGKey(0)
 
     def tokens_to_logits(tokens, cache):
-        jax.debug.print("tokens: {}", tokens)
-        logits = jax.nn.one_hot(tokens[..., -1:] + 1, 10)
-        logits = jnp.where(logits < 0.5, float('-inf'), logits)
-        logits = logits.squeeze(axis=1)
-        return logits, cache
-
-    new_tokens = temperature_sample(tokens, cache, tokens_to_logits, key, topk=5)
+      jax.debug.print('tokens: {}', tokens)
+      logits = jax.nn.one_hot(tokens[..., -1:] + 1, 10)
+      logits = jnp.where(logits < 0.5, float('-inf'), logits)
+      logits = logits.squeeze(axis=1)
+      return logits, cache
+
+    new_tokens = temperature_sample(
+        tokens, cache, tokens_to_logits, key, topk=5
+    )
 
     np.testing.assert_array_equal(new_tokens, [[5, 6, 7, 8]])
 
+
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.7.0/examples/lm1b/tokenizer.py` & `flax-0.7.1/examples/wmt/tokenizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,65 +10,69 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Provides op for tokenizing a dataset."""
 
+import dataclasses
 import os
 import tempfile
 import time
 from typing import Any, Dict, Iterable, Tuple
 
 from absl import logging
-import dataclasses
 import jax
-from sentencepiece import SentencePieceTrainer
 import tensorflow as tf
 import tensorflow_text as tftxt
 
+from sentencepiece import SentencePieceTrainer
+
 Features = Dict[str, tf.Tensor]
 
 
 def _dump_chars_to_textfile(
     dataset: tf.data.Dataset,
     maxchars: int = int(1e7),
-    data_keys=('inputs', 'targets')
+    data_keys=('inputs', 'targets'),
 ) -> Tuple[str, int]:
   """Write part of a TFDS sentence dataset to lines in a text file.
 
   Args:
     dataset: tf.dataset containing string-data.
     maxchars: int: approximate number of characters to save from dataset.
     data_keys: Tuple[str]: what keys in dataset to dump from.
 
   Returns:
     name of temp file with dataset bytes, exact number of characters dumped.
   """
   char_count = 0
   ds_iter = dataset.as_numpy_iterator()
   with tempfile.NamedTemporaryFile(
-      delete=False, prefix='/tmp/ds_chars') as outfp:
+      delete=False, prefix='/tmp/ds_chars'
+  ) as outfp:
     while char_count < maxchars:
       example = next(ds_iter)
       for k in data_keys:
         line = example[k] + b'\n'
         char_count += len(line)
         outfp.write(line)
   return outfp.name, char_count
 
 
-def _train_sentencepiece(dataset: tf.data.Dataset,
-                         *,
-                         vocab_size: int,
-                         maxchars: int = int(1e7),
-                         model_path: str,
-                         model_type: str = 'unigram',
-                         character_coverage: float = 1.0,
-                         data_keys=('inputs', 'targets')):
+def _train_sentencepiece(
+    dataset: tf.data.Dataset,
+    *,
+    vocab_size: int,
+    maxchars: int = int(1e7),
+    model_path: str,
+    model_type: str = 'unigram',
+    character_coverage: float = 1.0,
+    data_keys=('inputs', 'targets'),
+):
   """Train SentencePiece tokenizer from subset of tf dataset.
 
   Args:
     dataset: tf.dataset
     vocab_size: int: size of vocab tokens to train.
     maxchars: int: number of characters to use for sentencepiece training.
     model_path: str: path of model file to save vocab model to.
@@ -82,22 +86,26 @@
     path to the trained sentencepiece vocabulary model.
   """
   if model_path.startswith('gs://'):
     abs_model_path = model_path
   else:
     abs_model_path = os.path.abspath(os.path.expanduser(model_path))
   fname, _ = _dump_chars_to_textfile(
-      dataset, maxchars=maxchars, data_keys=data_keys)
+      dataset, maxchars=maxchars, data_keys=data_keys
+  )
   with tempfile.NamedTemporaryFile(
-      delete=False, prefix='/tmp/sp_tmp') as model_fp:
+      delete=False, prefix='/tmp/sp_tmp'
+  ) as model_fp:
     pass  # we just want a prefix'd tmp-filename
   argstr = ' '.join([
-      f'--input={fname}', f'--vocab_size={vocab_size}',
+      f'--input={fname}',
+      f'--vocab_size={vocab_size}',
       f'--character_coverage={character_coverage}',
-      f'--model_prefix={model_fp.name}', f'--model_type={model_type}'
+      f'--model_prefix={model_fp.name}',
+      f'--model_type={model_type}',
   ])
   SentencePieceTrainer.Train(argstr)
   if jax.process_index() == 0:
     # Use an intermediate filename that is renamed to the target name to address
     # create and fill delays.
     copy_rename_path = abs_model_path + '.rntmp'
     tf.io.gfile.copy(model_fp.name + '.model', copy_rename_path, overwrite=True)
@@ -106,49 +114,54 @@
   else:
     while not tf.io.gfile.exists(abs_model_path):
       time.sleep(1)
     time.sleep(1)
   return abs_model_path
 
 
-def _load_sentencepiece_tokenizer(model_path: str,
-                                  add_bos: bool = False,
-                                  add_eos: bool = True,
-                                  reverse: bool = False):
+def _load_sentencepiece_tokenizer(
+    model_path: str,
+    add_bos: bool = False,
+    add_eos: bool = True,
+    reverse: bool = False,
+):
   """Load a tf-text SentencePiece tokenizer from given model filepath."""
   with tf.io.gfile.GFile(model_path, 'rb') as model_fp:
     sp_model = model_fp.read()
   sp_tokenizer = tftxt.SentencepieceTokenizer(
-      model=sp_model, add_bos=add_bos, add_eos=add_eos, reverse=reverse)
+      model=sp_model, add_bos=add_bos, add_eos=add_eos, reverse=reverse
+  )
   return sp_tokenizer
 
 
-def load_or_train_tokenizer(dataset: tf.data.Dataset,
-                            *,
-                            vocab_path: str,
-                            vocab_size: int,
-                            max_corpus_chars: int,
-                            data_keys: Tuple[str, str] = ('inputs', 'targets')):
+def load_or_train_tokenizer(
+    dataset: tf.data.Dataset,
+    *,
+    vocab_path: str,
+    vocab_size: int,
+    max_corpus_chars: int,
+    data_keys: Tuple[str, str] = ('inputs', 'targets'),
+):
   """Loads the tokenizer at `vocab_path` or trains a one from `dataset`."""
   try:
     return _load_sentencepiece_tokenizer(vocab_path)
   except tf.errors.NotFoundError:
     logging.info('SentencePiece vocab not found, building one from data.')
     vocab_path = _train_sentencepiece(
         dataset,
         vocab_size=vocab_size,
         maxchars=max_corpus_chars,
         model_path=vocab_path,
-        data_keys=data_keys)
+        data_keys=data_keys,
+    )
     return _load_sentencepiece_tokenizer(vocab_path)
 
 
 @dataclasses.dataclass
 class TokenizeOp:
-
   sp_tokenizer: Any
   data_keys: Iterable[str] = ('inputs', 'targets')
 
   def __call__(self, features: Features) -> Features:
     for k in self.data_keys:
       features[k] = self.sp_tokenizer.tokenize(features[k])
     return features
```

### Comparing `flax-0.7.0/examples/lm1b/train.py` & `flax-0.7.1/examples/lm1b/train.py`

 * *Files 4% similar despite different names*

```diff
@@ -59,56 +59,64 @@
       schedule makes it less steep in the beginning (close to 0).
 
   Returns:
     A schedule that applies the reverse square root.
   """
 
   def schedule(count):
-    return init_value * (count + shift)**-.5 * shift**.5
+    return init_value * (count + shift) ** -0.5 * shift**0.5
 
   return schedule
 
 
 def create_learning_rate_schedule(learning_rate: float, warmup_steps: int):
   """Creates a rsqrt schedule with linear warmup."""
-  return optax.join_schedules([
-      optax.linear_schedule(
-          init_value=0, end_value=learning_rate, transition_steps=warmup_steps),
-      rsqrt_schedule(init_value=learning_rate, shift=warmup_steps),
-  ],
-                              boundaries=[warmup_steps])
-
-
-def compute_weighted_cross_entropy(logits,
-                                   targets,
-                                   weights=None,
-                                   label_smoothing=0.0):
+  return optax.join_schedules(
+      [
+          optax.linear_schedule(
+              init_value=0,
+              end_value=learning_rate,
+              transition_steps=warmup_steps,
+          ),
+          rsqrt_schedule(init_value=learning_rate, shift=warmup_steps),
+      ],
+      boundaries=[warmup_steps],
+  )
+
+
+def compute_weighted_cross_entropy(
+    logits, targets, weights=None, label_smoothing=0.0
+):
   """Compute weighted cross entropy and entropy for log probs and targets.
 
   Args:
    logits: [batch, length, num_classes] float array.
    targets: categorical targets [batch, length] int array.
    weights: None or array of shape [batch, length].
    label_smoothing: label smoothing constant, used to determine the on and off
      values.
 
   Returns:
     Tuple of scalar loss and batch normalizing factor.
   """
   if logits.ndim != targets.ndim + 1:
-    raise ValueError("Incorrect shapes. Got shape %s logits and %s targets" %
-                     (str(logits.shape), str(targets.shape)))
+    raise ValueError(
+        "Incorrect shapes. Got shape %s logits and %s targets"
+        % (str(logits.shape), str(targets.shape))
+    )
   vocab_size = logits.shape[-1]
   confidence = 1.0 - label_smoothing
   low_confidence = (1.0 - confidence) / (vocab_size - 1)
   normalizing_constant = -(
-      confidence * jnp.log(confidence) +
-      (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20))
+      confidence * jnp.log(confidence)
+      + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)
+  )
   soft_targets = common_utils.onehot(
-      targets, vocab_size, on_value=confidence, off_value=low_confidence)
+      targets, vocab_size, on_value=confidence, off_value=low_confidence
+  )
 
   loss = -jnp.sum(soft_targets * nn.log_softmax(logits), axis=-1)
   loss = loss - normalizing_constant
 
   normalizing_factor = np.prod(targets.shape)
   if weights is not None:
     loss = loss * weights
@@ -125,74 +133,82 @@
    targets: categorical targets [batch, length] int array.
    weights: None or array of shape [batch, length]
 
   Returns:
     Tuple of scalar loss and batch normalizing factor.
   """
   if logits.ndim != targets.ndim + 1:
-    raise ValueError("Incorrect shapes. Got shape %s logits and %s targets" %
-                     (str(logits.shape), str(targets.shape)))
+    raise ValueError(
+        "Incorrect shapes. Got shape %s logits and %s targets"
+        % (str(logits.shape), str(targets.shape))
+    )
   loss = jnp.equal(jnp.argmax(logits, axis=-1), targets)
   normalizing_factor = np.prod(logits.shape[:-1])
   if weights is not None:
     loss = loss * weights
     normalizing_factor = weights.sum()
 
   return loss.sum(), normalizing_factor
 
 
 def compute_metrics(logits, labels, weights, label_smoothing=0.0):
   """Compute summary metrics."""
-  loss, weight_sum = compute_weighted_cross_entropy(logits, labels, weights,
-                                                    label_smoothing)
+  loss, weight_sum = compute_weighted_cross_entropy(
+      logits, labels, weights, label_smoothing
+  )
   acc, _ = compute_weighted_accuracy(logits, labels, weights)
   metrics = {
       "loss": loss,
       "accuracy": acc,
       "denominator": weight_sum,
   }
   metrics = jax.lax.psum(metrics, axis_name="batch")
   return metrics
 
 
 # Primary training / eval / decode step functions.
 # -----------------------------------------------------------------------------
 
 
-def train_step(state,
-               batch,
-               config,
-               learning_rate_fn,
-               label_smoothing=0.0,
-               dropout_rng=None):
+def train_step(
+    state,
+    batch,
+    config,
+    learning_rate_fn,
+    label_smoothing=0.0,
+    dropout_rng=None,
+):
   """Perform a single training step."""
   # X_position and X_segmentation are needed only when using "packed examples"
   # where multiple sequences are packed into the same example with this
   # metadata.
   # if such features are not present they are ignored and the example is treated
   # like a normal, unpacked sequence example.
   train_keys = ["inputs", "inputs_position", "inputs_segmentation"]
-  (inputs, inputs_positions, inputs_segmentation
-   ) = (batch.get(k, None) for k in train_keys)
+  (inputs, inputs_positions, inputs_segmentation) = (
+      batch.get(k, None) for k in train_keys
+  )
 
   weights = jnp.where(inputs > 0, 1, 0).astype(jnp.float32)
 
   dropout_rng = jax.random.fold_in(dropout_rng, state.step)
 
   def loss_fn(params):
     """loss function used for training."""
     logits = models.TransformerLM(config).apply(
         {"params": params},
         inputs,
         inputs_positions=inputs_positions,
         inputs_segmentation=inputs_segmentation,
-        rngs={"dropout": dropout_rng})
+        rngs={"dropout": dropout_rng},
+    )
 
-    loss, weight_sum = compute_weighted_cross_entropy(logits, inputs, weights,
-                                                      label_smoothing)
+    loss, weight_sum = compute_weighted_cross_entropy(
+        logits, inputs, weights, label_smoothing
+    )
     mean_loss = loss / weight_sum
     return mean_loss, logits
 
   step = state.step
   lr = learning_rate_fn(step)
   grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
   (_, logits), grads = grad_fn(state.params)
@@ -209,39 +225,30 @@
   inputs = batch["inputs"]
   weights = jnp.where(inputs > 0, 1.0, 0.0)
   logits = models.TransformerLM(config).apply({"params": params}, inputs)
 
   return compute_metrics(logits, inputs, weights, label_smoothing)
 
 
-def predict_step(inputs,
-                 params,
-                 rngkey,
-                 eos_id,
-                 max_decode_len,
-                 config,
-                 temperature,
-                 top_k):
+def predict_step(
+    inputs, params, rngkey, eos_id, max_decode_len, config, temperature, top_k
+):
   """Predict language model on a batch."""
   target_shape = (inputs.shape[0], max_decode_len) + inputs.shape[2:]
   initial_variables = models.TransformerLM(config).init(
-      jax.random.PRNGKey(0),
-      jnp.ones(target_shape, config.dtype))
+      jax.random.PRNGKey(0), jnp.ones(target_shape, config.dtype)
+  )
   cache = initial_variables["cache"]
 
   def tokens_ids_to_logits(flat_ids, flat_cache):
     """Token slice to logits from decoder model."""
     # --> [batch * beam, 1, vocab]
     flat_logits, new_vars = models.TransformerLM(config).apply(
-        {
-            "params": params,
-            "cache": flat_cache
-        },
-        flat_ids,
-        mutable=["cache"])
+        {"params": params, "cache": flat_cache}, flat_ids, mutable=["cache"]
+    )
     new_flat_cache = new_vars["cache"]
     # Remove singleton sequence-length dimension:
     # [batch, 1, vocab] --> [batch, vocab]
     flat_logits = flat_logits.squeeze(axis=1)
     return flat_logits, new_flat_cache
 
   # Using the above-defined single-step decoder function, run a
@@ -249,15 +256,16 @@
   seqs = temperature_sampler.temperature_sample(
       inputs,
       cache,
       tokens_ids_to_logits,
       rngkey,
       temperature=temperature,
       topk=top_k,
-      eos_token=eos_id)
+      eos_token=eos_id,
+  )
 
   return seqs
 
 
 # Utils for prediction
 # -----------------------------------------------------------------------------
 
@@ -273,74 +281,84 @@
   host2devices = collections.defaultdict(list)
   for d in jax.devices():
     host2devices[d.process_index].append(d)
   devices = [host2devices[k][0] for k in host2devices]
   host_psum = jax.pmap(lambda x: jax.lax.psum(x, "i"), "i", devices=devices)
 
   def pre_pmap(xs):
-    return jax.tree_util.tree_map(lambda x: jnp.broadcast_to(x, (1,) + x.shape), xs)
+    return jax.tree_util.tree_map(
+        lambda x: jnp.broadcast_to(x, (1,) + x.shape), xs
+    )
 
   def post_pmap(xs):
     return jax.tree_util.tree_map(lambda x: x[0], xs)
 
   return post_pmap(host_psum(pre_pmap(in_tree)))
 
 
 def tohost(x):
   """Collect batches from all devices to host and flatten batch dimensions."""
   n_device, n_batch, *remaining_dims = x.shape
   return np.array(x).reshape((n_device * n_batch,) + tuple(remaining_dims))
 
 
-def evaluate(*, p_eval_step, params, eval_ds: tf.data.Dataset,
-             num_eval_steps: int):
+def evaluate(
+    *, p_eval_step, params, eval_ds: tf.data.Dataset, num_eval_steps: int
+):
   """Evaluate the target an return a dictionary with the metrics."""
   logging.info("Gathering evaluation metrics.")
   eval_metrics = []
   eval_iter = iter(eval_ds)  # pytype: disable=wrong-arg-types
   for _, eval_batch in zip(range(num_eval_steps), eval_iter):
     eval_batch = jax.tree_util.tree_map(lambda x: x._numpy(), eval_batch)  # pylint: disable=protected-access
     eval_batch = common_utils.shard(eval_batch)
     metrics = p_eval_step(params, eval_batch)
     eval_metrics.append(metrics)
   eval_metrics = common_utils.get_metrics(eval_metrics)
   eval_metrics_sums = jax.tree_util.tree_map(jnp.sum, eval_metrics)
   eval_denominator = eval_metrics_sums.pop("denominator")
   eval_summary = jax.tree_util.tree_map(
       lambda x: x / eval_denominator,  # pylint: disable=cell-var-from-loop
-      eval_metrics_sums)
+      eval_metrics_sums,
+  )
   return eval_summary
 
 
-def generate_prediction(*, p_pred_step, params,
-                        tokenized_prompts,
-                        eos_id,
-                        inference_rng,
-                        decode_tokens,
-                        max_predict_length: int):
+def generate_prediction(
+    *,
+    p_pred_step,
+    params,
+    tokenized_prompts,
+    eos_id,
+    inference_rng,
+    decode_tokens,
+    max_predict_length: int,
+):
   """Generate text from the prompt."""
   n_devices = jax.local_device_count()
 
   logging.info("Generating text.")
   predictions = []
   # Use batch of prompts provided by user.
   for pred_batch in jnp.array_split(
-      tokenized_prompts, int(np.ceil(len(tokenized_prompts) / n_devices))):
+      tokenized_prompts, int(np.ceil(len(tokenized_prompts) / n_devices))
+  ):
     cur_pred_batch_size = pred_batch.shape[0]
     if cur_pred_batch_size % n_devices:
-      padded_size = int(
-          np.ceil(cur_pred_batch_size / n_devices) * n_devices)
+      padded_size = int(np.ceil(cur_pred_batch_size / n_devices) * n_devices)
       pred_batch = jax.tree_util.tree_map(
-          lambda x: pad_examples(x, padded_size), pred_batch)  # pylint: disable=cell-var-from-loop
+          lambda x: pad_examples(x, padded_size), pred_batch
+      )  # pylint: disable=cell-var-from-loop
     pred_batch = common_utils.shard(pred_batch)
     inference_rng, sub_rng = random.split(inference_rng)
     inference_rngs = random.split(sub_rng, n_devices)
 
-    predicted = p_pred_step(pred_batch, params, inference_rngs,
-                            eos_id, max_predict_length)
+    predicted = p_pred_step(
+        pred_batch, params, inference_rngs, eos_id, max_predict_length
+    )
     predicted = tohost(predicted)
     # Iterate through non-padding examples of batch.
     for s in predicted[:cur_pred_batch_size]:
       prediction = decode_tokens(s)
       logging.info("Sample: %s", str(prediction))
       predictions.append(prediction)
 
@@ -367,36 +385,36 @@
     config.vocab_path = vocab_path
   tf.io.gfile.makedirs(os.path.split(vocab_path)[0])
 
   # Load Dataset
   # ---------------------------------------------------------------------------
   logging.info("Initializing dataset.")
   train_ds, eval_ds, _, encoder = input_pipeline.get_datasets(
-      n_devices=jax.local_device_count(),
-      config=config,
-      vocab_path=vocab_path)
+      n_devices=jax.local_device_count(), config=config, vocab_path=vocab_path
+  )
 
   train_iter = iter(train_ds)
   vocab_size = int(encoder.vocab_size())
   eos_id = temperature_sampler.EOS_ID  # Default Sentencepiece EOS token.
 
   def decode_tokens(toks):
-    valid_toks = toks[:np.argmax(toks == eos_id) + 1].astype(np.int32)
+    valid_toks = toks[: np.argmax(toks == eos_id) + 1].astype(np.int32)
     return encoder.detokenize(valid_toks).numpy().decode("utf-8")
 
   def encode_strings(strs, max_len):
     tokenized_batch = np.zeros((len(strs), max_len), np.int32)
     for i, s in enumerate(strs):
       toks = encoder.tokenize(s).numpy()
       # Remove EOS token in prompt.
-      tokenized_batch[i, :toks.shape[0]-1] = toks[:-1]
+      tokenized_batch[i, : toks.shape[0] - 1] = toks[:-1]
     return tokenized_batch
 
   tokenized_prompts = encode_strings(
-      [config.prompts], config.max_predict_length)
+      [config.prompts], config.max_predict_length
+  )
 
   logging.info("Initializing model, optimizer, and step functions.")
   # Build Model and Optimizer
   # ---------------------------------------------------------------------------
   train_config = models.TransformerConfig(
       vocab_size=vocab_size,
       output_vocab_size=vocab_size,
@@ -409,105 +427,114 @@
       mlp_dim=config.mlp_dim,
       max_len=max(config.max_target_length, config.max_eval_target_length),
       dropout_rate=config.dropout_rate,
       attention_dropout_rate=config.attention_dropout_rate,
       deterministic=False,
       decode=False,
       kernel_init=nn.initializers.xavier_uniform(),
-      bias_init=nn.initializers.normal(stddev=1e-6))
+      bias_init=nn.initializers.normal(stddev=1e-6),
+  )
   eval_config = train_config.replace(deterministic=True)
   predict_config = train_config.replace(deterministic=True, decode=True)
 
   start_step = 0
   rng = jax.random.PRNGKey(config.seed)
   rng, init_rng = jax.random.split(rng)
   rng, inference_rng = random.split(rng)
   input_shape = (config.per_device_batch_size, config.max_target_length)
 
   m = models.TransformerLM(eval_config)
-  initial_variables = jax.jit(m.init)(init_rng,
-                                      jnp.ones(input_shape, jnp.float32))
+  initial_variables = jax.jit(m.init)(
+      init_rng, jnp.ones(input_shape, jnp.float32)
+  )
 
   learning_rate_fn = create_learning_rate_schedule(
-      learning_rate=config.learning_rate, warmup_steps=config.warmup_steps)
+      learning_rate=config.learning_rate, warmup_steps=config.warmup_steps
+  )
 
   optimizer = optax.adamw(
-      learning_rate_fn, b1=0.9, b2=0.98, eps=1e-9,
-      weight_decay=config.weight_decay
-      )
+      learning_rate_fn,
+      b1=0.9,
+      b2=0.98,
+      eps=1e-9,
+      weight_decay=config.weight_decay,
+  )
   state = train_state.TrainState.create(
-      apply_fn=m.apply,
-      params=initial_variables["params"],
-      tx=optimizer
-      )
+      apply_fn=m.apply, params=initial_variables["params"], tx=optimizer
+  )
   # We access model params only from optimizer below.
   del initial_variables
 
   if config.restore_checkpoints:
     # Restore unreplicated optimizer + model state from last checkpoint.
     state = checkpoints.restore_checkpoint(workdir, state)
     # Grab last step.
     start_step = int(state.step)
 
   writer = metric_writers.create_default_writer(
-      workdir, just_logging=jax.process_index() > 0)
+      workdir, just_logging=jax.process_index() > 0
+  )
   if start_step == 0:
     writer.write_hparams(dict(config))
 
   # Replicate optimizer.
   state = jax_utils.replicate(state)
 
   # compile multidevice versions of train/eval/predict step fn.
   p_train_step = jax.pmap(
       functools.partial(
-          train_step,
-          config=train_config,
-          learning_rate_fn=learning_rate_fn),
+          train_step, config=train_config, learning_rate_fn=learning_rate_fn
+      ),
       axis_name="batch",
-      donate_argnums=(0,))  # pytype: disable=wrong-arg-types
+      donate_argnums=(0,),
+  )  # pytype: disable=wrong-arg-types
   p_eval_step = jax.pmap(
-      functools.partial(
-          eval_step, config=eval_config),
-      axis_name="batch")
+      functools.partial(eval_step, config=eval_config), axis_name="batch"
+  )
 
   p_pred_step = jax.pmap(
       functools.partial(
-          predict_step, config=predict_config,
+          predict_step,
+          config=predict_config,
           temperature=config.sampling_temperature,
-          top_k=config.sampling_top_k),
+          top_k=config.sampling_top_k,
+      ),
       axis_name="batch",
-      static_broadcasted_argnums=(3, 4))  # eos token, max_length are constant
+      static_broadcasted_argnums=(3, 4),
+  )  # eos token, max_length are constant
 
   # Main Train Loop
   # ---------------------------------------------------------------------------
 
   # We init the first set of dropout PRNG keys, but update it afterwards inside
   # the main pmap"d training update for performance.
   dropout_rngs = jax.random.split(rng, jax.local_device_count())
   del rng
 
   logging.info("Starting training loop.")
   hooks = []
   report_progress = periodic_actions.ReportProgress(
-      num_train_steps=config.num_train_steps, writer=writer)
+      num_train_steps=config.num_train_steps, writer=writer
+  )
   if jax.process_index() == 0:
     hooks += [
         report_progress,
-        periodic_actions.Profile(logdir=workdir, num_profile_steps=5)
+        periodic_actions.Profile(logdir=workdir, num_profile_steps=5),
     ]
   train_metrics = []
   with metric_writers.ensure_flushes(writer):
     for step in range(start_step, config.num_train_steps):
       is_last_step = step == config.num_train_steps - 1
 
       # Shard data to devices and do a training step.
       with jax.profiler.StepTraceAnnotation("train", step_num=step):
-        batch = common_utils.shard(jax.tree_util.tree_map(np.asarray, next(train_iter)))
-        state, metrics = p_train_step(
-            state, batch, dropout_rng=dropout_rngs)
+        batch = common_utils.shard(
+            jax.tree_util.tree_map(np.asarray, next(train_iter))
+        )
+        state, metrics = p_train_step(state, batch, dropout_rng=dropout_rngs)
         train_metrics.append(metrics)
 
       # Quick indication that training is happening.
       logging.log_first_n(logging.INFO, "Finished training step %d.", 5, step)
       for h in hooks:
         h(step)
 
@@ -515,47 +542,55 @@
       if step % config.eval_every_steps == 0 or is_last_step:
         with report_progress.timed("training_metrics"):
           logging.info("Gathering training metrics.")
           train_metrics = common_utils.get_metrics(train_metrics)
           lr = train_metrics.pop("learning_rate").mean()
           metrics_sums = jax.tree_util.tree_map(jnp.sum, train_metrics)
           denominator = metrics_sums.pop("denominator")
-          summary = jax.tree_util.tree_map(lambda x: x / denominator, metrics_sums)  # pylint: disable=cell-var-from-loop
+          summary = jax.tree_util.tree_map(
+              lambda x: x / denominator, metrics_sums
+          )  # pylint: disable=cell-var-from-loop
           summary["learning_rate"] = lr
           summary["perplexity"] = jnp.clip(
-              jnp.exp(summary["loss"]), a_max=1.0e4)
+              jnp.exp(summary["loss"]), a_max=1.0e4
+          )
           summary = {"train_" + k: v for k, v in summary.items()}
           writer.write_scalars(step, summary)
           train_metrics = []
 
         with report_progress.timed("eval"):
           eval_results = evaluate(
               p_eval_step=p_eval_step,
               params=state.params,
               eval_ds=eval_ds,
-              num_eval_steps=config.num_eval_steps)
+              num_eval_steps=config.num_eval_steps,
+          )
           # (clipped) perplexity after averaging log-perplexitie
           eval_results["perplexity"] = jnp.clip(
-              jnp.exp(eval_results["loss"]), a_max=1.0e4)
+              jnp.exp(eval_results["loss"]), a_max=1.0e4
+          )
           writer.write_scalars(
-              step, {"eval_" + k: v for k, v in eval_results.items()})
+              step, {"eval_" + k: v for k, v in eval_results.items()}
+          )
 
         with report_progress.timed("generate_text"):
           exemplars = generate_prediction(
               p_pred_step=p_pred_step,
               params=state.params,
               tokenized_prompts=tokenized_prompts,
               eos_id=eos_id,
               inference_rng=inference_rng,
               decode_tokens=decode_tokens,
-              max_predict_length=config.max_predict_length)
+              max_predict_length=config.max_predict_length,
+          )
           writer.write_texts(step, {"samples": exemplars})
 
       # Save a checkpoint on one host after every checkpoint_freq steps.
-      save_checkpoint = (step % config.checkpoint_every_steps == 0 or
-                         is_last_step)
+      save_checkpoint = (
+          step % config.checkpoint_every_steps == 0 or is_last_step
+      )
       if config.save_checkpoints and save_checkpoint:
         logging.info("Saving checkpoint step %d.", step)
         with report_progress.timed("checkpoint"):
           checkpoints.save_checkpoint_multiprocess(
               workdir, jax_utils.unreplicate(state), step
           )
```

### Comparing `flax-0.7.0/examples/lm1b/train_test.py` & `flax-0.7.1/examples/lm1b/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/mnist/README.md` & `flax-0.7.1/examples/mnist/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/mnist/configs/default.py` & `flax-0.7.1/examples/mnist/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/mnist/main.py` & `flax-0.7.1/examples/ogbg_molpcba/main.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,18 +8,18 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Main file for running the MNIST example.
+"""Main file for running the ogbg-molpcba example.
 
-This file is intentionally kept short. The majority of logic is in libraries
-than can be easily tested and imported in Colab.
+This file is intentionally kept short. The majority for logic is in libraries
+that can be easily tested and imported in Colab.
 """
 
 from absl import app
 from absl import flags
 from absl import logging
 from clu import platform
 import jax
@@ -32,34 +32,39 @@
 FLAGS = flags.FLAGS
 
 flags.DEFINE_string('workdir', None, 'Directory to store model data.')
 config_flags.DEFINE_config_file(
     'config',
     None,
     'File path to the training hyperparameter configuration.',
-    lock_config=True)
+    lock_config=True,
+)
 
 
 def main(argv):
   if len(argv) > 1:
     raise app.UsageError('Too many command-line arguments.')
 
   # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make
   # it unavailable to JAX.
   tf.config.experimental.set_visible_devices([], 'GPU')
 
-  logging.info('JAX process: %d / %d', jax.process_index(), jax.process_count())
+  # This example only supports single-host training on a single device.
+  logging.info('JAX host: %d / %d', jax.process_index(), jax.process_count())
   logging.info('JAX local devices: %r', jax.local_devices())
 
   # Add a note so that we can tell which task is which JAX host.
   # (Depending on the platform task 0 is not guaranteed to be host 0)
-  platform.work_unit().set_task_status(f'process_index: {jax.process_index()}, '
-                                       f'process_count: {jax.process_count()}')
-  platform.work_unit().create_artifact(platform.ArtifactType.DIRECTORY,
-                                       FLAGS.workdir, 'workdir')
+  platform.work_unit().set_task_status(
+      f'process_index: {jax.process_index()}, '
+      f'process_count: {jax.process_count()}'
+  )
+  platform.work_unit().create_artifact(
+      platform.ArtifactType.DIRECTORY, FLAGS.workdir, 'workdir'
+  )
 
   train.train_and_evaluate(FLAGS.config, FLAGS.workdir)
 
 
 if __name__ == '__main__':
   flags.mark_flags_as_required(['config', 'workdir'])
   app.run(main)
```

### Comparing `flax-0.7.0/examples/mnist/mnist.ipynb` & `flax-0.7.1/examples/mnist/mnist.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/mnist/mnist_benchmark.py` & `flax-0.7.1/examples/mnist/mnist_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/mnist/train.py` & `flax-0.7.1/examples/mnist/train.py`

 * *Files 5% similar despite different names*

```diff
@@ -50,14 +50,15 @@
     x = nn.Dense(features=10)(x)
     return x
 
 
 @jax.jit
 def apply_model(state, images, labels):
   """Computes gradients, loss and accuracy for a single batch."""
+
   def loss_fn(params):
     logits = state.apply_fn({'params': params}, images)
     one_hot = jax.nn.one_hot(labels, 10)
     loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))
     return loss, logits
 
   grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
@@ -73,15 +74,15 @@
 
 def train_epoch(state, train_ds, batch_size, rng):
   """Train for a single epoch."""
   train_ds_size = len(train_ds['image'])
   steps_per_epoch = train_ds_size // batch_size
 
   perms = jax.random.permutation(rng, len(train_ds['image']))
-  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch
+  perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch
   perms = perms.reshape((steps_per_epoch, batch_size))
 
   epoch_loss = []
   epoch_accuracy = []
 
   for perm in perms:
     batch_images = train_ds['image'][perm, ...]
@@ -97,30 +98,30 @@
 
 def get_datasets():
   """Load MNIST train and test datasets into memory."""
   ds_builder = tfds.builder('mnist')
   ds_builder.download_and_prepare()
   train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))
   test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))
-  train_ds['image'] = jnp.float32(train_ds['image']) / 255.
-  test_ds['image'] = jnp.float32(test_ds['image']) / 255.
+  train_ds['image'] = jnp.float32(train_ds['image']) / 255.0
+  test_ds['image'] = jnp.float32(test_ds['image']) / 255.0
   return train_ds, test_ds
 
 
 def create_train_state(rng, config):
   """Creates initial `TrainState`."""
   cnn = CNN()
   params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']
   tx = optax.sgd(config.learning_rate, config.momentum)
-  return train_state.TrainState.create(
-      apply_fn=cnn.apply, params=params, tx=tx)
+  return train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)
 
 
-def train_and_evaluate(config: ml_collections.ConfigDict,
-                       workdir: str) -> train_state.TrainState:
+def train_and_evaluate(
+    config: ml_collections.ConfigDict, workdir: str
+) -> train_state.TrainState:
   """Execute model training and evaluation loop.
 
   Args:
     config: Hyperparameter configuration for training and evaluation.
     workdir: Directory where the tensorboard summaries are written to.
 
   Returns:
@@ -133,24 +134,32 @@
   summary_writer.hparams(dict(config))
 
   rng, init_rng = jax.random.split(rng)
   state = create_train_state(init_rng, config)
 
   for epoch in range(1, config.num_epochs + 1):
     rng, input_rng = jax.random.split(rng)
-    state, train_loss, train_accuracy = train_epoch(state, train_ds,
-                                                    config.batch_size,
-                                                    input_rng)
-    _, test_loss, test_accuracy = apply_model(state, test_ds['image'],
-                                              test_ds['label'])
+    state, train_loss, train_accuracy = train_epoch(
+        state, train_ds, config.batch_size, input_rng
+    )
+    _, test_loss, test_accuracy = apply_model(
+        state, test_ds['image'], test_ds['label']
+    )
 
     logging.info(
-        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'
-        % (epoch, train_loss, train_accuracy * 100, test_loss,
-           test_accuracy * 100))
+        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f,'
+        ' test_accuracy: %.2f'
+        % (
+            epoch,
+            train_loss,
+            train_accuracy * 100,
+            test_loss,
+            test_accuracy * 100,
+        )
+    )
 
     summary_writer.scalar('train_loss', train_loss, epoch)
     summary_writer.scalar('train_accuracy', train_accuracy, epoch)
     summary_writer.scalar('test_loss', test_loss, epoch)
     summary_writer.scalar('test_accuracy', test_accuracy, epoch)
 
   summary_writer.flush()
```

### Comparing `flax-0.7.0/examples/mnist/train_test.py` & `flax-0.7.1/examples/mnist/train_test.py`

 * *Files 9% similar despite different names*

```diff
@@ -44,15 +44,19 @@
     rng = jax.random.PRNGKey(0)
     inputs = jnp.ones((1, 28, 28, 3), jnp.float32)
     output, variables = train.CNN().init_with_output(rng, inputs)
 
     self.assertEqual((1, 10), output.shape)
     self.assertEqual(
         CNN_PARAMS,
-        sum(np.prod(arr.shape) for arr in jax.tree_util.tree_leaves(variables["params"])))
+        sum(
+            np.prod(arr.shape)
+            for arr in jax.tree_util.tree_leaves(variables["params"])
+        ),
+    )
 
   def test_train_and_evaluate(self):
     """Tests training and evaluation code by running a single step."""
     # Create a temporary directory where tensorboard metrics are written.
     workdir = tempfile.mkdtemp()
 
     # Go two directories up to the root of the flax directory.
```

### Comparing `flax-0.7.0/examples/nlp_seq/README.md` & `flax-0.7.1/examples/nlp_seq/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/nlp_seq/input_pipeline.py` & `flax-0.7.1/examples/nlp_seq/input_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -39,14 +39,15 @@
   1    They     they    PRON    PRP    Case=Nom|Number=Plur       2    nsubj
   2    buy      buy     VERB    VBP    Number=Plur|PTense=Pres    0    root
   3    books    book    NOUN    NNS    Number=Plur                2    obj
   4    .        .       PUNCT   .      _                          2    punct
 
   For details, please see: http://universaldependencies.org/format.html.
   """
+
   ID = 0
   FORM = 1
   LEMMA = 2
   UPOS = 3
   XPOS = 4
   FEATS = 5
   HEAD = 6
@@ -75,25 +76,33 @@
         xpos_counter[split[CoNLLAttributes.XPOS.value]] += 1
 
   special_tokens = {PAD: PAD_ID, UNKNOWN: UNKNOWN_ID, ROOT: ROOT_ID}
 
   # create word form vocab
   vocabs = {'forms': {}, 'xpos': {}}
   vocabs['forms'].update(special_tokens)
-  vocabs['forms'].update({
-      form[0]: id for id, form in enumerate(
-          form_counter.most_common(max_num_forms), start=ROOT_ID + 1)
-  })
+  vocabs['forms'].update(
+      {
+          form[0]: id
+          for id, form in enumerate(
+              form_counter.most_common(max_num_forms), start=ROOT_ID + 1
+          )
+      }
+  )
 
   # create xpos vocab
   vocabs['xpos'].update(special_tokens)
-  vocabs['xpos'].update({
-      tag[0]: id
-      for id, tag in enumerate(xpos_counter.most_common(), start=ROOT_ID + 1)
-  })
+  vocabs['xpos'].update(
+      {
+          tag[0]: id
+          for id, tag in enumerate(
+              xpos_counter.most_common(), start=ROOT_ID + 1
+          )
+      }
+  )
 
   return vocabs
 
 
 def create_token(token, attributes, vocabs):
   """Map for a token a selected subset of attributes to indices.
 
@@ -120,16 +129,17 @@
     if attribute == CoNLLAttributes.FORM:
       selected_attributes.append(vocabs['forms'].get(token[index], UNKNOWN_ID))
     elif attribute == CoNLLAttributes.XPOS:
       selected_attributes.append(vocabs['xpos'].get(token[index], UNKNOWN_ID))
     elif attribute == CoNLLAttributes.HEAD:
       selected_attributes.append(int(token[index]))
     else:
-      raise ValueError('CoNLL index %s not covered by mapping.' %
-                       str(attribute.name))
+      raise ValueError(
+          'CoNLL index %s not covered by mapping.' % str(attribute.name)
+      )
   return selected_attributes
 
 
 def create_sentence_with_root(attributes, vocabs):
   """Create a sentence containing a root.
 
   Args:
@@ -146,18 +156,17 @@
   token_properties[CoNLLAttributes.HEAD.value] = '0'
   token = create_token(token_properties, attributes, vocabs)
   if len(token) == 1:
     token = token[0]
   return [token]
 
 
-def sentences_from_conll_data(corpus_filename,
-                              vocabs,
-                              attributes,
-                              max_sentence_length=1000):
+def sentences_from_conll_data(
+    corpus_filename, vocabs, attributes, max_sentence_length=1000
+):
   """Load and returns conll data in list format.
 
   Args:
     corpus_filename: filename of corpus.
     vocabs: dictionary of vocabs
     attributes: list of conll attributes to include into the batch
     max_sentence_length: cut off sentences longer as max tokens
@@ -183,22 +192,24 @@
 
         # Reset sentence.
         sentence = create_sentence_with_root(attributes, vocabs)
     if len(sentence) > 1:  # sentences does not only contain a root.
       yield sentence
 
 
-def sentence_dataset_dict(filename,
-                          vocabs,
-                          attributes_input,
-                          attributes_target,
-                          batch_size,
-                          bucket_size,
-                          repeat=None,
-                          prefetch_size=tf.data.experimental.AUTOTUNE):
+def sentence_dataset_dict(
+    filename,
+    vocabs,
+    attributes_input,
+    attributes_target,
+    batch_size,
+    bucket_size,
+    repeat=None,
+    prefetch_size=tf.data.experimental.AUTOTUNE,
+):
   """Combines sentences into a dataset of padded batches.
 
   Args:
     filename: file name of a corpus.
     vocabs: dictionary of dictionaries to map from strings to ids.
     attributes_input: attributes for the input.
     attributes_target: target attributes empty targets is not included.
@@ -213,35 +224,39 @@
   data_keys = ['inputs']
   if attributes_target:
     data_keys.append('targets')
 
   def generator():
     """Generator to create the data."""
     input_generator = sentences_from_conll_data(
-        filename, vocabs, attributes_input, max_sentence_length=bucket_size)
+        filename, vocabs, attributes_input, max_sentence_length=bucket_size
+    )
 
     if attributes_target:
       target_generator = sentences_from_conll_data(
-          filename, vocabs, attributes_target, max_sentence_length=bucket_size)
+          filename, vocabs, attributes_target, max_sentence_length=bucket_size
+      )
 
     for inputs in input_generator:
       data = {'inputs': inputs}
       if attributes_target:
         data['targets'] = next(target_generator)
       yield data
 
   output_types = {k: tf.float32 for k in data_keys}
   output_shapes = {k: (None,) for k in data_keys}
   dataset = tf.data.Dataset.from_generator(
-      generator, output_types=output_types, output_shapes=output_shapes)
+      generator, output_types=output_types, output_shapes=output_shapes
+  )
 
   # cache the dataset in memory and repeat.
   dataset = dataset.cache()
   dataset = dataset.repeat(repeat)
 
   # static padding up to bucket size.
   padded_shapes = {k: [bucket_size] for k in data_keys}
   dataset = dataset.padded_batch(
-      batch_size=batch_size, padded_shapes=(padded_shapes))
+      batch_size=batch_size, padded_shapes=(padded_shapes)
+  )
 
   dataset = dataset.prefetch(prefetch_size)
   return dataset
```

### Comparing `flax-0.7.0/examples/nlp_seq/input_pipeline_test.py` & `flax-0.7.1/examples/nlp_seq/input_pipeline_test.py`

 * *Files 18% similar despite different names*

```diff
@@ -54,60 +54,88 @@
       f.write(CONLL_DATA)
       f.write('\n')
 
   def test_vocab_creation(self):
     """Tests the creation of the vocab."""
     vocabs = input_pipeline.create_vocabs(self._filename)
     self.assertEqual(
-        vocabs['forms'], {
+        vocabs['forms'],
+        {
             '<p>': 0,
             '<u>': 1,
             '<r>': 2,
             'They': 3,
             'buy': 4,
             'books': 5,
             '.': 6,
             'NY': 7,
-        })
+        },
+    )
 
   def testInputBatch(self):
     """Test the batching of the dataset."""
     vocabs = input_pipeline.create_vocabs(self._filename)
 
     attributes_input = [input_pipeline.CoNLLAttributes.FORM]
     attributes_target = []  # empty target for tagging of unlabeled data.
     sentence_dataset = input_pipeline.sentence_dataset_dict(
-        self._filename, vocabs, attributes_input, attributes_target,
-        batch_size=2, bucket_size=10, repeat=1)
+        self._filename,
+        vocabs,
+        attributes_input,
+        attributes_target,
+        batch_size=2,
+        bucket_size=10,
+        repeat=1,
+    )
 
     sentence_dataset_iter = iter(sentence_dataset)
 
     batch = next(sentence_dataset_iter)
     inputs = batch['inputs'].numpy().tolist()
-    self.assertSameStructure(inputs, [[2., 3., 4., 5., 6., 0., 0., 0., 0., 0.],
-                                      [2., 3., 4., 5., 6., 0., 0., 0., 0., 0.]])
+    self.assertSameStructure(
+        inputs,
+        [
+            [2.0, 3.0, 4.0, 5.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0],
+            [2.0, 3.0, 4.0, 5.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0],
+        ],
+    )
     self.assertLen(batch, 1)  # make sure target is not included.
 
   def testInputTargetBatch(self):
     """Test the batching of the dataset."""
     vocabs = input_pipeline.create_vocabs(self._filename)
 
     attributes_input = [input_pipeline.CoNLLAttributes.FORM]
     attributes_target = [input_pipeline.CoNLLAttributes.XPOS]
     sentence_dataset = input_pipeline.sentence_dataset_dict(
-        self._filename, vocabs, attributes_input, attributes_target,
-        batch_size=2, bucket_size=10, repeat=1)
+        self._filename,
+        vocabs,
+        attributes_input,
+        attributes_target,
+        batch_size=2,
+        bucket_size=10,
+        repeat=1,
+    )
 
     sentence_dataset_iter = iter(sentence_dataset)
 
     batch = next(sentence_dataset_iter)
     inputs = batch['inputs'].numpy().tolist()
-    self.assertSameStructure(inputs, [[2., 3., 4., 5., 6., 0., 0., 0., 0., 0.],
-                                      [2., 3., 4., 5., 6., 0., 0., 0., 0., 0.]])
+    self.assertSameStructure(
+        inputs,
+        [
+            [2.0, 3.0, 4.0, 5.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0],
+            [2.0, 3.0, 4.0, 5.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0],
+        ],
+    )
     targets = batch['targets'].numpy().tolist()
-    self.assertSameStructure(targets,
-                             [[2., 4., 5., 3., 6., 0., 0., 0., 0., 0.],
-                              [2., 4., 5., 3., 6., 0., 0., 0., 0., 0.]])
+    self.assertSameStructure(
+        targets,
+        [
+            [2.0, 4.0, 5.0, 3.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0],
+            [2.0, 4.0, 5.0, 3.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0],
+        ],
+    )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/examples/nlp_seq/models.py` & `flax-0.7.1/examples/nlp_seq/models.py`

 * *Files 3% similar despite different names*

```diff
@@ -21,14 +21,15 @@
 import jax.numpy as jnp
 import numpy as np
 
 
 @struct.dataclass
 class TransformerConfig:
   """Global hyperparameters used to minimize obnoxious kwarg plumbing."""
+
   vocab_size: int
   output_vocab_size: int
   dtype: Any = jnp.float32
   emb_dim: int = 512
   num_heads: int = 8
   num_layers: int = 6
   qkv_dim: int = 512
@@ -54,29 +55,31 @@
   def init(key, shape, dtype=np.float32):
     """Sinusoidal init."""
     del key, dtype
     d_feature = shape[-1]
     pe = np.zeros((max_len, d_feature), dtype=np.float32)
     position = np.arange(0, max_len)[:, np.newaxis]
     div_term = np.exp(
-        np.arange(0, d_feature, 2) * -(np.log(10000.0) / d_feature))
+        np.arange(0, d_feature, 2) * -(np.log(10000.0) / d_feature)
+    )
     pe[:, 0::2] = np.sin(position * div_term)
     pe[:, 1::2] = np.cos(position * div_term)
     pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]
     return jnp.array(pe)
 
   return init
 
 
 class AddPositionEmbs(nn.Module):
   """Adds (optionally learned) positional embeddings to the inputs.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
   """
+
   config: TransformerConfig
 
   @nn.compact
   def __call__(self, inputs):
     """Applies AddPositionEmbs module.
 
     By default this layer uses a fixed sinusoidal embedding table. If a
@@ -87,71 +90,75 @@
       inputs: input data.
 
     Returns:
       output: `(bs, timesteps, in_dim)`
     """
     config = self.config
     # inputs.shape is (batch_size, seq_len, emb_dim)
-    assert inputs.ndim == 3, ('Number of dimensions should be 3,'
-                              ' but it is: %d' % inputs.ndim)
+    assert inputs.ndim == 3, (
+        'Number of dimensions should be 3, but it is: %d' % inputs.ndim
+    )
     length = inputs.shape[1]
     pos_emb_shape = (1, config.max_len, inputs.shape[-1])
     if config.posemb_init is None:
       # Use a fixed (non-learned) sinusoidal position embedding.
-      pos_embedding = sinusoidal_init(max_len=config.max_len)(None,
-                                                              pos_emb_shape,
-                                                              None)
+      pos_embedding = sinusoidal_init(max_len=config.max_len)(
+          None, pos_emb_shape, None
+      )
     else:
-      pos_embedding = self.param('pos_embedding', config.posemb_init,
-                                 pos_emb_shape)
+      pos_embedding = self.param(
+          'pos_embedding', config.posemb_init, pos_emb_shape
+      )
     pe = pos_embedding[:, :length, :]
     return inputs + pe
 
 
 class MlpBlock(nn.Module):
   """Transformer MLP / feed-forward block.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
     out_dim: optionally specify out dimension.
   """
+
   config: TransformerConfig
   out_dim: Optional[int] = None
 
   @nn.compact
   def __call__(self, inputs, deterministic=True):
     """Applies Transformer MlpBlock module."""
     config = self.config
-    actual_out_dim = (inputs.shape[-1] if self.out_dim is None
-                      else self.out_dim)
+    actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim
     x = nn.Dense(
         config.mlp_dim,
         dtype=config.dtype,
         kernel_init=config.kernel_init,
-        bias_init=config.bias_init)(
-            inputs)
+        bias_init=config.bias_init,
+    )(inputs)
     x = nn.elu(x)
     x = nn.Dropout(rate=config.dropout_rate)(x, deterministic=deterministic)
     output = nn.Dense(
         actual_out_dim,
         dtype=config.dtype,
         kernel_init=config.kernel_init,
-        bias_init=config.bias_init)(
-            x)
+        bias_init=config.bias_init,
+    )(x)
     output = nn.Dropout(rate=config.dropout_rate)(
-        output, deterministic=deterministic)
+        output, deterministic=deterministic
+    )
     return output
 
 
 class Encoder1DBlock(nn.Module):
   """Transformer encoder layer.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
   """
+
   config: TransformerConfig
 
   @nn.compact
   def __call__(self, inputs, deterministic):
     """Applies Encoder1DBlock module.
 
     Args:
@@ -171,16 +178,16 @@
         dtype=config.dtype,
         qkv_features=config.qkv_dim,
         kernel_init=config.kernel_init,
         bias_init=config.bias_init,
         use_bias=False,
         broadcast_dropout=False,
         dropout_rate=config.attention_dropout_rate,
-        deterministic=deterministic)(
-            x)
+        deterministic=deterministic,
+    )(x)
 
     x = nn.Dropout(rate=config.dropout_rate)(x, deterministic=deterministic)
     x = x + inputs
 
     # MLP block.
     y = nn.LayerNorm(dtype=config.dtype)(x)
     y = MlpBlock(config=config)(y, deterministic=deterministic)
@@ -206,23 +213,22 @@
     """
     assert inputs.ndim == 2  # (batch, len)
 
     config = self.config
 
     x = inputs.astype('int32')
     x = nn.Embed(
-        num_embeddings=config.vocab_size, features=config.emb_dim,
-        name='embed')(
-            x)
+        num_embeddings=config.vocab_size, features=config.emb_dim, name='embed'
+    )(x)
     x = nn.Dropout(rate=config.dropout_rate)(x, deterministic=not train)
     x = AddPositionEmbs(config)(x)
 
     for _ in range(config.num_layers):
       x = Encoder1DBlock(config)(x, deterministic=not train)
 
     x = nn.LayerNorm(dtype=config.dtype)(x)
     logits = nn.Dense(
         config.output_vocab_size,
         kernel_init=config.kernel_init,
-        bias_init=config.bias_init)(
-            x)
+        bias_init=config.bias_init,
+    )(x)
     return logits
```

### Comparing `flax-0.7.0/examples/nlp_seq/train.py` & `flax-0.7.1/examples/nlp_seq/train.py`

 * *Files 16% similar despite different names*

```diff
@@ -38,54 +38,59 @@
 
 import input_pipeline
 import models
 
 
 FLAGS = flags.FLAGS
 
-flags.DEFINE_string('model_dir', default='', help=('Directory for model data.'))
+flags.DEFINE_string('model_dir', default='', help='Directory for model data.')
 
-flags.DEFINE_string('experiment', default='xpos', help=('Experiment name.'))
+flags.DEFINE_string('experiment', default='xpos', help='Experiment name.')
 
-flags.DEFINE_integer(
-    'batch_size', default=64, help=('Batch size for training.'))
+flags.DEFINE_integer('batch_size', default=64, help='Batch size for training.')
 
 flags.DEFINE_integer(
     'eval_frequency',
     default=100,
-    help=('Frequency of eval during training, e.g. every 1000 steps.'))
+    help='Frequency of eval during training, e.g. every 1000 steps.',
+)
 
 flags.DEFINE_integer(
-    'num_train_steps', default=75000, help=('Number of train steps.'))
+    'num_train_steps', default=75000, help='Number of train steps.'
+)
 
-flags.DEFINE_float('learning_rate', default=0.05, help=('Learning rate.'))
+flags.DEFINE_float('learning_rate', default=0.05, help='Learning rate.')
 
 flags.DEFINE_float(
     'weight_decay',
     default=1e-1,
-    help=('Decay factor for AdamW style weight decay.'))
+    help='Decay factor for AdamW style weight decay.',
+)
 
-flags.DEFINE_integer('max_length', default=256,
-                     help=('Maximum length of examples.'))
+flags.DEFINE_integer(
+    'max_length', default=256, help='Maximum length of examples.'
+)
 
 flags.DEFINE_integer(
-    'random_seed', default=0, help=('Integer for PRNG random seed.'))
+    'random_seed', default=0, help='Integer for PRNG random seed.'
+)
 
-flags.DEFINE_string('train', default='', help=('Path to training data.'))
+flags.DEFINE_string('train', default='', help='Path to training data.')
 
-flags.DEFINE_string('dev', default='', help=('Path to development data.'))
+flags.DEFINE_string('dev', default='', help='Path to development data.')
 
 
 def create_learning_rate_scheduler(
     factors='constant * linear_warmup * rsqrt_decay',
     base_learning_rate=0.5,
     warmup_steps=8000,
     decay_factor=0.5,
     steps_per_decay=20000,
-    steps_per_cycle=100000):
+    steps_per_cycle=100000,
+):
   """creates learning rate schedule.
 
   Interprets factors in the factors string which can consist of:
   * constant: interpreted as the constant value,
   * linear_warmup: interpreted as linear warmup until warmup_steps,
   * rsqrt_decay: divide by square root of max(step, warmup_steps)
   * decay_every: Every k steps decay the learning rate by decay_factor.
@@ -115,20 +120,22 @@
         ret *= jnp.minimum(1.0, step / warmup_steps)
       elif name == 'rsqrt_decay':
         ret /= jnp.sqrt(jnp.maximum(step, warmup_steps))
       elif name == 'rsqrt_normalized_decay':
         ret *= jnp.sqrt(warmup_steps)
         ret /= jnp.sqrt(jnp.maximum(step, warmup_steps))
       elif name == 'decay_every':
-        ret *= (decay_factor**(step // steps_per_decay))
+        ret *= decay_factor ** (step // steps_per_decay)
       elif name == 'cosine_decay':
-        progress = jnp.maximum(0.0,
-                               (step - warmup_steps) / float(steps_per_cycle))
-        ret *= jnp.maximum(0.0,
-                           0.5 * (1.0 + jnp.cos(jnp.pi * (progress % 1.0))))
+        progress = jnp.maximum(
+            0.0, (step - warmup_steps) / float(steps_per_cycle)
+        )
+        ret *= jnp.maximum(
+            0.0, 0.5 * (1.0 + jnp.cos(jnp.pi * (progress % 1.0)))
+        )
       else:
         raise ValueError('Unknown factor %s.' % name)
     return jnp.asarray(ret, dtype=jnp.float32)
 
   return step_fn
 
 
@@ -140,16 +147,18 @@
    targets: categorical targets [batch, length] int array.
    weights: None or array of shape [batch x length]
 
   Returns:
     Tuple of scalar loss and batch normalizing factor.
   """
   if logits.ndim != targets.ndim + 1:
-    raise ValueError('Incorrect shapes. Got shape %s logits and %s targets' %
-                     (str(logits.shape), str(targets.shape)))
+    raise ValueError(
+        'Incorrect shapes. Got shape %s logits and %s targets'
+        % (str(logits.shape), str(targets.shape))
+    )
   onehot_targets = common_utils.onehot(targets, logits.shape[-1])
   loss = -jnp.sum(onehot_targets * nn.log_softmax(logits), axis=-1)
   normalizing_factor = onehot_targets.sum()
   if weights is not None:
     loss = loss * weights
     normalizing_factor = weights.sum()
 
@@ -164,16 +173,18 @@
    targets: categorical targets [batch, length] int array.
    weights: None or array of shape [batch x length]
 
   Returns:
     Tuple of scalar accuracy and batch normalizing factor.
   """
   if logits.ndim != targets.ndim + 1:
-    raise ValueError('Incorrect shapes. Got shape %s logits and %s targets' %
-                     (str(logits.shape), str(targets.shape)))
+    raise ValueError(
+        'Incorrect shapes. Got shape %s logits and %s targets'
+        % (str(logits.shape), str(targets.shape))
+    )
   loss = jnp.equal(jnp.argmax(logits, axis=-1), targets)
   normalizing_factor = np.prod(logits.shape[:-1])
   if weights is not None:
     loss = loss * weights
     normalizing_factor = weights.sum()
 
   return loss.sum(), normalizing_factor
@@ -188,43 +199,43 @@
       'accuracy': acc,
       'denominator': weight_sum,
   }
   metrics = np.sum(metrics, -1)
   return metrics
 
 
-def train_step(state,
-               batch,
-               model,
-               learning_rate_fn,
-               dropout_rng=None):
+def train_step(state, batch, model, learning_rate_fn, dropout_rng=None):
   """Perform a single training step."""
   train_keys = ['inputs', 'targets']
   (inputs, targets) = (batch.get(k, None) for k in train_keys)
 
   weights = jnp.where(targets > 0, 1, 0).astype(jnp.float32)
 
   dropout_rng = jax.random.fold_in(dropout_rng, state.step)
 
   def loss_fn(params):
     """loss function used for training."""
-    logits = model.apply({'params': params}, inputs=inputs, train=True,
-                         rngs={'dropout': dropout_rng})
+    logits = model.apply(
+        {'params': params},
+        inputs=inputs,
+        train=True,
+        rngs={'dropout': dropout_rng},
+    )
     loss, weight_sum = compute_weighted_cross_entropy(logits, targets, weights)
 
     mean_loss = loss / weight_sum
     return mean_loss, logits
 
   lr = learning_rate_fn(state.step)
   grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
   (_, logits), grads = grad_fn(state.params)
-  grads = jax.lax.pmean(grads, "batch")
+  grads = jax.lax.pmean(grads, 'batch')
   new_state = state.apply_gradients(grads=grads)
   metrics = compute_metrics(logits, targets, weights)
-  metrics["learning_rate"] = lr
+  metrics['learning_rate'] = lr
 
   return new_state, metrics
 
 
 def pad_examples(x, desired_batch_size):
   """Expand batch to desired size by zeros with the shape of last slice."""
   batch_pad = desired_batch_size - x.shape[0]
@@ -251,79 +262,85 @@
     raise app.UsageError('Please provide path to training set.')
   if batch_size % jax.device_count() > 0:
     raise ValueError('Batch size must be divisible by the number of devices')
   device_batch_size = batch_size // jax.device_count()
 
   if jax.process_index() == 0:
     train_summary_writer = tensorboard.SummaryWriter(
-        os.path.join(FLAGS.model_dir, FLAGS.experiment + '_train'))
+        os.path.join(FLAGS.model_dir, FLAGS.experiment + '_train')
+    )
     eval_summary_writer = tensorboard.SummaryWriter(
-        os.path.join(FLAGS.model_dir, FLAGS.experiment + '_eval'))
+        os.path.join(FLAGS.model_dir, FLAGS.experiment + '_eval')
+    )
 
   # create the training and development dataset
   vocabs = input_pipeline.create_vocabs(FLAGS.train)
   config = models.TransformerConfig(
       vocab_size=len(vocabs['forms']),
       output_vocab_size=len(vocabs['xpos']),
-      max_len=FLAGS.max_length)
+      max_len=FLAGS.max_length,
+  )
 
   attributes_input = [input_pipeline.CoNLLAttributes.FORM]
   attributes_target = [input_pipeline.CoNLLAttributes.XPOS]
   train_ds = input_pipeline.sentence_dataset_dict(
       FLAGS.train,
       vocabs,
       attributes_input,
       attributes_target,
       batch_size=batch_size,
-      bucket_size=config.max_len)
+      bucket_size=config.max_len,
+  )
   train_iter = iter(train_ds)
 
   eval_ds = input_pipeline.sentence_dataset_dict(
       FLAGS.dev,
       vocabs,
       attributes_input,
       attributes_target,
       batch_size=batch_size,
       bucket_size=config.max_len,
-      repeat=1)
+      repeat=1,
+  )
 
   model = models.Transformer(config)
 
   rng = random.PRNGKey(random_seed)
   rng, init_rng = random.split(rng)
 
   # call a jitted initialization function to get the initial parameter tree
   @jax.jit
   def initialize_variables(init_rng):
     init_batch = jnp.ones((config.max_len, 1), jnp.float32)
     init_variables = model.init(init_rng, inputs=init_batch, train=False)
     return init_variables
+
   init_variables = initialize_variables(init_rng)
 
   learning_rate_fn = create_learning_rate_scheduler(
-      base_learning_rate=learning_rate)
+      base_learning_rate=learning_rate
+  )
 
   optimizer = optax.adamw(
-      learning_rate_fn, b1=0.9, b2=0.98, eps=1e-9,
-      weight_decay=1e-1)
+      learning_rate_fn, b1=0.9, b2=0.98, eps=1e-9, weight_decay=1e-1
+  )
   state = train_state.TrainState.create(
-      apply_fn=model.apply,
-      params=init_variables["params"],
-      tx=optimizer)
+      apply_fn=model.apply, params=init_variables['params'], tx=optimizer
+  )
 
   # Replicate optimizer.
   state = jax_utils.replicate(state)
 
   p_train_step = jax.pmap(
       functools.partial(
-          train_step,
-          model=model,
-          learning_rate_fn=learning_rate_fn),
+          train_step, model=model, learning_rate_fn=learning_rate_fn
+      ),
       axis_name='batch',
-      donate_argnums=(0,))  # pytype: disable=wrong-arg-types
+      donate_argnums=(0,),
+  )  # pytype: disable=wrong-arg-types
 
   def eval_step(params, batch):
     """Calculate evaluation metrics on a batch."""
     inputs, targets = batch['inputs'], batch['targets']
     weights = jnp.where(targets > 0, 1.0, 0.0)
     logits = model.apply({'params': params}, inputs=inputs, train=False)
     return compute_metrics(logits, targets, weights)
@@ -333,15 +350,17 @@
   # We init the first set of dropout PRNG keys, but update it afterwards inside
   # the main pmap'd training update for performance.
   dropout_rngs = random.split(rng, jax.local_device_count())
   metrics_all = []
   tick = time.time()
   best_dev_score = 0
   for step, batch in zip(range(num_train_steps), train_iter):
-    batch = common_utils.shard(jax.tree_util.tree_map(lambda x: x._numpy(), batch))  # pylint: disable=protected-access
+    batch = common_utils.shard(
+        jax.tree_util.tree_map(lambda x: x._numpy(), batch)
+    )  # pylint: disable=protected-access
 
     state, metrics = p_train_step(state, batch, dropout_rng=dropout_rngs)
     metrics_all.append(metrics)
     if (step + 1) % eval_freq == 0:
       metrics_all = common_utils.get_metrics(metrics_all)
       lr = metrics_all.pop('learning_rate').mean()
       metrics_sums = jax.tree_util.tree_map(jnp.sum, metrics_all)
@@ -366,29 +385,35 @@
       for eval_batch in eval_iter:
         eval_batch = jax.tree_util.tree_map(lambda x: x._numpy(), eval_batch)  # pylint: disable=protected-access
         # Handle final odd-sized batch by padding instead of dropping it.
         cur_pred_batch_size = eval_batch['inputs'].shape[0]
         if cur_pred_batch_size != batch_size:
           # pad up to batch size
           eval_batch = jax.tree_util.tree_map(
-              lambda x: pad_examples(x, batch_size), eval_batch)
+              lambda x: pad_examples(x, batch_size), eval_batch
+          )
         eval_batch = common_utils.shard(eval_batch)
 
         metrics = p_eval_step(state.params, eval_batch)
 
         eval_metrics.append(metrics)
       eval_metrics = common_utils.get_metrics(eval_metrics)
       eval_metrics_sums = jax.tree_util.tree_map(jnp.sum, eval_metrics)
       eval_denominator = eval_metrics_sums.pop('denominator')
       eval_summary = jax.tree_util.tree_map(
           lambda x: x / eval_denominator,  # pylint: disable=cell-var-from-loop
-          eval_metrics_sums)
+          eval_metrics_sums,
+      )
 
-      logging.info('eval in step: %d, loss: %.4f, accuracy: %.4f', step,
-                   eval_summary['loss'], eval_summary['accuracy'])
+      logging.info(
+          'eval in step: %d, loss: %.4f, accuracy: %.4f',
+          step,
+          eval_summary['loss'],
+          eval_summary['accuracy'],
+      )
 
       if best_dev_score < eval_summary['accuracy']:
         best_dev_score = eval_summary['accuracy']
         # TODO: save model.
       eval_summary['best_dev_score'] = best_dev_score
       logging.info('best development model score %.4f', best_dev_score)
       if jax.process_index() == 0:
```

### Comparing `flax-0.7.0/examples/ogbg_molpcba/README.md` & `flax-0.7.1/examples/ogbg_molpcba/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/ogbg_molpcba/configs/default.py` & `flax-0.7.1/examples/ogbg_molpcba/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/ogbg_molpcba/configs/default_graph_net.py` & `flax-0.7.1/examples/ogbg_molpcba/configs/default_graph_net.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/ogbg_molpcba/configs/hparam_sweep.py` & `flax-0.7.1/examples/ogbg_molpcba/configs/hparam_sweep.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/ogbg_molpcba/configs/test.py` & `flax-0.7.1/examples/ogbg_molpcba/configs/test.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/ogbg_molpcba/input_pipeline.py` & `flax-0.7.1/examples/ogbg_molpcba/input_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,34 +21,35 @@
 import numpy as np
 import tensorflow as tf
 import tensorflow_datasets as tfds
 
 
 class GraphsTupleSize(NamedTuple):
   """Helper class to represent padding and graph sizes."""
+
   n_node: int
   n_edge: int
   n_graph: int
 
 
 def get_raw_datasets() -> Dict[str, tf.data.Dataset]:
   """Returns datasets as tf.data.Dataset, organized by split."""
   ds_builder = tfds.builder('ogbg_molpcba')
   ds_builder.download_and_prepare()
   ds_splits = ['train', 'validation', 'test']
-  datasets = {
-      split: ds_builder.as_dataset(split=split) for split in ds_splits
-  }
+  datasets = {split: ds_builder.as_dataset(split=split) for split in ds_splits}
   return datasets
 
 
-def get_datasets(batch_size: int,
-                 add_virtual_node: bool = True,
-                 add_undirected_edges: bool = True,
-                 add_self_loops: bool = True) -> Dict[str, tf.data.Dataset]:
+def get_datasets(
+    batch_size: int,
+    add_virtual_node: bool = True,
+    add_undirected_edges: bool = True,
+    add_self_loops: bool = True,
+) -> Dict[str, tf.data.Dataset]:
   """Returns datasets of batched GraphsTuples, organized by split."""
   if batch_size <= 1:
     raise ValueError('Batch size must be > 1 to account for padding graphs.')
 
   # Obtain the original datasets.
   datasets = get_raw_datasets()
 
@@ -58,84 +59,85 @@
       add_virtual_node=add_virtual_node,
       add_undirected_edges=add_undirected_edges,
       add_self_loops=add_self_loops,
   )
 
   # Process each split separately.
   for split_name in datasets:
-
     # Convert to GraphsTuple.
     datasets[split_name] = datasets[split_name].map(
         convert_to_graphs_tuple_fn,
         num_parallel_calls=tf.data.AUTOTUNE,
-        deterministic=True)
+        deterministic=True,
+    )
 
   # Compute the padding budget for the requested batch size.
-  budget = estimate_padding_budget_for_batch_size(datasets['train'], batch_size,
-                                                  num_estimation_graphs=100)
+  budget = estimate_padding_budget_for_batch_size(
+      datasets['train'], batch_size, num_estimation_graphs=100
+  )
 
   # Pad an example graph to see what the output shapes will be.
   # We will use this shape information when creating the tf.data.Dataset.
   example_graph = next(datasets['train'].as_numpy_iterator())
   example_padded_graph = jraph.pad_with_graphs(example_graph, *budget)
   padded_graphs_spec = specs_from_graphs_tuple(example_padded_graph)
 
   # Process each split separately.
   for split_name, dataset_split in datasets.items():
-
     # Repeat and shuffle the training split.
     if split_name == 'train':
       dataset_split = dataset_split.shuffle(100, reshuffle_each_iteration=True)
       dataset_split = dataset_split.repeat()
 
     # Batch and pad each split.
     batching_fn = functools.partial(
         jraph.dynamically_batch,
         graphs_tuple_iterator=iter(dataset_split),
         n_node=budget.n_node,
         n_edge=budget.n_edge,
-        n_graph=budget.n_graph)
+        n_graph=budget.n_graph,
+    )
     dataset_split = tf.data.Dataset.from_generator(
-        batching_fn,
-        output_signature=padded_graphs_spec)
+        batching_fn, output_signature=padded_graphs_spec
+    )
 
     # We cache the validation and test sets, since these are small.
     if split_name in ['validation', 'test']:
       dataset_split = dataset_split.cache()
 
     datasets[split_name] = dataset_split
   return datasets
 
 
-def convert_to_graphs_tuple(graph: Dict[str, tf.Tensor],
-                            add_virtual_node: bool,
-                            add_undirected_edges: bool,
-                            add_self_loops: bool) -> jraph.GraphsTuple:
+def convert_to_graphs_tuple(
+    graph: Dict[str, tf.Tensor],
+    add_virtual_node: bool,
+    add_undirected_edges: bool,
+    add_self_loops: bool,
+) -> jraph.GraphsTuple:
   """Converts a dictionary of tf.Tensors to a GraphsTuple."""
   num_nodes = tf.squeeze(graph['num_nodes'])
   num_edges = tf.squeeze(graph['num_edges'])
   nodes = graph['node_feat']
   edges = graph['edge_feat']
   edge_feature_dim = edges.shape[-1]
   labels = graph['labels']
   senders = graph['edge_index'][:, 0]
   receivers = graph['edge_index'][:, 1]
 
   # Add a virtual node connected to all other nodes.
   # The feature vectors for the virtual node
   # and the new edges are set to all zeros.
   if add_virtual_node:
-    nodes = tf.concat(
-        [nodes, tf.zeros_like(nodes[0, None])], axis=0)
-    senders = tf.concat(
-        [senders, tf.range(num_nodes)], axis=0)
+    nodes = tf.concat([nodes, tf.zeros_like(nodes[0, None])], axis=0)
+    senders = tf.concat([senders, tf.range(num_nodes)], axis=0)
     receivers = tf.concat(
-        [receivers, tf.fill((num_nodes,), num_nodes + 1)], axis=0)
-    edges = tf.concat(
-        [edges, tf.zeros((num_nodes, edge_feature_dim))], axis=0)
+        [receivers, tf.fill((num_nodes,), num_nodes + 1)], axis=0
+    )
+    edges = tf.concat([edges, tf.zeros((num_nodes, edge_feature_dim))], axis=0)
     num_edges += num_nodes
     num_nodes += 1
 
   # Make edges undirected, by adding edges with senders and receivers flipped.
   # The feature vector for the flipped edge is the same as the original edge.
   if add_undirected_edges:
     new_senders = tf.concat([senders, receivers], axis=0)
@@ -160,17 +162,16 @@
       senders=senders,
       receivers=receivers,
       globals=tf.expand_dims(labels, axis=0),
   )
 
 
 def estimate_padding_budget_for_batch_size(
-    dataset: tf.data.Dataset,
-    batch_size: int,
-    num_estimation_graphs: int) -> GraphsTupleSize:
+    dataset: tf.data.Dataset, batch_size: int, num_estimation_graphs: int
+) -> GraphsTupleSize:
   """Estimates the padding budget for a dataset of unbatched GraphsTuples.
 
   Args:
     dataset: A dataset of unbatched GraphsTuples.
     batch_size: The intended batch size. Note that no batching is performed by
       this function.
     num_estimation_graphs: How many graphs to take from the dataset to estimate
@@ -200,34 +201,42 @@
 
   num_nodes_per_graph_estimate = total_num_nodes / num_estimation_graphs
   num_edges_per_graph_estimate = total_num_edges / num_estimation_graphs
 
   padding_budget = GraphsTupleSize(
       n_node=next_multiple_of_64(num_nodes_per_graph_estimate * batch_size),
       n_edge=next_multiple_of_64(num_edges_per_graph_estimate * batch_size),
-      n_graph=batch_size)
+      n_graph=batch_size,
+  )
   return padding_budget
 
 
 def specs_from_graphs_tuple(graph: jraph.GraphsTuple):
   """Returns a tf.TensorSpec corresponding to this graph."""
 
   def get_tensor_spec(array: np.ndarray):
     shape = list(array.shape)
     dtype = array.dtype
     return tf.TensorSpec(shape=shape, dtype=dtype)
 
   specs = {}
   for field in [
-      'nodes', 'edges', 'senders', 'receivers', 'globals', 'n_node', 'n_edge'
+      'nodes',
+      'edges',
+      'senders',
+      'receivers',
+      'globals',
+      'n_node',
+      'n_edge',
   ]:
     field_sample = getattr(graph, field)
     specs[field] = get_tensor_spec(field_sample)
   return jraph.GraphsTuple(**specs)
 
 
 def get_graphs_tuple_size(graph: jraph.GraphsTuple):
   """Returns the number of nodes, edges and graphs in a GraphsTuple."""
   return GraphsTupleSize(
       n_node=np.sum(graph.n_node),
       n_edge=np.sum(graph.n_edge),
-      n_graph=np.shape(graph.n_node)[0])
+      n_graph=np.shape(graph.n_node)[0],
+  )
```

### Comparing `flax-0.7.0/examples/ogbg_molpcba/input_pipeline_test.py` & `flax-0.7.1/examples/ogbg_molpcba/input_pipeline_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -41,15 +41,16 @@
   def get_dummy_graphs():
     for _ in range(dataset_length):
       yield dummy_graph
 
   datasets = {}
   for split in ['train', 'validation', 'test']:
     datasets[split] = tf.data.Dataset.from_generator(
-        get_dummy_graphs, output_signature=graphs_spec)
+        get_dummy_graphs, output_signature=graphs_spec
+    )
   return datasets
 
 
 class InputPipelineTest(parameterized.TestCase):
 
   def setUp(self):
     super().setUp()
@@ -57,21 +58,23 @@
     self.datasets = get_dummy_datasets(dataset_length)
 
   @parameterized.product(
       valid_batch_size=[2, 5, 12, 15],
   )
   def test_estimate_padding_budget_valid(self, valid_batch_size):
     budget = input_pipeline.estimate_padding_budget_for_batch_size(
-        self.datasets['train'], valid_batch_size, num_estimation_graphs=1)
+        self.datasets['train'], valid_batch_size, num_estimation_graphs=1
+    )
     self.assertEqual(budget.n_graph, valid_batch_size)
 
   @parameterized.product(
       invalid_batch_size=[-1, 0, 1],
   )
   def test_estimate_padding_budget_invalid(self, invalid_batch_size):
     with self.assertRaises(ValueError):
       input_pipeline.estimate_padding_budget_for_batch_size(
-          self.datasets['train'], invalid_batch_size, num_estimation_graphs=1)
+          self.datasets['train'], invalid_batch_size, num_estimation_graphs=1
+      )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/examples/ogbg_molpcba/main.py` & `flax-0.7.1/examples/lm1b/main.py`

 * *Files 11% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Main file for running the ogbg-molpcba example.
+"""Main file for running the Language Modelling example with LM1B.
 
 This file is intentionally kept short. The majority for logic is in libraries
 that can be easily tested and imported in Colab.
 """
 
 from absl import app
 from absl import flags
@@ -24,43 +24,46 @@
 from clu import platform
 import jax
 from ml_collections import config_flags
 import tensorflow as tf
 
 import train
 
-
 FLAGS = flags.FLAGS
 
 flags.DEFINE_string('workdir', None, 'Directory to store model data.')
 config_flags.DEFINE_config_file(
     'config',
-    None,
+    'configs/default.py',
     'File path to the training hyperparameter configuration.',
-    lock_config=True)
+    lock_config=True,
+)
+flags.mark_flags_as_required(['config', 'workdir'])
 
 
 def main(argv):
   if len(argv) > 1:
     raise app.UsageError('Too many command-line arguments.')
 
   # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make
   # it unavailable to JAX.
   tf.config.experimental.set_visible_devices([], 'GPU')
 
-  # This example only supports single-host training on a single device.
-  logging.info('JAX host: %d / %d', jax.process_index(), jax.process_count())
+  logging.info('JAX process: %d / %d', jax.process_index(), jax.process_count())
   logging.info('JAX local devices: %r', jax.local_devices())
 
   # Add a note so that we can tell which task is which JAX host.
   # (Depending on the platform task 0 is not guaranteed to be host 0)
-  platform.work_unit().set_task_status(f'process_index: {jax.process_index()}, '
-                                       f'process_count: {jax.process_count()}')
-  platform.work_unit().create_artifact(platform.ArtifactType.DIRECTORY,
-                                       FLAGS.workdir, 'workdir')
+  platform.work_unit().set_task_status(
+      f'process_index: {jax.process_index()}, '
+      f'process_count: {jax.process_count()}'
+  )
+  platform.work_unit().create_artifact(
+      platform.ArtifactType.DIRECTORY, FLAGS.workdir, 'workdir'
+  )
 
   train.train_and_evaluate(FLAGS.config, FLAGS.workdir)
 
 
 if __name__ == '__main__':
-  flags.mark_flags_as_required(['config', 'workdir'])
+  jax.config.config_with_absl()
   app.run(main)
```

### Comparing `flax-0.7.0/examples/ogbg_molpcba/models.py` & `flax-0.7.1/examples/ogbg_molpcba/models.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,21 +17,23 @@
 from typing import Callable, Sequence
 
 from flax import linen as nn
 import jax.numpy as jnp
 import jraph
 
 
-def add_graphs_tuples(graphs: jraph.GraphsTuple,
-                      other_graphs: jraph.GraphsTuple) -> jraph.GraphsTuple:
+def add_graphs_tuples(
+    graphs: jraph.GraphsTuple, other_graphs: jraph.GraphsTuple
+) -> jraph.GraphsTuple:
   """Adds the nodes, edges and global features from other_graphs to graphs."""
   return graphs._replace(
       nodes=graphs.nodes + other_graphs.nodes,
       edges=graphs.edges + other_graphs.edges,
-      globals=graphs.globals + other_graphs.globals)
+      globals=graphs.globals + other_graphs.globals,
+  )
 
 
 class MLP(nn.Module):
   """A multi-layer perceptron."""
 
   feature_sizes: Sequence[int]
   dropout_rate: float = 0
@@ -40,16 +42,17 @@
 
   @nn.compact
   def __call__(self, inputs):
     x = inputs
     for size in self.feature_sizes:
       x = nn.Dense(features=size)(x)
       x = self.activation(x)
-      x = nn.Dropout(
-          rate=self.dropout_rate, deterministic=self.deterministic)(x)
+      x = nn.Dropout(rate=self.dropout_rate, deterministic=self.deterministic)(
+          x
+      )
     return x
 
 
 class GraphNet(nn.Module):
   """A complete Graph Network model defined with Jraph."""
 
   latent_size: int
@@ -64,59 +67,72 @@
 
   @nn.compact
   def __call__(self, graphs: jraph.GraphsTuple) -> jraph.GraphsTuple:
     # We will first linearly project the original features as 'embeddings'.
     embedder = jraph.GraphMapFeatures(
         embed_node_fn=nn.Dense(self.latent_size),
         embed_edge_fn=nn.Dense(self.latent_size),
-        embed_global_fn=nn.Dense(self.latent_size))
+        embed_global_fn=nn.Dense(self.latent_size),
+    )
     processed_graphs = embedder(graphs)
 
     # Now, we will apply a Graph Network once for each message-passing round.
     mlp_feature_sizes = [self.latent_size] * self.num_mlp_layers
     for _ in range(self.message_passing_steps):
       if self.use_edge_model:
         update_edge_fn = jraph.concatenated_args(
-            MLP(mlp_feature_sizes,
+            MLP(
+                mlp_feature_sizes,
                 dropout_rate=self.dropout_rate,
-                deterministic=self.deterministic))
+                deterministic=self.deterministic,
+            )
+        )
       else:
         update_edge_fn = None
 
       update_node_fn = jraph.concatenated_args(
-          MLP(mlp_feature_sizes,
+          MLP(
+              mlp_feature_sizes,
               dropout_rate=self.dropout_rate,
-              deterministic=self.deterministic))
+              deterministic=self.deterministic,
+          )
+      )
       update_global_fn = jraph.concatenated_args(
-          MLP(mlp_feature_sizes,
+          MLP(
+              mlp_feature_sizes,
               dropout_rate=self.dropout_rate,
-              deterministic=self.deterministic))
+              deterministic=self.deterministic,
+          )
+      )
 
       graph_net = jraph.GraphNetwork(
           update_node_fn=update_node_fn,
           update_edge_fn=update_edge_fn,
-          update_global_fn=update_global_fn)
+          update_global_fn=update_global_fn,
+      )
 
       if self.skip_connections:
         processed_graphs = add_graphs_tuples(
-            graph_net(processed_graphs), processed_graphs)
+            graph_net(processed_graphs), processed_graphs
+        )
       else:
         processed_graphs = graph_net(processed_graphs)
 
       if self.layer_norm:
         processed_graphs = processed_graphs._replace(
             nodes=nn.LayerNorm()(processed_graphs.nodes),
             edges=nn.LayerNorm()(processed_graphs.edges),
             globals=nn.LayerNorm()(processed_graphs.globals),
         )
 
     # Since our graph-level predictions will be at globals, we will
     # decode to get the required output logits.
     decoder = jraph.GraphMapFeatures(
-        embed_global_fn=nn.Dense(self.output_globals_size))
+        embed_global_fn=nn.Dense(self.output_globals_size)
+    )
     processed_graphs = decoder(processed_graphs)
 
     return processed_graphs
 
 
 class GraphConvNet(nn.Module):
   """A Graph Convolution Network + Pooling model defined with Jraph."""
@@ -125,65 +141,73 @@
   num_mlp_layers: int
   message_passing_steps: int
   output_globals_size: int
   dropout_rate: float = 0
   skip_connections: bool = True
   layer_norm: bool = True
   deterministic: bool = True
-  pooling_fn: Callable[[jnp.ndarray, jnp.ndarray, jnp.ndarray],  # pytype: disable=annotation-type-mismatch  # jax-ndarray
-                       jnp.ndarray] = jraph.segment_mean
+  pooling_fn: Callable[
+      [jnp.ndarray, jnp.ndarray, jnp.ndarray],  # pytype: disable=annotation-type-mismatch  # jax-ndarray
+      jnp.ndarray,
+  ] = jraph.segment_mean
 
   def pool(self, graphs: jraph.GraphsTuple) -> jraph.GraphsTuple:
     """Pooling operation, taken from Jraph."""
 
     # Equivalent to jnp.sum(n_node), but JIT-able.
     sum_n_node = graphs.nodes.shape[0]  # pytype: disable=attribute-error  # jax-ndarray
     # To aggregate nodes from each graph to global features,
     # we first construct tensors that map the node to the corresponding graph.
     # Example: if you have `n_node=[1,2]`, we construct the tensor [0, 1, 1].
     n_graph = graphs.n_node.shape[0]
     node_graph_indices = jnp.repeat(
         jnp.arange(n_graph),
         graphs.n_node,
         axis=0,
-        total_repeat_length=sum_n_node)
+        total_repeat_length=sum_n_node,
+    )
     # We use the aggregation function to pool the nodes per graph.
     pooled = self.pooling_fn(graphs.nodes, node_graph_indices, n_graph)  # pytype: disable=wrong-arg-types  # jax-ndarray
     return graphs._replace(globals=pooled)
 
   @nn.compact
   def __call__(self, graphs: jraph.GraphsTuple) -> jraph.GraphsTuple:
     # We will first linearly project the original node features as 'embeddings'.
-    embedder = jraph.GraphMapFeatures(
-        embed_node_fn=nn.Dense(self.latent_size))
+    embedder = jraph.GraphMapFeatures(embed_node_fn=nn.Dense(self.latent_size))
     processed_graphs = embedder(graphs)
 
     # Now, we will apply the GCN once for each message-passing round.
     for _ in range(self.message_passing_steps):
       mlp_feature_sizes = [self.latent_size] * self.num_mlp_layers
       update_node_fn = jraph.concatenated_args(
-          MLP(mlp_feature_sizes,
+          MLP(
+              mlp_feature_sizes,
               dropout_rate=self.dropout_rate,
-              deterministic=self.deterministic))
+              deterministic=self.deterministic,
+          )
+      )
       graph_conv = jraph.GraphConvolution(
-          update_node_fn=update_node_fn, add_self_edges=True)
+          update_node_fn=update_node_fn, add_self_edges=True
+      )
 
       if self.skip_connections:
         processed_graphs = add_graphs_tuples(
-            graph_conv(processed_graphs), processed_graphs)
+            graph_conv(processed_graphs), processed_graphs
+        )
       else:
         processed_graphs = graph_conv(processed_graphs)
 
       if self.layer_norm:
         processed_graphs = processed_graphs._replace(
             nodes=nn.LayerNorm()(processed_graphs.nodes),
         )
 
     # We apply the pooling operation to get a 'global' embedding.
     processed_graphs = self.pool(processed_graphs)
 
     # Now, we decode this to get the required output logits.
     decoder = jraph.GraphMapFeatures(
-        embed_global_fn=nn.Dense(self.output_globals_size))
+        embed_global_fn=nn.Dense(self.output_globals_size)
+    )
     processed_graphs = decoder(processed_graphs)
 
     return processed_graphs
```

### Comparing `flax-0.7.0/examples/ogbg_molpcba/models_test.py` & `flax-0.7.1/examples/ogbg_molpcba/models_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -44,103 +44,110 @@
         receivers=jnp.ones(total_n_edge, dtype=jnp.int32),
         nodes=jnp.ones((total_n_node, feature_dim)),
         edges=jnp.zeros((total_n_edge, feature_dim)),
         globals=jnp.zeros((n_graph, feature_dim)),
     )
 
   @parameterized.product(
-      dropout_rate=[0., 0.5, 1.], output_size=[50, 100], num_layers=[2])
+      dropout_rate=[0.0, 0.5, 1.0], output_size=[50, 100], num_layers=[2]
+  )
   def test_mlp(self, dropout_rate, output_size, num_layers):
     # Input definition.
     nodes = self.graphs.nodes
 
     # Model definition.
     mlp = models.MLP(
         feature_sizes=[output_size] * num_layers,
         dropout_rate=dropout_rate,
         activation=lambda x: x,
-        deterministic=False)
+        deterministic=False,
+    )
     nodes_after_mlp, _ = mlp.init_with_output(self.rngs, nodes)
 
     # Test that dropout actually worked.
     num_masked_entries = jnp.sum(nodes_after_mlp == 0)
     num_total_entries = jnp.size(nodes_after_mlp)
-    self.assertLessEqual(num_masked_entries,
-                         (dropout_rate + 0.05) * num_total_entries)
-    self.assertLessEqual((dropout_rate - 0.05) * num_total_entries,
-                         num_masked_entries)
+    self.assertLessEqual(
+        num_masked_entries, (dropout_rate + 0.05) * num_total_entries
+    )
+    self.assertLessEqual(
+        (dropout_rate - 0.05) * num_total_entries, num_masked_entries
+    )
 
     # Test the shape of the output.
     self.assertEqual(nodes_after_mlp.shape[-1], output_size)
 
   @parameterized.parameters(
       {
           'latent_size': 5,
           'output_globals_size': 15,
           'use_edge_model': True,
-      }, {
+      },
+      {
           'latent_size': 5,
           'output_globals_size': 15,
           'use_edge_model': False,
-      })
-  def test_graph_net(self, latent_size: int, output_globals_size: int,
-                     use_edge_model: bool):
+      },
+  )
+  def test_graph_net(
+      self, latent_size: int, output_globals_size: int, use_edge_model: bool
+  ):
     # Input definition.
     graphs = self.graphs
     num_nodes = jnp.sum(graphs.n_node)
     num_edges = jnp.sum(graphs.n_edge)
     num_graphs = graphs.n_node.shape[0]
 
     # Model definition.
     net = models.GraphNet(
         latent_size=latent_size,
         num_mlp_layers=2,
         message_passing_steps=2,
         output_globals_size=output_globals_size,
-        use_edge_model=use_edge_model)
+        use_edge_model=use_edge_model,
+    )
     output, _ = net.init_with_output(self.rngs, graphs)
 
     # Output should be graph with the same topology, but a
     # different number of features.
     self.assertIsInstance(output, jraph.GraphsTuple)
     self.assertSequenceAlmostEqual(output.n_node, graphs.n_node)
     self.assertSequenceAlmostEqual(output.n_edge, graphs.n_edge)
     self.assertSequenceAlmostEqual(output.senders, graphs.senders)
     self.assertSequenceAlmostEqual(output.receivers, graphs.receivers)
     self.assertEqual(output.nodes.shape, (num_nodes, latent_size))
     self.assertEqual(output.edges.shape, (num_edges, latent_size))
     self.assertEqual(output.globals.shape, (num_graphs, output_globals_size))
 
-  @parameterized.parameters({
-      'latent_size': 15,
-      'output_globals_size': 15
-  }, {
-      'latent_size': 5,
-      'output_globals_size': 5
-  })
+  @parameterized.parameters(
+      {'latent_size': 15, 'output_globals_size': 15},
+      {'latent_size': 5, 'output_globals_size': 5},
+  )
   def test_graph_conv_net(self, latent_size: int, output_globals_size: int):
     graphs = self.graphs
     num_nodes = jnp.sum(graphs.n_node)
     num_graphs = graphs.n_node.shape[0]
 
     # Model definition.
     net = models.GraphConvNet(
         latent_size=latent_size,
         num_mlp_layers=2,
         message_passing_steps=2,
-        output_globals_size=output_globals_size)
+        output_globals_size=output_globals_size,
+    )
     output, _ = net.init_with_output(self.rngs, graphs)
 
     # Output should be graph with the same topology, but a
     # different number of features.
     self.assertIsInstance(output, jraph.GraphsTuple)
     self.assertSequenceAlmostEqual(output.n_node, graphs.n_node)
     self.assertSequenceAlmostEqual(output.n_edge, graphs.n_edge)
-    self.assertSequenceAlmostEqual(output.edges.flatten(),
-                                   graphs.edges.flatten())
+    self.assertSequenceAlmostEqual(
+        output.edges.flatten(), graphs.edges.flatten()
+    )
     self.assertSequenceAlmostEqual(output.senders, graphs.senders)
     self.assertSequenceAlmostEqual(output.receivers, graphs.receivers)
     self.assertEqual(output.nodes.shape, (num_nodes, latent_size))
     self.assertEqual(output.globals.shape, (num_graphs, output_globals_size))
 
 
 if __name__ == '__main__':
```

### Comparing `flax-0.7.0/examples/ogbg_molpcba/ogbg_molpcba.ipynb` & `flax-0.7.1/examples/ogbg_molpcba/ogbg_molpcba.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py` & `flax-0.7.1/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py`

 * *Files 8% similar despite different names*

```diff
@@ -71,24 +71,21 @@
     # assertions.
     self.assertGreaterEqual(end_test_mean_average_precision, 0.24)
     self.assertGreaterEqual(end_validation_mean_average_precision, 0.25)
 
     # Use the reporting API to report single or multiple metrics/extras.
     self.report_wall_time(benchmark_time)
     self.report_metrics({
-        'sec_per_epoch':
-            sec_per_epoch,
-        'test_accuracy':
-            end_test_accuracy,
-        'test_mean_average_precision':
-            end_test_mean_average_precision,
-        'validation_accuracy':
-            end_validation_accuracy,
-        'validation_mean_average_precision':
-            end_validation_mean_average_precision,
+        'sec_per_epoch': sec_per_epoch,
+        'test_accuracy': end_test_accuracy,
+        'test_mean_average_precision': end_test_mean_average_precision,
+        'validation_accuracy': end_validation_accuracy,
+        'validation_mean_average_precision': (
+            end_validation_mean_average_precision
+        ),
     })
     self.report_extras({
         'model_name': 'Graph Convolutional Network',
         'description': 'GPU (1x V100) test for ogbg_molpcba.',
         'implementation': 'linen',
     })
 
@@ -121,24 +118,21 @@
 
     _, _, validation_aps = zip(*summaries['validation_mean_average_precision'])
     end_validation_mean_average_precision = validation_aps[-1]
 
     # Use the reporting API to report single or multiple metrics/extras.
     self.report_wall_time(benchmark_time)
     self.report_metrics({
-        'sec_per_epoch':
-            sec_per_epoch,
-        'test_accuracy':
-            end_test_accuracy,
-        'test_mean_average_precision':
-            end_test_mean_average_precision,
-        'validation_accuracy':
-            end_validation_accuracy,
-        'validation_mean_average_precision':
-            end_validation_mean_average_precision,
+        'sec_per_epoch': sec_per_epoch,
+        'test_accuracy': end_test_accuracy,
+        'test_mean_average_precision': end_test_mean_average_precision,
+        'validation_accuracy': end_validation_accuracy,
+        'validation_mean_average_precision': (
+            end_validation_mean_average_precision
+        ),
     })
     self.report_extras({
         'model_name': 'Graph Convolutional Network',
         'description': 'CPU test for ogbg_molpcba.',
         'implementation': 'linen',
     })
```

### Comparing `flax-0.7.0/examples/ogbg_molpcba/train.py` & `flax-0.7.1/examples/ogbg_molpcba/train.py`

 * *Files 4% similar despite different names*

```diff
@@ -36,89 +36,94 @@
 import sklearn.metrics
 import tensorflow as tf
 
 import input_pipeline
 import models
 
 
-def create_model(config: ml_collections.ConfigDict,
-                 deterministic: bool) -> nn.Module:
+def create_model(
+    config: ml_collections.ConfigDict, deterministic: bool
+) -> nn.Module:
   """Creates a Flax model, as specified by the config."""
   if config.model == 'GraphNet':
     return models.GraphNet(
         latent_size=config.latent_size,
         num_mlp_layers=config.num_mlp_layers,
         message_passing_steps=config.message_passing_steps,
         output_globals_size=config.num_classes,
         dropout_rate=config.dropout_rate,
         skip_connections=config.skip_connections,
         layer_norm=config.layer_norm,
         use_edge_model=config.use_edge_model,
-        deterministic=deterministic)
+        deterministic=deterministic,
+    )
   if config.model == 'GraphConvNet':
     return models.GraphConvNet(
         latent_size=config.latent_size,
         num_mlp_layers=config.num_mlp_layers,
         message_passing_steps=config.message_passing_steps,
         output_globals_size=config.num_classes,
         dropout_rate=config.dropout_rate,
         skip_connections=config.skip_connections,
         layer_norm=config.layer_norm,
-        deterministic=deterministic)
+        deterministic=deterministic,
+    )
   raise ValueError(f'Unsupported model: {config.model}.')
 
 
 def create_optimizer(
-    config: ml_collections.ConfigDict) -> optax.GradientTransformation:
+    config: ml_collections.ConfigDict,
+) -> optax.GradientTransformation:
   """Creates an optimizer, as specified by the config."""
   if config.optimizer == 'adam':
-    return optax.adam(
-        learning_rate=config.learning_rate)
+    return optax.adam(learning_rate=config.learning_rate)
   if config.optimizer == 'sgd':
     return optax.sgd(
-        learning_rate=config.learning_rate,
-        momentum=config.momentum)
+        learning_rate=config.learning_rate, momentum=config.momentum
+    )
   raise ValueError(f'Unsupported optimizer: {config.optimizer}.')
 
 
-def binary_cross_entropy_with_mask(*, logits: jnp.ndarray, labels: jnp.ndarray,
-                                   mask: jnp.ndarray):
+def binary_cross_entropy_with_mask(
+    *, logits: jnp.ndarray, labels: jnp.ndarray, mask: jnp.ndarray
+):
   """Binary cross entropy loss for unnormalized logits, with masked elements."""
   assert logits.shape == labels.shape == mask.shape
   assert len(logits.shape) == 2
 
   # To prevent propagation of NaNs during grad().
   # We mask over the loss for invalid targets later.
   labels = jnp.where(mask, labels, -1)
 
   # Numerically stable implementation of BCE loss.
   # This mimics TensorFlow's tf.nn.sigmoid_cross_entropy_with_logits().
-  positive_logits = (logits >= 0)
+  positive_logits = logits >= 0
   relu_logits = jnp.where(positive_logits, logits, 0)
   abs_logits = jnp.where(positive_logits, logits, -logits)
-  return relu_logits - (logits * labels) + (
-      jnp.log(1 + jnp.exp(-abs_logits)))
+  return relu_logits - (logits * labels) + (jnp.log(1 + jnp.exp(-abs_logits)))
 
 
-def predictions_match_labels(*, logits: jnp.ndarray, labels: jnp.ndarray,
-                             **kwargs) -> jnp.ndarray:
+def predictions_match_labels(
+    *, logits: jnp.ndarray, labels: jnp.ndarray, **kwargs
+) -> jnp.ndarray:
   """Returns a binary array indicating where predictions match the labels."""
   del kwargs  # Unused.
-  preds = (logits > 0)
+  preds = logits > 0
   return (preds == labels).astype(jnp.float32)
 
 
 def add_prefix_to_keys(result: Dict[str, Any], prefix: str) -> Dict[str, Any]:
   """Adds a prefix to the keys of a dict, returning a new dict."""
   return {f'{prefix}_{key}': val for key, val in result.items()}
 
 
 @flax.struct.dataclass
 class MeanAveragePrecision(
-    metrics.CollectingMetric.from_outputs(('labels', 'logits', 'mask'))):
+    metrics.CollectingMetric.from_outputs(('labels', 'logits', 'mask'))
+):
   """Computes the mean average precision (mAP) over different tasks."""
 
   def compute(self):
     # Matches the official OGB evaluation scheme for mean average precision.
     values = super().compute()
     labels = values['labels']
     logits = values['logits']
@@ -133,54 +138,55 @@
 
     for task in range(num_tasks):
       # AP is only defined when there is at least one negative data
       # and at least one positive data.
       is_labeled = mask[:, task]
       if len(np.unique(labels[is_labeled, task])) >= 2:
         average_precisions[task] = sklearn.metrics.average_precision_score(
-            labels[is_labeled, task], probs[is_labeled, task])
+            labels[is_labeled, task], probs[is_labeled, task]
+        )
 
     # When all APs are NaNs, return NaN. This avoids raising a RuntimeWarning.
     if np.isnan(average_precisions).all():
       return np.nan
     return np.nanmean(average_precisions)
 
 
 @flax.struct.dataclass
 class EvalMetrics(metrics.Collection):
-
   accuracy: metrics.Average.from_fun(predictions_match_labels)
   loss: metrics.Average.from_output('loss')
   mean_average_precision: MeanAveragePrecision
 
 
 @flax.struct.dataclass
 class TrainMetrics(metrics.Collection):
-
   accuracy: metrics.Average.from_fun(predictions_match_labels)
   loss: metrics.Average.from_output('loss')
 
 
 def replace_globals(graphs: jraph.GraphsTuple) -> jraph.GraphsTuple:
   """Replaces the globals attribute with a constant feature for each graph."""
-  return graphs._replace(
-      globals=jnp.ones([graphs.n_node.shape[0], 1]))
+  return graphs._replace(globals=jnp.ones([graphs.n_node.shape[0], 1]))
 
 
-def get_predicted_logits(state: train_state.TrainState,
-                         graphs: jraph.GraphsTuple,
-                         rngs: Optional[Dict[str, jnp.ndarray]]) -> jnp.ndarray:
+def get_predicted_logits(
+    state: train_state.TrainState,
+    graphs: jraph.GraphsTuple,
+    rngs: Optional[Dict[str, jnp.ndarray]],
+) -> jnp.ndarray:
   """Get predicted logits from the network for input graphs."""
   pred_graphs = state.apply_fn(state.params, graphs, rngs=rngs)
   logits = pred_graphs.globals
   return logits
 
 
-def get_valid_mask(labels: jnp.ndarray,
-                   graphs: jraph.GraphsTuple) -> jnp.ndarray:
+def get_valid_mask(
+    labels: jnp.ndarray, graphs: jraph.GraphsTuple
+) -> jnp.ndarray:
   """Gets the binary mask indicating only valid labels and graphs."""
   # We have to ignore all NaN values - which indicate labels for which
   # the current graphs have no label.
   labels_mask = ~jnp.isnan(labels)
 
   # Since we have extra 'dummy' graphs in our batch due to padding, we want
   # to mask out any loss associated with the dummy graphs.
@@ -190,16 +196,17 @@
 
   # Combine the mask over labels with the mask over graphs.
   return labels_mask & graph_mask[:, None]
 
 
 @jax.jit
 def train_step(
-    state: train_state.TrainState, graphs: jraph.GraphsTuple,
-    rngs: Dict[str, jnp.ndarray]
+    state: train_state.TrainState,
+    graphs: jraph.GraphsTuple,
+    rngs: Dict[str, jnp.ndarray],
 ) -> Tuple[train_state.TrainState, metrics.Collection]:
   """Performs one update step over the current batch of graphs."""
 
   def loss_fn(params, graphs):
     curr_state = state.replace(params=params)
 
     # Extract labels.
@@ -208,25 +215,27 @@
     # Replace the global feature for graph classification.
     graphs = replace_globals(graphs)
 
     # Compute logits and resulting loss.
     logits = get_predicted_logits(curr_state, graphs, rngs)
     mask = get_valid_mask(labels, graphs)
     loss = binary_cross_entropy_with_mask(
-        logits=logits, labels=labels, mask=mask)
+        logits=logits, labels=labels, mask=mask
+    )
     mean_loss = jnp.sum(jnp.where(mask, loss, 0)) / jnp.sum(mask)
 
     return mean_loss, (loss, logits, labels, mask)
 
   grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
   (_, (loss, logits, labels, mask)), grads = grad_fn(state.params, graphs)
   state = state.apply_gradients(grads=grads)
 
   metrics_update = TrainMetrics.single_from_model_output(
-      loss=loss, logits=logits, labels=labels, mask=mask)
+      loss=loss, logits=logits, labels=labels, mask=mask
+  )
   return state, metrics_update
 
 
 @jax.jit
 def evaluate_step(
     state: train_state.TrainState,
     graphs: jraph.GraphsTuple,
@@ -245,20 +254,23 @@
   # Get the mask for valid labels and graphs.
   mask = get_valid_mask(labels, graphs)
 
   # Compute the various metrics.
   loss = binary_cross_entropy_with_mask(logits=logits, labels=labels, mask=mask)
 
   return EvalMetrics.single_from_model_output(
-      loss=loss, logits=logits, labels=labels, mask=mask)
+      loss=loss, logits=logits, labels=labels, mask=mask
+  )
 
 
-def evaluate_model(state: train_state.TrainState,
-                   datasets: Dict[str, tf.data.Dataset],
-                   splits: Iterable[str]) -> Dict[str, metrics.Collection]:
+def evaluate_model(
+    state: train_state.TrainState,
+    datasets: Dict[str, tf.data.Dataset],
+    splits: Iterable[str],
+) -> Dict[str, metrics.Collection]:
   """Evaluates the model on metrics over the specified splits."""
 
   # Loop over each split independently.
   eval_metrics = {}
   for split in splits:
     split_metrics = None
 
@@ -272,16 +284,17 @@
       else:
         split_metrics = split_metrics.merge(split_metrics_update)
     eval_metrics[split] = split_metrics
 
   return eval_metrics  # pytype: disable=bad-return-type
 
 
-def train_and_evaluate(config: ml_collections.ConfigDict,
-                       workdir: str) -> train_state.TrainState:
+def train_and_evaluate(
+    config: ml_collections.ConfigDict, workdir: str
+) -> train_state.TrainState:
   """Execute model training and evaluation loop.
 
   Args:
     config: Hyperparameter configuration for training and evaluation.
     workdir: Directory where the TensorBoard summaries are written to.
 
   Returns:
@@ -296,15 +309,16 @@
 
   # Get datasets, organized by split.
   logging.info('Obtaining datasets.')
   datasets = input_pipeline.get_datasets(
       config.batch_size,
       add_virtual_node=config.add_virtual_node,
       add_undirected_edges=config.add_undirected_edges,
-      add_self_loops=config.add_self_loops)
+      add_self_loops=config.add_self_loops,
+  )
   train_iter = iter(datasets['train'])
 
   # Create and initialize the network.
   logging.info('Initializing network.')
   rng = jax.random.PRNGKey(0)
   rng, init_rng = jax.random.split(rng)
   init_graphs = next(datasets['train'].as_numpy_iterator())
@@ -315,15 +329,16 @@
 
   # Create the optimizer.
   tx = create_optimizer(config)
 
   # Create the training state.
   net = create_model(config, deterministic=False)
   state = train_state.TrainState.create(
-      apply_fn=net.apply, params=params, tx=tx)
+      apply_fn=net.apply, params=params, tx=tx
+  )
 
   # Set up checkpointing of the model.
   # The input pipeline cannot be checkpointed in its current form,
   # due to the use of stateful operations.
   checkpoint_dir = os.path.join(workdir, 'checkpoints')
   ckpt = checkpoint.Checkpoint(checkpoint_dir, max_to_keep=2)
   state = ckpt.restore_or_initialize(state)
@@ -331,60 +346,63 @@
 
   # Create the evaluation state, corresponding to a deterministic model.
   eval_net = create_model(config, deterministic=True)
   eval_state = state.replace(apply_fn=eval_net.apply)
 
   # Hooks called periodically during training.
   report_progress = periodic_actions.ReportProgress(
-      num_train_steps=config.num_train_steps, writer=writer)
+      num_train_steps=config.num_train_steps, writer=writer
+  )
   profiler = periodic_actions.Profile(num_profile_steps=5, logdir=workdir)
   hooks = [report_progress, profiler]
 
   # Begin training loop.
   logging.info('Starting training.')
   train_metrics = None
   for step in range(initial_step, config.num_train_steps + 1):
-
     # Split PRNG key, to ensure different 'randomness' for every step.
     rng, dropout_rng = jax.random.split(rng)
 
     # Perform one step of training.
     with jax.profiler.StepTraceAnnotation('train', step_num=step):
       graphs = jax.tree_util.tree_map(np.asarray, next(train_iter))
       state, metrics_update = train_step(
-          state, graphs, rngs={'dropout': dropout_rng})
+          state, graphs, rngs={'dropout': dropout_rng}
+      )
 
       # Update metrics.
       if train_metrics is None:
         train_metrics = metrics_update
       else:
         train_metrics = train_metrics.merge(metrics_update)
 
     # Quick indication that training is happening.
     logging.log_first_n(logging.INFO, 'Finished training step %d.', 10, step)
     for hook in hooks:
       hook(step)
 
     # Log, if required.
-    is_last_step = (step == config.num_train_steps - 1)
+    is_last_step = step == config.num_train_steps - 1
     if step % config.log_every_steps == 0 or is_last_step:
-      writer.write_scalars(step,
-                           add_prefix_to_keys(train_metrics.compute(), 'train'))
+      writer.write_scalars(
+          step, add_prefix_to_keys(train_metrics.compute(), 'train')
+      )
       train_metrics = None
 
     # Evaluate on validation and test splits, if required.
     if step % config.eval_every_steps == 0 or is_last_step:
       eval_state = eval_state.replace(params=state.params)
 
       splits = ['validation', 'test']
       with report_progress.timed('eval'):
         eval_metrics = evaluate_model(eval_state, datasets, splits=splits)
       for split in splits:
         writer.write_scalars(
-            step, add_prefix_to_keys(eval_metrics[split].compute(), split))
+            step, add_prefix_to_keys(eval_metrics[split].compute(), split)
+        )
 
     # Checkpoint model, if required.
     if step % config.checkpoint_every_steps == 0 or is_last_step:
       with report_progress.timed('checkpoint'):
         ckpt.save(state)
 
   return state
```

### Comparing `flax-0.7.0/examples/ogbg_molpcba/train_test.py` & `flax-0.7.1/examples/ogbg_molpcba/train_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -69,65 +69,69 @@
   def get_dummy_graphs():
     for _ in range(dataset_length):
       yield dummy_graph
 
   datasets = {}
   for split in ['train', 'validation', 'test']:
     datasets[split] = tf.data.Dataset.from_generator(
-        get_dummy_graphs, output_signature=dummy_graph_spec)
+        get_dummy_graphs, output_signature=dummy_graph_spec
+    )
   return datasets
 
 
 def get_dummy_datasets(
-    dataset_length: int,
-    batch_size: Optional[int] = None) -> Dict[str, tf.data.Dataset]:
+    dataset_length: int, batch_size: Optional[int] = None
+) -> Dict[str, tf.data.Dataset]:
   """Returns dummy datasets, mocking input_pipeline.get_datasets()."""
 
   datasets = get_dummy_raw_datasets(dataset_length)
 
   # Construct the GraphsTuple converter function.
   convert_to_graphs_tuple_fn = functools.partial(
       input_pipeline.convert_to_graphs_tuple,
       add_virtual_node=True,
       add_undirected_edges=True,
       add_self_loops=True,
   )
 
   # Process each split separately.
   for split_name in datasets:
-
     # Convert to GraphsTuple.
     datasets[split_name] = datasets[split_name].map(
         convert_to_graphs_tuple_fn,
         num_parallel_calls=tf.data.AUTOTUNE,
-        deterministic=True)
+        deterministic=True,
+    )
 
   # If batch size is None, do not batch.
   if batch_size is not None:
     budget = input_pipeline.estimate_padding_budget_for_batch_size(
-        datasets['train'], batch_size, num_estimation_graphs=1)
+        datasets['train'], batch_size, num_estimation_graphs=1
+    )
 
     # Pad an example graph to see what the output shapes will be.
     # We will use this shape information when creating the tf.data.Dataset.
     example_graph = next(datasets['train'].as_numpy_iterator())
     example_padded_graph = jraph.pad_with_graphs(example_graph, *budget)
     padded_graphs_spec = input_pipeline.specs_from_graphs_tuple(
-        example_padded_graph)
+        example_padded_graph
+    )
 
     # Batch and pad each split separately.
     for split, dataset_split in datasets.items():
       batching_fn = functools.partial(
           jraph.dynamically_batch,
           graphs_tuple_iterator=iter(dataset_split),
           n_node=budget.n_node,
           n_edge=budget.n_edge,
-          n_graph=budget.n_graph)
+          n_graph=budget.n_graph,
+      )
       datasets[split] = tf.data.Dataset.from_generator(
-          batching_fn,
-          output_signature=padded_graphs_spec)
+          batching_fn, output_signature=padded_graphs_spec
+      )
   return datasets
 
 
 class OgbgMolpcbaTrainTest(parameterized.TestCase):
 
   def setUp(self):
     super().setUp()
@@ -143,102 +147,120 @@
 
     # Create dummy datasets.
     self.datasets = get_dummy_datasets(dataset_length=20, batch_size=10)
     self.raw_datasets = get_dummy_raw_datasets(dataset_length=20)
 
   @parameterized.product(
       probs=[[[0.8, 0.9, 0.3, 0.5]]],
-      labels=[[[1, 0, 1, 1]], [[1, 0, 1, jnp.nan]], [[1, 0, jnp.nan, jnp.nan]],
-              [[1, jnp.nan, jnp.nan, jnp.nan]]],
+      labels=[
+          [[1, 0, 1, 1]],
+          [[1, 0, 1, jnp.nan]],
+          [[1, 0, jnp.nan, jnp.nan]],
+          [[1, jnp.nan, jnp.nan, jnp.nan]],
+      ],
   )
   def test_binary_cross_entropy_loss(self, probs, labels):
     probs = jnp.asarray(probs)
     labels = jnp.asarray(labels)
 
     logits = jnp.log(probs / (1 - probs))
     mask = ~jnp.isnan(labels)
 
     loss_array = train.binary_cross_entropy_with_mask(
-        logits=logits, labels=labels, mask=mask)
+        logits=logits, labels=labels, mask=mask
+    )
     loss = average_with_mask(loss_array, mask)
     expected_loss_array = -(jnp.log(probs) * labels) - (
-        jnp.log(1 - probs) * (1 - labels))
+        jnp.log(1 - probs) * (1 - labels)
+    )
     expected_loss = average_with_mask(expected_loss_array, mask)
 
     self.assertAlmostEqual(loss, expected_loss, places=5)
 
   @parameterized.named_parameters(
       dict(
           testcase_name='no_valid_tasks',
-          logits=[[-1., 1.], [1., 1.], [2., -1.]],
+          logits=[[-1.0, 1.0], [1.0, 1.0], [2.0, -1.0]],
           labels=[[jnp.nan, jnp.nan], [jnp.nan, jnp.nan], [jnp.nan, jnp.nan]],
-          expected_result=jnp.nan),
+          expected_result=jnp.nan,
+      ),
       dict(
           testcase_name='1_valid_task',
-          logits=[[-1., 1.], [1., 1.], [2., -1.]],
+          logits=[[-1.0, 1.0], [1.0, 1.0], [2.0, -1.0]],
           labels=[[0, jnp.nan], [1, jnp.nan], [1, jnp.nan]],
-          expected_result=1.),
+          expected_result=1.0,
+      ),
       dict(
           testcase_name='2_valid_tasks',
-          logits=[[-1., 1.], [1., 1.], [2., -1.]],
+          logits=[[-1.0, 1.0], [1.0, 1.0], [2.0, -1.0]],
           labels=[[0, jnp.nan], [1, 0], [1, 1]],
-          expected_result=0.75),
+          expected_result=0.75,
+      ),
   )
   def test_mean_average_precision(self, logits, labels, expected_result):
     logits = jnp.asarray(logits)
     labels = jnp.asarray(labels)
     mask = ~jnp.isnan(labels)
 
     mean_average_precision = train.MeanAveragePrecision.from_model_output(
-        logits=logits, labels=labels, mask=mask).compute()
+        logits=logits, labels=labels, mask=mask
+    ).compute()
 
     if jnp.isnan(expected_result):
       self.assertTrue(jnp.isnan(mean_average_precision))
     else:
       self.assertAlmostEqual(expected_result, mean_average_precision)
 
   @parameterized.parameters(
       dict(
-          loss=[[0.5, 1.], [1.5, 1.3], [2., 1.2]],
-          logits=[[-1., 1.], [1., 1.], [2., 0.]],
+          loss=[[0.5, 1.0], [1.5, 1.3], [2.0, 1.2]],
+          logits=[[-1.0, 1.0], [1.0, 1.0], [2.0, 0.0]],
           labels=[[0, jnp.nan], [1, 0], [0, 1]],
           mask=[[True, False], [True, True], [False, False]],
-          expected_results={'loss': 1.1, 'accuracy': 2/3,
-                            'mean_average_precision': 1.0}),
+          expected_results={
+              'loss': 1.1,
+              'accuracy': 2 / 3,
+              'mean_average_precision': 1.0,
+          },
+      ),
   )
   def test_eval_metrics(self, loss, logits, labels, mask, expected_results):
     loss = jnp.asarray(loss)
     logits = jnp.asarray(logits)
     labels = jnp.asarray(labels)
     mask = jnp.asarray(mask)
 
     # Ignore RuntimeWarning caused by MeanAveragePrecision calculation.
     with warnings.catch_warnings():
       warnings.simplefilter('ignore', category=RuntimeWarning)
       eval_metrics = train.EvalMetrics.single_from_model_output(
-          loss=loss, logits=logits, labels=labels, mask=mask).compute()
+          loss=loss, logits=logits, labels=labels, mask=mask
+      ).compute()
 
     for metric in expected_results:
       self.assertAlmostEqual(expected_results[metric], eval_metrics[metric])
 
   @parameterized.parameters(
-      dict(loss=[[0.5, 1.], [1.5, 1.3], [2., 1.2]],
-           logits=[[-1., 1.], [1., 1.], [2., 0.]],
-           labels=[[0, jnp.nan], [1, 0], [0, 1]],
-           mask=[[True, False], [True, True], [False, False]],
-           expected_results={'loss': 1.1, 'accuracy': 2/3}),
+      dict(
+          loss=[[0.5, 1.0], [1.5, 1.3], [2.0, 1.2]],
+          logits=[[-1.0, 1.0], [1.0, 1.0], [2.0, 0.0]],
+          labels=[[0, jnp.nan], [1, 0], [0, 1]],
+          mask=[[True, False], [True, True], [False, False]],
+          expected_results={'loss': 1.1, 'accuracy': 2 / 3},
+      ),
   )
   def test_train_metrics(self, loss, logits, labels, mask, expected_results):
     loss = jnp.asarray(loss)
     logits = jnp.asarray(logits)
     labels = jnp.asarray(labels)
     mask = jnp.asarray(mask)
 
     train_metrics = train.TrainMetrics.single_from_model_output(
-        loss=loss, logits=logits, labels=labels, mask=mask).compute()
+        loss=loss, logits=logits, labels=labels, mask=mask
+    ).compute()
     for metric in expected_results:
       self.assertAlmostEqual(expected_results[metric], train_metrics[metric])
 
   def test_train_step(self):
     # Get the default configuration.
     config = default.get_config()
 
@@ -251,20 +273,22 @@
 
     # Create the optimizer.
     optimizer = train.create_optimizer(config)
 
     # Create the training state.
     net = train.create_model(config, deterministic=False)
     state = train_state.TrainState.create(
-        apply_fn=net.apply, params=params, tx=optimizer)
+        apply_fn=net.apply, params=params, tx=optimizer
+    )
 
     # Perform one step of updates.
     # We use the same batch of graphs that we used for initialization.
     state, train_metrics = train.train_step(
-        state, init_graphs, rngs={'dropout': rng})
+        state, init_graphs, rngs={'dropout': rng}
+    )
 
     # Check that none of the parameters are NaNs!
     params = flax.core.unfreeze(state.params)
     flat_params = {
         '/'.join(k): v
         for k, v in flax.traverse_util.flatten_dict(params).items()
     }
@@ -281,25 +305,27 @@
     # Get the default configuration.
     config = default.get_config()
 
     # Initialize the network with a dummy graph.
     _, init_rng = jax.random.split(self.rng)
     init_graphs = next(self.datasets['train'].as_numpy_iterator())
     init_graphs_preprocessed = init_graphs._replace(
-        globals=jnp.zeros([init_graphs.n_node.shape[0], 1]))
+        globals=jnp.zeros([init_graphs.n_node.shape[0], 1])
+    )
     init_net = train.create_model(config, deterministic=True)
     params = jax.jit(init_net.init)(init_rng, init_graphs_preprocessed)
 
     # Create the optimizer.
     optimizer = train.create_optimizer(config)
 
     # Create the evaluation state.
     eval_net = train.create_model(config, deterministic=True)
     eval_state = train_state.TrainState.create(
-        apply_fn=eval_net.apply, params=params, tx=optimizer)
+        apply_fn=eval_net.apply, params=params, tx=optimizer
+    )
 
     # Perform one step of evaluation.
     # We use the same batch of graphs that we used for initialization.
     evaluate_metrics = train.evaluate_step(eval_state, init_graphs)
 
     # Check that the metrics are well defined.
     evaluate_metrics_vals = evaluate_metrics.compute()
```

### Comparing `flax-0.7.0/examples/ppo/README.md` & `flax-0.7.1/examples/ppo/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/ppo/agent.py` & `flax-0.7.1/examples/ppo/agent.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,17 +24,18 @@
 import numpy as np
 
 import env_utils
 
 
 @functools.partial(jax.jit, static_argnums=0)
 def policy_action(
-  apply_fn: Callable[..., Any],
-  params: flax.core.frozen_dict.FrozenDict,
-  state: np.ndarray):
+    apply_fn: Callable[..., Any],
+    params: flax.core.frozen_dict.FrozenDict,
+    state: np.ndarray,
+):
   """Forward pass of the network.
 
   Args:
     params: the parameters of the actor-critic model
     module: the actor-critic model
     state: the input for the forward pass
 
@@ -42,28 +43,30 @@
     out: a tuple (log_probabilities, values)
   """
   out = apply_fn({'params': params}, state)
   return out
 
 
 ExpTuple = collections.namedtuple(
-    'ExpTuple', ['state', 'action', 'reward', 'value', 'log_prob', 'done'])
+    'ExpTuple', ['state', 'action', 'reward', 'value', 'log_prob', 'done']
+)
 
 
 class RemoteSimulator:
   """Wrap functionality for an agent emulating Atari in a separate process.
 
   An object of this class is created for every agent.
   """
 
   def __init__(self, game: str):
     """Start the remote process and create Pipe() to communicate with it."""
     parent_conn, child_conn = multiprocessing.Pipe()
     self.proc = multiprocessing.Process(
-        target=rcv_action_send_exp, args=(child_conn, game))
+        target=rcv_action_send_exp, args=(child_conn, game)
+    )
     self.proc.daemon = True
     self.conn = parent_conn
     self.proc.start()
 
 
 def rcv_action_send_exp(conn, game: str):
   """Run the remote agents.
```

### Comparing `flax-0.7.0/examples/ppo/configs/default.py` & `flax-0.7.1/examples/ppo/configs/default.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Definitions of default hyperparameters."""
 
 import ml_collections
 
+
 def get_config():
   """Get the default configuration.
 
   The default hyperparameters originate from PPO paper arXiv:1707.06347
   and openAI baselines 2::
   https://github.com/openai/baselines/blob/master/baselines/ppo2/defaults.py
   """
```

### Comparing `flax-0.7.0/examples/ppo/env_utils.py` & `flax-0.7.1/examples/ppo/env_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,37 +17,40 @@
 import collections
 
 import gymnasium as gym
 import numpy as np
 
 import seed_rl_atari_preprocessing
 
+
 class ClipRewardEnv(gym.RewardWrapper):
   """Adapted from OpenAI baselines.
 
   github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py
   """
 
   def __init__(self, env):
     gym.RewardWrapper.__init__(self, env)
 
   def reward(self, reward):
     """Bin reward to {+1, 0, -1} by its sign."""
     return np.sign(reward)
 
+
 class FrameStack:
   """Implements stacking of `num_frames` last frames of the game.
 
   Wraps an AtariPreprocessing object.
   """
 
   def __init__(
       self,
       preproc: seed_rl_atari_preprocessing.AtariPreprocessing,
-      num_frames: int):
+      num_frames: int,
+  ):
     self.preproc = preproc
     self.num_frames = num_frames
     self.frames = collections.deque(maxlen=num_frames)
 
   def reset(self):
     ob = self.preproc.reset()
     for _ in range(self.num_frames):
@@ -59,23 +62,25 @@
     self.frames.append(ob)
     return self._get_array(), reward, done, info
 
   def _get_array(self):
     assert len(self.frames) == self.num_frames
     return np.concatenate(self.frames, axis=-1)
 
+
 def create_env(game: str, clip_rewards: bool):
   """Create a FrameStack object that serves as environment for the `game`."""
   env = gym.make(game)
   if clip_rewards:
-    env = ClipRewardEnv(env) # bin rewards to {-1., 0., 1.}
+    env = ClipRewardEnv(env)  # bin rewards to {-1., 0., 1.}
   preproc = seed_rl_atari_preprocessing.AtariPreprocessing(env)
   stack = FrameStack(preproc, num_frames=4)
   return stack
 
+
 def get_num_actions(game: str):
   """Get the number of possible actions of a given Atari game.
 
   This determines the number of outputs in the actor part of the
   actor-critic model.
   """
   env = gym.make(game)
```

### Comparing `flax-0.7.0/examples/ppo/models.py` & `flax-0.7.1/examples/ppo/models.py`

 * *Files 27% similar despite different names*

```diff
@@ -34,23 +34,38 @@
 
     Network is used to both estimate policy (logits) and expected state value;
     in other words, hidden layers' params are shared between policy and value
     networks, see e.g.:
     github.com/openai/baselines/blob/master/baselines/ppo1/cnn_policy.py
     """
     dtype = jnp.float32
-    x = x.astype(dtype) / 255.
-    x = nn.Conv(features=32, kernel_size=(8, 8), strides=(4, 4), name='conv1',
-                dtype=dtype)(x)
+    x = x.astype(dtype) / 255.0
+    x = nn.Conv(
+        features=32,
+        kernel_size=(8, 8),
+        strides=(4, 4),
+        name='conv1',
+        dtype=dtype,
+    )(x)
     x = nn.relu(x)
-    x = nn.Conv(features=64, kernel_size=(4, 4), strides=(2, 2), name='conv2',
-                dtype=dtype)(x)
+    x = nn.Conv(
+        features=64,
+        kernel_size=(4, 4),
+        strides=(2, 2),
+        name='conv2',
+        dtype=dtype,
+    )(x)
     x = nn.relu(x)
-    x = nn.Conv(features=64, kernel_size=(3, 3), strides=(1, 1), name='conv3',
-                dtype=dtype)(x)
+    x = nn.Conv(
+        features=64,
+        kernel_size=(3, 3),
+        strides=(1, 1),
+        name='conv3',
+        dtype=dtype,
+    )(x)
     x = nn.relu(x)
     x = x.reshape((x.shape[0], -1))  # flatten
     x = nn.Dense(features=512, name='hidden', dtype=dtype)(x)
     x = nn.relu(x)
     logits = nn.Dense(features=self.num_outputs, name='logits', dtype=dtype)(x)
     policy_log_probabilities = nn.log_softmax(logits)
     value = nn.Dense(features=1, name='value', dtype=dtype)(x)
```

### Comparing `flax-0.7.0/examples/ppo/ppo_lib.py` & `flax-0.7.1/examples/ppo/ppo_lib.py`

 * *Files 4% similar despite different names*

```diff
@@ -38,15 +38,16 @@
 @jax.jit
 @functools.partial(jax.vmap, in_axes=(1, 1, 1, None, None), out_axes=1)
 def gae_advantages(
     rewards: np.ndarray,
     terminal_masks: np.ndarray,
     values: np.ndarray,
     discount: float,
-    gae_param: float):
+    gae_param: float,
+):
   """Use Generalized Advantage Estimation (GAE) to compute advantages.
 
   As defined by eqs. (11-12) in PPO paper arXiv: 1707.06347. Implementation uses
   key observation that A_{t} = delta_t + gamma*lambda*A_{t+1}.
 
   Args:
     rewards: array shaped (actor_steps, num_agents), rewards from the game
@@ -55,19 +56,21 @@
     values: array shaped (actor_steps, num_agents), values estimated by critic
     discount: RL discount usually denoted with gamma
     gae_param: GAE parameter usually denoted with lambda
 
   Returns:
     advantages: calculated advantages shaped (actor_steps, num_agents)
   """
-  assert rewards.shape[0] + 1 == values.shape[0], ('One more value needed; Eq. '
-                                                   '(12) in PPO paper requires '
-                                                   'V(s_{t+1}) for delta_t')
+  assert rewards.shape[0] + 1 == values.shape[0], (
+      'One more value needed; Eq. '
+      '(12) in PPO paper requires '
+      'V(s_{t+1}) for delta_t'
+  )
   advantages = []
-  gae = 0.
+  gae = 0.0
   for t in reversed(range(len(rewards))):
     # Masks used to set next state value to 0 for terminal states.
     value_diff = discount * values[t + 1] * terminal_masks[t] - values[t]
     delta = rewards[t] + value_diff
     # Masks[t] used to ensure that values before and after a terminal state
     # are independent of each other.
     gae = delta + discount * gae_param * terminal_masks[t] * gae
@@ -78,15 +81,16 @@
 
 def loss_fn(
     params: flax.core.FrozenDict,
     apply_fn: Callable[..., Any],
     minibatch: Tuple,
     clip_param: float,
     vf_coeff: float,
-    entropy_coeff: float):
+    entropy_coeff: float,
+):
   """Evaluate the loss function.
 
   Compute loss as a sum of three components: the negative of the PPO clipped
   surrogate objective, the value function loss and the negative of the entropy
   bonus.
 
   Args:
@@ -108,36 +112,39 @@
   states, actions, old_log_probs, returns, advantages = minibatch
   log_probs, values = agent.policy_action(apply_fn, params, states)
   values = values[:, 0]  # Convert shapes: (batch, 1) to (batch, ).
   probs = jnp.exp(log_probs)
 
   value_loss = jnp.mean(jnp.square(returns - values), axis=0)
 
-  entropy = jnp.sum(-probs*log_probs, axis=1).mean()
+  entropy = jnp.sum(-probs * log_probs, axis=1).mean()
 
   log_probs_act_taken = jax.vmap(lambda lp, a: lp[a])(log_probs, actions)
   ratios = jnp.exp(log_probs_act_taken - old_log_probs)
   # Advantage normalization (following the OpenAI baselines).
   advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
   pg_loss = ratios * advantages
-  clipped_loss = advantages * jax.lax.clamp(1. - clip_param, ratios,
-                                            1. + clip_param)
+  clipped_loss = advantages * jax.lax.clamp(
+      1.0 - clip_param, ratios, 1.0 + clip_param
+  )
   ppo_loss = -jnp.mean(jnp.minimum(pg_loss, clipped_loss), axis=0)
 
-  return ppo_loss + vf_coeff*value_loss - entropy_coeff*entropy
+  return ppo_loss + vf_coeff * value_loss - entropy_coeff * entropy
+
 
 @functools.partial(jax.jit, static_argnums=(2,))
 def train_step(
     state: train_state.TrainState,
     trajectories: Tuple,
     batch_size: int,
     *,
     clip_param: float,
     vf_coeff: float,
-    entropy_coeff: float):
+    entropy_coeff: float
+):
   """Compilable train step.
 
   Runs an entire epoch of training (i.e. the loop over minibatches within
   an epoch is included here for performance reasons).
 
   Args:
     state: the train state
@@ -154,41 +161,47 @@
 
   Returns:
     optimizer: new optimizer after the parameters update
     loss: loss summed over training steps
   """
   iterations = trajectories[0].shape[0] // batch_size
   trajectories = jax.tree_util.tree_map(
-      lambda x: x.reshape((iterations, batch_size) + x.shape[1:]), trajectories)
-  loss = 0.
+      lambda x: x.reshape((iterations, batch_size) + x.shape[1:]), trajectories
+  )
+  loss = 0.0
   for batch in zip(*trajectories):
     grad_fn = jax.value_and_grad(loss_fn)
-    l, grads = grad_fn(state.params, state.apply_fn, batch, clip_param, vf_coeff,
-                      entropy_coeff)
+    l, grads = grad_fn(
+        state.params, state.apply_fn, batch, clip_param, vf_coeff, entropy_coeff
+    )
     loss += l
     state = state.apply_gradients(grads=grads)
   return state, loss
 
+
 def get_experience(
     state: train_state.TrainState,
     simulators: List[agent.RemoteSimulator],
-    steps_per_actor: int):
+    steps_per_actor: int,
+):
   """Collect experience from agents.
 
   Runs `steps_per_actor` time steps of the game for each of the `simulators`.
   """
   all_experience = []
   # Range up to steps_per_actor + 1 to get one more value needed for GAE.
   for _ in range(steps_per_actor + 1):
     sim_states = []
     for sim in simulators:
       sim_state = sim.conn.recv()
       sim_states.append(sim_state)
     sim_states = np.concatenate(sim_states, axis=0)
-    log_probs, values = agent.policy_action(state.apply_fn, state.params, sim_states)
+    log_probs, values = agent.policy_action(
+        state.apply_fn, state.params, sim_states
+    )
     log_probs, values = jax.device_get((log_probs, values))
     probs = np.exp(np.array(log_probs))
     for i, sim in enumerate(simulators):
       probabilities = probs[i]
       action = np.random.choice(probs.shape[1], p=probabilities)
       sim.conn.send(action)
     experiences = []
@@ -197,20 +210,22 @@
       value = values[i, 0]
       log_prob = log_probs[i][action]
       sample = agent.ExpTuple(sim_state, action, reward, value, log_prob, done)
       experiences.append(sample)
     all_experience.append(experiences)
   return all_experience
 
+
 def process_experience(
     experience: List[List[agent.ExpTuple]],
     actor_steps: int,
     num_agents: int,
     gamma: float,
-    lambda_: float):
+    lambda_: float,
+):
   """Process experience for training, including advantage estimation.
 
   Args:
     experience: collected from agents in the form of nested lists/namedtuple
     actor_steps: number of steps each agent has completed
     num_agents: number of agents that collected experience
     gamma: dicount parameter
@@ -241,103 +256,125 @@
   for a in range(num_agents):
     values[-1, a] = experience[-1][a].value
   advantages = gae_advantages(rewards, dones, values, gamma, lambda_)
   returns = advantages + values[:-1, :]
   # After preprocessing, concatenate data from all agents.
   trajectories = (states, actions, log_probs, returns, advantages)
   trajectory_len = num_agents * actor_steps
-  trajectories = tuple(map(
-      lambda x: np.reshape(x, (trajectory_len,) + x.shape[2:]), trajectories))
+  trajectories = tuple(
+      map(
+          lambda x: np.reshape(x, (trajectory_len,) + x.shape[2:]), trajectories
+      )
+  )
   return trajectories
 
 
 @functools.partial(jax.jit, static_argnums=1)
 def get_initial_params(key: np.ndarray, model: nn.Module):
   input_dims = (1, 84, 84, 4)  # (minibatch, height, width, stacked frames)
   init_shape = jnp.ones(input_dims, jnp.float32)
   initial_params = model.init(key, init_shape)['params']
   return initial_params
 
 
-def create_train_state(params, model: nn.Module,
-                       config: ml_collections.ConfigDict, train_steps: int) -> train_state.TrainState:
+def create_train_state(
+    params,
+    model: nn.Module,
+    config: ml_collections.ConfigDict,
+    train_steps: int,
+) -> train_state.TrainState:
   if config.decaying_lr_and_clip_param:
     lr = optax.linear_schedule(
-        init_value=config.learning_rate, end_value=0.,
-        transition_steps=train_steps)
+        init_value=config.learning_rate,
+        end_value=0.0,
+        transition_steps=train_steps,
+    )
   else:
     lr = config.learning_rate
   tx = optax.adam(lr)
   state = train_state.TrainState.create(
-      apply_fn=model.apply,
-      params=params,
-      tx=tx)
+      apply_fn=model.apply, params=params, tx=tx
+  )
   return state
 
 
 def train(
-    model: models.ActorCritic,
-    config: ml_collections.ConfigDict,
-    model_dir: str):
+    model: models.ActorCritic, config: ml_collections.ConfigDict, model_dir: str
+):
   """Main training loop.
 
   Args:
     model: the actor-critic model
     config: object holding hyperparameters and the training information
     model_dir: path to dictionary where checkpoints and logging info are stored
 
   Returns:
     optimizer: the trained optimizer
   """
 
   game = config.game + 'NoFrameskip-v4'
-  simulators = [agent.RemoteSimulator(game)
-                for _ in range(config.num_agents)]
+  simulators = [agent.RemoteSimulator(game) for _ in range(config.num_agents)]
   summary_writer = tensorboard.SummaryWriter(model_dir)
   summary_writer.hparams(dict(config))
   loop_steps = config.total_frames // (config.num_agents * config.actor_steps)
   log_frequency = 40
   checkpoint_frequency = 500
   # train_step does multiple steps per call for better performance
   # compute number of steps per call here to convert between the number of
   # train steps and the inner number of optimizer steps
-  iterations_per_step = (config.num_agents * config.actor_steps
-      // config.batch_size)
+  iterations_per_step = (
+      config.num_agents * config.actor_steps // config.batch_size
+  )
 
   initial_params = get_initial_params(jax.random.PRNGKey(0), model)
-  state = create_train_state(initial_params, model, config,
-                             loop_steps * config.num_epochs * iterations_per_step)
+  state = create_train_state(
+      initial_params,
+      model,
+      config,
+      loop_steps * config.num_epochs * iterations_per_step,
+  )
   del initial_params
   state = checkpoints.restore_checkpoint(model_dir, state)
   # number of train iterations done by each train_step
 
   start_step = int(state.step) // config.num_epochs // iterations_per_step
   logging.info('Start training from step: %s', start_step)
 
   for step in range(start_step, loop_steps):
     # Bookkeeping and testing.
     if step % log_frequency == 0:
       score = test_episodes.policy_test(1, state.apply_fn, state.params, game)
       frames = step * config.num_agents * config.actor_steps
       summary_writer.scalar('game_score', score, frames)
-      logging.info('Step %s:\nframes seen %s\nscore %s\n\n', step, frames, score)
+      logging.info(
+          'Step %s:\nframes seen %s\nscore %s\n\n', step, frames, score
+      )
 
     # Core training code.
-    alpha = 1. - step / loop_steps if config.decaying_lr_and_clip_param else 1.
-    all_experiences = get_experience(
-        state, simulators, config.actor_steps)
+    alpha = (
+        1.0 - step / loop_steps if config.decaying_lr_and_clip_param else 1.0
+    )
+    all_experiences = get_experience(state, simulators, config.actor_steps)
     trajectories = process_experience(
-        all_experiences, config.actor_steps, config.num_agents, config.gamma,
-        config.lambda_)
+        all_experiences,
+        config.actor_steps,
+        config.num_agents,
+        config.gamma,
+        config.lambda_,
+    )
     clip_param = config.clip_param * alpha
     for _ in range(config.num_epochs):
       permutation = np.random.permutation(
-          config.num_agents * config.actor_steps)
+          config.num_agents * config.actor_steps
+      )
       trajectories = tuple(x[permutation] for x in trajectories)
       state, _ = train_step(
-          state, trajectories, config.batch_size,
+          state,
+          trajectories,
+          config.batch_size,
           clip_param=clip_param,
           vf_coeff=config.vf_coeff,
-          entropy_coeff=config.entropy_coeff)
+          entropy_coeff=config.entropy_coeff,
+      )
     if (step + 1) % checkpoint_frequency == 0:
       checkpoints.save_checkpoint(model_dir, state, step + 1)
   return state
```

### Comparing `flax-0.7.0/examples/ppo/ppo_lib_test.py` & `flax-0.7.1/examples/ppo/ppo_lib_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -25,46 +25,59 @@
 import env_utils
 import models
 import ppo_lib
 
 
 # test GAE
 class TestGAE(absltest.TestCase):
+
   def test_gae_shape_on_random(self):
     # create random data, simulating 4 parallel envs and 20 time_steps
     envs, steps = 10, 100
-    rewards = np.random.choice([-1., 0., 1.], size=(steps, envs),
-                                p=[0.01, 0.98, 0.01])
+    rewards = np.random.choice(
+        [-1.0, 0.0, 1.0], size=(steps, envs), p=[0.01, 0.98, 0.01]
+    )
     terminal_masks = np.ones(shape=(steps, envs), dtype=np.float64)
     values = np.random.random(size=(steps + 1, envs))
     discount = 0.99
     gae_param = 0.95
-    adv = ppo_lib.gae_advantages(rewards, terminal_masks, values, discount,
-                                 gae_param)
+    adv = ppo_lib.gae_advantages(
+        rewards, terminal_masks, values, discount, gae_param
+    )
     self.assertEqual(adv.shape, (steps, envs))
 
   def test_gae_hardcoded(self):
-    #test on small example that can be verified by hand
-    rewards = np.array([[1., 0.], [0., 0.], [-1., 1.]])
-    #one of the two episodes terminated in the middle
-    terminal_masks = np.array([[1., 1.], [0., 1.], [1., 1.]])
-    values = np.array([[1., 1.], [1., 1.], [1., 1.], [1., 1.]])
+    # test on small example that can be verified by hand
+    rewards = np.array([[1.0, 0.0], [0.0, 0.0], [-1.0, 1.0]])
+    # one of the two episodes terminated in the middle
+    terminal_masks = np.array([[1.0, 1.0], [0.0, 1.0], [1.0, 1.0]])
+    values = np.array([[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]])
     discount = 0.5
     gae_param = 0.25
-    correct_gae = np.array([[0.375, -0.5546875], [-1., -0.4375], [-1.5, 0.5]])
-    actual_gae = ppo_lib.gae_advantages(rewards, terminal_masks, values,
-                                        discount, gae_param)
+    correct_gae = np.array([[0.375, -0.5546875], [-1.0, -0.4375], [-1.5, 0.5]])
+    actual_gae = ppo_lib.gae_advantages(
+        rewards, terminal_masks, values, discount, gae_param
+    )
     np_testing.assert_allclose(actual_gae, correct_gae)
+
+
 # test environment and preprocessing
 class TestEnvironmentPreprocessing(absltest.TestCase):
+
   def choose_random_game(self):
-    games = ['BeamRider', 'Breakout', 'Pong',
-             'Qbert', 'Seaquest', 'SpaceInvaders']
+    games = [
+        'BeamRider',
+        'Breakout',
+        'Pong',
+        'Qbert',
+        'Seaquest',
+        'SpaceInvaders',
+    ]
     ind = np.random.choice(len(games))
-    return games[ind] + "NoFrameskip-v4"
+    return games[ind] + 'NoFrameskip-v4'
 
   def test_creation(self):
     frame_shape = (84, 84, 4)
     game = self.choose_random_game()
     env = env_utils.create_env(game, clip_rewards=True)
     obs = env.reset()
     self.assertEqual(obs.shape, frame_shape)
@@ -74,43 +87,47 @@
     game = self.choose_random_game()
     env = env_utils.create_env(game, clip_rewards=True)
     obs = env.reset()
     actions = [1, 2, 3, 0]
     for a in actions:
       obs, reward, done, info = env.step(a)
       self.assertEqual(obs.shape, frame_shape)
-      self.assertTrue(reward <= 1. and reward >= -1.)
+      self.assertTrue(reward <= 1.0 and reward >= -1.0)
       self.assertTrue(isinstance(done, bool))
       self.assertTrue(isinstance(info, dict))
 
+
 # test the model (creation and forward pass)
 class TestModel(absltest.TestCase):
+
   def choose_random_outputs(self):
     return np.random.choice([4, 5, 6, 7, 8, 9])
 
   def test_model(self):
     outputs = self.choose_random_outputs()
     module = models.ActorCritic(num_outputs=outputs)
     params = ppo_lib.get_initial_params(jax.random.PRNGKey(0), module)
     test_batch_size, obs_shape = 10, (84, 84, 4)
     random_input = np.random.random(size=(test_batch_size,) + obs_shape)
-    log_probs, values = agent.policy_action(
-        module.apply, params, random_input)
+    log_probs, values = agent.policy_action(module.apply, params, random_input)
     self.assertEqual(values.shape, (test_batch_size, 1))
     sum_probs = np.sum(np.exp(log_probs), axis=1)
-    self.assertEqual(sum_probs.shape, (test_batch_size, ))
-    np_testing.assert_allclose(sum_probs, np.ones((test_batch_size, )),
-                                atol=1e-6)
+    self.assertEqual(sum_probs.shape, (test_batch_size,))
+    np_testing.assert_allclose(
+        sum_probs, np.ones((test_batch_size,)), atol=1e-6
+    )
+
 
 # test one optimization step
 class TestOptimizationStep(absltest.TestCase):
+
   def generate_random_data(self, num_actions):
-    data_len = 256 # equal to one default-sized batch
+    data_len = 256  # equal to one default-sized batch
     state_shape = (84, 84, 4)
-    states = np.random.randint(0, 255, size=((data_len, ) + state_shape))
+    states = np.random.randint(0, 255, size=((data_len,) + state_shape))
     actions = np.random.choice(num_actions, size=data_len)
     old_log_probs = np.random.random(size=data_len)
     returns = np.random.random(size=data_len)
     advantages = np.random.random(size=data_len)
     return states, actions, old_log_probs, returns, advantages
 
   def test_optimization_step(self):
@@ -119,20 +136,24 @@
     clip_param = 0.1
     vf_coeff = 0.5
     entropy_coeff = 0.01
     batch_size = 256
     module = models.ActorCritic(num_outputs)
     initial_params = ppo_lib.get_initial_params(jax.random.PRNGKey(0), module)
     config = ml_collections.ConfigDict({
-      'learning_rate': 2.5e-4,
-      'decaying_lr_and_clip_param': True,
+        'learning_rate': 2.5e-4,
+        'decaying_lr_and_clip_param': True,
     })
     state = ppo_lib.create_train_state(initial_params, module, config, 1000)
     state, _ = ppo_lib.train_step(
-        state, trn_data, batch_size,
+        state,
+        trn_data,
+        batch_size,
         clip_param=clip_param,
         vf_coeff=vf_coeff,
-        entropy_coeff=entropy_coeff)
+        entropy_coeff=entropy_coeff,
+    )
     self.assertIsInstance(state, train_state.TrainState)
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/examples/ppo/ppo_main.py` & `flax-0.7.1/examples/ppo/ppo_main.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,28 +26,31 @@
 
 
 FLAGS = flags.FLAGS
 
 flags.DEFINE_string(
     'workdir',
     default='/tmp/ppo_training',
-    help=('Directory to save checkpoints and logging info.'))
+    help='Directory to save checkpoints and logging info.',
+)
 
 config_flags.DEFINE_config_file(
     'config',
-    "configs/default.py",
+    'configs/default.py',
     'File path to the default configuration file.',
-    lock_config=True)
+    lock_config=True,
+)
 
 
 def main(argv):
   # Make sure tf does not allocate gpu memory.
   tf.config.experimental.set_visible_devices([], 'GPU')
   config = FLAGS.config
   game = config.game + 'NoFrameskip-v4'
   num_actions = env_utils.get_num_actions(game)
   print(f'Playing {game} with {num_actions} actions')
   model = models.ActorCritic(num_outputs=num_actions)
   ppo_lib.train(model, config, FLAGS.workdir)
 
+
 if __name__ == '__main__':
   app.run(main)
```

### Comparing `flax-0.7.0/examples/ppo/seed_rl_atari_preprocessing.py` & `flax-0.7.1/examples/ppo/seed_rl_atari_preprocessing.py`

 * *Files 2% similar despite different names*

```diff
@@ -47,59 +47,73 @@
   More generally, this class follows the preprocessing guidelines set down in
   Machado et al. (2018), "Revisiting the Arcade Learning Environment:
   Evaluation Protocols and Open Problems for General Agents".
   It also provides random starting no-ops, which are used in the Rainbow, Apex
   and R2D2 papers.
   """
 
-  def __init__(self, environment: gym.Env, frame_skip=4, terminal_on_life_loss=False,
-               screen_size=84, max_random_noops=0):
+  def __init__(
+      self,
+      environment: gym.Env,
+      frame_skip=4,
+      terminal_on_life_loss=False,
+      screen_size=84,
+      max_random_noops=0,
+  ):
     """Constructor for an Atari 2600 preprocessor.
     Args:
       environment: Gym environment whose observations are preprocessed.
       frame_skip: int, the frequency at which the agent experiences the game.
       terminal_on_life_loss: bool, If True, the step() method returns
         is_terminal=True whenever a life is lost. See Mnih et al. 2015.
       screen_size: int, size of a resized Atari 2600 frame.
       max_random_noops: int, maximum number of no-ops to apply at the beginning
         of each episode to reduce determinism. These no-ops are applied at a
         low-level, before frame skipping.
     Raises:
       ValueError: if frame_skip or screen_size are not strictly positive.
     """
     if frame_skip <= 0:
-      raise ValueError('Frame skip should be strictly positive, got {}'.
-                       format(frame_skip))
+      raise ValueError(
+          'Frame skip should be strictly positive, got {}'.format(frame_skip)
+      )
     if screen_size <= 0:
-      raise ValueError('Target screen size should be strictly positive, got {}'.
-                       format(screen_size))
+      raise ValueError(
+          'Target screen size should be strictly positive, got {}'.format(
+              screen_size
+          )
+      )
 
     self.environment = environment
     self.terminal_on_life_loss = terminal_on_life_loss
     self.frame_skip = frame_skip
     self.screen_size = screen_size
     self.max_random_noops = max_random_noops
 
     obs_dims = self.environment.observation_space
     # Stores temporary observations used for pooling over two successive
     # frames.
     self.screen_buffer = [
         np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),
-        np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)
+        np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),
     ]
 
     self.game_over = False
     self.lives = 0  # Will need to be set by reset().
 
   @property
   def observation_space(self):
     # Return the observation space adjusted to match the shape of the processed
     # observations.
-    return Box(low=0, high=255, shape=(self.screen_size, self.screen_size, 1),
-               dtype=np.uint8)
+    return Box(
+        low=0,
+        high=255,
+        shape=(self.screen_size, self.screen_size, 1),
+        dtype=np.uint8,
+    )
 
   @property
   def action_space(self):
     return self.environment.action_space
 
   @property
   def reward_range(self):
@@ -129,15 +143,15 @@
     Returns:
       observation: numpy array, the initial observation emitted by the
         environment.
     """
     self.environment.reset()
     self.apply_random_noops()
 
-    self.lives = self.environment.ale.lives()
+    self.lives = self.environment.unwrapped.ale.lives()
     self._fetch_grayscale_observation(self.screen_buffer[0])
     self.screen_buffer[1].fill(0)
     return self._pool_and_resize()
 
   def render(self, mode):
     """Renders the current screen, before preprocessing.
     This calls the Gym API's render() method.
@@ -165,24 +179,24 @@
       observation: numpy array, the observation following the action.
       reward: float, the reward following the action.
       is_terminal: bool, whether the environment has reached a terminal state.
         This is true when a life is lost and terminal_on_life_loss, or when the
         episode is over.
       info: Gym API's info data structure.
     """
-    accumulated_reward = 0.
+    accumulated_reward = 0.0
 
     for time_step in range(self.frame_skip):
       # We bypass the Gym observation altogether and directly fetch the
       # grayscale image from the ALE. This is a little faster.
       _, reward, game_over, _, info = self.environment.step(action)
       accumulated_reward += reward
 
       if self.terminal_on_life_loss:
-        new_lives = self.environment.ale.lives()
+        new_lives = self.environment.unwrapped.ale.lives()
         is_terminal = game_over or new_lives < self.lives
         self.lives = new_lives
       else:
         is_terminal = game_over
 
       if is_terminal:
         break
@@ -201,26 +215,31 @@
     """Returns the current observation in grayscale.
     The returned observation is stored in 'output'.
     Args:
       output: numpy array, screen buffer to hold the returned observation.
     Returns:
       observation: numpy array, the current observation in grayscale.
     """
-    self.environment.ale.getScreenGrayscale(output)
+    self.environment.unwrapped.ale.getScreenGrayscale(output)
     return output
 
   def _pool_and_resize(self):
     """Transforms two frames into a Nature DQN observation.
     For efficiency, the transformation is done in-place in self.screen_buffer.
     Returns:
       transformed_screen: numpy array, pooled, resized screen.
     """
     # Pool if there are enough screens to do so.
     if self.frame_skip > 1:
-      np.maximum(self.screen_buffer[0], self.screen_buffer[1],
-                 out=self.screen_buffer[0])
-
-    transformed_image = cv2.resize(self.screen_buffer[0],
-                                   (self.screen_size, self.screen_size),
-                                   interpolation=cv2.INTER_LINEAR)
+      np.maximum(
+          self.screen_buffer[0],
+          self.screen_buffer[1],
+          out=self.screen_buffer[0],
+      )
+
+    transformed_image = cv2.resize(
+        self.screen_buffer[0],
+        (self.screen_size, self.screen_size),
+        interpolation=cv2.INTER_LINEAR,
+    )
     int_image = np.asarray(transformed_image, dtype=np.uint8)
     return np.expand_dims(int_image, axis=2)
```

### Comparing `flax-0.7.0/examples/ppo/test_episodes.py` & `flax-0.7.1/examples/ppo/test_episodes.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,15 +24,16 @@
 import env_utils
 
 
 def policy_test(
     n_episodes: int,
     apply_fn: Callable[..., Any],
     params: flax.core.frozen_dict.FrozenDict,
-    game: str):
+    game: str,
+):
   """Perform a test of the policy in Atari environment.
 
   Args:
     n_episodes: number of full Atari episodes to test on
     apply_fn: the actor-critic apply function
     params: actor-critic model parameters, they define the policy being tested
     game: defines the Atari game to test on
```

### Comparing `flax-0.7.0/examples/seq2seq/README.md` & `flax-0.7.1/examples/seq2seq/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/seq2seq/input_pipeline.py` & `flax-0.7.1/examples/seq2seq/input_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,26 +16,24 @@
 
 import random
 from typing import Any, Dict, Generator, Optional, Tuple
 
 import jax.numpy as jnp
 import numpy as np
 
-Array = Any    # pylint: disable=invalid-name
+Array = Any  # pylint: disable=invalid-name
 
 
 class CharacterTable:
   """Encodes/decodes between strings and integer representations."""
 
   def __init__(self, chars: str, max_len_query_digit: int = 3) -> None:
     self._chars = sorted(set(chars))
-    self._char_indices = {
-        ch: idx + 2 for idx, ch in enumerate(self._chars)}
-    self._indices_char = {
-        idx + 2: ch for idx, ch in enumerate(self._chars)}
+    self._char_indices = {ch: idx + 2 for idx, ch in enumerate(self._chars)}
+    self._indices_char = {idx + 2: ch for idx, ch in enumerate(self._chars)}
     self._indices_char[self.pad_id] = '_'
     # Maximum length of a single input digit.
     self._max_len_query_digit = max_len_query_digit
 
   @property
   def pad_id(self) -> int:
     return 0
@@ -71,15 +69,16 @@
   @property
   def decoder_input_shape(self) -> Tuple[int, int, int]:
     return (1, self.max_output_len, self.vocab_size)
 
   def encode(self, inputs: str) -> np.ndarray:
     """Encodes from string to list of integers."""
     return np.array(
-        [self._char_indices[char] for char in inputs] + [self.eos_id])
+        [self._char_indices[char] for char in inputs] + [self.eos_id]
+    )
 
   def decode(self, inputs: Array) -> str:
     """Decodes from list of integers to string."""
     chars = []
     for elem in inputs.tolist():
       if elem == self.eos_id:
         break
@@ -88,38 +87,39 @@
 
   def one_hot(self, tokens: np.ndarray) -> np.ndarray:
     vecs = np.zeros((tokens.size, self.vocab_size), dtype=np.float32)
     vecs[np.arange(tokens.size), tokens] = 1
     return vecs
 
   def encode_onehot(
-      self, batch_inputs: Array, max_len: Optional[int] = None) -> np.ndarray:
+      self, batch_inputs: Array, max_len: Optional[int] = None
+  ) -> np.ndarray:
     """One-hot encodes a string input."""
 
     if max_len is None:
       max_len = self.max_input_len
 
     def encode_str(s):
       tokens = self.encode(s)
       unpadded_len = len(tokens)
       if unpadded_len > max_len:
-        raise ValueError(
-            f'Sequence too long ({len(tokens)}>{max_len}): \'{s}\'')
+        raise ValueError(f"Sequence too long ({len(tokens)}>{max_len}): '{s}'")
       tokens = np.pad(tokens, [(0, max_len - len(tokens))], mode='constant')
       return self.one_hot(tokens)
 
     return np.array([encode_str(inp) for inp in batch_inputs])
 
   def decode_onehot(self, batch_inputs: Array) -> np.ndarray:
     """Decodes a batch of one-hot encoding to strings."""
     decode_inputs = lambda inputs: self.decode(inputs.argmax(axis=-1))
     return np.array(list(map(decode_inputs, batch_inputs)))
 
   def generate_examples(
-      self, num_examples: int) -> Generator[Tuple[str, str], None, None]:
+      self, num_examples: int
+  ) -> Generator[Tuple[str, str], None, None]:
     """Yields `num_examples` examples."""
     for _ in range(num_examples):
       max_digit = pow(10, self._max_len_query_digit) - 1
       # TODO(marcvanzee): Use jax.random here.
       key = tuple(sorted((random.randint(0, 99), random.randint(0, max_digit))))
       inputs = f'{key[0]}+{key[1]}'
       # Preprend output by the decoder's start token.
@@ -134,21 +134,22 @@
         'answer': self.encode_onehot(outputs),
     }
 
 
 def mask_sequences(sequence_batch: Array, lengths: Array) -> Array:
   """Sets positions beyond the length of each sequence to 0."""
   return sequence_batch * (
-      lengths[:, np.newaxis] > np.arange(sequence_batch.shape[1])[np.newaxis])
+      lengths[:, np.newaxis] > np.arange(sequence_batch.shape[1])[np.newaxis]
+  )
 
 
 def get_sequence_lengths(sequence_batch: Array, eos_id: int) -> Array:
   """Returns the length of each one-hot sequence, including the EOS token."""
   # sequence_batch.shape = (batch_size, seq_length, vocab_size)
   eos_row = sequence_batch[:, :, eos_id]
   eos_idx = jnp.argmax(eos_row, axis=-1)  # returns first occurrence
   # `eos_idx` is 0 if EOS is not present, so we use full length in that case.
   return jnp.where(
       eos_row[jnp.arange(eos_row.shape[0]), eos_idx],
       eos_idx + 1,
-      sequence_batch.shape[1]  # if there is no EOS, use full length
+      sequence_batch.shape[1],  # if there is no EOS, use full length
   )
```

### Comparing `flax-0.7.0/examples/seq2seq/models.py` & `flax-0.7.1/examples/seq2seq/models.py`

 * *Files 6% similar despite different names*

```diff
@@ -33,14 +33,15 @@
 class DecoderLSTMCell(nn.RNNCellBase):
   """DecoderLSTM Module wrapped in a lifted scan transform.
 
   Attributes:
     teacher_force: See docstring on Seq2seq module.
     vocab_size: Size of the vocabulary.
   """
+
   features: int
   teacher_force: bool
   vocab_size: int
 
   @nn.compact
   def __call__(
       self, carry: Tuple[LSTMCarry, Array], x: Array
@@ -53,15 +54,16 @@
     logits = nn.Dense(features=self.vocab_size)(y)
     # Sample the predicted token using a categorical distribution over the
     # logits.
     categorical_rng = self.make_rng('lstm')
     predicted_token = jax.random.categorical(categorical_rng, logits)
     # Convert to one-hot encoding.
     prediction = jax.nn.one_hot(
-        predicted_token, self.vocab_size, dtype=jnp.float32)
+        predicted_token, self.vocab_size, dtype=jnp.float32
+    )
 
     return (lstm_state, prediction), (logits, prediction)
 
   @property
   def num_feature_axes(self) -> int:
     return 1
 
@@ -74,22 +76,24 @@
       every step. If False, only the first input (i.e., the "=" token) is used,
       followed by samples taken from the previous output logits.
     hidden_size: int, the number of hidden dimensions in the encoder and decoder
       LSTMs.
     vocab_size: the size of the vocabulary.
     eos_id: EOS id.
   """
+
   teacher_force: bool
   hidden_size: int
   vocab_size: int
   eos_id: int = 1
 
   @nn.compact
-  def __call__(self, encoder_inputs: Array,
-               decoder_inputs: Array) -> Tuple[Array, Array]:
+  def __call__(
+      self, encoder_inputs: Array, decoder_inputs: Array
+  ) -> Tuple[Array, Array]:
     """Applies the seq2seq model.
 
     Args:
       encoder_inputs: [batch_size, max_input_length, vocab_size].
         padded batch of input sequences to encode.
       decoder_inputs: [batch_size, max_output_length, vocab_size].
         padded batch of expected decoded sequences for teacher forcing.
@@ -101,27 +105,36 @@
 
     Returns:
       Pair (logits, predictions), which are two arrays of length `batch_size`
       containing respectively decoded logits and predictions (in one hot
       encoding format).
     """
     # Encode inputs.
-    encoder = nn.RNN(nn.LSTMCell(self.hidden_size), return_carry=True, name='encoder')
-    decoder = nn.RNN(DecoderLSTMCell(decoder_inputs.shape[-1], self.teacher_force, self.vocab_size),
-      split_rngs={'params': False, 'lstm': True}, name='decoder')
+    encoder = nn.RNN(
+        nn.LSTMCell(self.hidden_size), return_carry=True, name='encoder'
+    )
+    decoder = nn.RNN(
+        DecoderLSTMCell(
+            decoder_inputs.shape[-1], self.teacher_force, self.vocab_size
+        ),
+        split_rngs={'params': False, 'lstm': True},
+        name='decoder',
+    )
 
     seq_lengths = self.get_seq_lengths(encoder_inputs)
 
     encoder_state, _ = encoder(encoder_inputs, seq_lengths=seq_lengths)
-    logits, predictions = decoder(decoder_inputs[:, :-1], initial_carry=(encoder_state, decoder_inputs[:, 0]))
+    logits, predictions = decoder(
+        decoder_inputs[:, :-1],
+        initial_carry=(encoder_state, decoder_inputs[:, 0]),
+    )
 
     return logits, predictions
 
   def get_seq_lengths(self, inputs: Array) -> Array:
     """Get segmentation mask for inputs."""
     # undo one-hot encoding
     inputs = jnp.argmax(inputs, axis=-1)
     # calculate sequence lengths
     seq_lengths = jnp.argmax(inputs == self.eos_id, axis=-1)
 
     return seq_lengths
-
```

### Comparing `flax-0.7.0/examples/seq2seq/seq2seq.ipynb` & `flax-0.7.1/examples/seq2seq/seq2seq.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/seq2seq/train.py` & `flax-0.7.1/examples/seq2seq/train.py`

 * *Files 6% similar despite different names*

```diff
@@ -41,73 +41,83 @@
 PRNGKey = Any
 
 flags.DEFINE_string('workdir', default='.', help='Where to store log output.')
 
 flags.DEFINE_float(
     'learning_rate',
     default=0.003,
-    help=('The learning rate for the Adam optimizer.'))
+    help='The learning rate for the Adam optimizer.',
+)
 
-flags.DEFINE_integer(
-    'batch_size', default=128, help=('Batch size for training.'))
+flags.DEFINE_integer('batch_size', default=128, help='Batch size for training.')
 
 flags.DEFINE_integer(
-    'hidden_size', default=512, help=('Hidden size of the LSTM.'))
+    'hidden_size', default=512, help='Hidden size of the LSTM.'
+)
 
 flags.DEFINE_integer(
-    'num_train_steps', default=10000, help=('Number of train steps.'))
+    'num_train_steps', default=10000, help='Number of train steps.'
+)
 
 flags.DEFINE_integer(
     'decode_frequency',
     default=200,
-    help=('Frequency of decoding during training, e.g. every 1000 steps.'))
+    help='Frequency of decoding during training, e.g. every 1000 steps.',
+)
 
 flags.DEFINE_integer(
     'max_len_query_digit',
     default=3,
-    help=('Maximum length of a single input digit.'))
+    help='Maximum length of a single input digit.',
+)
 
 
 def get_model(ctable: CTable, *, teacher_force: bool = False) -> models.Seq2seq:
-  return models.Seq2seq(teacher_force=teacher_force,
-                        hidden_size=FLAGS.hidden_size, eos_id=ctable.eos_id,
-                        vocab_size=ctable.vocab_size)
+  return models.Seq2seq(
+      teacher_force=teacher_force,
+      hidden_size=FLAGS.hidden_size,
+      eos_id=ctable.eos_id,
+      vocab_size=ctable.vocab_size,
+  )
 
 
-def get_initial_params(model: models.Seq2seq, rng: PRNGKey,
-                       ctable: CTable) -> Dict[str, Any]:
+def get_initial_params(
+    model: models.Seq2seq, rng: PRNGKey, ctable: CTable
+) -> Dict[str, Any]:
   """Returns the initial parameters of a seq2seq model."""
   rng1, rng2 = jax.random.split(rng)
   variables = model.init(
       {'params': rng1, 'lstm': rng2},
       jnp.ones(ctable.encoder_input_shape, jnp.float32),
-      jnp.ones(ctable.decoder_input_shape, jnp.float32)
+      jnp.ones(ctable.decoder_input_shape, jnp.float32),
   )
   return variables['params']
 
 
 def get_train_state(rng: PRNGKey, ctable: CTable) -> train_state.TrainState:
   """Returns a train state."""
   model = get_model(ctable)
   params = get_initial_params(model, rng, ctable)
   tx = optax.adam(FLAGS.learning_rate)
   state = train_state.TrainState.create(
-      apply_fn=model.apply, params=params, tx=tx)
+      apply_fn=model.apply, params=params, tx=tx
+  )
   return state
 
 
 def cross_entropy_loss(logits: Array, labels: Array, lengths: Array) -> float:
   """Returns cross-entropy loss."""
   xe = jnp.sum(nn.log_softmax(logits) * labels, axis=-1)
   masked_xe = jnp.mean(mask_sequences(xe, lengths))
   return -masked_xe
 
 
-def compute_metrics(logits: Array, labels: Array,
-                    eos_id: int) -> Dict[str, float]:
+def compute_metrics(
+    logits: Array, labels: Array, eos_id: int
+) -> Dict[str, float]:
   """Computes metrics and returns them."""
   lengths = get_sequence_lengths(labels, eos_id)
   loss = cross_entropy_loss(logits, labels, lengths)
   # Computes sequence accuracy, which is the same as the accuracy during
   # inference, since teacher forcing is irrelevant when all output are correct.
   token_accuracy = jnp.argmax(logits, -1) == jnp.argmax(labels, -1)
   sequence_accuracy = (
@@ -118,61 +128,71 @@
       'loss': loss,
       'accuracy': accuracy,
   }
   return metrics
 
 
 @jax.jit
-def train_step(state: train_state.TrainState, batch: Array, lstm_rng: PRNGKey,
-               eos_id: int) -> Tuple[train_state.TrainState, Dict[str, float]]:
+def train_step(
+    state: train_state.TrainState, batch: Array, lstm_rng: PRNGKey, eos_id: int
+) -> Tuple[train_state.TrainState, Dict[str, float]]:
   """Trains one step."""
   labels = batch['answer'][:, 1:]
   lstm_key = jax.random.fold_in(lstm_rng, state.step)
 
   def loss_fn(params):
-    logits, _ = state.apply_fn({'params': params},
-                               batch['query'],
-                               batch['answer'],
-                               rngs={'lstm': lstm_key})
+    logits, _ = state.apply_fn(
+        {'params': params},
+        batch['query'],
+        batch['answer'],
+        rngs={'lstm': lstm_key},
+    )
     loss = cross_entropy_loss(
-        logits, labels, get_sequence_lengths(labels, eos_id))
+        logits, labels, get_sequence_lengths(labels, eos_id)
+    )
     return loss, logits
 
   grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
   (_, logits), grads = grad_fn(state.params)
   state = state.apply_gradients(grads=grads)
   metrics = compute_metrics(logits, labels, eos_id)
 
   return state, metrics
 
 
 def log_decode(question: str, inferred: str, golden: str):
   """Logs the given question, inferred query, and correct query."""
-  suffix = '(CORRECT)' if inferred == golden else (f'(INCORRECT) '
-                                                   f'correct={golden}')
+  suffix = (
+      '(CORRECT)' if inferred == golden else (f'(INCORRECT) correct={golden}')
+  )
   logging.info('DECODE: %s = %s %s', question, inferred, suffix)
 
 
 @functools.partial(jax.jit, static_argnums=3)
-def decode(params: Dict[str, Any], inputs: Array, decode_rng: PRNGKey,
-           ctable: CTable) -> Array:
+def decode(
+    params: Dict[str, Any], inputs: Array, decode_rng: PRNGKey, ctable: CTable
+) -> Array:
   """Decodes inputs."""
   init_decoder_input = ctable.one_hot(ctable.encode('=')[0:1])
-  init_decoder_inputs = jnp.tile(init_decoder_input,
-                                 (inputs.shape[0], ctable.max_output_len, 1))
+  init_decoder_inputs = jnp.tile(
+      init_decoder_input, (inputs.shape[0], ctable.max_output_len, 1)
+  )
   model = get_model(ctable, teacher_force=False)
-  _, predictions = model.apply({'params': params},
-                               inputs,
-                               init_decoder_inputs,
-                               rngs={'lstm': decode_rng})
+  _, predictions = model.apply(
+      {'params': params}, inputs, init_decoder_inputs, rngs={'lstm': decode_rng}
+  )
   return predictions
 
 
-def decode_batch(state: train_state.TrainState, batch: Dict[str, Array],
-                 decode_rng: PRNGKey, ctable: CTable):
+def decode_batch(
+    state: train_state.TrainState,
+    batch: Dict[str, Array],
+    decode_rng: PRNGKey,
+    ctable: CTable,
+):
   """Decodes and log results for a batch."""
   inputs, outputs = batch['query'], batch['answer'][:, 1:]
   decode_rng = jax.random.fold_in(decode_rng, state.step)
   inferred = decode(state.params, inputs, decode_rng, ctable)
   questions = ctable.decode_onehot(inputs)
   infers = ctable.decode_onehot(inferred)
   goldens = ctable.decode_onehot(outputs)
```

### Comparing `flax-0.7.0/examples/seq2seq/train_test.py` & `flax-0.7.1/examples/seq2seq/train_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -31,20 +31,24 @@
 
 
 def create_ctable(chars='0123456789+= '):
   return input_pipeline.CharacterTable(chars)
 
 
 def create_train_state(ctable):
-  model = models.Seq2seq(teacher_force=False,
-      hidden_size=train.FLAGS.hidden_size, vocab_size=ctable.vocab_size)
+  model = models.Seq2seq(
+      teacher_force=False,
+      hidden_size=train.FLAGS.hidden_size,
+      vocab_size=ctable.vocab_size,
+  )
   params = train.get_initial_params(model, jax.random.PRNGKey(0), ctable)
   tx = optax.adam(train.FLAGS.learning_rate)
   state = train_state.TrainState.create(
-      apply_fn=model.apply, params=params, tx=tx)
+      apply_fn=model.apply, params=params, tx=tx
+  )
   return state
 
 
 class TrainTest(absltest.TestCase):
 
   def test_character_table(self):
     ctable = create_ctable()
@@ -54,40 +58,34 @@
     # The text is possibly padded with whitespace, but the trimmed output should
     # be equal to the input.
     self.assertEqual(text, dec_text.strip())
 
   def test_mask_sequences(self):
     np.testing.assert_equal(
         input_pipeline.mask_sequences(
-            np.arange(1, 13).reshape((4, 3)),
-            np.array([3, 2, 1, 0])
+            np.arange(1, 13).reshape((4, 3)), np.array([3, 2, 1, 0])
         ),
-        np.array(
-            [[1, 2, 3],
-             [4, 5, 0],
-             [7, 0, 0],
-             [0, 0, 0]]
-        )
+        np.array([[1, 2, 3], [4, 5, 0], [7, 0, 0], [0, 0, 0]]),
     )
 
   def test_get_sequence_lengths(self):
     oh_sequence_batch = jax.vmap(
-        functools.partial(jax.nn.one_hot, num_classes=4))(
-            np.array([[0, 1, 0], [1, 0, 2], [1, 2, 0], [1, 2, 3]]))
+        functools.partial(jax.nn.one_hot, num_classes=4)
+    )(np.array([[0, 1, 0], [1, 0, 2], [1, 2, 0], [1, 2, 3]]))
     np.testing.assert_equal(
         input_pipeline.get_sequence_lengths(oh_sequence_batch, eos_id=0),
-        np.array([1, 2, 3, 3], np.int32)
+        np.array([1, 2, 3, 3], np.int32),
     )
     np.testing.assert_equal(
         input_pipeline.get_sequence_lengths(oh_sequence_batch, eos_id=1),
-        np.array([2, 1, 1, 1], np.int32)
+        np.array([2, 1, 1, 1], np.int32),
     )
     np.testing.assert_equal(
         input_pipeline.get_sequence_lengths(oh_sequence_batch, eos_id=2),
-        np.array([3, 3, 2, 2], np.int32)
+        np.array([3, 3, 2, 2], np.int32),
     )
 
   def test_train_one_step(self):
     ctable = create_ctable()
     batch = ctable.get_batch(128)
 
     state = create_train_state(ctable)
@@ -100,9 +98,10 @@
   def test_decode_batch(self):
     ctable = create_ctable()
     batch = ctable.get_batch(5)
     key = random.PRNGKey(0)
     state = create_train_state(ctable)
     train.decode_batch(state, batch, key, ctable)
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/examples/sst2/README.md` & `flax-0.7.1/examples/sst2/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/sst2/build_vocabulary.py` & `flax-0.7.1/examples/sst2/build_vocabulary.py`

 * *Files 3% similar despite different names*

```diff
@@ -22,21 +22,23 @@
 import tensorflow_datasets as tfds
 import tensorflow_text as tftext
 
 import vocabulary
 
 
 def get_tokenized_sequences(
-        dataset: tf.data.Dataset,
-        tokenizer: tftext.Tokenizer = tftext.WhitespaceTokenizer(),
-        input_key: str = 'sentence') -> Iterable[Sequence[bytes]]:
+    dataset: tf.data.Dataset,
+    tokenizer: tftext.Tokenizer = tftext.WhitespaceTokenizer(),
+    input_key: str = 'sentence',
+) -> Iterable[Sequence[bytes]]:
   """Returns tokenized sequences for vocabulary building."""
   dataset = dataset.map(
       lambda example: tokenizer.tokenize(example[input_key]),
-      num_parallel_calls=tf.data.experimental.AUTOTUNE)
+      num_parallel_calls=tf.data.experimental.AUTOTUNE,
+  )
   yield from tfds.as_numpy(dataset)
 
 
 if __name__ == '__main__':
   logging.set_verbosity(logging.INFO)
   start_time = time.time()
 
@@ -46,11 +48,12 @@
   # Tokenizes the sequences in the dataset and keeps only those.
   tokenized_sequences = get_tokenized_sequences(dataset)
 
   # Builds the vocabulary from the tokenized sequences.
   # A token needs to appear at least 3 times to be in the vocabulary. You can
   # play with this. It is there to make sure we don't overfit on rare words.
   vocab = vocabulary.Vocabulary(
-      tokenized_sequences=tokenized_sequences, min_freq=3)
+      tokenized_sequences=tokenized_sequences, min_freq=3
+  )
   vocab.save('vocab.txt')
 
   logging.info('Total time elapsed: %f s', time.time() - start_time)
```

### Comparing `flax-0.7.0/examples/sst2/configs/default.py` & `flax-0.7.1/examples/sst2/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/sst2/input_pipeline.py` & `flax-0.7.1/examples/sst2/input_pipeline.py`

 * *Files 6% similar despite different names*

```diff
@@ -109,48 +109,57 @@
   bucket_batch_sizes = [batch_size] * (len(bucket_boundaries) + 1)
   bucket_fn = tf.data.experimental.bucket_by_sequence_length(
       example_size_fn,
       bucket_boundaries,
       bucket_batch_sizes,
       padded_shapes=padded_shapes,
       pad_to_bucket_boundary=True,
-      drop_remainder=drop_remainder)
+      drop_remainder=drop_remainder,
+  )
 
   if shuffle:
     # For shuffling we need to know how many training examples we have.
     num_examples = get_num_examples(dataset)
     num_batches = num_examples // batch_size
-    return dataset.shuffle(
-        num_examples, seed=shuffle_seed,
-        reshuffle_each_iteration=True).apply(bucket_fn).shuffle(
-            num_batches,
-            seed=shuffle_seed,
-            reshuffle_each_iteration=True).prefetch(
-                tf.data.experimental.AUTOTUNE)
+    return (
+        dataset.shuffle(
+            num_examples, seed=shuffle_seed, reshuffle_each_iteration=True
+        )
+        .apply(bucket_fn)
+        .shuffle(num_batches, seed=shuffle_seed, reshuffle_each_iteration=True)
+        .prefetch(tf.data.experimental.AUTOTUNE)
+    )
   return dataset.apply(bucket_fn).prefetch(tf.data.experimental.AUTOTUNE)
 
 
-def vocab_to_hashtable(vocab: vocabulary.Vocabulary,
-                       unk_idx: int) -> tf.lookup.StaticHashTable:
+def vocab_to_hashtable(
+    vocab: vocabulary.Vocabulary, unk_idx: int
+) -> tf.lookup.StaticHashTable:
   """Returns a TF lookup table (token -> ID) from a vocabulary."""
   return tf.lookup.StaticHashTable(
       tf.lookup.KeyValueTensorInitializer(
-          list(vocab.keys()), list(vocab.values())), default_value=unk_idx)
+          list(vocab.keys()), list(vocab.values())
+      ),
+      default_value=unk_idx,
+  )
 
 
-def vocab_to_inverse_hashtable(vocab: vocabulary.Vocabulary,
-                               unk_token: bytes) -> tf.lookup.StaticHashTable:
+def vocab_to_inverse_hashtable(
+    vocab: vocabulary.Vocabulary, unk_token: bytes
+) -> tf.lookup.StaticHashTable:
   """Returns an inverse TF lookup table (ID -> token) from a vocabulary."""
   return tf.lookup.StaticHashTable(
       tf.lookup.KeyValueTensorInitializer(
           list(vocab.values()),
           list(vocab.keys()),
           key_dtype=tf.int64,
-          value_dtype=tf.string),
-      default_value=unk_token)
+          value_dtype=tf.string,
+      ),
+      default_value=unk_token,
+  )
 
 
 def _is_text_field(feature_name_and_type):
   """Identifies a text field when given a feature (name, type) pair."""
   _, feature_type = feature_name_and_type
   return isinstance(feature_type, tfds.features.Text)
 
@@ -160,19 +169,21 @@
   _, feature_type = feature_name_and_type
   return isinstance(feature_type, tfds.features.ClassLabel)
 
 
 class TextDataset:
   """A text dataset with one sequence as input and a label."""
 
-  def __init__(self,
-               tfds_name: str = 'glue/sst2',
-               vocab_path: str = 'vocab.txt',
-               tokenizer: text.Tokenizer = text.WhitespaceTokenizer(),
-               split='train'):
+  def __init__(
+      self,
+      tfds_name: str = 'glue/sst2',
+      vocab_path: str = 'vocab.txt',
+      tokenizer: text.Tokenizer = text.WhitespaceTokenizer(),
+      split='train',
+  ):
     """Initializes the SST2 data source."""
     self.dataset, self.info = tfds.load(tfds_name, split=split, with_info=True)
 
     # Look up the feature name of the text and label in the dataset.
     # We assume there is one text input and one label.
     text_fields = filter(_is_text_field, self.info.features.items())
     label_fields = filter(_is_class_label, self.info.features.items())
@@ -182,76 +193,83 @@
     # Load the vocabulary.
     self.vocab = vocabulary.Vocabulary(vocab_path=vocab_path)
 
     # Convert the sentences to sequences of token IDs and compute length.
     self.tokenizer = tokenizer
     self.tf_vocab = vocab_to_hashtable(self.vocab, unk_idx=self.vocab.unk_idx)
     self.examples = self.dataset.map(
-        self.prepare_example, num_parallel_calls=AUTOTUNE).cache()
+        self.prepare_example, num_parallel_calls=AUTOTUNE
+    ).cache()
 
   @property
   def padded_shapes(self):
     """The padded shapes used by batching functions."""
     # None means variable length; pads to the longest sequence in the batch.
     return {'idx': [], 'token_ids': [None], 'label': [], 'length': []}
 
   def example_length_fn(self, example: Example) -> tf.Tensor:
     """Returns the length of the example for the purpose of the bucketing."""
     return tf.size(example['token_ids'])
 
   def add_bos_eos(self, sequence: tf.Tensor) -> tf.Tensor:
     """Prepends BOS ID and appends EOS ID to a sequence of token IDs."""
-    return tf.concat(
-        [[self.vocab.bos_idx], sequence, [self.vocab.eos_idx]], 0)
+    return tf.concat([[self.vocab.bos_idx], sequence, [self.vocab.eos_idx]], 0)
 
   def prepare_example(self, example: Example) -> Example:
     """Prepares an example by converting text to token IDs."""
     tokens = self.tokenizer.tokenize(example[self.text_feature_name])
     label = example[self.label_feature_name]
     del example[self.text_feature_name]
     del example[self.label_feature_name]
     example['token_ids'] = self.add_bos_eos(self.tf_vocab.lookup(tokens))
     example['length'] = tf.size(example['token_ids'])
     example['label'] = label
     return example
 
-  def get_batches(self,
-                  batch_size: int,
-                  drop_remainder: bool = False,
-                  shuffle: bool = False,
-                  shuffle_seed: Optional[int] = None,
-                  fixed_pad_length: Optional[int] = None,
-                  dataset: Optional[tf.data.Dataset] = None):
+  def get_batches(
+      self,
+      batch_size: int,
+      drop_remainder: bool = False,
+      shuffle: bool = False,
+      shuffle_seed: Optional[int] = None,
+      fixed_pad_length: Optional[int] = None,
+      dataset: Optional[tf.data.Dataset] = None,
+  ):
     """Returns an iterator with padded batches for the provided dataset."""
     if dataset is None:
       dataset = self.examples
     if shuffle:
       buffer_size = get_num_examples(dataset)
       dataset = dataset.shuffle(
-          buffer_size, seed=shuffle_seed, reshuffle_each_iteration=True)
+          buffer_size, seed=shuffle_seed, reshuffle_each_iteration=True
+      )
     padded_shapes = {k: v for k, v in self.padded_shapes.items()}
     if fixed_pad_length is not None:
       padded_shapes['token_ids'] = fixed_pad_length
     return dataset.padded_batch(
-        batch_size, padded_shapes=padded_shapes, drop_remainder=drop_remainder)
+        batch_size, padded_shapes=padded_shapes, drop_remainder=drop_remainder
+    )
 
-  def get_bucketed_batches(self,
-                           batch_size: int,
-                           bucket_size: int,
-                           max_input_length: int,
-                           drop_remainder: bool = False,
-                           shuffle: bool = False,
-                           shuffle_seed: Optional[int] = None,
-                           dataset: Optional[tf.data.Dataset] = None):
+  def get_bucketed_batches(
+      self,
+      batch_size: int,
+      bucket_size: int,
+      max_input_length: int,
+      drop_remainder: bool = False,
+      shuffle: bool = False,
+      shuffle_seed: Optional[int] = None,
+      dataset: Optional[tf.data.Dataset] = None,
+  ):
     """Returns an iterator with bucketed batches for the provided dataset."""
     if dataset is None:
       dataset = self.examples
     return get_bucketed_batches(
         dataset,
         batch_size,
         bucket_size,
         max_input_length,
         self.padded_shapes,
         self.example_length_fn,
         shuffle=shuffle,
         shuffle_seed=shuffle_seed,
-        drop_remainder=drop_remainder)
+        drop_remainder=drop_remainder,
+    )
```

### Comparing `flax-0.7.0/examples/sst2/input_pipeline_test.py` & `flax-0.7.1/examples/sst2/input_pipeline_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,50 +42,53 @@
     vocab.save(vocab_path)
     return vocab_path
 
   def _get_dataset(self, vocab_path):
     """Uses mock data to create the dataset."""
     # Go two directories up to the root of the flax directory.
     flax_root_dir = pathlib.Path(__file__).parents[2]
-    data_dir = str(flax_root_dir) + \
-        '/.tfds/metadata'  # pylint: disable=unused-variable
+    data_dir = str(flax_root_dir) + '/.tfds/metadata'  # pylint: disable=unused-variable
     with tfds.testing.mock_data(num_examples=128, data_dir=data_dir):
       return input_pipeline.TextDataset(vocab_path=vocab_path, split='train')
 
   def test_bucketed_dataset(self):
     """Each batch should have a length that is a multiple of bucket_size."""
     batch_size = 2
     bucket_size = 8
     for batch in self.dataset.get_bucketed_batches(
-            batch_size=batch_size,
-            bucket_size=bucket_size, max_input_length=60, shuffle=False).take(3):
+        batch_size=batch_size,
+        bucket_size=bucket_size,
+        max_input_length=60,
+        shuffle=False,
+    ).take(3):
       # Because of bucketing, sequence length must be multiple of bucket_size.
       length = batch['token_ids'].numpy().shape[-1]
       self.assertEqual(0, length % bucket_size)
       self.assertEqual((batch_size,), batch['length'].numpy().shape)
       self.assertEqual((batch_size,), batch['label'].numpy().shape)
 
   def test_batched_dataset(self):
     """Tests that the length of a batch matches the longest sequence."""
     batch_size = 2
     for batch in self.dataset.get_batches(
-            batch_size=batch_size, shuffle=False).take(1):
+        batch_size=batch_size, shuffle=False
+    ).take(1):
       # Each batch is padded to the maximum sentence length in the batch.
       max_length_in_batch = max(batch['length'].numpy())
       length = batch['token_ids'].numpy().shape[-1]
       self.assertEqual(max_length_in_batch, length)
       self.assertEqual((batch_size,), batch['length'].numpy().shape)
       self.assertEqual((batch_size,), batch['label'].numpy().shape)
 
   def test_batched_dataset_fixed_length(self):
     """Tests that each batch has the fixed length."""
     batch_size = 2
     fixed_pad_length = 77
     for batch in self.dataset.get_batches(
-            batch_size=batch_size, shuffle=False,
-            fixed_pad_length=fixed_pad_length).take(1):
+        batch_size=batch_size, shuffle=False, fixed_pad_length=fixed_pad_length
+    ).take(1):
       length = batch['token_ids'].numpy().shape[-1]
       self.assertEqual(fixed_pad_length, length)
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/examples/sst2/main.py` & `flax-0.7.1/examples/wmt/main.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,15 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Main file for running the SST2 example.
+"""Main file for running the WMT example.
+
 This file is intentionally kept short. The majority for logic is in libraries
 that can be easily tested and imported in Colab.
 """
 
 from absl import app
 from absl import flags
 from absl import logging
@@ -29,17 +30,18 @@
 
 
 FLAGS = flags.FLAGS
 
 flags.DEFINE_string('workdir', None, 'Directory to store model data.')
 config_flags.DEFINE_config_file(
     'config',
-    None,
+    'configs/default.py',
     'File path to the training hyperparameter configuration.',
-    lock_config=True)
+    lock_config=True,
+)
 flags.mark_flags_as_required(['config', 'workdir'])
 
 
 def main(argv):
   if len(argv) > 1:
     raise app.UsageError('Too many command-line arguments.')
 
@@ -48,17 +50,21 @@
   tf.config.experimental.set_visible_devices([], 'GPU')
 
   logging.info('JAX process: %d / %d', jax.process_index(), jax.process_count())
   logging.info('JAX local devices: %r', jax.local_devices())
 
   # Add a note so that we can tell which task is which JAX host.
   # (Depending on the platform task 0 is not guaranteed to be host 0)
-  platform.work_unit().set_task_status(f'process_index: {jax.process_index()}, '
-                                       f'process_count: {jax.process_count()}')
-  platform.work_unit().create_artifact(platform.ArtifactType.DIRECTORY,
-                                       FLAGS.workdir, 'workdir')
+  platform.work_unit().set_task_status(
+      f'process_index: {jax.process_index()}, '
+      f'process_count: {jax.process_count()}'
+  )
+  platform.work_unit().create_artifact(
+      platform.ArtifactType.DIRECTORY, FLAGS.workdir, 'workdir'
+  )
 
   train.train_and_evaluate(FLAGS.config, FLAGS.workdir)
 
 
 if __name__ == '__main__':
+  jax.config.config_with_absl()
   app.run(main)
```

### Comparing `flax-0.7.0/examples/sst2/models.py` & `flax-0.7.1/examples/sst2/models.py`

 * *Files 9% similar despite different names*

```diff
@@ -81,23 +81,25 @@
 
 class WordDropout(nn.Module):
   """Applies word dropout to a batch of input IDs.
 
   This is basically the same as `nn.Dropout`, but allows specifying the
   value of dropped out items.
   """
+
   dropout_rate: float
   unk_idx: int
   deterministic: Optional[bool] = None
 
   @nn.compact
   def __call__(self, inputs: Array, deterministic: Optional[bool] = None):
     deterministic = nn.module.merge_param(
-        'deterministic', self.deterministic, deterministic)
-    if deterministic or self.dropout_rate == 0.:
+        'deterministic', self.deterministic, deterministic
+    )
+    if deterministic or self.dropout_rate == 0.0:
       return inputs
     rng = self.make_rng('dropout')
     mask = jax.random.bernoulli(rng, p=self.dropout_rate, shape=inputs.shape)
     return jnp.where(mask, jnp.array([self.unk_idx]), inputs)
 
 
 class Embedder(nn.Module):
@@ -108,96 +110,107 @@
     embedding_size: The dimensionality of the embeddings.
     embedding_init: The initializer used to initialize the embeddings.
     frozen: Freezes the embeddings table, keeping it fixed at initial values.
     dropout_rate: Percentage of units to drop after embedding the inputs.
     word_dropout_rate: Percentage of input words to replace with unk_idx.
     unk_idx: The index (integer) to use to replace inputs for word dropout.
   """
+
   vocab_size: int
   embedding_size: int
-  embedding_init: Callable[...,
-                           Array] = nn.initializers.normal(stddev=0.1)
+  embedding_init: Callable[..., Array] = nn.initializers.normal(stddev=0.1)
   frozen: bool = False
-  dropout_rate: float = 0.
-  word_dropout_rate: float = 0.
+  dropout_rate: float = 0.0
+  word_dropout_rate: float = 0.0
   unk_idx: Optional[int] = None
   deterministic: Optional[bool] = None
   dtype: jnp.dtype = jnp.float32
 
   def setup(self):
     self.embedding = self.param(
         'embedding',
         self.embedding_init,
-        (self.vocab_size,
-         self.embedding_size),
-        self.dtype)
+        (self.vocab_size, self.embedding_size),
+        self.dtype,
+    )
     self.dropout_layer = nn.Dropout(rate=self.dropout_rate)
     self.word_dropout_layer = WordDropout(
-        dropout_rate=self.word_dropout_rate,
-        unk_idx=self.unk_idx)
+        dropout_rate=self.word_dropout_rate, unk_idx=self.unk_idx
+    )
 
-  def __call__(self, inputs: Array,
-               deterministic: Optional[bool] = None) -> Array:
+  def __call__(
+      self, inputs: Array, deterministic: Optional[bool] = None
+  ) -> Array:
     """Embeds the input sequences and applies word dropout and dropout.
 
     Args:
       inputs: Batch of input token ID sequences <int64>[batch_size, seq_length].
       deterministic: Disables dropout when set to True.
 
     Returns:
       The embedded inputs, shape: <float32>[batch_size, seq_length,
       embedding_size].
     """
     deterministic = nn.module.merge_param(
-        'deterministic', self.deterministic, deterministic)
+        'deterministic', self.deterministic, deterministic
+    )
     inputs = self.word_dropout_layer(inputs, deterministic=deterministic)
     embedded_inputs = self.embedding[inputs]
 
     # Keep the embeddings fixed at initial (e.g. pretrained) values.
     if self.frozen:
       embedded_inputs = jax.lax.stop_gradient(embedded_inputs)
 
     return self.dropout_layer(embedded_inputs, deterministic=deterministic)
 
 
 class SimpleLSTM(nn.Module):
   """A simple unidirectional LSTM."""
+
   hidden_size: int
 
   @functools.partial(
       nn.transforms.scan,
       variable_broadcast='params',
-      in_axes=1, out_axes=1,
-      split_rngs={'params': False})
+      in_axes=1,
+      out_axes=1,
+      split_rngs={'params': False},
+  )
   @nn.compact
   def __call__(self, carry, x):
     return nn.OptimizedLSTMCell(self.hidden_size)(carry, x)
 
   def initialize_carry(self, input_shape):
     # Use fixed random key since default state init fn is just zeros.
     return nn.OptimizedLSTMCell(self.hidden_size, parent=None).initialize_carry(
-        jax.random.PRNGKey(0), input_shape)
+        jax.random.PRNGKey(0), input_shape
+    )
 
 
 class SimpleBiLSTM(nn.Module):
   """A simple bi-directional LSTM."""
+
   hidden_size: int
 
   def setup(self):
     self.forward_lstm = SimpleLSTM(self.hidden_size)
     self.backward_lstm = SimpleLSTM(self.hidden_size)
 
   def __call__(self, embedded_inputs, lengths):
     # Forward LSTM.
-    initial_state = self.forward_lstm.initialize_carry(embedded_inputs[:, 0].shape)
+    initial_state = self.forward_lstm.initialize_carry(
+        embedded_inputs[:, 0].shape
+    )
     _, forward_outputs = self.forward_lstm(initial_state, embedded_inputs)
 
     # Backward LSTM.
     reversed_inputs = flip_sequences(embedded_inputs, lengths)
-    initial_state = self.backward_lstm.initialize_carry(reversed_inputs[:, 0].shape)
+    initial_state = self.backward_lstm.initialize_carry(
+        reversed_inputs[:, 0].shape
+    )
     _, backward_outputs = self.backward_lstm(initial_state, reversed_inputs)
     backward_outputs = flip_sequences(backward_outputs, lengths)
 
     # Concatenate the forward and backward representations.
     outputs = jnp.concatenate([forward_outputs, backward_outputs], -1)
     return outputs
 
@@ -209,14 +222,15 @@
     hidden_size: The size of the hidden layer.
     output_size: The size of the output.
     activation: The activation function to apply to the hidden layer.
     dropout_rate: The dropout rate applied to the hidden layer.
     output_bias: If False, do not use a bias term in the last layer.
     deterministic: Disables dropout if set to True.
   """
+
   hidden_size: int
   output_size: int
   activation: Callable[..., Any] = nn.tanh
   dropout_rate: float = 0.0
   output_bias: bool = False
   deterministic: Optional[bool] = None
 
@@ -232,15 +246,16 @@
       inputs: <float32>[batch_size, ..., input_features].
       deterministic: Disables dropout when set to True.
 
     Returns:
       The MLP output <float32>[batch_size, ..., output_size]
     """
     deterministic = nn.module.merge_param(
-        'deterministic', self.deterministic, deterministic)
+        'deterministic', self.deterministic, deterministic
+    )
     hidden = self.intermediate_layer(inputs)
     hidden = self.activation(hidden)
     hidden = self.dropout_layer(hidden, deterministic=deterministic)
     output = self.output_layer(hidden)
     return output
 
 
@@ -256,14 +271,15 @@
   Bahdanau et al., 2015. Neural Machine Translation by Jointly Learning to
   Align and Translate. ICLR. https://arxiv.org/abs/1409.0473
   ```
 
   Attributes:
     hidden_size: The hidden size of the MLP that computes the attention score.
   """
+
   hidden_size: int
 
   @nn.compact
   def __call__(self, keys: Array, mask: Array) -> Array:
     """Applies model to the input keys and mask.
 
     Args:
@@ -295,47 +311,56 @@
   Attributes:
     hidden_size: The hidden size of the MLP classifier.
     output_size: The number of output classes for the classifier.
     dropout_rate: The dropout rate applied over the encoded_inputs, the summary
       of the inputs, and inside the MLP. Applied when `deterministic` is False.
     deterministic: Disables dropout if True.
   """
+
   hidden_size: int
   output_size: int
-  dropout_rate: float = 0.
+  dropout_rate: float = 0.0
   deterministic: Optional[bool] = None
 
   def setup(self):
     self.dropout_layer = nn.Dropout(rate=self.dropout_rate)
     self.keys_only_mlp_attention = KeysOnlyMlpAttention(
-        hidden_size=self.hidden_size)
+        hidden_size=self.hidden_size
+    )
     self.mlp = MLP(
         hidden_size=self.hidden_size,
         output_size=self.output_size,
         output_bias=False,
-        dropout_rate=self.dropout_rate)
+        dropout_rate=self.dropout_rate,
+    )
 
-  def __call__(self, encoded_inputs: Array, lengths: Array,
-               deterministic: Optional[bool] = None) -> Array:
+  def __call__(
+      self,
+      encoded_inputs: Array,
+      lengths: Array,
+      deterministic: Optional[bool] = None,
+  ) -> Array:
     """Applies model to the encoded inputs.
 
     Args:
       encoded_inputs: The inputs (e.g., sentences) that have already been
         encoded by some encoder, e.g., an LSTM. <float32>[batch_size,
         seq_length, encoded_inputs_size].
       lengths: The lengths of the inputs. <int64>[batch_size].
       deterministic: Disables dropout when set to True.
 
     Returns:
       An array of logits <float32>[batch_size, output_size].
     """
     deterministic = nn.module.merge_param(
-        'deterministic', self.deterministic, deterministic)
+        'deterministic', self.deterministic, deterministic
+    )
     encoded_inputs = self.dropout_layer(
-        encoded_inputs, deterministic=deterministic)
+        encoded_inputs, deterministic=deterministic
+    )
 
     # Compute attention. attention.shape: <float32>[batch_size, seq_len].
     mask = sequence_mask(lengths, encoded_inputs.shape[1])
     attention = self.keys_only_mlp_attention(encoded_inputs, mask)
 
     # Summarize the inputs by taking their weighted sum using attention scores.
     context = jnp.expand_dims(attention, 1) @ encoded_inputs
@@ -362,37 +387,50 @@
 
   def setup(self):
     self.embedder = Embedder(
         vocab_size=self.vocab_size,
         embedding_size=self.embedding_size,
         dropout_rate=self.dropout_rate,
         word_dropout_rate=self.word_dropout_rate,
-        unk_idx=self.unk_idx)
+        unk_idx=self.unk_idx,
+    )
     self.encoder = SimpleBiLSTM(hidden_size=self.hidden_size)
     self.classifier = AttentionClassifier(
         hidden_size=self.hidden_size,
         output_size=self.output_size,
-        dropout_rate=self.dropout_rate)
+        dropout_rate=self.dropout_rate,
+    )
 
-  def embed_token_ids(self, token_ids: Array,
-                      deterministic: Optional[bool] = None) -> Array:
+  def embed_token_ids(
+      self, token_ids: Array, deterministic: Optional[bool] = None
+  ) -> Array:
     deterministic = nn.module.merge_param(
-        'deterministic', self.deterministic, deterministic)
+        'deterministic', self.deterministic, deterministic
+    )
     return self.embedder(token_ids, deterministic=deterministic)
 
   def logits_from_embedded_inputs(
-      self, embedded_inputs: Array, lengths: Array,
-      deterministic: Optional[bool] = None) -> Array:
+      self,
+      embedded_inputs: Array,
+      lengths: Array,
+      deterministic: Optional[bool] = None,
+  ) -> Array:
     deterministic = nn.module.merge_param(
-        'deterministic', self.deterministic, deterministic)
+        'deterministic', self.deterministic, deterministic
+    )
     encoded_inputs = self.encoder(embedded_inputs, lengths)
-    return self.classifier(
-        encoded_inputs, lengths, deterministic=deterministic)
+    return self.classifier(encoded_inputs, lengths, deterministic=deterministic)
 
-  def __call__(self, token_ids: Array, lengths: Array,
-               deterministic: Optional[bool] = None) -> Array:
+  def __call__(
+      self,
+      token_ids: Array,
+      lengths: Array,
+      deterministic: Optional[bool] = None,
+  ) -> Array:
     """Embeds the token IDs, encodes them, and classifies with attention."""
     embedded_inputs = self.embed_token_ids(
-        token_ids, deterministic=deterministic)
+        token_ids, deterministic=deterministic
+    )
     logits = self.logits_from_embedded_inputs(
-        embedded_inputs, lengths, deterministic=deterministic)
-    return logits
+        embedded_inputs, lengths, deterministic=deterministic
+    )
+    return logits
```

### Comparing `flax-0.7.0/examples/sst2/models_test.py` & `flax-0.7.1/examples/sst2/models_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,64 +11,65 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for sst2.models."""
 from absl.testing import absltest
 from absl.testing import parameterized
+import models
 import jax
 from jax import numpy as jnp
 import jax.test_util
 import numpy as np
 
-import models
-
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 
 class ModelTest(parameterized.TestCase):
 
   def test_embedder_returns_correct_output_shape(self):
     """Tests if the embedder returns the correct shape."""
     vocab_size = 5
     embedding_size = 3
     model = models.Embedder(
-        vocab_size=vocab_size,
-        embedding_size=embedding_size)
+        vocab_size=vocab_size, embedding_size=embedding_size
+    )
     rng = jax.random.PRNGKey(0)
     token_ids = np.array([[2, 4, 3], [2, 6, 3]], dtype=np.int32)
     output, _ = model.init_with_output(rng, token_ids, deterministic=True)
     self.assertEqual((token_ids.shape) + (embedding_size,), output.shape)
 
   def test_lstm_returns_correct_output_shape(self):
     """Tests if the simple LSTM returns the correct shape."""
     batch_size = 2
     seq_len = 3
     embedding_size = 4
     hidden_size = 5
     model = models.SimpleLSTM(5)
     rng = jax.random.PRNGKey(0)
     inputs = np.random.RandomState(0).normal(
-        size=[batch_size, seq_len, embedding_size])
+        size=[batch_size, seq_len, embedding_size]
+    )
     initial_state = model.initialize_carry(inputs[:, 0].shape)
     (_, output), _ = model.init_with_output(rng, initial_state, inputs)
     self.assertEqual((batch_size, seq_len, hidden_size), output.shape)
 
   def test_bilstm_returns_correct_output_shape(self):
     """Tests if the simple BiLSTM returns the correct shape."""
     batch_size = 2
     seq_len = 3
     embedding_size = 4
     hidden_size = 5
     model = models.SimpleBiLSTM(hidden_size=hidden_size)
     rng = jax.random.PRNGKey(0)
     inputs = np.random.RandomState(0).normal(
-        size=[batch_size, seq_len, embedding_size])
+        size=[batch_size, seq_len, embedding_size]
+    )
     lengths = np.array([2, 3], dtype=np.int32)
     outputs, _ = model.init_with_output(rng, inputs, lengths)
     # We expect 2*hidden_size because we concatenate forward+backward LSTMs.
     self.assertEqual((batch_size, seq_len, 2 * hidden_size), outputs.shape)
 
   def test_text_classifier_returns_correct_output_shape(self):
     """Tests if a TextClassifier model returns the correct shape."""
@@ -84,19 +85,20 @@
         embedding_size=embedding_size,
         hidden_size=hidden_size,
         vocab_size=vocab_size,
         output_size=output_size,
         dropout_rate=dropout_rate,
         word_dropout_rate=word_dropout_rate,
         unk_idx=unk_idx,
-        deterministic=True)
+        deterministic=True,
+    )
 
     rng = jax.random.PRNGKey(0)
     token_ids = np.array([[2, 4, 3], [2, 6, 3]], dtype=np.int32)
     lengths = np.array([2, 3], dtype=np.int32)
     output, _ = model.init_with_output(rng, token_ids, lengths)
     batch_size = token_ids.shape[0]
     self.assertEqual((batch_size, output_size), output.shape)
 
 
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.7.0/examples/sst2/sst2.ipynb` & `flax-0.7.1/examples/sst2/sst2.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/sst2/train.py` & `flax-0.7.1/examples/sst2/train.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,24 +33,25 @@
 Array = jnp.ndarray
 Example = Dict[str, Array]
 TrainState = train_state.TrainState
 
 
 class Metrics(struct.PyTreeNode):
   """Computed metrics."""
+
   loss: float
   accuracy: float
   count: Optional[int] = None
 
 
 @jax.vmap
 def sigmoid_cross_entropy_with_logits(*, labels: Array, logits: Array) -> Array:
   """Sigmoid cross entropy loss."""
   zeros = jnp.zeros_like(logits, dtype=logits.dtype)
-  condition = (logits >= zeros)
+  condition = logits >= zeros
   relu_logits = jnp.where(condition, logits, zeros)
   neg_abs_logits = jnp.where(condition, -logits, logits)
   return relu_logits - logits * labels + jnp.log1p(jnp.exp(neg_abs_logits))
 
 
 def get_initial_params(rng, model):
   """Returns randomly initialized parameters."""
@@ -61,42 +62,45 @@
 
 
 def create_train_state(rng, config: ml_collections.ConfigDict, model):
   """Create initial training state."""
   params = get_initial_params(rng, model)
   tx = optax.chain(
       optax.sgd(learning_rate=config.learning_rate, momentum=config.momentum),
-      optax.add_decayed_weights(weight_decay=config.weight_decay))
+      optax.add_decayed_weights(weight_decay=config.weight_decay),
+  )
   state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)
   return state
 
 
 def compute_metrics(*, labels: Array, logits: Array) -> Metrics:
   """Computes the metrics, summed across the batch if a batch is provided."""
   if labels.ndim == 1:  # Prevent the labels from broadcasting over the logits.
     labels = jnp.expand_dims(labels, axis=1)
   loss = sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)
-  binary_predictions = (logits >= 0.)
+  binary_predictions = logits >= 0.0
   binary_accuracy = jnp.equal(binary_predictions, labels)
   return Metrics(
       loss=jnp.sum(loss),
       accuracy=jnp.sum(binary_accuracy),
-      count=logits.shape[0])
+      count=logits.shape[0],
+  )
 
 
 def model_from_config(config: ml_collections.ConfigDict):
   """Builds a text classification model from a config."""
   model = models.TextClassifier(
       embedding_size=config.embedding_size,
       hidden_size=config.hidden_size,
       vocab_size=config.vocab_size,
       output_size=config.output_size,
       dropout_rate=config.dropout_rate,
       word_dropout_rate=config.word_dropout_rate,
-      unk_idx=config.unk_idx)
+      unk_idx=config.unk_idx,
+  )
   return model
 
 
 def train_step(
     state: TrainState,
     batch: Dict[str, Array],
     rngs: Dict[str, Any],
@@ -105,134 +109,155 @@
   # Make sure to get a new RNG at every step.
   step = state.step
   rngs = {name: jax.random.fold_in(rng, step) for name, rng in rngs.items()}
 
   def loss_fn(params):
     variables = {'params': params}
     logits = state.apply_fn(
-        variables, batch['token_ids'], batch['length'],
+        variables,
+        batch['token_ids'],
+        batch['length'],
         deterministic=False,
-        rngs=rngs)
+        rngs=rngs,
+    )
 
     labels = batch['label']
     if labels.ndim == 1:
       labels = jnp.expand_dims(labels, 1)
     loss = jnp.mean(
-        sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))
+        sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)
+    )
     return loss, logits
 
   grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
   value, grads = grad_fn(state.params)
   (_, logits) = value
 
   new_state = state.apply_gradients(grads=grads)
   metrics = compute_metrics(labels=batch['label'], logits=logits)
   return new_state, metrics
 
 
-def eval_step(state: TrainState, batch: Dict[str, Array],
-              rngs: Dict[str, Any]) -> Metrics:
+def eval_step(
+    state: TrainState, batch: Dict[str, Array], rngs: Dict[str, Any]
+) -> Metrics:
   """Evaluate for a single step. Model should be in deterministic mode."""
   variables = {'params': state.params}
   logits = state.apply_fn(
-      variables, batch['token_ids'], batch['length'],
+      variables,
+      batch['token_ids'],
+      batch['length'],
       deterministic=True,
-      rngs=rngs)
+      rngs=rngs,
+  )
   metrics = compute_metrics(labels=batch['label'], logits=logits)
   return metrics
 
 
-def normalize_batch_metrics(
-        batch_metrics: Sequence[Metrics]) -> Metrics:
+def normalize_batch_metrics(batch_metrics: Sequence[Metrics]) -> Metrics:
   """Consolidates and normalizes a list of per-batch metrics dicts."""
   # Here we sum the metrics that were already summed per batch.
   total_loss = np.sum([metrics.loss for metrics in batch_metrics])
   total_accuracy = np.sum([metrics.accuracy for metrics in batch_metrics])
   total = np.sum([metrics.count for metrics in batch_metrics])
   # Divide each metric by the total number of items in the data set.
   return Metrics(
-      loss=total_loss.item() / total, accuracy=total_accuracy.item() / total)
+      loss=total_loss.item() / total, accuracy=total_accuracy.item() / total
+  )
 
 
 def batch_to_numpy(batch: Dict[str, tf.Tensor]) -> Dict[str, Array]:
   """Converts a batch with TF tensors to a batch of NumPy arrays."""
   # _numpy() reuses memory, does not make a copy.
   # pylint: disable=protected-access
   return jax.tree_util.tree_map(lambda x: x._numpy(), batch)
 
 
 def evaluate_model(
-        eval_step_fn: Callable[..., Any],
-        state: TrainState,
-        batches: Union[Iterable[Example], tf.data.Dataset],
-        epoch: int,
-        rngs: Optional[Dict[str, Any]] = None
+    eval_step_fn: Callable[..., Any],
+    state: TrainState,
+    batches: Union[Iterable[Example], tf.data.Dataset],
+    epoch: int,
+    rngs: Optional[Dict[str, Any]] = None,
 ) -> Metrics:
   """Evaluate a model on a dataset."""
   batch_metrics = []
   for i, batch in enumerate(batches):
     batch = batch_to_numpy(batch)
     if rngs is not None:  # New RNG for each step.
       rngs = {name: jax.random.fold_in(rng, i) for name, rng in rngs.items()}
 
     metrics = eval_step_fn(state, batch, rngs)
     batch_metrics.append(metrics)
 
   batch_metrics = jax.device_get(batch_metrics)
   metrics = normalize_batch_metrics(batch_metrics)
-  logging.info('eval  epoch %03d loss %.4f accuracy %.2f', epoch,
-               metrics.loss, metrics.accuracy * 100)
+  logging.info(
+      'eval  epoch %03d loss %.4f accuracy %.2f',
+      epoch,
+      metrics.loss,
+      metrics.accuracy * 100,
+  )
   return metrics
 
 
-def train_epoch(train_step_fn: Callable[..., Tuple[TrainState, Metrics]],
-                state: TrainState,
-                train_batches: tf.data.Dataset,
-                epoch: int,
-                rngs: Optional[Dict[str, Any]] = None
-                ) -> Tuple[TrainState, Metrics]:
+def train_epoch(
+    train_step_fn: Callable[..., Tuple[TrainState, Metrics]],
+    state: TrainState,
+    train_batches: tf.data.Dataset,
+    epoch: int,
+    rngs: Optional[Dict[str, Any]] = None,
+) -> Tuple[TrainState, Metrics]:
   """Train for a single epoch."""
   batch_metrics = []
   for batch in train_batches:
     batch = batch_to_numpy(batch)
     state, metrics = train_step_fn(state, batch, rngs)
     batch_metrics.append(metrics)
 
   # Compute the metrics for this epoch.
   batch_metrics = jax.device_get(batch_metrics)
   metrics = normalize_batch_metrics(batch_metrics)
 
-  logging.info('train epoch %03d loss %.4f accuracy %.2f', epoch,
-               metrics.loss, metrics.accuracy * 100)
+  logging.info(
+      'train epoch %03d loss %.4f accuracy %.2f',
+      epoch,
+      metrics.loss,
+      metrics.accuracy * 100,
+  )
 
   return state, metrics
 
 
-def train_and_evaluate(config: ml_collections.ConfigDict,
-                       workdir: str) -> TrainState:
+def train_and_evaluate(
+    config: ml_collections.ConfigDict, workdir: str
+) -> TrainState:
   """Execute model training and evaluation loop.
 
   Args:
     config: Hyperparameter configuration for training and evaluation.
     workdir: Directory where the tensorboard summaries are written to.
   Returns:
     The final train state that includes the trained parameters.
   """
   # Prepare datasets.
   train_dataset = input_pipeline.TextDataset(
-      tfds_name='glue/sst2', split='train')
+      tfds_name='glue/sst2', split='train'
+  )
   eval_dataset = input_pipeline.TextDataset(
-      tfds_name='glue/sst2', split='validation')
+      tfds_name='glue/sst2', split='validation'
+  )
   train_batches = train_dataset.get_bucketed_batches(
       config.batch_size,
       config.bucket_size,
       max_input_length=config.max_input_length,
       drop_remainder=True,
       shuffle=True,
-      shuffle_seed=config.seed)
+      shuffle_seed=config.seed,
+  )
   eval_batches = eval_dataset.get_batches(batch_size=config.batch_size)
 
   # Keep track of vocab size in the config so that the embedder knows it.
   config.vocab_size = len(train_dataset.vocab)
 
   # Compile step functions.
   train_step_fn = jax.jit(train_step)
@@ -245,31 +270,25 @@
 
   summary_writer = tensorboard.SummaryWriter(workdir)
   summary_writer.hparams(dict(config))
 
   # Main training loop.
   logging.info('Starting training...')
   for epoch in range(1, config.num_epochs + 1):
-
     # Train for one epoch.
     rng, epoch_rng = jax.random.split(rng)
     rngs = {'dropout': epoch_rng}
     state, train_metrics = train_epoch(
-        train_step_fn, state, train_batches, epoch, rngs)
+        train_step_fn, state, train_batches, epoch, rngs
+    )
 
     # Evaluate current model on the validation data.
     eval_metrics = evaluate_model(eval_step_fn, state, eval_batches, epoch)
 
     # Write metrics to TensorBoard.
     summary_writer.scalar('train_loss', train_metrics.loss, epoch)
-    summary_writer.scalar(
-        'train_accuracy',
-        train_metrics.accuracy * 100,
-        epoch)
+    summary_writer.scalar('train_accuracy', train_metrics.accuracy * 100, epoch)
     summary_writer.scalar('eval_loss', eval_metrics.loss, epoch)
-    summary_writer.scalar(
-        'eval_accuracy',
-        eval_metrics.accuracy * 100,
-        epoch)
+    summary_writer.scalar('eval_accuracy', eval_metrics.accuracy * 100, epoch)
 
   summary_writer.flush()
   return state
```

### Comparing `flax-0.7.0/examples/sst2/train_test.py` & `flax-0.7.1/examples/sst2/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/sst2/vocab.txt` & `flax-0.7.1/examples/sst2/vocab.txt`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/sst2/vocabulary.py` & `flax-0.7.1/examples/sst2/vocabulary.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,41 +19,47 @@
 
 from absl import logging
 
 
 class Vocabulary:
   """Represents a vocabulary that can be built from a dataset."""
 
-  def __init__(self,
-               vocab_path: Optional[str] = None,
-               tokenized_sequences: Optional[Iterable[Sequence[bytes]]] = None,
-               min_freq: int = 1,
-               pad_token: bytes = b'<pad>',
-               unk_token: bytes = b'<unk>',
-               bos_token: bytes = b'<s>',
-               eos_token: bytes = b'</s>'):
+  def __init__(
+      self,
+      vocab_path: Optional[str] = None,
+      tokenized_sequences: Optional[Iterable[Sequence[bytes]]] = None,
+      min_freq: int = 1,
+      pad_token: bytes = b'<pad>',
+      unk_token: bytes = b'<unk>',
+      bos_token: bytes = b'<s>',
+      eos_token: bytes = b'</s>',
+  ):
     """Loads the vocab from disk (if `vocab_path` is given) or builds it from `tokenized_sequences`."""
     self.pad_token = pad_token
     self.unk_token = unk_token
     self.bos_token = bos_token
     self.eos_token = eos_token
     self.special_tokens = (pad_token, unk_token, bos_token, eos_token)
 
     if vocab_path:
       self.load(vocab_path)
     elif tokenized_sequences is not None:
       self.build(tokenized_sequences, min_freq=min_freq)
     else:
       raise ValueError(
-          ('Vocabulary needs either `vocab_path` or `tokenized_sequences` to '
-           'be provided, got %r and %r.') % (vocab_path, tokenized_sequences))
-
-  def build(self,
-            tokenized_sequences: Iterable[Sequence[bytes]],
-            min_freq: int = 1):
+          (
+              'Vocabulary needs either `vocab_path` or `tokenized_sequences` to'
+              ' be provided, got %r and %r.'
+          )
+          % (vocab_path, tokenized_sequences)
+      )
+
+  def build(
+      self, tokenized_sequences: Iterable[Sequence[bytes]], min_freq: int = 1
+  ):
     """Builds a vocabulary over tokens with optional minimum frequency.
 
     Args:
       tokenized_sequences: Iterable of token sequences to build the vocabulary.
       min_freq: The minimum frequency of each token to be included. Default: 1.
     """
     # Count all the tokens.
@@ -67,15 +73,16 @@
       vocab[token] = len(vocab)
 
     # Add all other tokens to the vocab if their frequency is >= min_freq.
     for token, freq in sorted(
         # Sort by frequency (from high to low), and then by token string.
         # This makes sure high frequency tokens get a low token ID.
         counter.items(),
-        key=lambda token_freq: (-token_freq[1], token_freq[0])):
+        key=lambda token_freq: (-token_freq[1], token_freq[0]),
+    ):
       if freq >= min_freq:
         vocab[token] = len(vocab)
 
     logging.info('Number of unfiltered tokens: %d', len(counter))
     logging.info('Vocabulary size: %d', len(vocab))
     self.vocab = vocab
```

### Comparing `flax-0.7.0/examples/vae/README.md` & `flax-0.7.1/examples/vae/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/vae/input_pipeline.py` & `flax-0.7.1/examples/vae/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/vae/models.py` & `flax-0.7.1/examples/vae/models.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 from flax import linen as nn
 from jax import random
 import jax.numpy as jnp
 
 
 class Encoder(nn.Module):
   """VAE Encoder."""
+
   latents: int
 
   @nn.compact
   def __call__(self, x):
     x = nn.Dense(500, name='fc1')(x)
     x = nn.relu(x)
     mean_x = nn.Dense(self.latents, name='fc2_mean')(x)
@@ -41,14 +42,15 @@
     z = nn.relu(z)
     z = nn.Dense(784, name='fc2')(z)
     return z
 
 
 class VAE(nn.Module):
   """Full VAE model."""
+
   latents: int = 20
 
   def setup(self):
     self.encoder = Encoder(self.latents)
     self.decoder = Decoder()
 
   def __call__(self, x, z_rng):
```

### Comparing `flax-0.7.0/examples/vae/reconstruction.png` & `flax-0.7.1/examples/vae/reconstruction.png`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/vae/sample.png` & `flax-0.7.1/examples/vae/sample.png`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/vae/train.py` & `flax-0.7.1/examples/vae/train.py`

 * *Files 4% similar despite different names*

```diff
@@ -46,45 +46,47 @@
 def kl_divergence(mean, logvar):
   return -0.5 * jnp.sum(1 + logvar - jnp.square(mean) - jnp.exp(logvar))
 
 
 @jax.vmap
 def binary_cross_entropy_with_logits(logits, labels):
   logits = nn.log_sigmoid(logits)
-  return -jnp.sum(labels * logits + (1. - labels) * jnp.log(-jnp.expm1(logits)))
+  return -jnp.sum(
+      labels * logits + (1.0 - labels) * jnp.log(-jnp.expm1(logits))
+  )
 
 
 def compute_metrics(recon_x, x, mean, logvar):
   bce_loss = binary_cross_entropy_with_logits(recon_x, x).mean()
   kld_loss = kl_divergence(mean, logvar).mean()
-  return {
-      'bce': bce_loss,
-      'kld': kld_loss,
-      'loss': bce_loss + kld_loss
-  }
+  return {'bce': bce_loss, 'kld': kld_loss, 'loss': bce_loss + kld_loss}
 
 
 def train_step(state, batch, z_rng, latents):
   def loss_fn(params):
-    recon_x, mean, logvar = models.model(latents).apply({'params': params},
-                                                      batch, z_rng)
+    recon_x, mean, logvar = models.model(latents).apply(
+        {'params': params}, batch, z_rng
+    )
 
     bce_loss = binary_cross_entropy_with_logits(recon_x, batch).mean()
     kld_loss = kl_divergence(mean, logvar).mean()
     loss = bce_loss + kld_loss
     return loss
+
   grads = jax.grad(loss_fn)(state.params)
   return state.apply_gradients(grads=grads)
 
 
 def eval_f(params, images, z, z_rng, latents):
   def eval_model(vae):
     recon_images, mean, logvar = vae(images, z_rng)
-    comparison = jnp.concatenate([images[:8].reshape(-1, 28, 28, 1),
-                                  recon_images[:8].reshape(-1, 28, 28, 1)])
+    comparison = jnp.concatenate([
+        images[:8].reshape(-1, 28, 28, 1),
+        recon_images[:8].reshape(-1, 28, 28, 1),
+    ])
 
     generate_images = vae.generate(z)
     generate_images = generate_images.reshape(-1, 28, 28, 1)
     metrics = compute_metrics(recon_images, images, mean, logvar)
     return metrics, comparison, generate_images
 
   return nn.apply(eval_model, models.model(latents))({'params': params})
@@ -111,25 +113,28 @@
       params=params,
       tx=optax.adam(learning_rate),
   )
 
   rng, z_key, eval_rng = random.split(rng, 3)
   z = random.normal(z_key, (64, latents))
 
-  steps_per_epoch = ds_builder.info.splits["train"].num_examples // batch_size
+  steps_per_epoch = ds_builder.info.splits['train'].num_examples // batch_size
 
   for epoch in range(num_epochs):
     for _ in range(steps_per_epoch):
       batch = next(train_ds)
       rng, key = random.split(rng)
       state = train_step(state, batch, key, latents)
 
-    metrics, comparison, sample = eval_f(state.params, test_ds, z, eval_rng,
-                                         latents)
+    metrics, comparison, sample = eval_f(
+        state.params, test_ds, z, eval_rng, latents
+    )
     vae_utils.save_image(
-        comparison, f'results/reconstruction_{epoch}.png', nrow=8)
+        comparison, f'results/reconstruction_{epoch}.png', nrow=8
+    )
     vae_utils.save_image(sample, f'results/sample_{epoch}.png', nrow=8)
 
-    print('eval epoch: {}, loss: {:.4f}, BCE: {:.4f}, KLD: {:.4f}'.format(
-        epoch + 1, metrics['loss'], metrics['bce'], metrics['kld']
-    ))
-
+    print(
+        'eval epoch: {}, loss: {:.4f}, BCE: {:.4f}, KLD: {:.4f}'.format(
+            epoch + 1, metrics['loss'], metrics['bce'], metrics['kld']
+        )
+    )
```

### Comparing `flax-0.7.0/examples/vae/utils.py` & `flax-0.7.1/examples/vae/utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -50,39 +50,49 @@
     padding (int, optional): amount of padding. Default: ``2``.
     pad_value (float, optional): Value for the padded pixels. Default: ``0``.
     format_img(Optional):  If omitted, the format to use is determined from the
       filename extension. If a file object was used instead of a filename,
       this parameter should always be used.
   """
 
-  if not (isinstance(ndarray, jnp.ndarray) or
-          (isinstance(ndarray, list) and all(isinstance(t, jnp.ndarray) for t
-                                             in ndarray))):
+  if not (
+      isinstance(ndarray, jnp.ndarray)
+      or (
+          isinstance(ndarray, list)
+          and all(isinstance(t, jnp.ndarray) for t in ndarray)
+      )
+  ):
     raise TypeError(f'array_like of tensors expected, got {type(ndarray)}')
 
   ndarray = jnp.asarray(ndarray)
 
   if ndarray.ndim == 4 and ndarray.shape[-1] == 1:  # single-channel images
     ndarray = jnp.concatenate((ndarray, ndarray, ndarray), -1)
 
   # make the mini-batch of images into a grid
   nmaps = ndarray.shape[0]
   xmaps = min(nrow, nmaps)
   ymaps = int(math.ceil(float(nmaps) / xmaps))
-  height, width = (int(ndarray.shape[1] + padding),
-                   int(ndarray.shape[2] + padding))
+  height, width = (
+      int(ndarray.shape[1] + padding),
+      int(ndarray.shape[2] + padding),
+  )
   num_channels = ndarray.shape[3]
-  grid = jnp.full((height * ymaps + padding, width * xmaps + padding,
-                   num_channels), pad_value).astype(jnp.float32)
+  grid = jnp.full(
+      (height * ymaps + padding, width * xmaps + padding, num_channels),
+      pad_value,
+  ).astype(jnp.float32)
   k = 0
   for y in range(ymaps):
     for x in range(xmaps):
       if k >= nmaps:
         break
-      grid = grid.at[y * height + padding:(y + 1) * height,
-                     x * width + padding:(x + 1) * width].set(ndarray[k])
+      grid = grid.at[
+          y * height + padding : (y + 1) * height,
+          x * width + padding : (x + 1) * width,
+      ].set(ndarray[k])
       k = k + 1
 
   # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer
   ndarr = np.array(jnp.clip(grid * 255.0 + 0.5, 0, 255).astype(jnp.uint8))
   im = Image.fromarray(ndarr.copy())
   im.save(fp, format=format_img)
```

### Comparing `flax-0.7.0/examples/wmt/README.md` & `flax-0.7.1/examples/wmt/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/wmt/bleu.py` & `flax-0.7.1/examples/wmt/bleu.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,15 +56,16 @@
     self.punct_nondigit_re = re.compile(r"([" + punctuation + r"])([^\d])")
     self.symbol_re = re.compile("([" + self.property_chars("S") + "])")
 
   def property_chars(self, prefix):
     return "".join(
         chr(x)
         for x in range(sys.maxunicode)
-        if unicodedata.category(chr(x)).startswith(prefix))
+        if unicodedata.category(chr(x)).startswith(prefix)
+    )
 
 
 uregex = UnicodeRegex()
 
 
 def bleu_tokenize(string):
   r"""Tokenize a string following the official BLEU implementation.
@@ -105,22 +106,20 @@
   Returns:
     The Counter containing all n-grams up to max_order in segment
     with a count of how many times each n-gram occurred.
   """
   ngram_counts = collections.Counter()
   for order in range(1, max_order + 1):
     for i in range(0, len(segment) - order + 1):
-      ngram = tuple(segment[i:i + order])
+      ngram = tuple(segment[i : i + order])
       ngram_counts[ngram] += 1
   return ngram_counts
 
 
-def compute_bleu_matches(reference_corpus,
-                 translation_corpus,
-                 max_order=4):
+def compute_bleu_matches(reference_corpus, translation_corpus, max_order=4):
   """Computes BLEU match stats of translations against one or more references.
 
   Args:
     reference_corpus: list of references for each translation. Each reference
       should be tokenized into a list of tokens.
     translation_corpus: list of translations to score. Each translation should
       be tokenized into a list of tokens.
@@ -134,54 +133,62 @@
   bp = 1.0
   geo_mean = 0
 
   matches_by_order = [0] * max_order
   possible_matches_by_order = [0] * max_order
   precisions = []
 
-  for (references, translations) in zip(reference_corpus, translation_corpus):
+  for references, translations in zip(reference_corpus, translation_corpus):
     reference_length += len(references)
     translation_length += len(translations)
     ref_ngram_counts = _get_ngrams(references, max_order)
     translation_ngram_counts = _get_ngrams(translations, max_order)
 
-    overlap = {ngram: min(count, translation_ngram_counts[ngram])
-                   for ngram, count in ref_ngram_counts.items()}
+    overlap = {
+        ngram: min(count, translation_ngram_counts[ngram])
+        for ngram, count in ref_ngram_counts.items()
+    }
 
     for ngram in overlap:
       matches_by_order[len(ngram) - 1] += overlap[ngram]
     for ngram in translation_ngram_counts:
-      possible_matches_by_order[len(ngram) -
-                                1] += translation_ngram_counts[ngram]
-
-  return (np.array(matches_by_order),
-          np.array(possible_matches_by_order),
-          np.array(reference_length),
-          np.array(translation_length))
+      possible_matches_by_order[len(ngram) - 1] += translation_ngram_counts[
+          ngram
+      ]
+
+  return (
+      np.array(matches_by_order),
+      np.array(possible_matches_by_order),
+      np.array(reference_length),
+      np.array(translation_length),
+  )
 
 
 def bleu_partial(ref_lines, hyp_lines, case_sensitive=False):
   """Compute n-gram statistics for two lists of references and translations."""
   if len(ref_lines) != len(hyp_lines):
-    raise ValueError("Reference and translation lists have different "
-                     "numbers of lines.")
+    raise ValueError(
+        "Reference and translation lists have different numbers of lines."
+    )
   if not case_sensitive:
     ref_lines = [x.lower() for x in ref_lines]
     hyp_lines = [x.lower() for x in hyp_lines]
   ref_tokens = [bleu_tokenize(x) for x in ref_lines]
   hyp_tokens = [bleu_tokenize(x) for x in hyp_lines]
   return compute_bleu_matches(ref_tokens, hyp_tokens)
 
 
-def complete_bleu(matches_by_order,
-                  possible_matches_by_order,
-                  reference_length,
-                  translation_length,
-                  max_order=4,
-                  use_bp=True):
+def complete_bleu(
+    matches_by_order,
+    possible_matches_by_order,
+    reference_length,
+    translation_length,
+    max_order=4,
+    use_bp=True,
+):
   """Compute BLEU score from aggregated n-gram statistics."""
   precisions = [0] * max_order
   smooth = 1.0
   geo_mean = 0.0
   for i in range(0, max_order):
     if possible_matches_by_order[i] > 0:
       precisions[i] = matches_by_order[i] / possible_matches_by_order[i]
@@ -203,15 +210,15 @@
     else:
       ratio = translation_length / reference_length
       if ratio <= 0.0:
         bp = 0.0
       elif ratio >= 1.0:
         bp = 1.0
       else:
-        bp = math.exp(1 - 1. / ratio)
+        bp = math.exp(1 - 1.0 / ratio)
   bleu = geo_mean * bp
   return float(bleu) * 100.0
 
 
 def bleu_local(ref_lines, hyp_lines, case_sensitive=False):
   """Compute BLEU for two lists of reference and hypothesis translations."""
   stats = bleu_partial(ref_lines, hyp_lines, case_sensitive=case_sensitive)
```

### Comparing `flax-0.7.0/examples/wmt/configs/default.py` & `flax-0.7.1/examples/wmt/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/examples/wmt/decode.py` & `flax-0.7.1/examples/wmt/decode.py`

 * *Files 6% similar despite different names*

```diff
@@ -87,20 +87,23 @@
 
   Returns:
     New pytree with new beam arrays.
     [batch_size, old_beam_size, ...] --> [batch_size, new_beam_size, ...]
   """
   batch_indices = jnp.reshape(
       jnp.arange(batch_size * new_beam_size) // new_beam_size,
-      (batch_size, new_beam_size))
+      (batch_size, new_beam_size),
+  )
+
   def gather_fn(x):
     if x.ndim == 0:  # ignore scalars (e.g. cache index)
       return x
     else:
       return x[batch_indices, beam_indices]
+
   return jax.tree_util.tree_map(gather_fn, nested)
 
 
 def gather_topk_beams(nested, score_or_log_prob, batch_size, new_beam_size):
   """Gathers the top-k beam slices given by score_or_log_prob array.
 
   Args:
@@ -121,14 +124,15 @@
 
 # Beam search state:
 
 
 @flax.struct.dataclass
 class BeamState:
   """Holds beam search state data."""
+
   # The position of the decoding loop in the length dimension.
   cur_index: jax.Array  # scalar int32: current decoded length index
   # The active sequence log probabilities and finished sequence scores.
   live_logprobs: jax.Array  # float32: [batch_size, beam_size]
   finished_scores: jax.Array  # float32: [batch_size, beam_size]
   # The current active-beam-searching and finished sequences.
   live_seqs: jax.Array  # int32: [batch_size, beam_size, max_decode_len]
@@ -140,43 +144,47 @@
   cache: typing.Any  # Any pytree of arrays, e.g. flax attention Cache object
 
 
 def beam_init(batch_size, beam_size, max_decode_len, cache):
   """Initializes the beam search state data structure."""
   cur_index0 = jnp.array(0)
   live_logprobs0 = jnp.tile(
-      jnp.array([0.0] + [NEG_INF] * (beam_size - 1)),
-      [batch_size, 1])
+      jnp.array([0.0] + [NEG_INF] * (beam_size - 1)), [batch_size, 1]
+  )
   finished_scores0 = jnp.ones((batch_size, beam_size)) * NEG_INF
-  live_seqs0 = jnp.zeros(
-      (batch_size, beam_size, max_decode_len), jnp.int32)
-  finished_seqs0 = jnp.zeros(
-      (batch_size, beam_size, max_decode_len), jnp.int32)
+  live_seqs0 = jnp.zeros((batch_size, beam_size, max_decode_len), jnp.int32)
+  finished_seqs0 = jnp.zeros((batch_size, beam_size, max_decode_len), jnp.int32)
   finished_flags0 = jnp.zeros((batch_size, beam_size), jnp.bool_)
   # add beam dimension to attention cache pytree elements
-  beam_cache0 = jax.tree_util.tree_map(lambda x: add_beam_dim(x, beam_size), cache)
-  return BeamState(cur_index=cur_index0,
-                   live_logprobs=live_logprobs0,
-                   finished_scores=finished_scores0,
-                   live_seqs=live_seqs0,
-                   finished_seqs=finished_seqs0,
-                   finished_flags=finished_flags0,
-                   cache=beam_cache0)
+  beam_cache0 = jax.tree_util.tree_map(
+      lambda x: add_beam_dim(x, beam_size), cache
+  )
+  return BeamState(
+      cur_index=cur_index0,
+      live_logprobs=live_logprobs0,
+      finished_scores=finished_scores0,
+      live_seqs=live_seqs0,
+      finished_seqs=finished_seqs0,
+      finished_flags=finished_flags0,
+      cache=beam_cache0,
+  )
 
 
 # Beam search routine:
 
 
-def beam_search(inputs,
-                cache,
-                tokens_to_logits,
-                beam_size=4,
-                alpha=0.6,
-                eos_id=EOS_ID,
-                max_decode_len=None):
+def beam_search(
+    inputs,
+    cache,
+    tokens_to_logits,
+    beam_size=4,
+    alpha=0.6,
+    eos_id=EOS_ID,
+    max_decode_len=None,
+):
   """Beam search for transformer machine translation.
 
   Args:
     inputs: array: [batch_size, length] int32 sequence of tokens.
     cache: flax attention cache.
     tokens_to_logits: fast autoregressive decoder function taking single token
       slices and cache and returning next-token logits and updated cache.
@@ -194,74 +202,78 @@
 
   batch_size = inputs.shape[0]
   if max_decode_len is None:
     max_decode_len = inputs.shape[1]
   end_marker = jnp.array(eos_id)
 
   # initialize beam search state
-  beam_search_init_state = beam_init(batch_size,
-                                     beam_size,
-                                     max_decode_len,
-                                     cache)
+  beam_search_init_state = beam_init(
+      batch_size, beam_size, max_decode_len, cache
+  )
 
   def beam_search_loop_cond_fn(state):
     """Beam search loop termination condition."""
     # Have we reached max decoding length?
-    not_at_end = (state.cur_index < max_decode_len - 1)
+    not_at_end = state.cur_index < max_decode_len - 1
 
     # Is no further progress in the beam search possible?
     # Get the best possible scores from alive sequences.
     min_brevity_penalty = brevity_penalty(alpha, max_decode_len)
     best_live_scores = state.live_logprobs[:, -1:] / min_brevity_penalty
     # Get the worst scores from finished sequences.
     worst_finished_scores = jnp.min(
-        state.finished_scores, axis=1, keepdims=True)
+        state.finished_scores, axis=1, keepdims=True
+    )
     # Mask out scores from slots without any actual finished sequences.
     worst_finished_scores = jnp.where(
-        state.finished_flags, worst_finished_scores, NEG_INF)
+        state.finished_flags, worst_finished_scores, NEG_INF
+    )
     # If no best possible live score is better than current worst finished
     # scores, the search cannot improve the finished set further.
     search_terminated = jnp.all(worst_finished_scores > best_live_scores)
 
     # If we're not at the max decode length, and the search hasn't terminated,
     # continue looping.
     return not_at_end & (~search_terminated)
 
   def beam_search_loop_body_fn(state):
     """Beam search loop state update function."""
     # Collect the current position slice along length to feed the fast
     # autoregressive decoder model.  Flatten the beam dimension into batch
     # dimension for feeding into the model.
     # --> [batch * beam, 1]
-    flat_ids = flatten_beam_dim(lax.dynamic_slice(
-        state.live_seqs,
-        (0, 0, state.cur_index),
-        (batch_size, beam_size, 1)))
+    flat_ids = flatten_beam_dim(
+        lax.dynamic_slice(
+            state.live_seqs, (0, 0, state.cur_index), (batch_size, beam_size, 1)
+        )
+    )
     # Flatten beam dimension into batch to be compatible with model.
     # {[batch, beam, ...], ...} --> {[batch * beam, ...], ...}
     flat_cache = jax.tree_util.tree_map(flatten_beam_dim, state.cache)
 
     # Call fast-decoder model on current tokens to get next-position logits.
     # --> [batch * beam, vocab]
     flat_logits, new_flat_cache = tokens_to_logits(flat_ids, flat_cache)
 
     # unflatten beam dimension
     # [batch * beam, vocab] --> [batch, beam, vocab]
     logits = unflatten_beam_dim(flat_logits, batch_size, beam_size)
     # Unflatten beam dimension in attention cache arrays
     # {[batch * beam, ...], ...} --> {[batch, beam, ...], ...}
     new_cache = jax.tree_util.tree_map(
-        lambda x: unflatten_beam_dim(x, batch_size, beam_size), new_flat_cache)
+        lambda x: unflatten_beam_dim(x, batch_size, beam_size), new_flat_cache
+    )
 
     # Gather log probabilities from logits
     candidate_log_probs = jax.nn.log_softmax(logits)
     # Add new logprobs to existing prefix logprobs.
     # --> [batch, beam, vocab]
-    log_probs = (candidate_log_probs +
-                 jnp.expand_dims(state.live_logprobs, axis=2))
+    log_probs = candidate_log_probs + jnp.expand_dims(
+        state.live_logprobs, axis=2
+    )
 
     # We'll need the vocab size, gather it from the log probability dimension.
     vocab_size = log_probs.shape[2]
 
     # Each item in batch has beam_size * vocab_size candidate sequences.
     # For each item, get the top 2*k candidates with the highest log-
     # probabilities. We gather the top 2*K beams here so that even if the best
@@ -273,94 +285,114 @@
     # Gather the top 2*K scores from _all_ beams.
     # --> [batch, 2*beams], [batch, 2*beams]
     topk_log_probs, topk_indices = lax.top_k(flat_log_probs, k=beams_to_keep)
     # Recover the beam index by floor division.
     topk_beam_indices = topk_indices // vocab_size
     # Gather 2*k top beams.
     # --> [batch, 2*beams, length]
-    topk_seq = gather_beams(state.live_seqs,
-                            topk_beam_indices,
-                            batch_size, beams_to_keep)
+    topk_seq = gather_beams(
+        state.live_seqs, topk_beam_indices, batch_size, beams_to_keep
+    )
 
     # Append the most probable 2*K token IDs to the top 2*K sequences
     # Recover token id by modulo division and expand Id array for broadcasting.
     # --> [batch, 2*beams, 1]
     topk_ids = jnp.expand_dims(topk_indices % vocab_size, axis=2)
     # Update sequences for the 2*K top-k new sequences.
     # --> [batch, 2*beams, length]
     topk_seq = lax.dynamic_update_slice(
-        topk_seq, topk_ids, (0, 0, state.cur_index + 1))
+        topk_seq, topk_ids, (0, 0, state.cur_index + 1)
+    )
 
     # Update LIVE (in-progress) sequences:
     # Did any of these sequences reach an end marker?
     # --> [batch, 2*beams]
-    newly_finished = (topk_seq[:, :, state.cur_index + 1] == end_marker)
+    newly_finished = topk_seq[:, :, state.cur_index + 1] == end_marker
     # To prevent these newly finished sequences from being added to the LIVE
     # set of active beam search sequences, set their log probs to a very large
     # negative value.
     new_log_probs = topk_log_probs + newly_finished * NEG_INF
     # Determine the top k beam indices (from top 2*k beams) from log probs.
     # --> [batch, beams]
     _, new_topk_indices = lax.top_k(new_log_probs, k=beam_size)
     new_topk_indices = jnp.flip(new_topk_indices, axis=1)
     # Gather the top k beams (from top 2*k beams).
     # --> [batch, beams, length], [batch, beams]
     top_alive_seq, top_alive_log_probs = gather_beams(
-        [topk_seq, new_log_probs], new_topk_indices, batch_size, beam_size)
+        [topk_seq, new_log_probs], new_topk_indices, batch_size, beam_size
+    )
 
     # Determine the top k beam indices from the original set of all beams.
     # --> [batch, beams]
     top_alive_indices = gather_beams(
-        topk_beam_indices, new_topk_indices, batch_size, beam_size)
+        topk_beam_indices, new_topk_indices, batch_size, beam_size
+    )
     # With these, gather the top k beam-associated caches.
     # --> {[batch, beams, ...], ...}
     top_alive_cache = gather_beams(
-        new_cache, top_alive_indices, batch_size, beam_size)
+        new_cache, top_alive_indices, batch_size, beam_size
+    )
 
     # Update FINISHED (reached end of sentence) sequences:
     # Calculate new seq scores from log probabilities.
     new_scores = topk_log_probs / brevity_penalty(alpha, state.cur_index + 1)
     # Mask out the still unfinished sequences by adding large negative value.
     # --> [batch, 2*beams]
     new_scores += (~newly_finished) * NEG_INF
 
     # Combine sequences, scores, and flags along the beam dimension and compare
     # new finished sequence scores to existing finished scores and select the
     # best from the new set of beams.
     finished_seqs = jnp.concatenate(  # --> [batch, 3*beams, length]
-        [state.finished_seqs, topk_seq], axis=1)
+        [state.finished_seqs, topk_seq], axis=1
+    )
     finished_scores = jnp.concatenate(  # --> [batch, 3*beams]
-        [state.finished_scores, new_scores], axis=1)
+        [state.finished_scores, new_scores], axis=1
+    )
     finished_flags = jnp.concatenate(  # --> [batch, 3*beams]
-        [state.finished_flags, newly_finished], axis=1)
+        [state.finished_flags, newly_finished], axis=1
+    )
     # --> [batch, beams, length], [batch, beams], [batch, beams]
-    top_finished_seq, top_finished_scores, top_finished_flags = (
-        gather_topk_beams([finished_seqs, finished_scores, finished_flags],
-                          finished_scores, batch_size, beam_size))
-
-    return BeamState(cur_index=state.cur_index + 1,
-                     live_logprobs=top_alive_log_probs,
-                     finished_scores=top_finished_scores,
-                     live_seqs=top_alive_seq,
-                     finished_seqs=top_finished_seq,
-                     finished_flags=top_finished_flags,
-                     cache=top_alive_cache)
+    (
+        top_finished_seq,
+        top_finished_scores,
+        top_finished_flags,
+    ) = gather_topk_beams(
+        [finished_seqs, finished_scores, finished_flags],
+        finished_scores,
+        batch_size,
+        beam_size,
+    )
+
+    return BeamState(
+        cur_index=state.cur_index + 1,
+        live_logprobs=top_alive_log_probs,
+        finished_scores=top_finished_scores,
+        live_seqs=top_alive_seq,
+        finished_seqs=top_finished_seq,
+        finished_flags=top_finished_flags,
+        cache=top_alive_cache,
+    )
 
   # Run while loop and get final beam search state.
-  final_state = lax.while_loop(beam_search_loop_cond_fn,
-                               beam_search_loop_body_fn,
-                               beam_search_init_state)
+  final_state = lax.while_loop(
+      beam_search_loop_cond_fn, beam_search_loop_body_fn, beam_search_init_state
+  )
 
   # Account for the edge-case where there are no finished sequences for a
   # particular batch item. If so, return live sequences for that batch item.
   # --> [batch]
   none_finished = jnp.any(final_state.finished_flags, axis=1)
   # --> [batch, beams, length]
-  finished_seqs = jnp.where(none_finished[:, None, None],
-                            final_state.finished_seqs,
-                            final_state.live_seqs)
+  finished_seqs = jnp.where(
+      none_finished[:, None, None],
+      final_state.finished_seqs,
+      final_state.live_seqs,
+  )
   # --> [batch, beams]
-  finished_scores = jnp.where(none_finished[:, None],
-                              final_state.finished_scores,
-                              final_state.live_logprobs)
+  finished_scores = jnp.where(
+      none_finished[:, None],
+      final_state.finished_scores,
+      final_state.live_logprobs,
+  )
 
   return finished_seqs, finished_scores
```

### Comparing `flax-0.7.0/examples/wmt/input_pipeline.py` & `flax-0.7.1/examples/wmt/input_pipeline.py`

 * *Files 3% similar despite different names*

```diff
@@ -39,18 +39,20 @@
 
   def __call__(self, features: Features) -> Features:
     features['inputs'] = features.pop(self.input_lang)
     features['targets'] = features.pop(self.target_lang)
     return features
 
 
-def get_raw_dataset(dataset_builder: tfds.core.DatasetBuilder,
-                    split: str,
-                    *,
-                    reverse_translation: bool = False) -> tf.data.Dataset:
+def get_raw_dataset(
+    dataset_builder: tfds.core.DatasetBuilder,
+    split: str,
+    *,
+    reverse_translation: bool = False
+) -> tf.data.Dataset:
   """Loads a raw WMT dataset and normalizes feature keys.
 
   Args:
     dataset_builder: TFDS dataset builder that can build `slit`.
     split: Split to use. This must be the full split. We shard the split across
       multiple hosts and currently don't support sharding subsplits.
     reverse_translation: bool: whether to reverse the translation direction.
@@ -58,26 +60,31 @@
 
   Returns:
     Dataset with source and target language features mapped to 'inputs' and
     'targets'.
   """
   num_examples = dataset_builder.info.splits[split].num_examples
   per_host_split = deterministic_data.get_read_instruction_for_host(
-      split, num_examples, drop_remainder=False)
+      split, num_examples, drop_remainder=False
+  )
   ds = dataset_builder.as_dataset(split=per_host_split, shuffle_files=False)
   ds = ds.map(
       NormalizeFeatureNamesOp(
-          dataset_builder.info, reverse_translation=reverse_translation),
-      num_parallel_calls=AUTOTUNE)
+          dataset_builder.info, reverse_translation=reverse_translation
+      ),
+      num_parallel_calls=AUTOTUNE,
+  )
   return ds
 
 
-def pack_dataset(dataset: tf.data.Dataset,
-                 key2length: Union[int, Dict[str, int]],
-                 keys: Optional[List[str]] = None) -> tf.data.Dataset:
+def pack_dataset(
+    dataset: tf.data.Dataset,
+    key2length: Union[int, Dict[str, int]],
+    keys: Optional[List[str]] = None,
+) -> tf.data.Dataset:
   """Creates a 'packed' version of a dataset on-the-fly.
 
   Adapted from the mesh-tf implementation.
 
   This is meant to replace the irritation of having to create a separate
   "packed" version of a dataset to train efficiently on TPU.
   Each example in the output dataset represents several examples in the
@@ -114,46 +121,51 @@
     a tf.data.Dataset
   """
   shapes = tf.nest.map_structure(lambda spec: spec.shape, dataset.element_spec)
   if keys is None:
     keys = list(shapes.keys())
   for k in keys:
     if k not in shapes:
-      raise ValueError('Key %s not found in dataset.  Available keys are %s' %
-                       (k, shapes.keys()))
-    if not shapes[k].is_compatible_with(tf.TensorShape([None])): # type: ignore[wrong-arg-types]
+      raise ValueError(
+          'Key %s not found in dataset.  Available keys are %s'
+          % (k, shapes.keys())
+      )
+    if not shapes[k].is_compatible_with(tf.TensorShape([None])):  # type: ignore[wrong-arg-types]
       raise ValueError('Tensors to be packed must be one-dimensional.')
   # make sure that the length dictionary contains all keys as well as the
   # keys suffixed by "_segmentation" and "_position"
   if isinstance(key2length, int):
     key2length = {k: key2length for k in keys}
   for k in keys:
     for suffix in ['_segmentation', '_position']:
       key2length[k + suffix] = key2length[k]
 
   # trim to length
   dataset = dataset.map(
-      lambda x: {k: x[k][:key2length[k]] for k in keys},
-      num_parallel_calls=AUTOTUNE)
+      lambda x: {k: x[k][: key2length[k]] for k in keys},
+      num_parallel_calls=AUTOTUNE,
+  )
   # Setting batch_size=length ensures that the concatenated sequences (if they
   # have length >=1) are sufficient to fill at least one packed example.
   batch_size = max(key2length.values())
   dataset = dataset.padded_batch(
-      batch_size, padded_shapes={k: [-1] for k in keys})
+      batch_size, padded_shapes={k: [-1] for k in keys}
+  )
   dataset = _pack_with_tf_ops(dataset, keys, key2length)
 
   # Set the Tensor shapes correctly since they get lost in the process.
   def my_fn(x):
     return {k: tf.reshape(v, [key2length[k]]) for k, v in x.items()}
 
   return dataset.map(my_fn, num_parallel_calls=AUTOTUNE)
 
 
-def _pack_with_tf_ops(dataset: tf.data.Dataset, keys: List[str],
-                      key2length: Dict[str, int]) -> tf.data.Dataset:
+def _pack_with_tf_ops(
+    dataset: tf.data.Dataset, keys: List[str], key2length: Dict[str, int]
+) -> tf.data.Dataset:
   """Helper-function for packing a dataset which has already been batched.
 
   Helper for pack_dataset()  Uses tf.while_loop.
 
   Args:
     dataset: a dataset containing padded batches of examples.
     keys: a list of strings
@@ -170,15 +182,16 @@
 
   def write_packed_example(partial, outputs):
     new_partial = empty_example.copy()
     new_outputs = {}
     for k in keys_etc:
       new_outputs[k] = outputs[k].write(
           outputs[k].size(),
-          tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))
+          tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]),
+      )
     return new_partial, new_outputs
 
   def map_fn(x):
     """Internal function to flat_map over.
 
     Consumes a batch of input examples and produces a variable number of output
     examples.
@@ -190,17 +203,19 @@
     """
     partial = empty_example.copy()
     i = tf.zeros([], dtype=tf.int32)
     dynamic_batch_size = tf.shape(x[keys[0]])[0]
     outputs = {}
     for k in keys:
       outputs[k] = tf.TensorArray(
-          tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
+          tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]]
+      )
       outputs[k + '_position'] = tf.TensorArray(
-          tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
+          tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]]
+      )
 
     def body_fn(i, partial, outputs):
       """Body function for while_loop.
 
       Args:
         i: integer scalar
         partial: dictionary of Tensor (partially-constructed example)
@@ -209,80 +224,83 @@
       Returns:
         A triple containing the new values of the inputs.
       """
       can_append = True
       one_example = {}
       for k in keys:
         val = tf.cast(x[k][i], tf.int32)
-        val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
+        val = val[: tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
         one_example[k] = val
       for k in keys:
         can_append = tf.logical_and(
             can_append,
             tf.less_equal(
-                tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]))
+                tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]
+            ),
+        )
 
       def false_fn():
         return write_packed_example(partial, outputs)
 
       def true_fn():
         return partial, outputs
 
       partial, outputs = tf.cond(can_append, true_fn, false_fn)
       new_partial = {}
       for k in keys:
-        new_seq = one_example[k][:key2length[k]]
+        new_seq = one_example[k][: key2length[k]]
         new_seq_len = tf.size(new_seq)
         new_partial[k] = tf.concat([partial[k], new_seq], 0)
         new_partial[k + '_position'] = tf.concat(
-            [partial[k + '_position'],
-             tf.range(new_seq_len)], 0)
+            [partial[k + '_position'], tf.range(new_seq_len)], 0
+        )
       partial = new_partial
       return i + 1, partial, outputs
 
     # For loop over all examples in the batch.
     i, partial, outputs = tf.while_loop(
         cond=lambda *_: True,
         body=body_fn,
         loop_vars=(i, partial, outputs),
         shape_invariants=(
-            tf.TensorShape([]), # type: ignore[wrong-arg-types]
-            {k: tf.TensorShape([None]) for k in keys_etc}, # type: ignore[wrong-arg-types]
-            {k: tf.TensorShape(None) for k in keys_etc}, # type: ignore[wrong-arg-types]
+            tf.TensorShape([]),  # type: ignore[wrong-arg-types]
+            {k: tf.TensorShape([None]) for k in keys_etc},  # type: ignore[wrong-arg-types]
+            {k: tf.TensorShape(None) for k in keys_etc},  # type: ignore[wrong-arg-types]
         ),
-        maximum_iterations=dynamic_batch_size)
+        maximum_iterations=dynamic_batch_size,
+    )
     _, outputs = write_packed_example(partial, outputs)
     packed = {k: outputs[k].stack() for k in keys_etc}
     for k in keys:
-      packed[k + '_segmentation'] = (
-          tf.cumsum(
-              tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1) *
-          tf.cast(tf.not_equal(packed[k], 0), tf.int32))
+      packed[k + '_segmentation'] = tf.cumsum(
+          tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1
+      ) * tf.cast(tf.not_equal(packed[k], 0), tf.int32)
     return packed
 
   dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)
   return dataset.unbatch()
 
 
 # -----------------------------------------------------------------------------
 # Main dataset prep routines.
 # -----------------------------------------------------------------------------
-def preprocess_wmt_data(dataset,
-                        shuffle: bool,
-                        num_epochs: Optional[int] = 1,
-                        pack_examples: bool = True,
-                        shuffle_buffer_size: int = 1024,
-                        max_length: int = 512,
-                        batch_size: int = 256,
-                        drop_remainder: bool = True,
-                        prefetch_size: int = AUTOTUNE):
+def preprocess_wmt_data(
+    dataset,
+    shuffle: bool,
+    num_epochs: Optional[int] = 1,
+    pack_examples: bool = True,
+    shuffle_buffer_size: int = 1024,
+    max_length: int = 512,
+    batch_size: int = 256,
+    drop_remainder: bool = True,
+    prefetch_size: int = AUTOTUNE,
+):
   """Shuffle and batch/pack the given dataset."""
 
   def length_filter(max_len):
-
     def filter_fn(x):
       source, target = x['inputs'], x['targets']
       l = tf.maximum(tf.shape(source)[0], tf.shape(target)[0])
       return tf.less(l, max_len + 1)
 
     return filter_fn
 
@@ -295,82 +313,87 @@
 
   if pack_examples:
     dataset = pack_dataset(dataset, max_length)
     dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)
   else:  # simple (static-shape) padded batching
     dataset = dataset.padded_batch(
         batch_size,
-        padded_shapes={
-            'inputs': max_length,
-            'targets': max_length
-        },
-        padding_values={
-            'inputs': 0,
-            'targets': 0
-        },
-        drop_remainder=drop_remainder)
+        padded_shapes={'inputs': max_length, 'targets': max_length},
+        padding_values={'inputs': 0, 'targets': 0},
+        drop_remainder=drop_remainder,
+    )
 
   if prefetch_size:
     dataset = dataset.prefetch(prefetch_size)
 
   return dataset
 
 
-def get_wmt_datasets(config: ml_collections.ConfigDict,
-                     *,
-                     n_devices: int,
-                     reverse_translation: bool = True,
-                     vocab_path: Optional[str] = None):
+def get_wmt_datasets(
+    config: ml_collections.ConfigDict,
+    *,
+    n_devices: int,
+    reverse_translation: bool = True,
+    vocab_path: Optional[str] = None
+):
   """Load and return dataset of batched examples for use during training."""
   if vocab_path is None:
     vocab_path = os.path.expanduser('~/wmt_sentencepiece_model')
 
   train_ds_builder = tfds.builder(config.dataset_name)
   train_data = get_raw_dataset(
-      train_ds_builder, 'train', reverse_translation=reverse_translation)
+      train_ds_builder, 'train', reverse_translation=reverse_translation
+  )
 
   if config.eval_dataset_name:
     eval_ds_builder = tfds.builder(config.eval_dataset_name)
   else:
     eval_ds_builder = train_ds_builder
   eval_data = get_raw_dataset(
       eval_ds_builder,
       config.eval_split,
-      reverse_translation=reverse_translation)
+      reverse_translation=reverse_translation,
+  )
 
   # Tokenize data.
   sp_tokenizer = tokenizer.load_or_train_tokenizer(
       train_data,
       vocab_path=vocab_path,
       vocab_size=config.vocab_size,
-      max_corpus_chars=config.max_corpus_chars)
+      max_corpus_chars=config.max_corpus_chars,
+  )
   train_data = train_data.map(
-      tokenizer.TokenizeOp(sp_tokenizer), num_parallel_calls=AUTOTUNE)
+      tokenizer.TokenizeOp(sp_tokenizer), num_parallel_calls=AUTOTUNE
+  )
   eval_data = eval_data.map(
-      tokenizer.TokenizeOp(sp_tokenizer), num_parallel_calls=AUTOTUNE)
+      tokenizer.TokenizeOp(sp_tokenizer), num_parallel_calls=AUTOTUNE
+  )
 
   batch_size = config.per_device_batch_size * n_devices
 
   train_ds = preprocess_wmt_data(
       train_data,
       shuffle=True,
       num_epochs=None,
       pack_examples=True,
       batch_size=batch_size,
-      max_length=config.max_target_length)
+      max_length=config.max_target_length,
+  )
 
   eval_ds = preprocess_wmt_data(
       eval_data,
       shuffle=False,
       pack_examples=False,
       batch_size=batch_size,
-      max_length=config.max_eval_target_length)
+      max_length=config.max_eval_target_length,
+  )
 
   predict_ds = preprocess_wmt_data(
       eval_data,
       shuffle=False,
       pack_examples=False,
       batch_size=batch_size,
       max_length=config.max_predict_length,
-      drop_remainder=False)
+      drop_remainder=False,
+  )
 
   return train_ds, eval_ds, predict_ds, sp_tokenizer
```

### Comparing `flax-0.7.0/examples/wmt/input_pipeline_test.py` & `flax-0.7.1/examples/wmt/input_pipeline_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -49,43 +49,53 @@
 
     # Go two directories up to the root of the flax directory.
     flax_root_dir = pathlib.Path(__file__).parents[2]
     data_dir = str(flax_root_dir) + '/.tfds/metadata'  # pylint: disable=unused-variable
 
     with tfds.testing.mock_data(num_examples=128, data_dir=data_dir):
       train_ds, eval_ds, predict_ds, _ = input_pipeline.get_wmt_datasets(
-          n_devices=2, config=config, vocab_path=vocab_path)
+          n_devices=2, config=config, vocab_path=vocab_path
+      )
     return train_ds, eval_ds, predict_ds
 
   def test_train_ds(self):
     expected_shape = [2, _TARGET_LENGTH]  # 2 devices.
     # For training we pack multiple short examples in one example.
     # *_position and *_segmentation indicate the boundaries.
     for batch in self.train_ds.take(3):
-      self.assertEqual({k: v.shape.as_list() for k, v in batch.items()}, {
-          'inputs': expected_shape,
-          'inputs_position': expected_shape,
-          'inputs_segmentation': expected_shape,
-          'targets': expected_shape,
-          'targets_position': expected_shape,
-          'targets_segmentation': expected_shape,
-      })
+      self.assertEqual(
+          {k: v.shape.as_list() for k, v in batch.items()},
+          {
+              'inputs': expected_shape,
+              'inputs_position': expected_shape,
+              'inputs_segmentation': expected_shape,
+              'targets': expected_shape,
+              'targets_position': expected_shape,
+              'targets_segmentation': expected_shape,
+          },
+      )
 
   def test_eval_ds(self):
     expected_shape = [2, _EVAL_TARGET_LENGTH]  # 2 devices.
     for batch in self.eval_ds.take(3):
-      self.assertEqual({k: v.shape.as_list() for k, v in batch.items()}, {
-          'inputs': expected_shape,
-          'targets': expected_shape,
-      })
+      self.assertEqual(
+          {k: v.shape.as_list() for k, v in batch.items()},
+          {
+              'inputs': expected_shape,
+              'targets': expected_shape,
+          },
+      )
 
   def test_predict_ds(self):
     expected_shape = [2, _PREDICT_TARGET_LENGTH]  # 2 devices.
     for batch in self.predict_ds.take(3):
-      self.assertEqual({k: v.shape.as_list() for k, v in batch.items()}, {
-          'inputs': expected_shape,
-          'targets': expected_shape,
-      })
+      self.assertEqual(
+          {k: v.shape.as_list() for k, v in batch.items()},
+          {
+              'inputs': expected_shape,
+              'targets': expected_shape,
+          },
+      )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/examples/wmt/main.py` & `flax-0.7.1/examples/imagenet/main.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Main file for running the WMT example.
+"""Main file for running the ImageNet example.
 
 This file is intentionally kept short. The majority for logic is in libraries
 that can be easily tested and imported in Colab.
 """
 
 from absl import app
 from absl import flags
@@ -30,18 +30,18 @@
 
 
 FLAGS = flags.FLAGS
 
 flags.DEFINE_string('workdir', None, 'Directory to store model data.')
 config_flags.DEFINE_config_file(
     'config',
-    'configs/default.py',
+    None,
     'File path to the training hyperparameter configuration.',
-    lock_config=True)
-flags.mark_flags_as_required(['config', 'workdir'])
+    lock_config=True,
+)
 
 
 def main(argv):
   if len(argv) > 1:
     raise app.UsageError('Too many command-line arguments.')
 
   # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make
@@ -49,18 +49,21 @@
   tf.config.experimental.set_visible_devices([], 'GPU')
 
   logging.info('JAX process: %d / %d', jax.process_index(), jax.process_count())
   logging.info('JAX local devices: %r', jax.local_devices())
 
   # Add a note so that we can tell which task is which JAX host.
   # (Depending on the platform task 0 is not guaranteed to be host 0)
-  platform.work_unit().set_task_status(f'process_index: {jax.process_index()}, '
-                                       f'process_count: {jax.process_count()}')
-  platform.work_unit().create_artifact(platform.ArtifactType.DIRECTORY,
-                                       FLAGS.workdir, 'workdir')
+  platform.work_unit().set_task_status(
+      f'process_index: {jax.process_index()}, '
+      f'process_count: {jax.process_count()}'
+  )
+  platform.work_unit().create_artifact(
+      platform.ArtifactType.DIRECTORY, FLAGS.workdir, 'workdir'
+  )
 
   train.train_and_evaluate(FLAGS.config, FLAGS.workdir)
 
 
 if __name__ == '__main__':
-  jax.config.config_with_absl()
+  flags.mark_flags_as_required(['config', 'workdir'])
   app.run(main)
```

### Comparing `flax-0.7.0/examples/wmt/models.py` & `flax-0.7.1/examples/wmt/models.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,14 +28,15 @@
 import jax.numpy as jnp
 import numpy as np
 
 
 @struct.dataclass
 class TransformerConfig:
   """Global hyperparameters used to minimize obnoxious kwarg plumbing."""
+
   vocab_size: int
   output_vocab_size: int
   share_embeddings: bool = False
   logits_via_embedding: bool = False
   dtype: Any = jnp.float32
   emb_dim: int = 512
   num_heads: int = 8
@@ -53,21 +54,20 @@
 
 
 def shift_right(x, axis=1):
   """Shift the input to the right by padding on axis 1."""
   pad_widths = [(0, 0)] * len(x.shape)
   pad_widths[axis] = (1, 0)
   padded = jnp.pad(
-      x, pad_widths, mode='constant', constant_values=x.dtype.type(0))
+      x, pad_widths, mode='constant', constant_values=x.dtype.type(0)
+  )
   return padded[:, :-1]
 
 
-def sinusoidal_init(max_len=2048,
-                    min_scale=1.0,
-                    max_scale=10000.0):
+def sinusoidal_init(max_len=2048, min_scale=1.0, max_scale=10000.0):
   """1D Sinusoidal Position Embedding Initializer.
 
   Args:
       max_len: maximum possible length for the input.
       min_scale: float: minimum frequency-scale in sine grating.
       max_scale: float: maximum frequency-scale in sine grating.
 
@@ -79,36 +79,35 @@
     """Sinusoidal init."""
     del key, dtype
     d_feature = shape[-1]
     pe = np.zeros((max_len, d_feature), dtype=np.float32)
     position = np.arange(0, max_len)[:, np.newaxis]
     scale_factor = -np.log(max_scale / min_scale) / (d_feature // 2 - 1)
     div_term = min_scale * np.exp(np.arange(0, d_feature // 2) * scale_factor)
-    pe[:, :d_feature // 2] = np.sin(position * div_term)
-    pe[:, d_feature // 2: 2 * (d_feature // 2)] = np.cos(position * div_term)
+    pe[:, : d_feature // 2] = np.sin(position * div_term)
+    pe[:, d_feature // 2 : 2 * (d_feature // 2)] = np.cos(position * div_term)
     pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]
     return jnp.array(pe)
 
   return init
 
 
 class AddPositionEmbs(nn.Module):
   """Adds (optionally learned) positional embeddings to the inputs.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
     decode: whether to run in single-position autoregressive mode.
   """
+
   config: TransformerConfig
   decode: bool = False
 
   @nn.compact
-  def __call__(self,
-               inputs,
-               inputs_positions=None):
+  def __call__(self, inputs, inputs_positions=None):
     """Applies AddPositionEmbs module.
 
     By default this layer uses a fixed sinusoidal embedding table. If a
     learned position embedding is desired, pass an initializer to
     posemb_init in the configuration.
 
     Args:
@@ -116,40 +115,41 @@
       inputs_positions: input position indices for packed sequences.
 
     Returns:
       output: `(bs, timesteps, in_dim)`
     """
     config = self.config
     # inputs.shape is (batch_size, seq_len, emb_dim)
-    assert inputs.ndim == 3, ('Number of dimensions should be 3,'
-                              ' but it is: %d' % inputs.ndim)
+    assert inputs.ndim == 3, (
+        'Number of dimensions should be 3, but it is: %d' % inputs.ndim
+    )
     length = inputs.shape[1]
     pos_emb_shape = (1, config.max_len, inputs.shape[-1])
     if config.posemb_init is None:
       # Use a fixed (non-learned) sinusoidal position embedding.
-      pos_embedding = sinusoidal_init(max_len=config.max_len)(None,
-                                                              pos_emb_shape,
-                                                              None)
+      pos_embedding = sinusoidal_init(max_len=config.max_len)(
+          None, pos_emb_shape, None
+      )
     else:
-      pos_embedding = self.param('pos_embedding', config.posemb_init,
-                                 pos_emb_shape)
+      pos_embedding = self.param(
+          'pos_embedding', config.posemb_init, pos_emb_shape
+      )
     pe = pos_embedding[:, :length, :]
 
     # We use a cache position index for tracking decoding position.
     if self.decode:
       is_initialized = self.has_variable('cache', 'cache_index')
-      cache_index = self.variable('cache', 'cache_index',
-                                  lambda: jnp.array(0, dtype=jnp.uint32))
+      cache_index = self.variable(
+          'cache', 'cache_index', lambda: jnp.array(0, dtype=jnp.uint32)
+      )
       if is_initialized:
         i = cache_index.value
         cache_index.value = i + 1
         _, _, df = pos_embedding.shape
-        pe = lax.dynamic_slice(pos_embedding,
-                               jnp.array((0, i, 0)),
-                               (1, 1, df))
+        pe = lax.dynamic_slice(pos_embedding, jnp.array((0, i, 0)), (1, 1, df))
     if inputs_positions is None:
       # normal unpacked case:
       return inputs + pe
     else:
       # for packed data we need to use known position indices:
       return inputs + jnp.take(pe[0], inputs_positions, axis=0)
 
@@ -157,55 +157,56 @@
 class MlpBlock(nn.Module):
   """Transformer MLP / feed-forward block.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
     out_dim: optionally specify out dimension.
   """
+
   config: TransformerConfig
   out_dim: Optional[int] = None
 
   @nn.compact
   def __call__(self, inputs):
     """Applies Transformer MlpBlock module."""
     config = self.config
-    actual_out_dim = (inputs.shape[-1] if self.out_dim is None
-                      else self.out_dim)
+    actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim
     x = nn.Dense(
         config.mlp_dim,
         dtype=config.dtype,
         kernel_init=config.kernel_init,
-        bias_init=config.bias_init)(
-            inputs)
+        bias_init=config.bias_init,
+    )(inputs)
     x = nn.relu(x)
     x = nn.Dropout(rate=config.dropout_rate)(
-        x, deterministic=config.deterministic)
+        x, deterministic=config.deterministic
+    )
     output = nn.Dense(
         actual_out_dim,
         dtype=config.dtype,
         kernel_init=config.kernel_init,
-        bias_init=config.bias_init)(
-            x)
+        bias_init=config.bias_init,
+    )(x)
     output = nn.Dropout(rate=config.dropout_rate)(
-        output, deterministic=config.deterministic)
+        output, deterministic=config.deterministic
+    )
     return output
 
 
 class Encoder1DBlock(nn.Module):
   """Transformer encoder layer.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
   """
+
   config: TransformerConfig
 
   @nn.compact
-  def __call__(self,
-               inputs,
-               encoder_mask=None):
+  def __call__(self, inputs, encoder_mask=None):
     """Applies Encoder1DBlock module.
 
     Args:
       inputs: input data.
       encoder_mask: encoder self-attention mask.
 
     Returns:
@@ -221,18 +222,20 @@
         dtype=config.dtype,
         qkv_features=config.qkv_dim,
         kernel_init=config.kernel_init,
         bias_init=config.bias_init,
         use_bias=False,
         broadcast_dropout=False,
         dropout_rate=config.attention_dropout_rate,
-        deterministic=config.deterministic)(x, encoder_mask)
+        deterministic=config.deterministic,
+    )(x, encoder_mask)
 
     x = nn.Dropout(rate=config.dropout_rate)(
-        x, deterministic=config.deterministic)
+        x, deterministic=config.deterministic
+    )
     x = x + inputs
 
     # MLP block.
     y = nn.LayerNorm(dtype=config.dtype)(x)
     y = MlpBlock(config=config)(y)
 
     return x + y
@@ -240,22 +243,21 @@
 
 class EncoderDecoder1DBlock(nn.Module):
   """Transformer encoder-decoder layer.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
   """
+
   config: TransformerConfig
 
   @nn.compact
-  def __call__(self,
-               targets,
-               encoded,
-               decoder_mask=None,
-               encoder_decoder_mask=None):
+  def __call__(
+      self, targets, encoded, decoder_mask=None, encoder_decoder_mask=None
+  ):
     """Applies EncoderDecoder1DBlock module.
 
     Args:
       targets: input data for decoder
       encoded: input data from encoder
       decoder_mask: decoder self-attention mask.
       encoder_decoder_mask: encoder-decoder attention mask.
@@ -274,34 +276,38 @@
         qkv_features=config.qkv_dim,
         kernel_init=config.kernel_init,
         bias_init=config.bias_init,
         use_bias=False,
         broadcast_dropout=False,
         dropout_rate=config.attention_dropout_rate,
         deterministic=config.deterministic,
-        decode=config.decode)(x, decoder_mask)
+        decode=config.decode,
+    )(x, decoder_mask)
     x = nn.Dropout(rate=config.dropout_rate)(
-        x, deterministic=config.deterministic)
+        x, deterministic=config.deterministic
+    )
     x = x + targets
 
     # Encoder-Decoder block.
     y = nn.LayerNorm(dtype=config.dtype)(x)
     y = nn.MultiHeadDotProductAttention(
         num_heads=config.num_heads,
         dtype=config.dtype,
         qkv_features=config.qkv_dim,
         kernel_init=config.kernel_init,
         bias_init=config.bias_init,
         use_bias=False,
         broadcast_dropout=False,
         dropout_rate=config.attention_dropout_rate,
-        deterministic=config.deterministic)(y, encoded, encoder_decoder_mask)
+        deterministic=config.deterministic,
+    )(y, encoded, encoder_decoder_mask)
 
     y = nn.Dropout(rate=config.dropout_rate)(
-        y, deterministic=config.deterministic)
+        y, deterministic=config.deterministic
+    )
     y = y + x
 
     # MLP block.
     z = nn.LayerNorm(dtype=config.dtype)(y)
     z = MlpBlock(config=config)(z)
 
     return y + z
@@ -310,22 +316,20 @@
 class Encoder(nn.Module):
   """Transformer Model Encoder for sequence to sequence translation.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
     shared_embedding: a shared embedding layer to use.
   """
+
   config: TransformerConfig
   shared_embedding: Any = None
 
   @nn.compact
-  def __call__(self,
-               inputs,
-               inputs_positions=None,
-               encoder_mask=None):
+  def __call__(self, inputs, inputs_positions=None, encoder_mask=None):
     """Applies Transformer model on the inputs.
 
     Args:
       inputs: input data
       inputs_positions: input subsequence positions for packed examples.
       encoder_mask: decoder self-attention mask.
 
@@ -336,54 +340,60 @@
     assert inputs.ndim == 2  # (batch, len)
 
     # Input Embedding
     if self.shared_embedding is None:
       input_embed = nn.Embed(
           num_embeddings=config.vocab_size,
           features=config.emb_dim,
-          embedding_init=nn.initializers.normal(stddev=1.0))
+          embedding_init=nn.initializers.normal(stddev=1.0),
+      )
     else:
       input_embed = self.shared_embedding
     x = inputs.astype('int32')
     x = input_embed(x)
-    x = AddPositionEmbs(
-        config=config, decode=False, name='posembed_input')(
-            x, inputs_positions=inputs_positions)
+    x = AddPositionEmbs(config=config, decode=False, name='posembed_input')(
+        x, inputs_positions=inputs_positions
+    )
     x = nn.Dropout(rate=config.dropout_rate)(
-        x, deterministic=config.deterministic)
+        x, deterministic=config.deterministic
+    )
 
     x = x.astype(config.dtype)
 
     # Input Encoder
     for lyr in range(config.num_layers):
-      x = Encoder1DBlock(
-          config=config, name=f'encoderblock_{lyr}')(x, encoder_mask)
+      x = Encoder1DBlock(config=config, name=f'encoderblock_{lyr}')(
+          x, encoder_mask
+      )
 
     encoded = nn.LayerNorm(dtype=config.dtype, name='encoder_norm')(x)
 
     return encoded
 
 
 class Decoder(nn.Module):
   """Transformer Model Decoder for sequence to sequence translation.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
     shared_embedding: a shared embedding layer to use.
   """
+
   config: TransformerConfig
   shared_embedding: Any = None
 
   @nn.compact
-  def __call__(self,
-               encoded,
-               targets,
-               targets_positions=None,
-               decoder_mask=None,
-               encoder_decoder_mask=None):
+  def __call__(
+      self,
+      encoded,
+      targets,
+      targets_positions=None,
+      decoder_mask=None,
+      encoder_decoder_mask=None,
+  ):
     """Applies Transformer model on the inputs.
 
     Args:
       encoded: encoded input data from encoder.
       targets: target inputs.
       targets_positions: input subsequence positions for packed examples.
       decoder_mask: decoder self-attention mask.
@@ -398,123 +408,133 @@
     assert targets.ndim == 2  # (batch, len)
 
     # Target Embedding
     if self.shared_embedding is None:
       output_embed = nn.Embed(
           num_embeddings=config.output_vocab_size,
           features=config.emb_dim,
-          embedding_init=nn.initializers.normal(stddev=1.0))
+          embedding_init=nn.initializers.normal(stddev=1.0),
+      )
     else:
       output_embed = self.shared_embedding
 
     y = targets.astype('int32')
     if not config.decode:
       y = shift_right(y)
     y = output_embed(y)
     y = AddPositionEmbs(
-        config=config, decode=config.decode, name='posembed_output')(
-            y, inputs_positions=targets_positions)
+        config=config, decode=config.decode, name='posembed_output'
+    )(y, inputs_positions=targets_positions)
     y = nn.Dropout(rate=config.dropout_rate)(
-        y, deterministic=config.deterministic)
+        y, deterministic=config.deterministic
+    )
 
     y = y.astype(config.dtype)
 
     # Target-Input Decoder
     for lyr in range(config.num_layers):
       y = EncoderDecoder1DBlock(
-          config=config, name=f'encoderdecoderblock_{lyr}')(
-              y,
-              encoded,
-              decoder_mask=decoder_mask,
-              encoder_decoder_mask=encoder_decoder_mask)
+          config=config, name=f'encoderdecoderblock_{lyr}'
+      )(
+          y,
+          encoded,
+          decoder_mask=decoder_mask,
+          encoder_decoder_mask=encoder_decoder_mask,
+      )
     y = nn.LayerNorm(dtype=config.dtype, name='encoderdecoder_norm')(y)
 
     # Decoded Logits
     if config.logits_via_embedding:
       # Use the transpose of embedding matrix for logit transform.
       logits = output_embed.attend(y.astype(jnp.float32))
       # Correctly normalize pre-softmax logits for this shared case.
       logits = logits / jnp.sqrt(y.shape[-1])
     else:
       logits = nn.Dense(
           config.output_vocab_size,
           dtype=config.dtype,
           kernel_init=config.kernel_init,
           bias_init=config.bias_init,
-          name='logitdense')(
-              y)
+          name='logitdense',
+      )(y)
     return logits
 
 
 class Transformer(nn.Module):
   """Transformer Model for sequence to sequence translation.
 
   Attributes:
     config: TransformerConfig dataclass containing hyperparameters.
   """
+
   config: TransformerConfig
 
   def setup(self):
     config = self.config
 
     if config.share_embeddings:
       if config.output_vocab_size is not None:
-        assert config.output_vocab_size == config.vocab_size, (
-            "can't share embedding with different vocab sizes.")
+        assert (
+            config.output_vocab_size == config.vocab_size
+        ), "can't share embedding with different vocab sizes."
       self.shared_embedding = nn.Embed(
           num_embeddings=config.vocab_size,
           features=config.emb_dim,
-          embedding_init=nn.initializers.normal(stddev=1.0))
+          embedding_init=nn.initializers.normal(stddev=1.0),
+      )
     else:
       self.shared_embedding = None
 
     self.encoder = Encoder(
-        config=config, shared_embedding=self.shared_embedding)
+        config=config, shared_embedding=self.shared_embedding
+    )
     self.decoder = Decoder(
-        config=config, shared_embedding=self.shared_embedding)
+        config=config, shared_embedding=self.shared_embedding
+    )
 
-  def encode(self,
-             inputs,
-             inputs_positions=None,
-             inputs_segmentation=None):
+  def encode(self, inputs, inputs_positions=None, inputs_segmentation=None):
     """Applies Transformer encoder-branch on the inputs.
 
     Args:
       inputs: input data.
       inputs_positions: input subsequence positions for packed examples.
       inputs_segmentation: input segmentation info for packed examples.
 
     Returns:
       encoded feature array from the transformer encoder.
     """
     config = self.config
     # Make padding attention mask.
     encoder_mask = nn.make_attention_mask(
-        inputs > 0, inputs > 0, dtype=config.dtype)
+        inputs > 0, inputs > 0, dtype=config.dtype
+    )
     # Add segmentation block-diagonal attention mask if using segmented data.
     if inputs_segmentation is not None:
       encoder_mask = nn.combine_masks(
           encoder_mask,
           nn.make_attention_mask(
               inputs_segmentation,
               inputs_segmentation,
               jnp.equal,
-              dtype=config.dtype))
+              dtype=config.dtype,
+          ),
+      )
     return self.encoder(
-        inputs,
-        inputs_positions=inputs_positions,
-        encoder_mask=encoder_mask)
+        inputs, inputs_positions=inputs_positions, encoder_mask=encoder_mask
+    )
 
-  def decode(self,
-             encoded,
-             inputs,  # only needed for masks
-             targets,
-             targets_positions=None,
-             inputs_segmentation=None,
-             targets_segmentation=None):
+  def decode(
+      self,
+      encoded,
+      inputs,  # only needed for masks
+      targets,
+      targets_positions=None,
+      inputs_segmentation=None,
+      targets_segmentation=None,
+  ):
     """Applies Transformer decoder-branch on encoded-input and target.
 
     Args:
       encoded: encoded input data from encoder.
       inputs: input data (only needed for masking).
       targets: target data.
       targets_positions: target subsequence positions for packed examples.
@@ -527,69 +547,83 @@
     config = self.config
 
     # Make padding attention masks.
     if config.decode:
       # for fast autoregressive decoding only a special encoder-decoder mask is used
       decoder_mask = None
       encoder_decoder_mask = nn.make_attention_mask(
-          jnp.ones_like(targets) > 0, inputs > 0, dtype=config.dtype)
+          jnp.ones_like(targets) > 0, inputs > 0, dtype=config.dtype
+      )
     else:
       decoder_mask = nn.combine_masks(
           nn.make_attention_mask(targets > 0, targets > 0, dtype=config.dtype),
-          nn.make_causal_mask(targets, dtype=config.dtype))
+          nn.make_causal_mask(targets, dtype=config.dtype),
+      )
       encoder_decoder_mask = nn.make_attention_mask(
-          targets > 0, inputs > 0, dtype=config.dtype)
+          targets > 0, inputs > 0, dtype=config.dtype
+      )
 
     # Add segmentation block-diagonal attention masks if using segmented data.
     if inputs_segmentation is not None:
       decoder_mask = nn.combine_masks(
           decoder_mask,
           nn.make_attention_mask(
               targets_segmentation,
               targets_segmentation,
               jnp.equal,
-              dtype=config.dtype))
+              dtype=config.dtype,
+          ),
+      )
       encoder_decoder_mask = nn.combine_masks(
           encoder_decoder_mask,
           nn.make_attention_mask(
               targets_segmentation,
               inputs_segmentation,
               jnp.equal,
-              dtype=config.dtype))
+              dtype=config.dtype,
+          ),
+      )
     logits = self.decoder(
         encoded,
         targets,
         targets_positions=targets_positions,
         decoder_mask=decoder_mask,
-        encoder_decoder_mask=encoder_decoder_mask)
+        encoder_decoder_mask=encoder_decoder_mask,
+    )
     return logits.astype(self.config.dtype)
 
-  def __call__(self,
-               inputs,
-               targets,
-               inputs_positions=None,
-               targets_positions=None,
-               inputs_segmentation=None,
-               targets_segmentation=None):
+  def __call__(
+      self,
+      inputs,
+      targets,
+      inputs_positions=None,
+      targets_positions=None,
+      inputs_segmentation=None,
+      targets_segmentation=None,
+  ):
     """Applies Transformer model on the inputs.
 
     Args:
       inputs: input data.
       targets: target data.
       inputs_positions: input subsequence positions for packed examples.
       targets_positions: target subsequence positions for packed examples.
       inputs_segmentation: input segmentation info for packed examples.
       targets_segmentation: target segmentation info for packed examples.
 
     Returns:
       logits array from full transformer.
     """
-    encoded = self.encode(inputs,
-                          inputs_positions=inputs_positions,
-                          inputs_segmentation=inputs_segmentation)
-
-    return self.decode(encoded,
-                       inputs,  # only used for masks
-                       targets,
-                       targets_positions=targets_positions,
-                       inputs_segmentation=inputs_segmentation,
-                       targets_segmentation=targets_segmentation)
+    encoded = self.encode(
+        inputs,
+        inputs_positions=inputs_positions,
+        inputs_segmentation=inputs_segmentation,
+    )
+
+    return self.decode(
+        encoded,
+        inputs,  # only used for masks
+        targets,
+        targets_positions=targets_positions,
+        inputs_segmentation=inputs_segmentation,
+        targets_segmentation=targets_segmentation,
+    )
```

### Comparing `flax-0.7.0/examples/wmt/tokenizer.py` & `flax-0.7.1/examples/lm1b/tokenizer.py`

 * *Files 7% similar despite different names*

```diff
@@ -10,66 +10,68 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Provides op for tokenizing a dataset."""
 
-import dataclasses
 import os
 import tempfile
 import time
 from typing import Any, Dict, Iterable, Tuple
 
 from absl import logging
+import dataclasses
 import jax
+from sentencepiece import SentencePieceTrainer
 import tensorflow as tf
 import tensorflow_text as tftxt
 
-from sentencepiece import SentencePieceTrainer
-
 Features = Dict[str, tf.Tensor]
 
 
 def _dump_chars_to_textfile(
     dataset: tf.data.Dataset,
     maxchars: int = int(1e7),
-    data_keys=('inputs', 'targets')
+    data_keys=('inputs', 'targets'),
 ) -> Tuple[str, int]:
   """Write part of a TFDS sentence dataset to lines in a text file.
 
   Args:
     dataset: tf.dataset containing string-data.
     maxchars: int: approximate number of characters to save from dataset.
     data_keys: Tuple[str]: what keys in dataset to dump from.
 
   Returns:
     name of temp file with dataset bytes, exact number of characters dumped.
   """
   char_count = 0
   ds_iter = dataset.as_numpy_iterator()
   with tempfile.NamedTemporaryFile(
-      delete=False, prefix='/tmp/ds_chars') as outfp:
+      delete=False, prefix='/tmp/ds_chars'
+  ) as outfp:
     while char_count < maxchars:
       example = next(ds_iter)
       for k in data_keys:
         line = example[k] + b'\n'
         char_count += len(line)
         outfp.write(line)
   return outfp.name, char_count
 
 
-def _train_sentencepiece(dataset: tf.data.Dataset,
-                         *,
-                         vocab_size: int,
-                         maxchars: int = int(1e7),
-                         model_path: str,
-                         model_type: str = 'unigram',
-                         character_coverage: float = 1.0,
-                         data_keys=('inputs', 'targets')):
+def _train_sentencepiece(
+    dataset: tf.data.Dataset,
+    *,
+    vocab_size: int,
+    maxchars: int = int(1e7),
+    model_path: str,
+    model_type: str = 'unigram',
+    character_coverage: float = 1.0,
+    data_keys=('inputs', 'targets'),
+):
   """Train SentencePiece tokenizer from subset of tf dataset.
 
   Args:
     dataset: tf.dataset
     vocab_size: int: size of vocab tokens to train.
     maxchars: int: number of characters to use for sentencepiece training.
     model_path: str: path of model file to save vocab model to.
@@ -83,22 +85,26 @@
     path to the trained sentencepiece vocabulary model.
   """
   if model_path.startswith('gs://'):
     abs_model_path = model_path
   else:
     abs_model_path = os.path.abspath(os.path.expanduser(model_path))
   fname, _ = _dump_chars_to_textfile(
-      dataset, maxchars=maxchars, data_keys=data_keys)
+      dataset, maxchars=maxchars, data_keys=data_keys
+  )
   with tempfile.NamedTemporaryFile(
-      delete=False, prefix='/tmp/sp_tmp') as model_fp:
+      delete=False, prefix='/tmp/sp_tmp'
+  ) as model_fp:
     pass  # we just want a prefix'd tmp-filename
   argstr = ' '.join([
-      f'--input={fname}', f'--vocab_size={vocab_size}',
+      f'--input={fname}',
+      f'--vocab_size={vocab_size}',
       f'--character_coverage={character_coverage}',
-      f'--model_prefix={model_fp.name}', f'--model_type={model_type}'
+      f'--model_prefix={model_fp.name}',
+      f'--model_type={model_type}',
   ])
   SentencePieceTrainer.Train(argstr)
   if jax.process_index() == 0:
     # Use an intermediate filename that is renamed to the target name to address
     # create and fill delays.
     copy_rename_path = abs_model_path + '.rntmp'
     tf.io.gfile.copy(model_fp.name + '.model', copy_rename_path, overwrite=True)
@@ -107,49 +113,54 @@
   else:
     while not tf.io.gfile.exists(abs_model_path):
       time.sleep(1)
     time.sleep(1)
   return abs_model_path
 
 
-def _load_sentencepiece_tokenizer(model_path: str,
-                                  add_bos: bool = False,
-                                  add_eos: bool = True,
-                                  reverse: bool = False):
+def _load_sentencepiece_tokenizer(
+    model_path: str,
+    add_bos: bool = False,
+    add_eos: bool = True,
+    reverse: bool = False,
+):
   """Load a tf-text SentencePiece tokenizer from given model filepath."""
   with tf.io.gfile.GFile(model_path, 'rb') as model_fp:
     sp_model = model_fp.read()
   sp_tokenizer = tftxt.SentencepieceTokenizer(
-      model=sp_model, add_bos=add_bos, add_eos=add_eos, reverse=reverse)
+      model=sp_model, add_bos=add_bos, add_eos=add_eos, reverse=reverse
+  )
   return sp_tokenizer
 
 
-def load_or_train_tokenizer(dataset: tf.data.Dataset,
-                            *,
-                            vocab_path: str,
-                            vocab_size: int,
-                            max_corpus_chars: int,
-                            data_keys: Tuple[str, str] = ('inputs', 'targets')):
+def load_or_train_tokenizer(
+    dataset: tf.data.Dataset,
+    *,
+    vocab_path: str,
+    vocab_size: int,
+    max_corpus_chars: int,
+    data_keys: Tuple[str, str] = ('inputs', 'targets'),
+):
   """Loads the tokenizer at `vocab_path` or trains a one from `dataset`."""
   try:
     return _load_sentencepiece_tokenizer(vocab_path)
   except tf.errors.NotFoundError:
     logging.info('SentencePiece vocab not found, building one from data.')
     vocab_path = _train_sentencepiece(
         dataset,
         vocab_size=vocab_size,
         maxchars=max_corpus_chars,
         model_path=vocab_path,
-        data_keys=data_keys)
+        data_keys=data_keys,
+    )
     return _load_sentencepiece_tokenizer(vocab_path)
 
 
 @dataclasses.dataclass
 class TokenizeOp:
-
   sp_tokenizer: Any
   data_keys: Iterable[str] = ('inputs', 'targets')
 
   def __call__(self, features: Features) -> Features:
     for k in self.data_keys:
       features[k] = self.sp_tokenizer.tokenize(features[k])
     return features
```

### Comparing `flax-0.7.0/examples/wmt/train.py` & `flax-0.7.1/examples/wmt/train.py`

 * *Files 4% similar despite different names*

```diff
@@ -64,56 +64,64 @@
       schedule makes it less steep in the beginning (close to 0).
 
   Returns:
     A schedule `count -> learning_rate`.
   """
 
   def schedule(count):
-    return init_value * (count + shift)**-.5 * shift**.5
+    return init_value * (count + shift) ** -0.5 * shift**0.5
 
   return schedule
 
 
 def create_learning_rate_schedule(learning_rate: float, warmup_steps: int):
   """Creates a rsqrt schedule with linear warmup."""
-  return optax.join_schedules([
-      optax.linear_schedule(
-          init_value=0, end_value=learning_rate, transition_steps=warmup_steps),
-      rsqrt_schedule(init_value=learning_rate, shift=warmup_steps),
-  ],
-                              boundaries=[warmup_steps])
+  return optax.join_schedules(
+      [
+          optax.linear_schedule(
+              init_value=0,
+              end_value=learning_rate,
+              transition_steps=warmup_steps,
+          ),
+          rsqrt_schedule(init_value=learning_rate, shift=warmup_steps),
+      ],
+      boundaries=[warmup_steps],
+  )
 
 
-def compute_weighted_cross_entropy(logits,
-                                   targets,
-                                   weights=None,
-                                   label_smoothing=0.0):
+def compute_weighted_cross_entropy(
+    logits, targets, weights=None, label_smoothing=0.0
+):
   """Compute weighted cross entropy and entropy for log probs and targets.
 
   Args:
    logits: [batch, length, num_classes] float array.
    targets: categorical targets [batch, length] int array.
    weights: None or array of shape [batch, length].
    label_smoothing: label smoothing constant, used to determine the on and off
      values.
 
   Returns:
     Tuple of scalar loss and batch normalizing factor.
   """
   if logits.ndim != targets.ndim + 1:
-    raise ValueError("Incorrect shapes. Got shape %s logits and %s targets" %
-                     (str(logits.shape), str(targets.shape)))
+    raise ValueError(
+        "Incorrect shapes. Got shape %s logits and %s targets"
+        % (str(logits.shape), str(targets.shape))
+    )
   vocab_size = logits.shape[-1]
   confidence = 1.0 - label_smoothing
   low_confidence = (1.0 - confidence) / (vocab_size - 1)
   normalizing_constant = -(
-      confidence * jnp.log(confidence) +
-      (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20))
+      confidence * jnp.log(confidence)
+      + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)
+  )
   soft_targets = common_utils.onehot(
-      targets, vocab_size, on_value=confidence, off_value=low_confidence)
+      targets, vocab_size, on_value=confidence, off_value=low_confidence
+  )
 
   loss = -jnp.sum(soft_targets * nn.log_softmax(logits), axis=-1)
   loss = loss - normalizing_constant
 
   normalizing_factor = np.prod(targets.shape)
   if weights is not None:
     loss = loss * weights
@@ -130,61 +138,76 @@
    targets: categorical targets [batch, length] int array.
    weights: None or array of shape [batch, length]
 
   Returns:
     Tuple of scalar loss and batch normalizing factor.
   """
   if logits.ndim != targets.ndim + 1:
-    raise ValueError("Incorrect shapes. Got shape %s logits and %s targets" %
-                     (str(logits.shape), str(targets.shape)))
+    raise ValueError(
+        "Incorrect shapes. Got shape %s logits and %s targets"
+        % (str(logits.shape), str(targets.shape))
+    )
   loss = jnp.equal(jnp.argmax(logits, axis=-1), targets)
   normalizing_factor = np.prod(logits.shape[:-1])
   if weights is not None:
     loss = loss * weights
     normalizing_factor = weights.sum()
 
   return loss.sum(), normalizing_factor
 
 
 def compute_metrics(logits, labels, weights, label_smoothing=0.0):
   """Compute summary metrics."""
-  loss, weight_sum = compute_weighted_cross_entropy(logits, labels, weights,
-                                                    label_smoothing)
+  loss, weight_sum = compute_weighted_cross_entropy(
+      logits, labels, weights, label_smoothing
+  )
   acc, _ = compute_weighted_accuracy(logits, labels, weights)
   metrics = {
       "loss": loss,
       "accuracy": acc,
       "denominator": weight_sum,
   }
   metrics = jax.lax.psum(metrics, axis_name="batch")
   return metrics
 
 
 # Primary training / eval / decode step functions.
 # -----------------------------------------------------------------------------
 
 
-def train_step(state,
-               batch,
-               config,
-               learning_rate_fn,
-               label_smoothing=0.0,
-               dropout_rng=None):
+def train_step(
+    state,
+    batch,
+    config,
+    learning_rate_fn,
+    label_smoothing=0.0,
+    dropout_rng=None,
+):
   """Perform a single training step."""
   # X_position and X_segmentation are needed only when using "packed examples"
   # where multiple sequences are packed into the same example with this
   # metadata.
   # if such features are not present they are ignored and the example is treated
   # like a normal, unpacked sequence example.
   train_keys = [
-      "inputs", "targets", "inputs_position", "targets_position",
-      "inputs_segmentation", "targets_segmentation"
+      "inputs",
+      "targets",
+      "inputs_position",
+      "targets_position",
+      "inputs_segmentation",
+      "targets_segmentation",
   ]
-  (inputs, targets, inputs_positions, targets_positions, inputs_segmentation,
-   targets_segmentation) = (batch.get(k, None) for k in train_keys)
+  (
+      inputs,
+      targets,
+      inputs_positions,
+      targets_positions,
+      inputs_segmentation,
+      targets_segmentation,
+  ) = (batch.get(k, None) for k in train_keys)
 
   weights = jnp.where(targets > 0, 1, 0).astype(jnp.float32)
 
   dropout_rng = jax.random.fold_in(dropout_rng, state.step)
 
   def loss_fn(params):
     """loss function used for training."""
@@ -192,26 +215,30 @@
         {"params": params},
         inputs,
         targets,
         inputs_positions=inputs_positions,
         targets_positions=targets_positions,
         inputs_segmentation=inputs_segmentation,
         targets_segmentation=targets_segmentation,
-        rngs={"dropout": dropout_rng})
+        rngs={"dropout": dropout_rng},
+    )
 
-    loss, weight_sum = compute_weighted_cross_entropy(logits, targets, weights,
-                                                      label_smoothing)
+    loss, weight_sum = compute_weighted_cross_entropy(
+        logits, targets, weights, label_smoothing
+    )
     mean_loss = loss / weight_sum
     return mean_loss, logits
+
   step = state.step
 
   if state.dynamic_scale:
     # dynamic scale takes care of averaging gradients across replicas
     grad_fn = state.dynamic_scale.value_and_grad(
-        loss_fn, has_aux=True, axis_name="batch")
+        loss_fn, has_aux=True, axis_name="batch"
+    )
     dynamic_scale, is_fin, (_, logits), grads = grad_fn(state.params)
     state = state.replace(dynamic_scale=dynamic_scale)
   else:
     grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
     (_, logits), grads = grad_fn(state.params)
     grads = jax.lax.pmean(grads, axis_name="batch")
 
@@ -221,18 +248,20 @@
 
   if state.dynamic_scale:
     # if is_fin == False the gradients contain Inf/NaNs and optimizer state and
     # params should be restored (= skip this step).
     select_fn = functools.partial(jnp.where, is_fin)
     new_state = new_state.replace(
         opt_state=jax.tree_util.tree_map(
-            select_fn, new_state.opt_state, state.opt_state),
+            select_fn, new_state.opt_state, state.opt_state
+        ),
         params=jax.tree_util.tree_map(
-            select_fn, new_state.params, state.params)
-        )
+            select_fn, new_state.params, state.params
+        ),
+    )
     metrics["loss_scale"] = dynamic_scale.scale * metrics["denominator"]
 
   return new_state, metrics
 
 
 def eval_step(params, batch, config, label_smoothing=0.0):
   """Calculate evaluation metrics on a batch."""
@@ -243,53 +272,50 @@
   return compute_metrics(logits, targets, weights, label_smoothing)
 
 
 def initialize_cache(inputs, max_decode_len, config):
   """Initialize a cache for a given input shape and max decode length."""
   target_shape = (inputs.shape[0], max_decode_len) + inputs.shape[2:]
   initial_variables = models.Transformer(config).init(
-      jax.random.PRNGKey(0), jnp.ones(inputs.shape, config.dtype),
-      jnp.ones(target_shape, config.dtype))
+      jax.random.PRNGKey(0),
+      jnp.ones(inputs.shape, config.dtype),
+      jnp.ones(target_shape, config.dtype),
+  )
   return initial_variables["cache"]
 
 
-def predict_step(inputs,
-                 params,
-                 cache,
-                 eos_id,
-                 max_decode_len,
-                 config,
-                 beam_size=4):
+def predict_step(
+    inputs, params, cache, eos_id, max_decode_len, config, beam_size=4
+):
   """Predict translation with fast decoding beam search on a batch."""
   # Prepare transformer fast-decoder call for beam search: for beam search, we
   # need to set up our decoder model to handle a batch size equal to
   # batch_size * beam_size, where each batch item"s data is expanded in-place
   # rather than tiled.
   # i.e. if we denote each batch element subtensor as el[n]:
   # [el0, el1, el2] --> beamsize=2 --> [el0,el0,el1,el1,el2,el2]
   encoded_inputs = decode.flat_batch_beam_expand(
-      models.Transformer(config).apply({"params": params},
-                                       inputs,
-                                       method=models.Transformer.encode),
-      beam_size)
+      models.Transformer(config).apply(
+          {"params": params}, inputs, method=models.Transformer.encode
+      ),
+      beam_size,
+  )
   raw_inputs = decode.flat_batch_beam_expand(inputs, beam_size)
 
   def tokens_ids_to_logits(flat_ids, flat_cache):
     """Token slice to logits from decoder model."""
     # --> [batch * beam, 1, vocab]
     flat_logits, new_vars = models.Transformer(config).apply(
-        {
-            "params": params,
-            "cache": flat_cache
-        },
+        {"params": params, "cache": flat_cache},
         encoded_inputs,
         raw_inputs,  # only needed for input padding mask
         flat_ids,
         mutable=["cache"],
-        method=models.Transformer.decode)
+        method=models.Transformer.decode,
+    )
     new_flat_cache = new_vars["cache"]
     # Remove singleton sequence-length dimension:
     # [batch * beam, 1, vocab] --> [batch * beam, vocab]
     flat_logits = flat_logits.squeeze(axis=1)
     return flat_logits, new_flat_cache
 
   # Using the above-defined single-step decoder function, run a
@@ -297,15 +323,16 @@
   beam_seqs, _ = decode.beam_search(
       inputs,
       cache,
       tokens_ids_to_logits,
       beam_size=beam_size,
       alpha=0.6,
       eos_id=eos_id,
-      max_decode_len=max_decode_len)
+      max_decode_len=max_decode_len,
+  )
 
   # Beam search returns [n_batch, n_beam, n_length + 1] with beam dimension
   # sorted in increasing order of log-probability.
   # Return the highest scoring beam sequence, drop first dummy 0 token.
   return beam_seqs[:, -1, 1:]
 
 
@@ -324,78 +351,94 @@
   host2devices = collections.defaultdict(list)
   for d in jax.devices():
     host2devices[d.process_index].append(d)
   devices = [host2devices[k][0] for k in host2devices]
   host_psum = jax.pmap(lambda x: jax.lax.psum(x, "i"), "i", devices=devices)
 
   def pre_pmap(xs):
-    return jax.tree_util.tree_map(lambda x: jnp.broadcast_to(x, (1,) + x.shape), xs)
+    return jax.tree_util.tree_map(
+        lambda x: jnp.broadcast_to(x, (1,) + x.shape), xs
+    )
 
   def post_pmap(xs):
     return jax.tree_util.tree_map(lambda x: x[0], xs)
 
   return post_pmap(host_psum(pre_pmap(in_tree)))
 
 
 def tohost(x):
   """Collect batches from all devices to host and flatten batch dimensions."""
   n_device, n_batch, *remaining_dims = x.shape
   return np.array(x).reshape((n_device * n_batch,) + tuple(remaining_dims))
 
 
-def evaluate(*, p_eval_step, params, eval_ds: tf.data.Dataset,
-             num_eval_steps: int):
+def evaluate(
+    *, p_eval_step, params, eval_ds: tf.data.Dataset, num_eval_steps: int
+):
   """Evaluate the params an return a dictionary with the metrics."""
   logging.info("Gathering evaluation metrics.")
   eval_metrics = []
   eval_iter = iter(eval_ds)  # pytype: disable=wrong-arg-types
   for _, eval_batch in zip(range(num_eval_steps), eval_iter):
     eval_batch = jax.tree_util.tree_map(lambda x: x._numpy(), eval_batch)  # pylint: disable=protected-access
     eval_batch = common_utils.shard(eval_batch)
     metrics = p_eval_step(params, eval_batch)
     eval_metrics.append(metrics)
   eval_metrics = common_utils.get_metrics(eval_metrics)
   eval_metrics_sums = jax.tree_util.tree_map(jnp.sum, eval_metrics)
   eval_denominator = eval_metrics_sums.pop("denominator")
   eval_summary = jax.tree_util.tree_map(
       lambda x: x / eval_denominator,  # pylint: disable=cell-var-from-loop
-      eval_metrics_sums)
+      eval_metrics_sums,
+  )
   return eval_summary
 
 
-def translate_and_calculate_bleu(*, p_pred_step, p_init_cache, params,
-                                 predict_ds: tf.data.Dataset, decode_tokens,
-                                 max_predict_length: int):
+def translate_and_calculate_bleu(
+    *,
+    p_pred_step,
+    p_init_cache,
+    params,
+    predict_ds: tf.data.Dataset,
+    decode_tokens,
+    max_predict_length: int,
+):
   """Translates the `predict_ds` and calculates the BLEU score."""
   n_devices = jax.local_device_count()
   logging.info("Translating evaluation dataset.")
   sources, references, predictions = [], [], []
   for pred_batch in predict_ds:
     pred_batch = jax.tree_util.tree_map(lambda x: x._numpy(), pred_batch)  # pylint: disable=protected-access
     # Handle final odd-sized batch by padding instead of dropping it.
     cur_pred_batch_size = pred_batch["inputs"].shape[0]
     if cur_pred_batch_size % n_devices:
       padded_size = int(np.ceil(cur_pred_batch_size / n_devices) * n_devices)
       pred_batch = jax.tree_util.tree_map(
           lambda x: pad_examples(x, padded_size),  # pylint: disable=cell-var-from-loop
-          pred_batch)
+          pred_batch,
+      )
     pred_batch = common_utils.shard(pred_batch)
     cache = p_init_cache(pred_batch["inputs"])
-    predicted = p_pred_step(pred_batch["inputs"], params, cache, decode.EOS_ID,
-                            max_predict_length)
+    predicted = p_pred_step(
+        pred_batch["inputs"], params, cache, decode.EOS_ID, max_predict_length
+    )
     predicted = tohost(predicted)
     inputs = tohost(pred_batch["inputs"])
     targets = tohost(pred_batch["targets"])
     # Iterate through non-padding examples of batch.
     for i, s in enumerate(predicted[:cur_pred_batch_size]):
       sources.append(decode_tokens(inputs[i]))
       references.append(decode_tokens(targets[i]))
       predictions.append(decode_tokens(s))
-  logging.info("Translation: %d predictions %d references %d sources.",
-               len(predictions), len(references), len(sources))
+  logging.info(
+      "Translation: %d predictions %d references %d sources.",
+      len(predictions),
+      len(references),
+      len(sources),
+  )
 
   # Calculate BLEU score for translated eval corpus against reference.
   bleu_matches = bleu.bleu_partial(references, predictions)
   all_bleu_matches = per_host_sum_pmap(bleu_matches)
   bleu_score = bleu.complete_bleu(*all_bleu_matches)
   # Save translation samples for tensorboard.
   exemplars = ""
@@ -433,22 +476,23 @@
   # Load Dataset
   # ---------------------------------------------------------------------------
   logging.info("Initializing dataset.")
   train_ds, eval_ds, predict_ds, encoder = input_pipeline.get_wmt_datasets(
       n_devices=jax.local_device_count(),
       config=config,
       reverse_translation=config.reverse_translation,
-      vocab_path=vocab_path)
+      vocab_path=vocab_path,
+  )
 
   train_iter = iter(train_ds)
   vocab_size = int(encoder.vocab_size())
   eos_id = decode.EOS_ID  # Default Sentencepiece EOS token.
 
   def decode_tokens(toks):
-    valid_toks = toks[:np.argmax(toks == eos_id) + 1].astype(np.int32)
+    valid_toks = toks[: np.argmax(toks == eos_id) + 1].astype(np.int32)
     return encoder.detokenize(valid_toks).numpy().decode("utf-8")
 
   if config.num_predict_steps > 0:
     predict_ds = predict_ds.take(config.num_predict_steps)
 
   logging.info("Initializing model, optimizer, and step functions.")
 
@@ -469,32 +513,36 @@
       mlp_dim=config.mlp_dim,
       max_len=max(config.max_target_length, config.max_eval_target_length),
       dropout_rate=config.dropout_rate,
       attention_dropout_rate=config.attention_dropout_rate,
       deterministic=False,
       decode=False,
       kernel_init=nn.initializers.xavier_uniform(),
-      bias_init=nn.initializers.normal(stddev=1e-6))
+      bias_init=nn.initializers.normal(stddev=1e-6),
+  )
   eval_config = train_config.replace(deterministic=True)
   predict_config = train_config.replace(deterministic=True, decode=True)
 
   start_step = 0
   rng = jax.random.PRNGKey(config.seed)
   rng, init_rng = jax.random.split(rng)
   input_shape = (config.per_device_batch_size, config.max_target_length)
   target_shape = (config.per_device_batch_size, config.max_target_length)
 
   m = models.Transformer(eval_config)
-  initial_variables = jax.jit(m.init)(init_rng,
-                                      jnp.ones(input_shape, jnp.float32),
-                                      jnp.ones(target_shape, jnp.float32))
+  initial_variables = jax.jit(m.init)(
+      init_rng,
+      jnp.ones(input_shape, jnp.float32),
+      jnp.ones(target_shape, jnp.float32),
+  )
 
   # Create train state with Adam optimizer and weight decay.
   learning_rate_fn = create_learning_rate_schedule(
-      learning_rate=config.learning_rate, warmup_steps=config.warmup_steps)
+      learning_rate=config.learning_rate, warmup_steps=config.warmup_steps
+  )
   dynamic_scale = None
   if dtype == jnp.float16:
     dynamic_scale = dynamic_scale_lib.DynamicScale()
   state = TrainState.create(
       apply_fn=m.apply,
       params=initial_variables["params"],
       tx=optax.adamw(
@@ -513,73 +561,81 @@
   if config.restore_checkpoints:
     # Restore unreplicated optimizer + model state from last checkpoint.
     state = checkpoints.restore_checkpoint(workdir, state)
     # Grab last step.
     start_step = int(state.step)
 
   writer = metric_writers.create_default_writer(
-      workdir, just_logging=jax.process_index() > 0)
+      workdir, just_logging=jax.process_index() > 0
+  )
   if start_step == 0:
     writer.write_hparams(dict(config))
 
   # Replicate state.
   state = jax_utils.replicate(state)
 
   # compile multidevice versions of train/eval/predict step and cache init fn.
   p_train_step = jax.pmap(
       functools.partial(
           train_step,
           config=train_config,
           learning_rate_fn=learning_rate_fn,
-          label_smoothing=config.label_smoothing),
+          label_smoothing=config.label_smoothing,
+      ),
       axis_name="batch",
-      donate_argnums=(0,))  # pytype: disable=wrong-arg-types
+      donate_argnums=(0,),
+  )  # pytype: disable=wrong-arg-types
   p_eval_step = jax.pmap(
-      functools.partial(
-          eval_step, config=eval_config),
-      axis_name="batch")
+      functools.partial(eval_step, config=eval_config), axis_name="batch"
+  )
   p_init_cache = jax.pmap(
       functools.partial(
           initialize_cache,
           max_decode_len=config.max_predict_length,
-          config=predict_config),
-      axis_name="batch")
+          config=predict_config,
+      ),
+      axis_name="batch",
+  )
   p_pred_step = jax.pmap(
       functools.partial(
-          predict_step, config=predict_config, beam_size=config.beam_size),
+          predict_step, config=predict_config, beam_size=config.beam_size
+      ),
       axis_name="batch",
-      static_broadcasted_argnums=(3, 4))  # eos token, max_length are constant
+      static_broadcasted_argnums=(3, 4),
+  )  # eos token, max_length are constant
 
   # Main Train Loop
   # ---------------------------------------------------------------------------
 
   # We init the first set of dropout PRNG keys, but update it afterwards inside
   # the main pmap"d training update for performance.
   dropout_rngs = jax.random.split(rng, jax.local_device_count())
   del rng
 
   logging.info("Starting training loop.")
   hooks = []
   report_progress = periodic_actions.ReportProgress(
-      num_train_steps=config.num_train_steps, writer=writer)
+      num_train_steps=config.num_train_steps, writer=writer
+  )
   if jax.process_index() == 0:
     hooks += [
         report_progress,
-        periodic_actions.Profile(logdir=workdir, num_profile_steps=5)
+        periodic_actions.Profile(logdir=workdir, num_profile_steps=5),
     ]
   train_metrics = []
   with metric_writers.ensure_flushes(writer):
     for step in range(start_step, config.num_train_steps):
       is_last_step = step == config.num_train_steps - 1
 
       # Shard data to devices and do a training step.
       with jax.profiler.StepTraceAnnotation("train", step_num=step):
-        batch = common_utils.shard(jax.tree_util.tree_map(np.asarray, next(train_iter)))
-        state, metrics = p_train_step(
-            state, batch, dropout_rng=dropout_rngs)
+        batch = common_utils.shard(
+            jax.tree_util.tree_map(np.asarray, next(train_iter))
+        )
+        state, metrics = p_train_step(state, batch, dropout_rng=dropout_rngs)
         train_metrics.append(metrics)
 
       # Quick indication that training is happening.
       logging.log_first_n(logging.INFO, "Finished training step %d.", 5, step)
       for h in hooks:
         h(step)
 
@@ -587,42 +643,48 @@
       if step % config.eval_every_steps == 0 or is_last_step:
         with report_progress.timed("training_metrics"):
           logging.info("Gathering training metrics.")
           train_metrics = common_utils.get_metrics(train_metrics)
           lr = train_metrics.pop("learning_rate").mean()
           metrics_sums = jax.tree_util.tree_map(jnp.sum, train_metrics)
           denominator = metrics_sums.pop("denominator")
-          summary = jax.tree_util.tree_map(lambda x: x / denominator, metrics_sums)  # pylint: disable=cell-var-from-loop
+          summary = jax.tree_util.tree_map(
+              lambda x: x / denominator, metrics_sums
+          )  # pylint: disable=cell-var-from-loop
           summary["learning_rate"] = lr
           summary = {"train_" + k: v for k, v in summary.items()}
           writer.write_scalars(step, summary)
           train_metrics = []
 
         with report_progress.timed("eval"):
           eval_results = evaluate(
               p_eval_step=p_eval_step,
               params=state.params,
               eval_ds=eval_ds,
-              num_eval_steps=config.num_eval_steps)
+              num_eval_steps=config.num_eval_steps,
+          )
           writer.write_scalars(
-              step, {"eval_" + k: v for k, v in eval_results.items()})
+              step, {"eval_" + k: v for k, v in eval_results.items()}
+          )
 
         with report_progress.timed("translate_and_bleu"):
           exemplars, bleu_score = translate_and_calculate_bleu(
               p_pred_step=p_pred_step,
               p_init_cache=p_init_cache,
               params=state.params,
               predict_ds=predict_ds,
               decode_tokens=decode_tokens,
-              max_predict_length=config.max_predict_length)
+              max_predict_length=config.max_predict_length,
+          )
           writer.write_scalars(step, {"bleu": bleu_score})
           writer.write_texts(step, {"samples": exemplars})
 
       # Save a checkpoint on one host after every checkpoint_freq steps.
-      save_checkpoint = (step % config.checkpoint_every_steps == 0 or
-                         is_last_step)
-      if (config.save_checkpoints and save_checkpoint):
+      save_checkpoint = (
+          step % config.checkpoint_every_steps == 0 or is_last_step
+      )
+      if config.save_checkpoints and save_checkpoint:
         logging.info("Saving checkpoint step %d.", step)
         with report_progress.timed("checkpoint"):
           checkpoints.save_checkpoint_multiprocess(
               workdir, jax_utils.unreplicate(state), step
           )
```

### Comparing `flax-0.7.0/examples/wmt/train_test.py` & `flax-0.7.1/examples/wmt/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/flax/__init__.py` & `flax-0.7.1/flax/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,23 +8,24 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 """Flax API."""
 
+# pyformat: disable
+
 from .configurations import (
     config as config,
 )
 
 from . import core
 from . import jax_utils
 from . import linen
 from . import serialization
 from . import traverse_util
 
 # DO NOT REMOVE - Marker for internal deprecated API.
 # DO NOT REMOVE - Marker for internal logging.
-from .version import __version__
+from .version import __version__
```

### Comparing `flax-0.7.0/flax/configurations.py` & `flax-0.7.1/flax/configurations.py`

 * *Files 6% similar despite different names*

```diff
@@ -14,28 +14,28 @@
 
 r"""Global configuration options for Flax.
 
 Now a wrapper over jax.config, in which all config vars have a 'flax\_' prefix.
 
 To modify a config value on run time, call:
 ``flax.config.update('flax_<config_name>', <value>)``
-
 """
 
 import os
 from jax import config as jax_config
 
 from contextlib import contextmanager
 
 # Keep a wrapper at the flax namespace, in case we make our implementation
 # in the future.
 config = jax_config
 
 # Config parsing utils
 
+
 def define_bool_state(name, default, help):
   """Set up a boolean flag using JAX's config system.
 
   The flag will actually be stored as an environment variable of
   'FLAX_<UPPERCASE_NAME>'. JAX config ensures that the flag can be overwritten
   on runtime with `flax.config.update('flax_<config_name>', <value>)`.
   """
@@ -61,15 +61,16 @@
   val = val.lower()
   if val in ('y', 'yes', 't', 'true', 'on', '1'):
     return True
   elif val in ('n', 'no', 'f', 'false', 'off', '0'):
     return False
   else:
     raise ValueError(
-        'invalid truth value {!r} for environment {!r}'.format(val, varname))
+        'invalid truth value {!r} for environment {!r}'.format(val, varname)
+    )
 
 
 @contextmanager
 def temp_flip_flag(var_name: str, var_value: bool):
   """Context manager to temporarily flip feature flags for test functions.
 
   Args:
@@ -88,35 +89,43 @@
 
 # Whether to use the lazy rng implementation.
 flax_lazy_rng = static_bool_env('FLAX_LAZY_RNG', True)
 
 flax_filter_frames = define_bool_state(
     name='filter_frames',
     default=True,
-    help=('Whether to hide flax-internal stack frames from tracebacks.'))
+    help='Whether to hide flax-internal stack frames from tracebacks.',
+)
 
 flax_profile = define_bool_state(
     name='profile',
     default=True,
-    help=('Whether to run Module methods under jax.named_scope for profiles.'))
+    help='Whether to run Module methods under jax.named_scope for profiles.',
+)
 
 flax_use_orbax_checkpointing = define_bool_state(
     name='use_orbax_checkpointing',
     default=True,
-    help=('Whether to use Orbax to save checkpoints.'))
+    help='Whether to use Orbax to save checkpoints.',
+)
 
 flax_preserve_adopted_names = define_bool_state(
     name='preserve_adopted_names',
     default=False,
-    help=("When adopting outside modules, don't clobber existing names."))
+    help="When adopting outside modules, don't clobber existing names.",
+)
 
-#TODO(marcuschiam): remove this feature flag once regular dict migration is complete
+# TODO(marcuschiam): remove this feature flag once regular dict migration is complete
 flax_return_frozendict = define_bool_state(
     name='return_frozendict',
-    default=True,
-    help=('Whether to return FrozenDicts when calling init or apply.'))
+    default=False,
+    help='Whether to return FrozenDicts when calling init or apply.',
+)
 
 flax_fix_rng = define_bool_state(
-    name ='fix_rng_separator',
+    name='fix_rng_separator',
     default=False,
-    help=('Whether to add separator characters when folding in static data into PRNG keys.')
+    help=(
+        'Whether to add separator characters when folding in static data into'
+        ' PRNG keys.'
+    ),
 )
```

### Comparing `flax-0.7.0/flax/core/__init__.py` & `flax-0.7.1/flax/core/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -10,47 +10,44 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from .axes_scan import broadcast as broadcast
 from .frozen_dict import (
-  FrozenDict as FrozenDict,
-  freeze as freeze,
-  unfreeze as unfreeze,
-  copy as copy,
-  pop as pop,
-  pretty_repr as pretty_repr
+    FrozenDict as FrozenDict,
+    copy as copy,
+    freeze as freeze,
+    pop as pop,
+    pretty_repr as pretty_repr,
+    unfreeze as unfreeze,
 )
-
-from .tracers import (
-  current_trace as current_trace,
-  trace_level as trace_level,
-  check_trace_level as check_trace_level
-)
-
-from .scope import (
-  Scope as Scope,
-  Array as Array,
-  DenyList as DenyList,
-  apply as apply,
-  init as init,
-  lazy_init as lazy_init,
-  bind as bind)
-
 from .lift import (
-  scan as scan,
-  vmap as vmap,
-  jit as jit,
-  remat as remat,
-  remat_scan as remat_scan,
-  while_loop as while_loop,
-  custom_vjp as custom_vjp,
-  vjp as vjp,
-  jvp as jvp
+    custom_vjp as custom_vjp,
+    jit as jit,
+    jvp as jvp,
+    remat_scan as remat_scan,
+    remat as remat,
+    scan as scan,
+    vjp as vjp,
+    vmap as vmap,
+    while_loop as while_loop,
 )
-
 from .meta import (
-  AxisMetadata as AxisMetadata,
-  unbox as unbox,
-  map_axis_meta as map_axis_meta,
+    AxisMetadata as AxisMetadata,
+    map_axis_meta as map_axis_meta,
+    unbox as unbox,
+)
+from .scope import (
+    Array as Array,
+    DenyList as DenyList,
+    Scope as Scope,
+    apply as apply,
+    bind as bind,
+    init as init,
+    lazy_init as lazy_init,
+)
+from .tracers import (
+    check_trace_level as check_trace_level,
+    current_trace as current_trace,
+    trace_level as trace_level,
 )
```

### Comparing `flax-0.7.0/flax/core/axes_scan.py` & `flax-0.7.1/flax/core/axes_scan.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,24 +27,26 @@
 
 ScanAxis = Optional[int]
 
 
 class _Broadcast:
   pass
 
+
 broadcast = _Broadcast()
 
 
 def scan(
     fn: Callable[..., Any],
     in_axes: Any,
     out_axes: Any,
     length: Optional[int] = None,
     reverse: bool = False,
-    unroll: int = 1):
+    unroll: int = 1,
+):
   """A wrapper around `jax.lax.scan` with in_axes/out_axes api.
 
   Example::
     def body_fn(b, c, x):
       return b + 2, c + 1, 2 * x
 
     loop = scan(body_fn, in_axes=0, out_axes=0)
@@ -80,78 +82,94 @@
   """
 
   def transpose_to_front(ax, xs):
     if ax is broadcast:
       return ()
     if ax == 0:
       return xs
+
     def trans(x):
       perm = tuple(range(x.ndim))
       perm = (ax,) + tuple(np.delete(perm, ax))
       return jnp.transpose(x, perm)
+
     return jax.tree_util.tree_map(trans, xs)
 
   def transpose_from_front(ax, xs):
     if ax is broadcast:
       return ()
     if ax == 0:
       return xs
+
     def trans(x):
       if ax < 0:
         pax = x.ndim - ax
       else:
         pax = ax
       assert pax < x.ndim
       perm = tuple(range(1, pax + 1)) + (0,) + tuple(range(pax + 1, x.ndim))
       return jnp.transpose(x, perm)
+
     return jax.tree_util.tree_map(trans, xs)
 
   def scan_fn(broadcast_in, init, *args):
     xs = jax.tree_util.tree_map(transpose_to_front, in_axes, args)
 
     def body_fn(c, xs, init_mode=False):
       # inject constants
       xs = jax.tree_util.tree_map(
-          lambda ax, arg, x: (arg if ax is broadcast else x), in_axes, args, xs)
+          lambda ax, arg, x: (arg if ax is broadcast else x), in_axes, args, xs
+      )
       broadcast_out, c, ys = fn(broadcast_in, c, *xs)
 
       if init_mode:
         ys = jax.tree_util.tree_map(
-            lambda ax, y: (y if ax is broadcast else ()), out_axes, ys)
+            lambda ax, y: (y if ax is broadcast else ()), out_axes, ys
+        )
         return broadcast_out, ys
       else:
         ys = jax.tree_util.tree_map(
-            lambda ax, y: (() if ax is broadcast else y), out_axes, ys)
+            lambda ax, y: (() if ax is broadcast else y), out_axes, ys
+        )
         return c, ys
+
     broadcast_body = functools.partial(body_fn, init_mode=True)
 
     carry_avals = jax.tree_util.tree_map(
-        lambda x: core.ShapedArray(jnp.shape(x), jnp.result_type(x)),
-        init)
+        lambda x: core.ShapedArray(jnp.shape(x), jnp.result_type(x)), init
+    )
     scan_avals = jax.tree_util.tree_map(
-        lambda x: core.ShapedArray(jnp.shape(x)[1:], jnp.result_type(x)),
-        xs)
+        lambda x: core.ShapedArray(jnp.shape(x)[1:], jnp.result_type(x)), xs
+    )
     input_avals = (carry_avals, scan_avals)
 
     in_avals, in_tree = jax.tree_util.tree_flatten(input_avals)
     f_flat, out_tree = jax.api_util.flatten_fun_nokwargs(
-        lu.wrap_init(broadcast_body), in_tree)
+        lu.wrap_init(broadcast_body), in_tree
+    )
     in_pvals = list(map(pe.PartialVal.unknown, in_avals))
     _, out_pvals, _ = pe.trace_to_jaxpr_nounits(f_flat, in_pvals)
 
     out_flat = []
     for pv, const in out_pvals:
       if pv is not None:
         raise ValueError(
-            'broadcasted variable has a data dependency on the scan body.')
+            'broadcasted variable has a data dependency on the scan body.'
+        )
       out_flat.append(const)
-    broadcast_in, constants_out = jax.tree_util.tree_unflatten(out_tree(), out_flat)
-
-    c, ys = lax.scan(body_fn, init, xs, length=length,
-                     reverse=reverse, unroll=unroll)
+    broadcast_in, constants_out = jax.tree_util.tree_unflatten(
+        out_tree(), out_flat
+    )
+
+    c, ys = lax.scan(
+        body_fn, init, xs, length=length, reverse=reverse, unroll=unroll
+    )
     ys = jax.tree_util.tree_map(transpose_from_front, out_axes, ys)
     ys = jax.tree_util.tree_map(
-        lambda ax, const, y: (const if ax is broadcast else y), out_axes,
-        constants_out, ys)
+        lambda ax, const, y: (const if ax is broadcast else y),
+        out_axes,
+        constants_out,
+        ys,
+    )
     return broadcast_in, c, ys
 
   return scan_fn
```

### Comparing `flax-0.7.0/flax/core/flax_functional_engine.ipynb` & `flax-0.7.1/flax/core/flax_functional_engine.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/flax/core/frozen_dict.py` & `flax-0.7.1/flax/core/frozen_dict.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,14 +46,15 @@
   # skip the final line because it's empty and should not be indented.
   return '\n'.join(indent_str + line for line in lines[:-1]) + '\n'
 
 
 @jax.tree_util.register_pytree_with_keys_class
 class FrozenDict(Mapping[K, V]):
   """An immutable variant of the Python dict."""
+
   __slots__ = ('_dict', '_hash')
 
   def __init__(self, *args, __unsafe_skip_copy__=False, **kwargs):  # pylint: disable=invalid-name
     # make sure the dict is as
     xs = dict(*args, **kwargs)
     if __unsafe_skip_copy__:
       self._dict = xs
@@ -84,37 +85,39 @@
     return self.pretty_repr()
 
   def __reduce__(self):
     return FrozenDict, (self.unfreeze(),)
 
   def pretty_repr(self, num_spaces=4):
     """Returns an indented representation of the nested dictionary."""
+
     def pretty_dict(x):
       if not isinstance(x, dict):
         return repr(x)
       rep = ''
       for key, val in x.items():
         rep += f'{key}: {pretty_dict(val)},\n'
       if rep:
         return '{\n' + _indent(rep, num_spaces) + '}'
       else:
         return '{}'
+
     return f'FrozenDict({pretty_dict(self._dict)})'
 
   def __hash__(self):
     if self._hash is None:
       h = 0
       for key, value in self.items():
         h ^= hash((key, value))
       self._hash = h
     return self._hash
 
   def copy(self, add_or_replace: Mapping[K, V]) -> 'FrozenDict[K, V]':
     """Create a new FrozenDict with additional or replaced entries."""
-    return type(self)({**self, **unfreeze(add_or_replace)}) # type: ignore[arg-type]
+    return type(self)({**self, **unfreeze(add_or_replace)})  # type: ignore[arg-type]
 
   def keys(self):
     return FrozenKeysView(self)
 
   def values(self):
     return FrozenValuesView(self)
 
@@ -214,15 +217,18 @@
     for key, value in x.items():
       ys[key] = unfreeze(value)
     return ys
   else:
     return x
 
 
-def copy(x: Union[FrozenDict, Dict[str, Any]], add_or_replace: Union[FrozenDict, Dict[str, Any]]) -> Union[FrozenDict, Dict[str, Any]]:
+def copy(
+    x: Union[FrozenDict, Dict[str, Any]],
+    add_or_replace: Union[FrozenDict, Dict[str, Any]],
+) -> Union[FrozenDict, Dict[str, Any]]:
   """Create a new dict with additional and/or replaced entries. This is a utility
   function that can act on either a FrozenDict or regular dict and mimics the
   behavior of `FrozenDict.copy`.
 
   Example::
 
   new_variables = copy(variables, {'additional_entries': 1})
@@ -233,21 +239,23 @@
   Returns:
     A new dict with the additional and/or replaced entries.
   """
 
   if isinstance(x, FrozenDict):
     return x.copy(add_or_replace)
   elif isinstance(x, dict):
-    new_dict = jax.tree_map(lambda x: x, x) # make a deep copy of dict x
+    new_dict = jax.tree_map(lambda x: x, x)  # make a deep copy of dict x
     new_dict.update(add_or_replace)
     return new_dict
   raise TypeError(f'Expected FrozenDict or dict, got {type(x)}')
 
 
-def pop(x: Union[FrozenDict, Dict[str, Any]], key: str) -> Tuple[Union[FrozenDict, Dict[str, Any]], Any]:
+def pop(
+    x: Union[FrozenDict, Dict[str, Any]], key: str
+) -> Tuple[Union[FrozenDict, Dict[str, Any]], Any]:
   """Create a new dict where one entry is removed. This is a utility
   function that can act on either a FrozenDict or regular dict and
   mimics the behavior of `FrozenDict.pop`.
 
   Example::
 
     state, params = pop(variables, 'params')
@@ -258,15 +266,15 @@
   Returns:
     A pair with the new dict and the removed value.
   """
 
   if isinstance(x, FrozenDict):
     return x.pop(key)
   elif isinstance(x, dict):
-    new_dict = jax.tree_map(lambda x: x, x) # make a deep copy of dict x
+    new_dict = jax.tree_map(lambda x: x, x)  # make a deep copy of dict x
     value = new_dict.pop(key)
     return new_dict, value
   raise TypeError(f'Expected FrozenDict or dict, got {type(x)}')
 
 
 def pretty_repr(x: Any, num_spaces: int = 4) -> str:
   """Returns an indented representation of the nested dictionary.
@@ -280,40 +288,46 @@
   Returns:
     An indented string representation of the nested dictionary.
   """
 
   if isinstance(x, FrozenDict):
     return x.pretty_repr()
   else:
+
     def pretty_dict(x):
       if not isinstance(x, dict):
         return repr(x)
       rep = ''
       for key, val in x.items():
         rep += f'{key}: {pretty_dict(val)},\n'
       if rep:
         return '{\n' + _indent(rep, num_spaces) + '}'
       else:
         return '{}'
+
     return pretty_dict(x)
 
 
 def _frozen_dict_state_dict(xs):
   return {key: serialization.to_state_dict(value) for key, value in xs.items()}
 
 
 def _restore_frozen_dict(xs, states):
   diff = set(map(str, xs.keys())).difference(states.keys())
   if diff:
-    raise ValueError('The target dict keys and state dict keys do not match,'
-                   f' target dict contains keys {diff} which are not present in state dict '
-                   f'at path {serialization.current_path()}')
+    raise ValueError(
+        'The target dict keys and state dict keys do not match, target dict'
+        f' contains keys {diff} which are not present in state dict at path'
+        f' {serialization.current_path()}'
+    )
 
   return FrozenDict(
-      {key: serialization.from_state_dict(value, states[key], name=key)
-       for key, value in xs.items()})
+      {
+          key: serialization.from_state_dict(value, states[key], name=key)
+          for key, value in xs.items()
+      }
+  )
 
 
 serialization.register_serialization_state(
-    FrozenDict,
-    _frozen_dict_state_dict,
-    _restore_frozen_dict)
+    FrozenDict, _frozen_dict_state_dict, _restore_frozen_dict
+)
```

### Comparing `flax-0.7.0/flax/core/lift.py` & `flax-0.7.1/flax/core/lift.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,39 +13,62 @@
 # limitations under the License.
 
 """Jax transform lifting."""
 
 import collections
 import dataclasses
 import functools
-from typing import (Any, Callable, Dict, Generic, Iterable, List, Mapping,
-                    Optional, Sequence, Tuple, TypeVar, Union)
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Generic,
+    Iterable,
+    List,
+    Mapping,
+    Optional,
+    Sequence,
+    Tuple,
+    TypeVar,
+    Union,
+)
 import warnings
 
+from flax import traceback_util
+import jax
+from jax import random
+
 from . import axes_scan
 from . import meta
-from flax import traceback_util
 from .frozen_dict import freeze
 from .frozen_dict import unfreeze
-import jax
-from jax import random
-from .scope import (CollectionFilter, DenyList, PRNGSequenceFilter,  # pylint: disable=g-multiple-import
-                    Filter, Scope, group_collections, in_filter,
-                    intersect_filters, is_filter_empty, subtract_filters,
-                    union_filters)
+from .scope import (
+    CollectionFilter,
+    DenyList,  # pylint: disable=g-multiple-import
+    Filter,
+    PRNGSequenceFilter,
+    Scope,
+    group_collections,
+    in_filter,
+    intersect_filters,
+    is_filter_empty,
+    subtract_filters,
+    union_filters,
+)
 
 traceback_util.register_exclusion(__file__)
 
 T = TypeVar('T')
 
 
 def tree_map_rngs(fn, tree):
   """Needed for mapping JAX random.* functions over KeyArray leaves."""
   return jax.tree_util.tree_map(
-      fn, tree, is_leaf=lambda x: isinstance(x, random.KeyArray))
+      fn, tree, is_leaf=lambda x: isinstance(x, random.KeyArray)
+  )
 
 
 def _dedup_scopes(scopes):
   """Deduplicated scopes."""
   paths = []
   # must preseve insertion order for duplication to work correctly
   minimal_set = collections.OrderedDict((s, ()) for s in scopes)
@@ -78,20 +101,22 @@
   return scopes
 
 
 def _transpose(xs):
   return tuple(zip(*xs))
 
 
-def pack(fn: Callable[..., Any],
-         in_variable_filters: Sequence[CollectionFilter],
-         out_variable_filters: Sequence[CollectionFilter],
-         rng_filters: Sequence[PRNGSequenceFilter],
-         name=None,
-         enable_kwargs=False) -> Callable[..., Any]:
+def pack(
+    fn: Callable[..., Any],
+    in_variable_filters: Sequence[CollectionFilter],
+    out_variable_filters: Sequence[CollectionFilter],
+    rng_filters: Sequence[PRNGSequenceFilter],
+    name=None,
+    enable_kwargs=False,
+) -> Callable[..., Any]:
   """Pack variables and rngs for functional transformations.
 
   The pack function is the building block for all other lifted transformations.
 
   Args:
     fn: The function to pack. `fn` has the signature
       `(scope_fn, repack_fn, variable_groups, rng_groups, *args) ->
@@ -100,159 +125,181 @@
     out_variable_filters: Output variable filters.
     rng_filters: RNG filters.
     name: The name of the packed scope.
     enable_kwargs: Whether to enable kwargs or not.
   Returns:
     A callable which expects a scope as the first argument.
   """
+
   @functools.wraps(fn)
   def wrapper(scope_tree: Scope, *args, **kwargs):
     if not enable_kwargs and kwargs:
-      msg = 'kwargs are not supported in {}, so \"{}\" is(are) ignored'
+      msg = 'kwargs are not supported in {}, so "{}" is(are) ignored'
       warnings.warn(msg.format(name, ', '.join(kwargs.keys())), RuntimeWarning)
     # pylint: disable=protected-access
     scopes, treedef = jax.tree_util.tree_flatten(scope_tree)
     scopes, paths = _dedup_scopes(scopes)
 
     variable_groups_xs = []
 
     for scope in scopes:
       scope._validate_trace_level()
       scope._populate_collections()
-      variable_groups_xs.append(group_collections(
-          scope._variables, in_variable_filters))
+      variable_groups_xs.append(
+          group_collections(scope._variables, in_variable_filters)
+      )
     variable_groups_xs_t = _transpose(variable_groups_xs)
 
     # Make sure that in-only variable collections are frozen
     for variable_group_xs in variable_groups_xs_t:
       for variable_group in variable_group_xs:
         for col_name, collection in variable_group.items():
           col_in_out = any(
               in_filter(col_filter, col_name)
-              for col_filter in out_variable_filters)
+              for col_filter in out_variable_filters
+          )
           if not col_in_out:
             variable_group[col_name] = freeze(collection)
     rng_groups_xs = []
     inner_rng_counters = []
     for scope in scopes:
       rng_counters = scope.rng_counters
       rng_groups = group_collections(scope.rngs, rng_filters)
       rng_groups_xs.append(rng_groups)
       inner_rng_counters.append(rng_counters)
     rng_groups_xs_t = _transpose(rng_groups_xs)
 
     inner_scopes: List[Scope] = []
 
-    def scope_fn(variable_groups_xs_t,
-                 rng_groups_xs_t,
-                 mutable_filter: CollectionFilter = True):
+    def scope_fn(
+        variable_groups_xs_t,
+        rng_groups_xs_t,
+        mutable_filter: CollectionFilter = True,
+    ):
       nonlocal inner_scopes
       for inner_scope in inner_scopes:
         inner_scope.invalidate()
       inner_scopes = []
       mutable: Filter = False
       for out_filter in out_variable_filters:
         mutable = union_filters(mutable, out_filter)
       # could be () in the edge case where no rngs or variable_groups are lifted
       # in this case fallback to ((),) * len(scopes) to make sure the zip has
       # something to iterate over for each scope.
-      variable_groups_xs = _transpose(variable_groups_xs_t) or (
-          (),) * len(scopes)
+      variable_groups_xs = _transpose(variable_groups_xs_t) or ((),) * len(
+          scopes
+      )
       rng_groups_xs = _transpose(rng_groups_xs_t) or ((),) * len(scopes)
       assert len(variable_groups_xs) == len(scopes)
       assert len(rng_groups_xs) == len(scopes)
       for variable_groups, rng_groups, scope, rng_counters in zip(
-          variable_groups_xs, rng_groups_xs, scopes, inner_rng_counters):
+          variable_groups_xs, rng_groups_xs, scopes, inner_rng_counters
+      ):
         variables = {}
         rngs = {}
         for variable_group in variable_groups:
           variables.update(variable_group)
         for rng_group in rng_groups:
           rngs.update(rng_group)
         # make sure variable dicts are cloned and can't be manipulated by ref
         # sharing.
         variables = jax.tree_util.tree_map(lambda x: x, variables)
         scope_mutable = intersect_filters(
-            intersect_filters(scope.mutable, mutable), mutable_filter)
+            intersect_filters(scope.mutable, mutable), mutable_filter
+        )
         new_path = scope.path
         if name:
           if new_path:
             new_path = new_path[:-1] + (f'{name}({new_path[-1]})',)
           else:
             new_path = (f'{name}()',)
         inner_scope = Scope(
-            variables, name=scope.name, rngs=rngs,
-            mutable=scope_mutable, parent=None,
-            path=new_path, flags=scope.flags)
+            variables,
+            name=scope.name,
+            rngs=rngs,
+            mutable=scope_mutable,
+            parent=None,
+            path=new_path,
+            flags=scope.flags,
+        )
         inner_scope.rng_counters = rng_counters
         inner_scopes.append(inner_scope)
       inner_scopes = _dup_scopes(scopes, inner_scopes, paths)
       return treedef.unflatten(inner_scopes)
 
     def repack(inner_scope_tree):
       inner_scopes = treedef.flatten_up_to(inner_scope_tree)
       inner_scopes, inner_paths = _dedup_scopes(inner_scopes)
       inner_scopes = list(inner_scopes)
       assert [p for _, p in paths] == [p for _, p in inner_paths]
       out_variable_groups_xs = []
       for inner_scope in inner_scopes:
         inner_scope.invalidate()
         inner_scope._validate_trace_level()
-        mutable_variables = {key: val for key, val
-                             in inner_scope._variables.items()
-                             if in_filter(inner_scope.mutable, key)}
+        mutable_variables = {
+            key: val
+            for key, val in inner_scope._variables.items()
+            if in_filter(inner_scope.mutable, key)
+        }
         out_variable_groups = group_collections(
-            mutable_variables, tuple(out_variable_filters) + (True,))
+            mutable_variables, tuple(out_variable_filters) + (True,)
+        )
         remainder = tuple(out_variable_groups[-1].keys())
         if remainder:
           raise ValueError(f'unmapped output variables: {remainder}')
         out_variable_groups_xs.append(out_variable_groups[:-1])
 
       return _transpose(out_variable_groups_xs)
 
     try:
       if enable_kwargs:
         y, out_variable_groups_xs_t = fn(
-            scope_fn, repack,
-            variable_groups_xs_t, rng_groups_xs_t,
-            *args, **kwargs)
+            scope_fn,
+            repack,
+            variable_groups_xs_t,
+            rng_groups_xs_t,
+            *args,
+            **kwargs,
+        )
       else:
         y, out_variable_groups_xs_t = fn(
-            scope_fn, repack,
-            variable_groups_xs_t, rng_groups_xs_t,
-            *args)
+            scope_fn, repack, variable_groups_xs_t, rng_groups_xs_t, *args
+        )
     finally:
       for inner_scope in inner_scopes:
         inner_scope.invalidate()
     out_variable_groups_xs = _transpose(out_variable_groups_xs_t)
-    for scope, out_variable_groups, rng_counters in zip(scopes,
-                                                        out_variable_groups_xs,
-                                                        inner_rng_counters):
+    for scope, out_variable_groups, rng_counters in zip(
+        scopes, out_variable_groups_xs, inner_rng_counters
+    ):
       for out_variable_group in out_variable_groups:
         for col_name, collection in out_variable_group.items():
           if not scope.is_mutable_collection(col_name):
             # Some lifted transforms like scan return redundant variables.
             continue
           for var_name, value in collection.items():
             scope.put_variable(col_name, var_name, value)
     return y
+
   return wrapper
 
 
 id_fn = lambda x: x
 
 
-def map_variables(fn: Callable[..., Any],
-                  mapped_collections: CollectionFilter,
-                  map_in_fn: Callable[..., Any] = id_fn,
-                  map_out_fn: Callable[..., Any] = id_fn,
-                  init: bool = False,
-                  mutable: bool = False,
-                  rngs: PRNGSequenceFilter = True,
-                  variables: CollectionFilter = True) -> Callable[..., Any]:
+def map_variables(
+    fn: Callable[..., Any],
+    mapped_collections: CollectionFilter,
+    map_in_fn: Callable[..., Any] = id_fn,
+    map_out_fn: Callable[..., Any] = id_fn,
+    init: bool = False,
+    mutable: bool = False,
+    rngs: PRNGSequenceFilter = True,
+    variables: CollectionFilter = True,
+) -> Callable[..., Any]:
   """Map Variables inside a scope.
 
   Args:
     fn: the function to be transformed.
     mapped_collections: the collection(s) to be transformed.
     map_in_fn: creates a view of the target variables.
     map_out_fn: transforms the updated variables in the view after mutation.
@@ -267,16 +314,18 @@
   """
   is_target_out = mutable or init
 
   def wrapper(scope_fn, repack, variable_groups, rng_groups, *args, **kwargs):
     target, variables = variable_groups
     if init:
       scopes = scope_fn((target, variables), rng_groups)
-      has_mutable_cols = any(not is_filter_empty(scope.mutable)
-                             for scope in jax.tree_util.tree_leaves(scopes))
+      has_mutable_cols = any(
+          not is_filter_empty(scope.mutable)
+          for scope in jax.tree_util.tree_leaves(scopes)
+      )
       if has_mutable_cols:
         fn(scopes, *args, **kwargs)
         target, _ = repack(scopes)
         target = tuple(map_out_fn(x) for x in target)
     target = tuple(map_in_fn(unfreeze(x)) for x in target)
     mfilter = True
     if not is_target_out:
@@ -287,45 +336,52 @@
     y = fn(scopes, *args, **kwargs)
     out_target, out_vars = repack(scopes)
     if is_target_out:
       out_target = tuple(map_out_fn(x) for x in out_target)
     return y, (out_target, out_vars)
 
   in_vars = (mapped_collections, variables)
-  out_vars = in_vars if is_target_out else (False,
-                                            subtract_filters(
-                                                variables, mapped_collections))
+  out_vars = (
+      in_vars
+      if is_target_out
+      else (False, subtract_filters(variables, mapped_collections))
+  )
   return pack(
       wrapper,
       in_vars,
-      out_vars, (rngs,),
+      out_vars,
+      (rngs,),
       enable_kwargs=True,
-      name='map_variables')
+      name='map_variables',
+  )
 
 
 def swap_collection(fn: Callable[..., Any], col_a: str, col_b: str):
   """Swap two collections."""
+
   def swap(target):
     a = target[col_a] if col_a in target else {}
     b = target[col_b] if col_b in target else {}
     target[col_b], target[col_a] = a, b
     return target
 
   return map_variables(fn, (col_a, col_b), swap, swap, mutable=True)
 
 
 @dataclasses.dataclass(frozen=True)
 class In(Generic[T]):
   """Specifies a variable collection should only be lifted as input."""
+
   axis: T
 
 
 @dataclasses.dataclass(frozen=True)
 class Out(Generic[T]):
   """Specifies a variable collection should only be lifted as output."""
+
   axis: T
 
 
 def _split_in_out_axes(xs: Mapping[CollectionFilter, Any]):
   unpack = lambda v: v.axis if isinstance(v, (In, Out)) else v
   in_axes = {k: unpack(v) for k, v in xs.items() if not isinstance(v, Out)}
   out_axes = {k: unpack(v) for k, v in xs.items() if not isinstance(v, In)}
@@ -403,51 +459,58 @@
     ``vjpfun`` is a function from a cotangent vector with the same shape as
     ``primals_out`` to a tuple of cotangent vectors with the same shape as
     ``primals``, representing the vector-Jacobian product of ``fn`` evaluated at
     ``primals``. If ``has_aux`` is ``True``, returns a
     ``(primals_out, vjpfun, aux)`` tuple where ``aux`` is the auxiliary data
     returned by ``fn``.
   """
+
   def inner(scope_fn, repack_fn, variable_groups, rng_groups, *args):
     vjp_vars, other_vars = variable_groups
+
     @functools.wraps(fn)
     def wrapper(vjp_vars, *args):
       variable_groups = (vjp_vars, other_vars)
       scope = scope_fn(variable_groups, rng_groups)
       if has_aux:
         y, aux = fn(scope, *args)
       else:
         y = fn(scope, *args)
         aux = ()
       return y, (aux, repack_fn(scope))
+
     y, bwd, (aux, out_vars) = jax.vjp(
-        wrapper, vjp_vars, *args,
-        reduce_axes=reduce_axes, has_aux=True)
+        wrapper, vjp_vars, *args, reduce_axes=reduce_axes, has_aux=True
+    )
     treedef = jax.tree_util.tree_structure(scope)
-    bwd = jax.tree_util.Partial(
-        functools.partial(_bwd_wrapper, treedef), bwd)
+    bwd = jax.tree_util.Partial(functools.partial(_bwd_wrapper, treedef), bwd)
     if has_aux:
       return (y, bwd, aux), out_vars
     else:
       return (y, bwd), out_vars
+
   return pack(
-      inner, (vjp_variables, variables), (variables,), (rngs,),
+      inner,
+      (vjp_variables, variables),
+      (variables,),
+      (rngs,),
       name='vjp',
-      enable_kwargs=False)(scope, *primals)
+      enable_kwargs=False,
+  )(scope, *primals)
 
 
 def jvp(
     fn: Callable[..., Any],
     scope: Scope,
     primals,
     tangents,
     variable_tangents,
     variables: CollectionFilter = True,
     rngs: PRNGSequenceFilter = True,
-    ) -> Tuple[Any, Any]:
+) -> Tuple[Any, Any]:
   """A lifted version of ``jax.jvp``.
 
   See ``jax.jvp`` for the unlifted Jacobian-vector product (forward gradient).
 
   Note that no tangents are returned for variables. When variable tangents
   are required their value should be returned explicitly by `fn`
   using `scope.variables()`.
@@ -487,49 +550,60 @@
   Returns:
     A ``(primals_out, tangents_out)`` pair, where ``primals_out`` is
     ``fun(*primals)``, and ``tangents_out`` is the Jacobian-vector product of
     ``function`` evaluated at ``primals`` with ``tangents``. The
     ``tangents_out`` value has the same Python tree structure and shapes as
     ``primals_out``.
   """
+
   def inner(scope_fn, repack_fn, variable_groups, rng_groups, *args):
     jvp_vars, other_vars = variable_groups
+
     @functools.wraps(fn)
     def wrapper(vars_primals, args):
       variable_groups = (vars_primals, other_vars)
       scope = scope_fn(variable_groups, rng_groups)
       y = fn(scope, *args)
       return y, repack_fn(scope)
 
-    (y, out_vars), out_tangents = jax.jvp(wrapper, (jvp_vars, args),
-                                          (variable_tangents, tangents))
+    (y, out_vars), out_tangents = jax.jvp(
+        wrapper, (jvp_vars, args), (variable_tangents, tangents)
+    )
     return (y, out_tangents[0]), out_vars
+
   # filter out empty tangent collections because JAX will error on non-equal
   # tree structure for example: {"params": {}} != {}.
   treedef = jax.tree_util.tree_structure(scope)
 
-  variable_tangents = tuple({k: v  # pylint: disable=g-complex-comprehension
-                             for k, v in vt.items()
-                             if v}
-                            for vt in treedef.flatten_up_to(variable_tangents))
+  variable_tangents = tuple(
+      {k: v for k, v in vt.items() if v}  # pylint: disable=g-complex-comprehension
+      for vt in treedef.flatten_up_to(variable_tangents)
+  )
   target = tuple(variable_tangents[0].keys())
   return pack(
-      inner, (target, variables), (variables,), (rngs,),
-      name='jvp', enable_kwargs=False)(scope, *primals)
+      inner,
+      (target, variables),
+      (variables,),
+      (rngs,),
+      name='jvp',
+      enable_kwargs=False,
+  )(scope, *primals)
 
 
-def vmap(fn: Callable[..., Any],
-         variable_axes: Mapping[CollectionFilter, InOutAxis],
-         split_rngs: Mapping[PRNGSequenceFilter, bool],
-         in_axes=0,
-         out_axes=0,
-         axis_size: Optional[int] = None,
-         axis_name: Optional[str] = None,
-         spmd_axis_name: Optional[str] = None,
-         metadata_params: Dict[Any, Any] = {}) -> Callable[..., Any]:
+def vmap(
+    fn: Callable[..., Any],
+    variable_axes: Mapping[CollectionFilter, InOutAxis],
+    split_rngs: Mapping[PRNGSequenceFilter, bool],
+    in_axes=0,
+    out_axes=0,
+    axis_size: Optional[int] = None,
+    axis_name: Optional[str] = None,
+    spmd_axis_name: Optional[str] = None,
+    metadata_params: Dict[Any, Any] = {},
+) -> Callable[..., Any]:
   """A lifted version of ``jax.vmap``.
 
   See ``jax.vmap`` for the unlifted batch transform in Jax.
 
   ``vmap`` can be used to add a batch axis to a scope function.
   For example we could create a version of ``dense`` with
   a batch axis that does not share parameters::
@@ -593,48 +667,51 @@
       if axis is not None:
         leaves = jax.tree_util.tree_leaves(x)
         if leaves:
           return leaves[0].shape[axis]
       return ()
 
     # split rngs
-    axis_sizes = jax.tree_util.tree_map(find_axis_size,
-                                        (variable_in_axes, in_axes),
-                                        (variable_groups, args))
+    axis_sizes = jax.tree_util.tree_map(
+        find_axis_size, (variable_in_axes, in_axes), (variable_groups, args)
+    )
     axis_sizes = set(jax.tree_util.tree_leaves(axis_sizes))
     if axis_size is None and len(axis_sizes) == 1:
-      d_axis_size, = axis_sizes
+      (d_axis_size,) = axis_sizes
     elif len(axis_sizes) > 1:
       raise ValueError(f'Inconsistent batch axis sizes: {axis_sizes}')
     elif axis_size is None:
       raise ValueError('axis_size should be specified manually.')
     else:
       d_axis_size = axis_size
     split_fn = lambda rng: random.split(rng, d_axis_size)
 
     rng_groups = tuple(
         tree_map_rngs(split_fn, rng_group) if split else rng_group
-        for rng_group, split in zip(rng_groups, rng_splits))
+        for rng_group, split in zip(rng_groups, rng_splits)
+    )
 
     new_variable_groups = []
     for var_group, axis in zip(variable_groups, variable_in_axes):
       if axis is not None:
-        new_variable_groups.append(meta.remove_axis(
-            var_group, axis, metadata_params))
+        new_variable_groups.append(
+            meta.remove_axis(var_group, axis, metadata_params)
+        )
       else:
         new_variable_groups.append(var_group)
     variable_groups = tuple(new_variable_groups)
 
     @functools.partial(
         jax.vmap,
         in_axes=(variable_in_axes, rng_axes, in_axes),
         out_axes=(out_axes, variable_out_axes),
         axis_name=axis_name,
         axis_size=axis_size,
-        spmd_axis_name=spmd_axis_name)
+        spmd_axis_name=spmd_axis_name,
+    )
     @functools.wraps(fn)
     def mapped(variable_groups, rng_groups, args):
       scope = scope_fn(variable_groups, rng_groups)
       y = fn(scope, *args)
       return y, repack_fn(scope)
 
     y, vars_out = mapped(variable_groups, rng_groups, args)
@@ -644,33 +721,36 @@
         new_vars_out.append(meta.add_axis(var_group, axis, metadata_params))
       else:
         new_vars_out.append(var_group)
     vars_out = tuple(new_vars_out)
     return y, vars_out
 
   return pack(
-      inner, variable_in_groups, variable_out_groups, rng_groups,
-      name='vmap')
+      inner, variable_in_groups, variable_out_groups, rng_groups, name='vmap'
+  )
 
 
 ScanAxis = int
 InOutScanAxis = Union[ScanAxis, In[ScanAxis], Out[ScanAxis]]
 
-def scan(fn: Callable[..., Any],
-         variable_axes: Mapping[CollectionFilter, InOutScanAxis] = {},
-         variable_broadcast: CollectionFilter = False,
-         variable_carry: CollectionFilter = False,
-         split_rngs: Mapping[PRNGSequenceFilter, bool] = {},
-         in_axes=0, out_axes=0,
-         length: Optional[int] = None,
-         reverse: bool = False,
-         unroll: int = 1,
-         data_transform: Optional[Callable[..., Any]] = None,
-         metadata_params: Dict[Any, Any] = {},
-         ) -> Callable[..., Any]:
+
+def scan(
+    fn: Callable[..., Any],
+    variable_axes: Mapping[CollectionFilter, InOutScanAxis] = {},
+    variable_broadcast: CollectionFilter = False,
+    variable_carry: CollectionFilter = False,
+    split_rngs: Mapping[PRNGSequenceFilter, bool] = {},
+    in_axes=0,
+    out_axes=0,
+    length: Optional[int] = None,
+    reverse: bool = False,
+    unroll: int = 1,
+    data_transform: Optional[Callable[..., Any]] = None,
+    metadata_params: Dict[Any, Any] = {},
+) -> Callable[..., Any]:
   """A lifted version of ``jax.lax.scan``.
 
   See ``jax.lax.scan`` for the unlifted scan in Jax.
 
   To improve consistency with ``vmap``, this version of scan
   uses ``in_axes`` and ``out_axes`` to determine which arguments
   are scanned over and along which axis.
@@ -739,54 +819,59 @@
   """
   variable_in_axes, variable_out_axes = _split_in_out_axes(variable_axes)
   variable_in_groups, variable_in_axes = _unzip2(variable_in_axes.items())
   variable_out_groups, variable_out_axes = _unzip2(variable_out_axes.items())
   assert all(isinstance(ax, int) for ax in variable_in_axes)
   assert all(isinstance(ax, int) for ax in variable_out_axes)
   rng_groups, rng_splits = _unzip2(split_rngs.items())
-  rng_axes = tuple(0 if rng_split else axes_scan.broadcast
-                   for rng_split in rng_splits)
+  rng_axes = tuple(
+      0 if rng_split else axes_scan.broadcast for rng_split in rng_splits
+  )
 
-  def inner(scope_fn, repack_fn,
-            variable_groups, rng_groups,
-            init, *args):
+  def inner(scope_fn, repack_fn, variable_groups, rng_groups, init, *args):
     def find_length(axis, x):
       if axis is not axes_scan.broadcast:
         leaves = jax.tree_util.tree_leaves(x)
         if leaves:
           return leaves[0].shape[axis]
       return ()
+
     # split rngs
     lengths = jax.tree_util.tree_map(find_length, in_axes, args)
     lengths = set(jax.tree_util.tree_leaves(lengths))
     if length is None and len(lengths) == 1:
-      d_length, = lengths
+      (d_length,) = lengths
     elif len(lengths) > 1:
       raise ValueError(f'Inconsistent scan lengths: {lengths}')
     elif length is None:
       raise ValueError('length should be specified manually.')
     else:
       d_length = length
     split_fn = lambda rng: random.split(rng, d_length)
 
     rng_groups = tuple(
         tree_map_rngs(split_fn, rng_group) if split else rng_group
-        for rng_group, split in zip(rng_groups, rng_splits))
+        for rng_group, split in zip(rng_groups, rng_splits)
+    )
 
-    @functools.partial(axes_scan.scan,
-                       in_axes=(variable_in_axes, rng_axes, in_axes),
-                       out_axes=(out_axes, variable_out_axes),
-                       length=length, reverse=reverse,
-                       unroll=unroll)
+    @functools.partial(
+        axes_scan.scan,
+        in_axes=(variable_in_axes, rng_axes, in_axes),
+        out_axes=(out_axes, variable_out_axes),
+        length=length,
+        reverse=reverse,
+        unroll=unroll,
+    )
     def scanned(broadcast_vars, carry, scan_variable_groups, rng_groups, args):
       carry_vars, c = carry
       variable_groups = (broadcast_vars, carry_vars) + scan_variable_groups
       if data_transform is not None:
-        variable_groups, rng_groups = data_transform(variable_groups,
-                                                     rng_groups)
+        variable_groups, rng_groups = data_transform(
+            variable_groups, rng_groups
+        )
       scope = scope_fn(variable_groups, rng_groups)
       c, y = fn(scope, c, *args)
       out_vars = repack_fn(scope)
       broadcast_vars_out = out_vars[0]
       carry_vars = out_vars[1]
       scan_vars = out_vars[2:]
       # add immutable broadcast vars back to broadcast output
@@ -800,40 +885,51 @@
     broadcast_vars = variable_groups[0]
     carry_vars = variable_groups[1]
     scan_vars = variable_groups[2:]
     new_scan_vars = []
     for scan_group, axis in zip(scan_vars, variable_in_axes):
       new_scan_vars.append(meta.remove_axis(scan_group, axis, metadata_params))
     broadcast_vars, (carry_vars, c), (ys, scan_vars) = scanned(
-        broadcast_vars, (carry_vars, init), tuple(new_scan_vars),
-        rng_groups, args)
+        broadcast_vars,
+        (carry_vars, init),
+        tuple(new_scan_vars),
+        rng_groups,
+        args,
+    )
     new_scan_vars = []
     for scan_group, axis in zip(scan_vars, variable_out_axes):
       new_scan_vars.append(meta.add_axis(scan_group, axis, metadata_params))
     scan_vars = tuple(new_scan_vars)
-    out_vars = (broadcast_vars, carry_vars,) + scan_vars
+    out_vars = (
+        broadcast_vars,
+        carry_vars,
+    ) + scan_vars
     return (c, ys), out_vars
 
   return pack(
       inner,
       (variable_broadcast, variable_carry) + variable_in_groups,
       (variable_broadcast, variable_carry) + variable_out_groups,
       rng_groups,
-      name='scan')
+      name='scan',
+  )
 
 
 C = TypeVar('C')
 
 
-def while_loop(cond_fn: Callable[[Scope, C], bool],
-               body_fn: Callable[[Scope, C], C],
-               scope: Scope, init: C,
-               carry_variables: CollectionFilter = False,
-               broadcast_variables: CollectionFilter = True,
-               split_rngs: Mapping[PRNGSequenceFilter, bool] = {}) -> C:
+def while_loop(
+    cond_fn: Callable[[Scope, C], bool],
+    body_fn: Callable[[Scope, C], C],
+    scope: Scope,
+    init: C,
+    carry_variables: CollectionFilter = False,
+    broadcast_variables: CollectionFilter = True,
+    split_rngs: Mapping[PRNGSequenceFilter, bool] = {},
+) -> C:
   """Lifted version of jax.lax.while_loop.
 
   The lifted scope is passed to `cond_fn` and `body_fn`.
   Broadcasted variables are immutable. The carry variable are
   mutable but cannot change shape and dtype.
   This also means you cannot initialize variables inside
   the body. Consider calling `body_fn` once manually before
@@ -867,60 +963,69 @@
     split_rngs: Split PRNG sequences will be different for each loop iterations.
       If split is False the PRNGs will be the same across iterations.
   Returns:
     The final state after executing the while loop.
   """
   rng_groups, rng_splits = _unzip2(split_rngs.items())
 
-  def inner(scope_fn, repack_fn,
-            variable_groups, rng_groups):
+  def inner(scope_fn, repack_fn, variable_groups, rng_groups):
     carry_variables, broadcast_variables = variable_groups
 
     def make_loop_rngs(i):
       local_rng_groups = []
       for rng_group, rng_split in zip(rng_groups, rng_splits):
         if rng_split:
-          rng_group = tree_map_rngs(lambda rng: random.fold_in(rng, i),
-                                    rng_group)
+          rng_group = tree_map_rngs(
+              lambda rng: random.fold_in(rng, i), rng_group
+          )
         local_rng_groups.append(rng_group)
       return local_rng_groups
 
     def cond_wrapper(c):
       i, carry_variables, carry = c
-      scope = scope_fn((carry_variables, broadcast_variables),
-                       make_loop_rngs(-i),
-                       mutable_filter=False)
+      scope = scope_fn(
+          (carry_variables, broadcast_variables),
+          make_loop_rngs(-i),
+          mutable_filter=False,
+      )
       return cond_fn(scope, carry)
 
     def body_wrapper(c):
       i, carry_variables, carry = c
-      scope = scope_fn((carry_variables, broadcast_variables),
-                       make_loop_rngs(i))
+      scope = scope_fn(
+          (carry_variables, broadcast_variables), make_loop_rngs(i)
+      )
       carry = body_fn(scope, carry)
-      carry_variables, = repack_fn(scope)
+      (carry_variables,) = repack_fn(scope)
       return (i + 1, carry_variables, carry)
 
     c = (0, carry_variables, init)
-    _, carry_variables, carry = jax.lax.while_loop(cond_wrapper, body_wrapper,
-                                                   c)
+    _, carry_variables, carry = jax.lax.while_loop(
+        cond_wrapper, body_wrapper, c
+    )
     return carry, (carry_variables,)
 
   return pack(
       inner,
       (carry_variables, broadcast_variables),
       (carry_variables,),
       rng_groups,
-      name='while_loop')(scope)
+      name='while_loop',
+  )(scope)
 
 
-def cond(pred: Any,
-         true_fun: Callable[..., C], false_fun: Callable[..., C],
-         scope: Scope, *operands,
-         variables: CollectionFilter = True,
-         rngs: PRNGSequenceFilter = True) -> C:
+def cond(
+    pred: Any,
+    true_fun: Callable[..., C],
+    false_fun: Callable[..., C],
+    scope: Scope,
+    *operands,
+    variables: CollectionFilter = True,
+    rngs: PRNGSequenceFilter = True,
+) -> C:
   """Lifted version of ``jax.lax.cond``.
 
   The returned values from ``true_fun`` and ``false_fun``
   must have the same Pytree structure, shapes, and dtypes.
   The variables created or updated inside the
   branches must also have the same structure.
   Note that this constraint is violated when
@@ -953,39 +1058,37 @@
     variables: The variable collections passed to the conditional
       branches (default: all)
     rngs: The PRNG sequences passed to the conditionals (default: all)
   Returns:
     The result of the evaluated branch (``true_fun`` or ``false_fun``).
   """
   branches = [true_fun, false_fun]
-  def inner(scope_fn, repack_fn,
-            variable_groups, rng_groups):
+
+  def inner(scope_fn, repack_fn, variable_groups, rng_groups):
     def branch_wrapper(branch_fn, *operands):
       scope = scope_fn(variable_groups, rng_groups)
       y = branch_fn(scope, *operands)
       return y, repack_fn(scope)
+
     pure_branches = [
-        functools.partial(branch_wrapper, branch_fn)
-        for branch_fn in branches]
-    return jax.lax.cond(
-        pred, pure_branches[0], pure_branches[1], *operands)
+        functools.partial(branch_wrapper, branch_fn) for branch_fn in branches
+    ]
+    return jax.lax.cond(pred, pure_branches[0], pure_branches[1], *operands)
 
-  return pack(
-      inner,
-      (variables,),
-      (variables,),
-      (rngs,),
-      name='cond')(scope)
+  return pack(inner, (variables,), (variables,), (rngs,), name='cond')(scope)
 
 
-def switch(index: Any,
-           branches: Sequence[Callable[..., C]],
-           scope: Scope, *operands,
-           variables: CollectionFilter = True,
-           rngs: PRNGSequenceFilter = True) -> C:
+def switch(
+    index: Any,
+    branches: Sequence[Callable[..., C]],
+    scope: Scope,
+    *operands,
+    variables: CollectionFilter = True,
+    rngs: PRNGSequenceFilter = True,
+) -> C:
   """Lifted version of ``jax.lax.switch``.
 
   The returned values from ``branches``
   must have the same Pytree structure, shapes, and dtypes.
   The variables created or updated inside the
   branches must also have the same structure.
   Note that this constraint is violated when
@@ -1042,38 +1145,35 @@
     variables: The variable collections passed to the conditional
       branches (default: all)
     rngs: The PRNG sequences passed to the conditionals (default: all)
   Returns:
     The result of the evaluated branch.
   """
 
-  def inner(scope_fn, repack_fn,
-            variable_groups, rng_groups):
+  def inner(scope_fn, repack_fn, variable_groups, rng_groups):
     def branch_wrapper(branch_fn, *operands):
       scope = scope_fn(variable_groups, rng_groups)
       y = branch_fn(scope, *operands)
       return y, repack_fn(scope)
+
     pure_branches = [
-        functools.partial(branch_wrapper, branch_fn)
-        for branch_fn in branches]
+        functools.partial(branch_wrapper, branch_fn) for branch_fn in branches
+    ]
     return jax.lax.switch(index, pure_branches, *operands)
 
-  return pack(
-      inner,
-      (variables,),
-      (variables,),
-      (rngs,),
-      name='switch')(scope)
+  return pack(inner, (variables,), (variables,), (rngs,), name='switch')(scope)
 
 
-def custom_vjp(fn: Callable[..., Any],
-               forward_fn: Callable[..., Any],
-               backward_fn: Callable[..., Any],
-               grad_vars: CollectionFilter = 'params',
-               nondiff_argnums=()):
+def custom_vjp(
+    fn: Callable[..., Any],
+    forward_fn: Callable[..., Any],
+    backward_fn: Callable[..., Any],
+    grad_vars: CollectionFilter = 'params',
+    nondiff_argnums=(),
+):
   """Lifted version of `jax.custom_vjp`.
 
   `forward_fn` and `backward_fn` together define a custom vjp for `fn`.
   The original `fn` will run in case a vjp (backward gradient) is not computed.
 
   The `forward_fn` receives the same arguments as `fn` but is expected to return
   a tuple containing the output of `fn(scope, *args)` and the residuals that are
@@ -1115,23 +1215,25 @@
       arguments (except the scope and nondiff args).
     grad_vars: The collections for which a vjp will be computed
       (default: "params").
     nondiff_argnums: arguments for which no vjp is computed.
   Returns:
     A function with the same signature as `fn` with the custom vjp.
   """
+
   def inner(scope_fn, repack_fn, variable_groups, rng_groups, *args):
     grad_variables, other_variables = variable_groups
     scopes_treedef = None
 
     def f(grad_variables, *args):
       scope = scope_fn((grad_variables, other_variables), rng_groups)
       y = fn(scope, *args)
       vars_out = repack_fn(scope)
       return y, vars_out
+
     f = jax.custom_vjp(f, nondiff_argnums=nondiff_argnums)
 
     def f_fwd(grad_variables, *args):
       nonlocal scopes_treedef
       scopes = scope_fn((grad_variables, other_variables), rng_groups)
       scopes_treedef = jax.tree_util.tree_structure(scopes)
       y, res = forward_fn(scopes, *args)
@@ -1156,26 +1258,31 @@
 
     return f(grad_variables, *args)
 
   variable_in_groups = (grad_vars, True)
   variable_out_groups = (grad_vars, True)
   rng_groups = (True,)
   return pack(
-      inner, variable_in_groups, variable_out_groups, rng_groups,
-      name='custom_vjp')
+      inner,
+      variable_in_groups,
+      variable_out_groups,
+      rng_groups,
+      name='custom_vjp',
+  )
 
 
-def checkpoint(fn: Callable[..., Any],
-               variables: CollectionFilter = True,
-               rngs: PRNGSequenceFilter = True,
-               concrete: bool = False,
-               prevent_cse: bool = True,
-               static_argnums: Union[int, Tuple[int, ...]] = (),
-               policy: Optional[Callable[..., bool]] = None,
-               ) -> Callable[..., Any]:
+def checkpoint(
+    fn: Callable[..., Any],
+    variables: CollectionFilter = True,
+    rngs: PRNGSequenceFilter = True,
+    concrete: bool = False,
+    prevent_cse: bool = True,
+    static_argnums: Union[int, Tuple[int, ...]] = (),
+    policy: Optional[Callable[..., bool]] = None,
+) -> Callable[..., Any]:
   """Lifted version of ``jax.checkpoint``.
 
   This function is aliased to ``lift.remat`` just like ``jax.remat``.
 
   Args:
     fn: scope function for which intermediate computations should be
     re-computed when computing gradients.
@@ -1201,55 +1308,67 @@
       arguments as static can avoid ConcretizationTypeErrors when tracing, but
       at the cost of more retracing overheads.
     policy: Experimental checkpoint policy, see ``jax.checkpoint``.
   Returns:
     A wrapped version of ``fn``. When computing gradients intermediate
     computations will be re-computed when computing gradients.
   """
+
   def inner(scope_fn, repack_fn, variable_groups, rng_groups, *args, **kwargs):
     # add 2 to each static_argnums because we add two initial arguments to rematted
     static_argnums_ = jax.tree_util.tree_map(lambda x: x + 2, static_argnums)
-    @functools.partial(jax.remat,
-                       concrete=concrete, static_argnums=static_argnums_,
-                       prevent_cse=prevent_cse, policy=policy)
+
+    @functools.partial(
+        jax.remat,
+        concrete=concrete,
+        static_argnums=static_argnums_,
+        prevent_cse=prevent_cse,
+        policy=policy,
+    )
     @functools.wraps(fn)
     def rematted(variable_groups, rng_groups, *args, **kwargs):
       scope = scope_fn(variable_groups, rng_groups)
       y = fn(scope, *args, **kwargs)
       return y, repack_fn(scope)
 
     return rematted(variable_groups, rng_groups, *args, **kwargs)
 
   return pack(
-      inner, (variables,), (variables,), (rngs,),
+      inner,
+      (variables,),
+      (variables,),
+      (rngs,),
       name='remat',
-      enable_kwargs=True)
+      enable_kwargs=True,
+  )
 
 
 remat = checkpoint
 
 
 def _hashable_filter(x):
   """Hashable version of CollectionFilter."""
   if isinstance(x, Iterable):
     return tuple(x)  # convert un-hashable list & sets to tuple
   if isinstance(x, DenyList):
-    return DenyList(_hashable_filter(
-        x.deny))  # convert inner filter recursively
+    return DenyList(
+        _hashable_filter(x.deny)
+    )  # convert inner filter recursively
   return x
 
 
-def jit(fn: Callable[..., Any],
-        variables: CollectionFilter = True,
-        rngs: PRNGSequenceFilter = True,
-        static_argnums: Union[int, Iterable[int]] = (),
-        donate_argnums: Union[int, Iterable[int]] = (),
-        device=None,
-        backend: Union[str, None] = None,
-        ) -> Callable[..., Any]:
+def jit(
+    fn: Callable[..., Any],
+    variables: CollectionFilter = True,
+    rngs: PRNGSequenceFilter = True,
+    static_argnums: Union[int, Iterable[int]] = (),
+    donate_argnums: Union[int, Iterable[int]] = (),
+    device=None,
+    backend: Union[str, None] = None,
+) -> Callable[..., Any]:
   """Lifted version of ``jax.jit``.
 
   Args:
     fn: Scope function to be jitted.
     variables: The variable collections that are lifted. By default all
       collections are lifted.
     rngs: The PRNG sequences that are lifted. By default all PRNG sequences
@@ -1293,18 +1412,22 @@
   donate_argnums = tuple(i + 2 for i in donate_argnums if i > 0)
 
   # Close over scope_fn & repack_fn to avoid recompilation
   # this is impure but we use the fingerprint arg to differentiate between cases
   # where scope_fn or repack_fn actually produce non-identical results.
   scope_fn = None  # type: Optional[Callable]
   repack_fn = None  # type: Optional[Callable]
-  @functools.partial(jax.jit,
-                     static_argnums=static_argnums,
-                     donate_argnums=donate_argnums,
-                     device=device, backend=backend)
+
+  @functools.partial(
+      jax.jit,
+      static_argnums=static_argnums,
+      donate_argnums=donate_argnums,
+      device=device,
+      backend=backend,
+  )
   @functools.wraps(fn)
   def jitted(fingerprint, variable_groups, rng_groups, *args):
     nonlocal scope_fn, repack_fn
     # fingerprint is only used to differentiate the cache signature for cases
     # where different collections are mutable.
     del fingerprint
     scope = scope_fn(variable_groups, rng_groups)  # pylint: disable=not-callable
@@ -1328,15 +1451,15 @@
 def remat_scan(
     body_fn: Callable[..., Any],
     lengths: Sequence[int],
     policy: Optional[Callable[..., bool]] = None,
     variable_broadcast: CollectionFilter = False,
     variable_carry: CollectionFilter = False,
     variable_axes: Mapping[CollectionFilter, InOutScanAxis] = {True: 0},
-    split_rngs: Mapping[PRNGSequenceFilter, bool] = {True: True}
+    split_rngs: Mapping[PRNGSequenceFilter, bool] = {True: True},
 ) -> Callable[..., Any]:
   """Combines `lift.remat` and `lift.scan` for memory efficiency and constant time compilation.
 
   ``remat_scan`` allows for constant compile times and sublinear
   memory usage with respect to model depth. At a small constant
   penalty. This is typically beneficial for very deep models.
 
@@ -1371,26 +1494,37 @@
   """
   # TODO(jheek) should remat scan have scan inputs/outputs?
   scan_fn = functools.partial(
       scan,
       variable_broadcast=variable_broadcast,
       variable_carry=variable_carry,
       variable_axes=variable_axes,
-      split_rngs=split_rngs)
+      split_rngs=split_rngs,
+  )
   if len(lengths) == 1:
+
     def wrapper(scope, carry):
       return body_fn(scope, carry), ()
+
     fn = lambda scope, c: scan_fn(wrapper, length=lengths[0])(scope, c)[0]
   else:
+
     @functools.partial(remat, policy=policy, prevent_cse=False)
     def inner_loop(scope, carry):
-      carry = remat_scan(body_fn, lengths[1:], policy,
-                         variable_broadcast, variable_carry,
-                         variable_axes, split_rngs)(scope, carry)
+      carry = remat_scan(
+          body_fn,
+          lengths[1:],
+          policy,
+          variable_broadcast,
+          variable_carry,
+          variable_axes,
+          split_rngs,
+      )(scope, carry)
       return carry, ()
+
     fn = lambda scope, c: scan_fn(inner_loop, length=lengths[0])(scope, c)[0]
   return fn
 
 
 def _unzip2(xs):
   ys = tuple(zip(*xs))
   return ys if ys else ((), ())
```

### Comparing `flax-0.7.0/flax/core/meta.py` & `flax-0.7.1/flax/core/meta.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 
 from flax import errors
 from flax import struct
 import jax
 from jax.experimental import maps
 
 
-TAxisMetadata = Any # TypeVar('TAxisMetadata', bound='AxisMetadata')
+TAxisMetadata = Any  # TypeVar('TAxisMetadata', bound='AxisMetadata')
 
 
 class AxisMetadata(metaclass=abc.ABCMeta):
   """Abstract base class for boxed Metadata.
 
   ``AxisMetadata`` enables arbitrary, per axis metadata for variables.
   By using ``unbox`` the metadata is stripped away to obtain the original
@@ -78,16 +78,17 @@
     Returns:
       A new instance of the same type as self with `val` as the new ``unbox``
       content
     """
     pass
 
   @abc.abstractmethod
-  def add_axis(self: TAxisMetadata, index: int,
-               params: Dict[Any, Any]) -> TAxisMetadata:
+  def add_axis(
+      self: TAxisMetadata, index: int, params: Dict[Any, Any]
+  ) -> TAxisMetadata:
     """Adds a new axis to the axis metadata.
 
     Note that add_axis and remove_axis should act as each other's inverse
     (meaning: ``x.add_axis(i, p).remove_axis(i, p) == x``)
 
     Args:
       index: The position at which the new axis will be inserted
@@ -98,16 +99,17 @@
     Returns:
       A new instance of the same type as self and with the same ``unbox``
       content with updated axis metadata.
     """
     pass
 
   @abc.abstractmethod
-  def remove_axis(self: TAxisMetadata, index: int,
-                  params: Dict[Any, Any]) -> TAxisMetadata:
+  def remove_axis(
+      self: TAxisMetadata, index: int, params: Dict[Any, Any]
+  ) -> TAxisMetadata:
     """Removes an axis from the axis metadata.
 
     Note that add_axis and remove_axis should act as each other's inverse
     (meaning: ``x.remove_axis(i, p).add_axis(i, p) == x``)
 
     Args:
       index: The position of the axis that is to be removed
@@ -125,19 +127,21 @@
 def is_axis_metadata(val: Any) -> bool:
   """Returns whether the argument is an instance of AxisMetadata."""
   return isinstance(val, AxisMetadata)
 
 
 def map_axis_meta(fn: Callable[[AxisMetadata], Any], tree: Any) -> Any:
   """Maps over all PyTree nodes that are AxisMetadata instances."""
+
   def wrapper(x):
     if isinstance(x, AxisMetadata):
       return fn(x)
     else:
       return x
+
   return jax.tree_map(wrapper, tree, is_leaf=is_axis_metadata)
 
 
 def add_axis(tree: Any, index: int, params: Dict[Any, Any]) -> Any:
   """Add an axis to each AxisMetadata node in a PyTree."""
   return map_axis_meta(lambda x: x.add_axis(index, params), tree)
 
@@ -150,19 +154,21 @@
 def unbox(tree: Any) -> Any:
   """Strips all AxisMetadata boxes from a PyTree."""
   return map_axis_meta(lambda x: unbox(x.unbox()), tree)
 
 
 def replace_boxed(tree: Any, updates: Any) -> Any:
   """Updates all AxisMetadata boxes with the values in updates."""
+
   def inner_update(c, v):
     if isinstance(c, AxisMetadata):
       return c.replace_boxed(replace_boxed(c.unbox(), v))
     else:
       return v
+
   return jax.tree_map(inner_update, tree, updates, is_leaf=is_axis_metadata)
 
 
 PARTITION_NAME = 'partition_name'
 LogicalNames = Tuple[Union[str, None], ...]
 
 
@@ -224,27 +230,29 @@
         return c, ()
       c, _ = nn.scan(
           body, variable_axes={"params": 0}, split_rngs={"params": 0}, length=8,
           metadata_params={nn.meta.PARTITION_NAME: "layers"})(self, x)
       return c
 
   """
+
   value: Any
   names: LogicalNames = struct.field(pytree_node=False)
-  mesh: Optional[jax.sharding.Mesh] = struct.field(default=None, pytree_node=False)
+  mesh: Optional[jax.sharding.Mesh] = struct.field(
+      default=None, pytree_node=False
+  )
 
   def unbox(self, apply_constraint=True) -> Any:
     """Returns the wrapped value with the partitioning applied as a sharding constraint."""
     if apply_constraint and (_global_mesh_defined() or self.mesh is not None):
       axis_resource = self.get_partition_spec()
       if self.mesh is not None:
         sharding = jax.sharding.NamedSharding(self.mesh, axis_resource)
         return jax.lax.with_sharding_constraint(self.value, sharding)
-      return jax.lax.with_sharding_constraint(
-          self.value, axis_resource)
+      return jax.lax.with_sharding_constraint(self.value, axis_resource)
     else:
       return self.value
 
   def replace_boxed(self, val: Any) -> TAxisMetadata:
     return self.replace(value=val)
 
   def _get_partition_name(self, params: Dict[Any, Any]) -> str:
@@ -252,16 +260,16 @@
       raise errors.PartitioningUnspecifiedError(self)
     return params[PARTITION_NAME]
 
   def add_axis(self, index: int, params: Dict[Any, Any]) -> TAxisMetadata:
     axis_name = self._get_partition_name(params)
     names = list(self.names)
     while len(names) < index:
-      names.append(None) # type: ignore
-    names.insert(index, axis_name) # type: ignore
+      names.append(None)  # type: ignore
+    names.insert(index, axis_name)  # type: ignore
     return self.replace(names=tuple(names))
 
   def remove_axis(self, index: int, params: Dict[Any, Any]) -> TAxisMetadata:
     axis_name = self._get_partition_name(params)
     names = list(self.names)
     assert names.pop(index) == axis_name
     return self.replace(names=tuple(names))
@@ -275,15 +283,15 @@
     return jax.sharding.NamedSharding(mesh, self.get_partition_spec())
 
 
 def with_partitioning(
     fn: Callable[..., Any],
     names: LogicalNames,
     mesh: Optional[jax.sharding.Mesh] = None,
-  ) ->  Callable[..., Partitioned]:
+) -> Callable[..., Partitioned]:
   """Wraps a function's return value with Partitioned.
 
   Example::
 
     kernel_init = with_partitioning(
         nn.initializers.lecun_normal, (None, "data"))
     partitioned_dense = nn.Dense(features, kernel_init=kernel_init)
@@ -292,31 +300,34 @@
     fn: The function to be wrapped. Typically this is an initializer.
     names: The logical axis passed to ``Partitioned``.
     mesh: The mesh to use for the partitioning. If None, the global mesh
       resource is used if available.
   Returns:
     A function wrapping ``fn`` that will return an instance of ``Partitioned``.
   """
+
   @functools.wraps(fn)
   def wrapper(*args, **kwargs):
     return Partitioned(fn(*args, **kwargs), names, mesh=mesh)
+
   return wrapper
 
 
 def get_partition_spec(tree: Any) -> Any:
   """Extracts a PartitionSpec tree from a PyTree containing ``Partitioned`` values."""
+
   def f(x):
     if isinstance(x, Partitioned):
       return x.get_partition_spec()
     # Unboxed arrays, which should be replicated across all devices
     elif hasattr(x, 'shape'):
       return jax.sharding.PartitionSpec()
     else:
       return None
-  return jax.tree_map(f, tree,
-                      is_leaf=lambda x: isinstance(x, Partitioned))
+
+  return jax.tree_map(f, tree, is_leaf=lambda x: isinstance(x, Partitioned))
 
 
 def get_sharding(tree: Any, mesh: jax.sharding.Mesh) -> Any:
   """Extracts a jax.sharding tree from a PyTree containing ``Partitioned`` values and a mesh."""
   pspec_tree = get_partition_spec(tree)
   return jax.tree_map(lambda x: jax.sharding.NamedSharding(mesh, x), pspec_tree)
```

### Comparing `flax-0.7.0/flax/core/nn/__init__.py` & `flax-0.7.1/flax/core/nn/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,49 +12,47 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Flax Neural Network api."""
 
 # pylint: disable=g-multiple-import
 # re-export commonly used modules and functions
-from .attention import (
-  dot_product_attention as dot_product_attention,
-  multi_head_dot_product_attention as multi_head_dot_product_attention
-)
 from flax.linen import activation as activation
 from flax.linen import initializers as initializers
 from flax.linen.activation import (
-  celu as celu,
-  elu as elu,
-  gelu as gelu,
-  glu as glu,
-  leaky_relu as leaky_relu,
-  log_sigmoid as log_sigmoid,
-  log_softmax as log_softmax,
-  relu as relu,
-  sigmoid as sigmoid,
-  silu as silu,
-  soft_sign as soft_sign,
-  softmax as softmax,
-  softplus as softplus,
-  swish as swish,
-  tanh as tanh)
-from flax.linen.pooling import (
-  avg_pool as avg_pool,
-  max_pool as max_pool
+    celu as celu,
+    elu as elu,
+    gelu as gelu,
+    glu as glu,
+    leaky_relu as leaky_relu,
+    log_sigmoid as log_sigmoid,
+    log_softmax as log_softmax,
+    relu as relu,
+    sigmoid as sigmoid,
+    silu as silu,
+    soft_sign as soft_sign,
+    softmax as softmax,
+    softplus as softplus,
+    swish as swish,
+    tanh as tanh,
+)
+from flax.linen.pooling import (avg_pool as avg_pool, max_pool as max_pool)
+from .attention import (
+    dot_product_attention as dot_product_attention,
+    multi_head_dot_product_attention as multi_head_dot_product_attention,
 )
 from .linear import (
-  Embedding as Embedding,
-  conv as conv,
-  conv_transpose as conv_transpose,
-  dense as dense,
-  dense_general as dense_general,
-  embedding as embedding
+    Embedding as Embedding,
+    conv_transpose as conv_transpose,
+    conv as conv,
+    dense_general as dense_general,
+    dense as dense,
+    embedding as embedding,
 )
 from .normalization import (
-  batch_norm as batch_norm,
-  group_norm as group_norm,
-  layer_norm as layer_norm
+    batch_norm as batch_norm,
+    group_norm as group_norm,
+    layer_norm as layer_norm,
 )
 from .stochastic import dropout as dropout
 
 # pylint: enable=g-multiple-import
```

### Comparing `flax-0.7.0/flax/core/nn/attention.py` & `flax-0.7.1/flax/core/nn/attention.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,26 +29,28 @@
 from jax import random
 import jax.numpy as jnp
 from .linear import default_kernel_init
 from .linear import dense_general
 import numpy as np
 
 
-def dot_product_attention(scope,
-                          query,
-                          key,
-                          value,
-                          dtype=jnp.float32,
-                          bias=None,
-                          axis=None,
-                          broadcast_dropout=True,
-                          dropout_rng=None,
-                          dropout_rate=0.,
-                          deterministic=False,
-                          precision=None):
+def dot_product_attention(
+    scope,
+    query,
+    key,
+    value,
+    dtype=jnp.float32,
+    bias=None,
+    axis=None,
+    broadcast_dropout=True,
+    dropout_rng=None,
+    dropout_rate=0.0,
+    deterministic=False,
+    precision=None,
+):
   """Computes dot-product attention given query, key, and value.
 
   This is the core function for applying attention based on
   https://arxiv.org/abs/1706.03762. It calculates the attention weights given
   query and key and combines the values using the attention weights. This
   function supports multi-dimensional inputs.
 
@@ -71,27 +73,27 @@
     precision: numerical precision of the computation see `jax.lax.Precision`
       for details.
 
   Returns:
     Output of shape `[bs, dim1, dim2, ..., dimN,, num_heads, value_channels]`.
   """
   assert key.shape[:-1] == value.shape[:-1]
-  assert (query.shape[0:1] == key.shape[0:1] and
-          query.shape[-1] == key.shape[-1])
+  assert query.shape[0:1] == key.shape[0:1] and query.shape[-1] == key.shape[-1]
 
   if axis is None:
     axis = tuple(range(1, key.ndim - 2))
   if not isinstance(axis, Iterable):
     axis = (axis,)
   assert key.ndim == query.ndim
   assert key.ndim == value.ndim
   for ax in axis:
     if not (query.ndim >= 3 and 1 <= ax < query.ndim - 2):
-      raise ValueError('Attention axis must be between the batch '
-                       'axis and the last-two axes.')
+      raise ValueError(
+          'Attention axis must be between the batch axis and the last-two axes.'
+      )
   depth = query.shape[-1]
   n = key.ndim
   # batch_dims is  <bs, <non-attention dims>, num_heads>
   batch_dims = tuple(np.delete(range(n), axis + (n - 1,)))
   # q & k -> (bs, <non-attention dims>, num_heads, <attention dims>, channels)
   qk_perm = batch_dims + axis + (n - 1,)
   key = key.transpose(qk_perm)
@@ -100,50 +102,56 @@
   v_perm = batch_dims + (n - 1,) + axis
   value = value.transpose(v_perm)
 
   query = query / jnp.sqrt(depth).astype(dtype)
   batch_dims_t = tuple(range(len(batch_dims)))
   attn_weights = lax.dot_general(
       query,
-      key, (((n - 1,), (n - 1,)), (batch_dims_t, batch_dims_t)),
-      precision=precision)
+      key,
+      (((n - 1,), (n - 1,)), (batch_dims_t, batch_dims_t)),
+      precision=precision,
+  )
 
   # apply attention bias: masking, droput, proximity bias, ect.
   if bias is not None:
     attn_weights = attn_weights + bias
 
   # normalize the attention weights
   norm_dims = tuple(range(attn_weights.ndim - len(axis), attn_weights.ndim))
   attn_weights = lax.exp(
-      attn_weights -
-      jax.scipy.special.logsumexp(attn_weights, axis=norm_dims, keepdims=True))
+      attn_weights
+      - jax.scipy.special.logsumexp(attn_weights, axis=norm_dims, keepdims=True)
+  )
   attn_weights = attn_weights.astype(dtype)
 
   # apply dropout
-  if not deterministic and dropout_rate > 0.:
+  if not deterministic and dropout_rate > 0.0:
     if dropout_rng is None:
       dropout_rng = scope.make_rng('dropout')
     keep_prob = jax.lax.tie_in(attn_weights, 1.0 - dropout_rate)
     if broadcast_dropout:
       # dropout is broadcast across the batch+head+non-attention dimension
-      dropout_dims = attn_weights.shape[-(2 * len(axis)):]
-      dropout_shape = (tuple([1] * len(batch_dims_t)) + dropout_dims)
+      dropout_dims = attn_weights.shape[-(2 * len(axis)) :]
+      dropout_shape = tuple([1] * len(batch_dims_t)) + dropout_dims
       keep = random.bernoulli(dropout_rng, keep_prob, dropout_shape)
     else:
       keep = random.bernoulli(dropout_rng, keep_prob, attn_weights.shape)
-    multiplier = (keep.astype(attn_weights.dtype) /
-                  jnp.asarray(keep_prob, dtype=dtype))
+    multiplier = keep.astype(attn_weights.dtype) / jnp.asarray(
+        keep_prob, dtype=dtype
+    )
     attn_weights = attn_weights * multiplier
 
   # compute the new values given the attention weights
   wv_contracting_dims = (norm_dims, range(value.ndim - len(axis), value.ndim))
   y = lax.dot_general(
       attn_weights,
-      value, (wv_contracting_dims, (batch_dims_t, batch_dims_t)),
-      precision=precision)
+      value,
+      (wv_contracting_dims, (batch_dims_t, batch_dims_t)),
+      precision=precision,
+  )
 
   # back to (bs, dim1, dim2, ..., dimN, num_heads, channels)
   perm_inv = _invert_perm(qk_perm)
   y = y.transpose(perm_inv)
   return y
 
 
@@ -173,21 +181,22 @@
     padding_mask=None,
     key_padding_mask=None,
     segmentation=None,
     key_segmentation=None,
     cache=False,
     broadcast_dropout=True,
     dropout_rng=None,
-    dropout_rate=0.,
+    dropout_rate=0.0,
     deterministic=False,
     precision=None,
     kernel_init=default_kernel_init,
     bias_init=initializers.zeros_init(),
     bias=True,
-    attention_fn=dot_product_attention):
+    attention_fn=dot_product_attention,
+):
   """Applies multi-head dot product attention on the input data.
 
   Projects the inputs into multi-headed query, key, and value vectors,
   applies dot-product attention and project the results to an output vector.
 
   This can be used for encoder-decoder attention by specifying both `inputs_q`
   and `inputs_kv` orfor self-attention by only specifying `inputs_q` and
@@ -227,92 +236,102 @@
     query, key, value, and returns output of shape
     `[bs, dim1, dim2, ..., dimN,, num_heads, value_channels]``
 
   Returns:
     output of shape `[bs, dim1, dim2, ..., dimN, features]`.
   """
 
-  assert causal_mask or not cache, (
-      'Caching is only support for causal attention.')
+  assert (
+      causal_mask or not cache
+  ), 'Caching is only support for causal attention.'
 
   if inputs_kv is None:
     inputs_kv = inputs_q
 
   if attention_axis is None:
     attention_axis = tuple(range(1, inputs_q.ndim - 1))
 
   features = out_features or inputs_q.shape[-1]
   qkv_features = qkv_features or inputs_q.shape[-1]
 
-  assert qkv_features % num_heads == 0, (
-      'Memory dimension must be divisible by number of heads.')
+  assert (
+      qkv_features % num_heads == 0
+  ), 'Memory dimension must be divisible by number of heads.'
   head_dim = qkv_features // num_heads
 
   dense = functools.partial(
       dense_general,
       axis=-1,
       dtype=dtype,
       features=(num_heads, head_dim),
       kernel_init=kernel_init,
       bias_init=bias_init,
       bias=bias,
-      precision=precision)
+      precision=precision,
+  )
   # project inputs_q to multi-headed q/k/v
   # dimensions are then [bs, dims..., n_heads, n_features_per_head]
   query = scope.child(dense, 'query')(inputs_q)
   key = scope.child(dense, 'key')(inputs_kv)
   value = scope.child(dense, 'value')(inputs_kv)
 
   if cache:
     cache_entry: Union[Callable[[Any], CacheEntry], CacheEntry]
     if not scope.has_variable('cache', 'entry'):
       ndim, tail_shape = (key.ndim, key.shape[-2:])
+
       def init_fn(shape, dtype=jnp.float32):
         full_shape = shape + tail_shape
         if len(full_shape) != ndim:
-          raise ValueError('Shape should be a tuple with the shape of the batch'
-                           'and attention dims.')
+          raise ValueError(
+              'Shape should be a tuple with the shape of the batch'
+              'and attention dims.'
+          )
         return CacheEntry(
             key=jnp.zeros(full_shape, dtype),
             value=jnp.zeros(full_shape, dtype),
-            i=jnp.zeros((), jnp.uint32))
+            i=jnp.zeros((), jnp.uint32),
+        )
+
       cache_entry = init_fn
     else:
       cache_entry = scope.get_variable('cache', 'entry')
       if not isinstance(cache_entry, CacheEntry):
         raise ValueError('Cache is not initialized.')
 
       expected_shape = list(cache_entry.key.shape[:-2])
       for attn_dim in attention_axis:
         expected_shape[attn_dim] = 1
       expected_shape = tuple(expected_shape) + inputs_q.shape[-1:]
       if expected_shape != inputs_q.shape:
-        raise ValueError('Invalid shape provided, '
-                         'expected shape %s instead got %s.' %
-                         (expected_shape, inputs_q.shape))
+        raise ValueError(
+            'Invalid shape provided, expected shape %s instead got %s.'
+            % (expected_shape, inputs_q.shape)
+        )
 
       cshape = cache_entry.key.shape
       indices = [0] * len(cshape)
       i = cache_entry.i
       attn_size = np.prod(np.take(cshape, attention_axis))
       for attn_dim in attention_axis:
         attn_size //= cshape[attn_dim]
         indices[attn_dim] = i // attn_size
         i = i % attn_size
 
-      key = lax.dynamic_update_slice(cache_entry.key, key, indices) # type: ignore
-      value = lax.dynamic_update_slice(cache_entry.value, value, indices) # type: ignore
+      key = lax.dynamic_update_slice(cache_entry.key, key, indices)  # type: ignore
+      value = lax.dynamic_update_slice(cache_entry.value, value, indices)  # type: ignore
       one = jnp.array(1, jnp.uint32)
-      cache_entry = cache_entry.replace(i=cache_entry.i + one,
-                                        key=key,
-                                        value=value)
+      cache_entry = cache_entry.replace(
+          i=cache_entry.i + one, key=key, value=value
+      )
 
       # TODO(levskaya): verify this is still needed in translation decoding.
       key_padding_mask = jnp.broadcast_to(
-          (jnp.arange(cshape[1]) < cache_entry.i), cshape[:2])
+          (jnp.arange(cshape[1]) < cache_entry.i), cshape[:2]
+      )
       key_padding_mask = key_padding_mask.astype(jnp.float32)[..., None]
     scope.put_variable('cache', 'entry', cache_entry)
 
   # create attention masks
   mask_components = []
 
   if causal_mask:
@@ -330,38 +349,42 @@
     if key_padding_mask is None:
       key_padding_mask = padding_mask
     padding_mask = make_padding_mask(
         padding_mask_query=padding_mask,
         padding_mask_key=key_padding_mask,
         query_shape=query.shape,
         key_shape=key.shape,
-        attention_axis=attention_axis)
+        attention_axis=attention_axis,
+    )
     mask_components.append(padding_mask)
 
   if segmentation is not None:
     if key_segmentation is None:
       key_segmentation = segmentation
     segmentation_mask = make_padding_mask(
         padding_mask_query=segmentation,
         padding_mask_key=key_segmentation,
         query_shape=query.shape,
         key_shape=key.shape,
         attention_axis=attention_axis,
-        segmentation_mask=True)
+        segmentation_mask=True,
+    )
     mask_components.append(segmentation_mask)
 
   if mask_components:
     attention_mask = mask_components[0]
     for component in mask_components[1:]:
       attention_mask = jnp.logical_and(attention_mask, component)
 
     # attention mask in the form of attention bias
     attention_bias = lax.select(
-        attention_mask > 0, jnp.full(attention_mask.shape, 0.).astype(dtype),
-        jnp.full(attention_mask.shape, -1e10).astype(dtype))
+        attention_mask > 0,
+        jnp.full(attention_mask.shape, 0.0).astype(dtype),
+        jnp.full(attention_mask.shape, -1e10).astype(dtype),
+    )
   else:
     attention_bias = None
 
   # apply attention
   x = scope.child(attention_fn)(
       query,
       key,
@@ -369,41 +392,45 @@
       dtype=dtype,
       axis=attention_axis,
       bias=attention_bias,
       precision=precision,
       dropout_rng=dropout_rng,
       dropout_rate=dropout_rate,
       broadcast_dropout=broadcast_dropout,
-      deterministic=deterministic)
+      deterministic=deterministic,
+  )
 
   # back to the original inputs dimensions
   out = scope.child(dense_general, name='out')(
       x,
       features=features,
       axis=(-2, -1),
       kernel_init=kernel_init,
       bias_init=bias_init,
       bias=bias,
       dtype=dtype,
-      precision=precision)
+      precision=precision,
+  )
 
   return out
 
 
 # TODO(flax-dev): Consider refactoring MultiHeadDotProductAttention and moving
 # causal_mask and cache support into this class instead.
-#SelfAttention = MultiHeadDotProductAttention.partial(inputs_kv=None)
+# SelfAttention = MultiHeadDotProductAttention.partial(inputs_kv=None)
 
 
-def make_padding_mask(padding_mask_query,
-                      padding_mask_key,
-                      query_shape,
-                      key_shape,
-                      attention_axis=None,
-                      segmentation_mask=False):
+def make_padding_mask(
+    padding_mask_query,
+    padding_mask_key,
+    query_shape,
+    key_shape,
+    attention_axis=None,
+    segmentation_mask=False,
+):
   """Makes padding mask for attention weights.
 
   In case of 1d inputs (i.e., `[bs, len, features]`, the attention weights will
   be `[bs, len, len]` and this function makes a square matrix [len, len].
 
   Args:
     padding_mask_query: padding mask of query <bs, qdim1,.., qdimn>
@@ -487,13 +514,14 @@
     # Tie in the key to avoid the mask becoming a constant.
     # This way XLA can construct the mask during computation and fuse it
     # with the attention ops.
     x = lax.tie_in(key, jnp.arange(n, dtype=jnp.int32))
     y = lax.tie_in(key, jnp.arange(m, dtype=jnp.int32))
     mask = lax.ge(
         (lax.broadcast_in_dim(x, shape=(n, m), broadcast_dimensions=(0,))) + k,
-        lax.broadcast(y, [n]))
+        lax.broadcast(y, [n]),
+    )
     return mask
 
   k = -1 if self_mask else 0
   mask = tri(*mask_shape[-2:], k=k).reshape(mask_shape_final)
   return mask
```

### Comparing `flax-0.7.0/flax/core/nn/linear.py` & `flax-0.7.1/flax/core/nn/linear.py`

 * *Files 4% similar despite different names*

```diff
@@ -38,15 +38,16 @@
     features,
     axis=-1,
     batch_dims=(),
     bias=True,
     dtype=jnp.float32,
     kernel_init=default_kernel_init,
     bias_init=initializers.zeros_init(),
-    precision=None):
+    precision=None,
+):
   """Applies a linear transformation to the inputs along multiple dimensions.
 
   Args:
     inputs: The nd-array to be transformed.
     features: tuple with numbers of output features.
     axis: tuple with axes to apply the transformation on.
     batch_dims: tuple with batch axes.
@@ -68,70 +69,83 @@
   if not isinstance(batch_dims, Iterable):
     batch_dims = (batch_dims,)
   features, axis, batch_dims = tuple(features), tuple(axis), tuple(batch_dims)
 
   if batch_dims:
     max_dim = np.max(batch_dims)
     if set(batch_dims) != set(range(max_dim + 1)):
-      raise ValueError('batch_dims %s must be consecutive leading '
-                        'dimensions starting from 0.' % str(batch_dims))
+      raise ValueError(
+          'batch_dims %s must be consecutive leading '
+          'dimensions starting from 0.'
+          % str(batch_dims)
+      )
 
   ndim = inputs.ndim
   n_batch_dims = len(batch_dims)
   axis = _normalize_axes(axis, ndim)
   batch_dims = _normalize_axes(batch_dims, ndim)
   n_axis, n_features = len(axis), len(features)
 
   def kernel_init_wrap(rng, shape, dtype=jnp.float32):
     size_batch_dims = np.prod(shape[:n_batch_dims], dtype=np.int32)
-    flat_shape = (np.prod(shape[n_batch_dims:n_axis + n_batch_dims]),
-                  np.prod(shape[-n_features:]),)
-    kernel = jnp.concatenate([kernel_init(rng, flat_shape, dtype)
-                              for _ in range(size_batch_dims)], axis=0)
+    flat_shape = (
+        np.prod(shape[n_batch_dims : n_axis + n_batch_dims]),
+        np.prod(shape[-n_features:]),
+    )
+    kernel = jnp.concatenate(
+        [kernel_init(rng, flat_shape, dtype) for _ in range(size_batch_dims)],
+        axis=0,
+    )
     return jnp.reshape(kernel, shape)
 
   batch_shape = tuple(inputs.shape[ax] for ax in batch_dims)
   kernel_shape = tuple(inputs.shape[ax] for ax in axis) + features
   kernel = scope.param('kernel', kernel_init_wrap, batch_shape + kernel_shape)
   kernel = jnp.asarray(kernel, dtype)
 
   batch_ind = tuple(range(n_batch_dims))
   contract_ind = tuple(range(n_batch_dims, n_axis + n_batch_dims))
-  out = lax.dot_general(inputs,
-                        kernel,
-                        ((axis, contract_ind), (batch_dims, batch_ind)),
-                        precision=precision)
+  out = lax.dot_general(
+      inputs,
+      kernel,
+      ((axis, contract_ind), (batch_dims, batch_ind)),
+      precision=precision,
+  )
   if bias:
+
     def bias_init_wrap(rng, shape, dtype=jnp.float32):
       size_batch_dims = np.prod(shape[:n_batch_dims], dtype=np.int32)
       flat_shape = (np.prod(shape[-n_features:]),)
-      bias = jnp.concatenate([bias_init(rng, flat_shape, dtype)
-                              for _ in range(size_batch_dims)], axis=0)
+      bias = jnp.concatenate(
+          [bias_init(rng, flat_shape, dtype) for _ in range(size_batch_dims)],
+          axis=0,
+      )
       return jnp.reshape(bias, shape)
 
     bias = scope.param('bias', bias_init_wrap, batch_shape + features)
 
     # Reshape bias for broadcast.
-    expand_dims = sorted(
-        set(range(inputs.ndim)) - set(axis) - set(batch_dims))
+    expand_dims = sorted(set(range(inputs.ndim)) - set(axis) - set(batch_dims))
     for ax in expand_dims:
       bias = jnp.expand_dims(bias, ax)
     bias = jnp.asarray(bias, dtype)
     out = out + bias
   return out
 
 
-def dense(scope,
-          inputs,
-          features,
-          bias=True,
-          dtype=jnp.float32,
-          precision=None,
-          kernel_init=default_kernel_init,
-          bias_init=initializers.zeros_init()):
+def dense(
+    scope,
+    inputs,
+    features,
+    bias=True,
+    dtype=jnp.float32,
+    precision=None,
+    kernel_init=default_kernel_init,
+    bias_init=initializers.zeros_init(),
+):
   """Applies a linear transformation to the inputs along the last dimension.
 
   Args:
     inputs: The nd-array to be transformed.
     features: the number of output features.
     bias: whether to add a bias to the output (default: True).
     dtype: the dtype of the computation (default: float32).
@@ -141,17 +155,20 @@
     bias_init: initializer function for the bias.
   Returns:
     The transformed input.
   """
   inputs = jnp.asarray(inputs, dtype)
   kernel = scope.param('kernel', kernel_init, (inputs.shape[-1], features))
   kernel = jnp.asarray(kernel, dtype)
-  y = lax.dot_general(inputs, kernel,
-                      (((inputs.ndim - 1,), (0,)), ((), ())),
-                      precision=precision)
+  y = lax.dot_general(
+      inputs,
+      kernel,
+      (((inputs.ndim - 1,), (0,)), ((), ())),
+      precision=precision,
+  )
   if bias:
     bias = scope.param('bias', bias_init, (features,))
     bias = jnp.asarray(bias, dtype)
     y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))
   return y
 
 
@@ -160,28 +177,30 @@
   ndim = len(input_shape)
   lhs_spec = (0, ndim - 1) + tuple(range(1, ndim - 1))
   rhs_spec = (ndim - 1, ndim - 2) + tuple(range(0, ndim - 2))
   out_spec = lhs_spec
   return lax.ConvDimensionNumbers(lhs_spec, rhs_spec, out_spec)
 
 
-def conv(scope,
-         inputs,
-         features,
-         kernel_size,
-         strides=None,
-         padding='SAME',
-         input_dilation=None,
-         kernel_dilation=None,
-         feature_group_count=1,
-         bias=True,
-         dtype=jnp.float32,
-         precision=None,
-         kernel_init=default_kernel_init,
-         bias_init=initializers.zeros_init()):
+def conv(
+    scope,
+    inputs,
+    features,
+    kernel_size,
+    strides=None,
+    padding='SAME',
+    input_dilation=None,
+    kernel_dilation=None,
+    feature_group_count=1,
+    bias=True,
+    dtype=jnp.float32,
+    precision=None,
+    kernel_init=default_kernel_init,
+    bias_init=initializers.zeros_init(),
+):
   """Applies a convolution to the inputs.
 
   Args:
     inputs: input data with dimensions (batch, spatial_dims..., features).
     features: number of convolution filters.
     kernel_size: shape of the convolutional kernel.
     strides: a sequence of `n` integers, representing the inter-window
@@ -226,35 +245,38 @@
       kernel,
       strides,
       padding,
       lhs_dilation=input_dilation,
       rhs_dilation=kernel_dilation,
       dimension_numbers=dimension_numbers,
       feature_group_count=feature_group_count,
-      precision=precision)
+      precision=precision,
+  )
 
   if bias:
     bias = scope.param('bias', bias_init, (features,))
     bias = jnp.asarray(bias, dtype)
     y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))
   return y
 
 
-def conv_transpose(scope,
-                   inputs,
-                   features,
-                   kernel_size,
-                   strides=None,
-                   padding='SAME',
-                   kernel_dilation=None,
-                   bias=True,
-                   dtype=jnp.float32,
-                   precision=None,
-                   kernel_init=default_kernel_init,
-                   bias_init=initializers.zeros_init()):
+def conv_transpose(
+    scope,
+    inputs,
+    features,
+    kernel_size,
+    strides=None,
+    padding='SAME',
+    kernel_dilation=None,
+    bias=True,
+    dtype=jnp.float32,
+    precision=None,
+    kernel_init=default_kernel_init,
+    bias_init=initializers.zeros_init(),
+):
   """Applies a transposed convolution to the inputs. Behaviour mirrors that of
   `jax.lax.conv_transpose`.
 
   Args:
     scope: functional scope.
     inputs: input data with dimensions (batch, spatial_dims..., features).
     features: number of convolution filters.
@@ -281,26 +303,33 @@
   strides = strides or (1,) * (inputs.ndim - 2)
 
   in_features = inputs.shape[-1]
   kernel_shape = kernel_size + (in_features, features)
   kernel = scope.param('kernel', kernel_init, kernel_shape)
   kernel = jnp.asarray(kernel, dtype)
 
-  y = lax.conv_transpose(inputs, kernel, strides, padding,
-                          rhs_dilation=kernel_dilation, precision=precision)
+  y = lax.conv_transpose(
+      inputs,
+      kernel,
+      strides,
+      padding,
+      rhs_dilation=kernel_dilation,
+      precision=precision,
+  )
 
   if bias:
     bias = scope.param('bias', bias_init, (features,))
     bias = jnp.asarray(bias, dtype)
     y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))
   return y
 
 
-default_embed_init = initializers.variance_scaling(1.0, 'fan_in', 'normal',
-                                                   out_axis=0)
+default_embed_init = initializers.variance_scaling(
+    1.0, 'fan_in', 'normal', out_axis=0
+)
 
 
 @struct.dataclass
 class Embedding:
   table: np.ndarray
 
   def lookup(self, indices):
@@ -329,20 +358,22 @@
       inner-product of the array of query vectors against each embedding.
       Commonly used for weight-sharing between embeddings and logit transform
       in NLP models.
     """
     return jnp.dot(query, self.table.T)
 
 
-def embedding(scope: Scope, num_embeddings: int, features: int, init_fn=default_embed_init) -> Embedding:
+def embedding(
+    scope: Scope, num_embeddings: int, features: int, init_fn=default_embed_init
+) -> Embedding:
   """Creates embedding dataclass.
 
-    Args:
-      num_embeddings: number of embeddings.
-      features: Number of feature dimensions for each embedding.
-      embedding_init: embedding initializer.
+  Args:
+    num_embeddings: number of embeddings.
+    features: Number of feature dimensions for each embedding.
+    embedding_init: embedding initializer.
 
-    Returns:
-      Embedding dataclass with lookup and attend methods.
+  Returns:
+    Embedding dataclass with lookup and attend methods.
   """
   table = scope.param('table', init_fn, (num_embeddings, features))
-  return Embedding(table) # type: ignore
+  return Embedding(table)  # type: ignore
```

### Comparing `flax-0.7.0/flax/core/nn/normalization.py` & `flax-0.7.1/flax/core/nn/normalization.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,24 +20,30 @@
 import jax.numpy as jnp
 
 
 def _absolute_dims(ndim, dims):
   return tuple(ndim + dim if dim < 0 else dim for dim in dims)
 
 
-def batch_norm(scope: Scope,
-               x,
-               use_running_average=False,
-               axis=-1, momentum=0.99, epsilon=1e-5,
-               dtype=jnp.float32,
-               bias=True, scale=True,
-               bias_init=initializers.zeros_init(), scale_init=initializers.ones_init(),
-               axis_name=None, axis_index_groups=None,
-               kind='batch_stats'):
-
+def batch_norm(
+    scope: Scope,
+    x,
+    use_running_average=False,
+    axis=-1,
+    momentum=0.99,
+    epsilon=1e-5,
+    dtype=jnp.float32,
+    bias=True,
+    scale=True,
+    bias_init=initializers.zeros_init(),
+    scale_init=initializers.ones_init(),
+    axis_name=None,
+    axis_index_groups=None,
+    kind='batch_stats',
+):
   x = jnp.asarray(x, jnp.float32)
   axis = axis if isinstance(axis, tuple) else (axis,)
   axis = _absolute_dims(x.ndim, axis)
   redux = tuple(i for i in range(x.ndim) if i not in axis)
 
   def pmean(x):
     m = jnp.mean(x, redux, keepdims=True)
@@ -57,38 +63,39 @@
   if use_running_average:
     # if ra_mean is not None:
     #   raise ValueError('batch_stats should be provided if use_running_averages=True')
     mean = jnp.reshape(ra_mean.value, mean.shape)
     var = jnp.reshape(ra_var.value, var.shape)
   else:
     if not is_init:
-      beta = 1. - momentum
+      beta = 1.0 - momentum
       ra_mean.value += beta * (jnp.squeeze(mean) - ra_mean.value)
       ra_var.value += beta * (jnp.squeeze(var) - ra_var.value)
   y = x - mean
   mul = lax.rsqrt(var + epsilon)
   if scale:
-    mul = mul * scope.param(
-        'scale', scale_init, squeeze_shape).reshape(mean.shape)
+    mul = mul * scope.param('scale', scale_init, squeeze_shape).reshape(
+        mean.shape
+    )
   y = y * mul
   if bias:
-    y = y + scope.param(
-        'bias', bias_init, squeeze_shape).reshape(mean.shape)
+    y = y + scope.param('bias', bias_init, squeeze_shape).reshape(mean.shape)
   return jnp.asarray(y, dtype)
 
 
 def layer_norm(
     scope: Scope,
     x,
     epsilon=1e-6,
     dtype=jnp.float32,
     bias=True,
     scale=True,
     bias_init=initializers.zeros_init(),
-    scale_init=initializers.ones_init()):
+    scale_init=initializers.ones_init(),
+):
   """Applies layer normalization on the input.
   It normalizes the activations of the layer for each given example in a
   batch independently, rather than across a batch like Batch Normalization.
   i.e. applies a transformation that maintains the mean activation within
   each example close to 0 and the activation standard deviation close to 1.
   Args:
     x: the inputs
@@ -105,32 +112,35 @@
   """
   features = x.shape[-1]
   mean = jnp.mean(x, axis=-1, keepdims=True)
   mean2 = jnp.mean(lax.square(x), axis=-1, keepdims=True)
   var = mean2 - lax.square(mean)
   mul = lax.rsqrt(var + epsilon)
   if scale:
-    mul = mul * jnp.asarray(scope.param('scale', scale_init, (features,)),
-                            dtype)
+    mul = mul * jnp.asarray(
+        scope.param('scale', scale_init, (features,)), dtype
+    )
   y = (x - mean) * mul
   if bias:
     y = y + jnp.asarray(scope.param('bias', bias_init, (features,)), dtype)
   return y
 
 
-def group_norm(scope,
-               x,
-               num_groups=32,
-               group_size=None,
-               epsilon=1e-6,
-               dtype=jnp.float32,
-               bias=True,
-               scale=True,
-               bias_init=initializers.zeros_init(),
-               scale_init=initializers.ones_init()):
+def group_norm(
+    scope,
+    x,
+    num_groups=32,
+    group_size=None,
+    epsilon=1e-6,
+    dtype=jnp.float32,
+    bias=True,
+    scale=True,
+    bias_init=initializers.zeros_init(),
+    scale_init=initializers.ones_init(),
+):
   """Applies group normalization to the input (arxiv.org/abs/1803.08494).
   This op is similar to batch normalization, but statistics are shared across
   equally-sized groups of channels and not shared across batch dimension.
   Thus, group normalization does not depend on the batch composition and does
   not require maintaining internal state for storing statistics.
   The user should either specify the total number of channel groups or the
   number of channels per group.
@@ -149,36 +159,40 @@
       by the next layer.
     bias_init: Initializer for bias, by default, zero.
     scale_init: Initializer for scale, by default, one.
   Returns:
     Normalized inputs (the same shape as inputs).
   """
   x = jnp.asarray(x, jnp.float32)
-  if ((num_groups is None and group_size is None) or
-      (num_groups is not None and group_size is not None)):
-    raise ValueError('Either `num_groups` or `group_size` should be '
-                     'specified, but not both of them.')
+  if (num_groups is None and group_size is None) or (
+      num_groups is not None and group_size is not None
+  ):
+    raise ValueError(
+        'Either `num_groups` or `group_size` should be '
+        'specified, but not both of them.'
+    )
 
   if group_size is not None:
     channels = x.shape[-1]
     if channels % group_size != 0:
-      raise ValueError('Number of channels ({}) is not multiple of the '
-                       'group size ({}).'.format(channels, group_size))
+      raise ValueError(
+          'Number of channels ({}) is not multiple of the '
+          'group size ({}).'.format(channels, group_size)
+      )
     num_groups = channels // group_size
 
   input_shape = x.shape
   group_shape = x.shape[:-1] + (num_groups, x.shape[-1] // num_groups)
 
   x = x.reshape(group_shape)
 
   reduction_axis = list(range(1, x.ndim - 2)) + [x.ndim - 1]
 
   mean = jnp.mean(x, axis=reduction_axis, keepdims=True)
-  mean_of_squares = jnp.mean(jnp.square(x), axis=reduction_axis,
-                             keepdims=True)
+  mean_of_squares = jnp.mean(jnp.square(x), axis=reduction_axis, keepdims=True)
   var = mean_of_squares - jnp.square(mean)
 
   x = (x - mean) * lax.rsqrt(var + epsilon)
 
   x = x.reshape(input_shape)
 
   feature_shape = tuple([1 for d in input_shape[:-1]] + [input_shape[-1]])
```

### Comparing `flax-0.7.0/flax/core/nn/stochastic.py` & `flax-0.7.1/flax/core/nn/stochastic.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,39 +8,37 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Stochastic modules.
-"""
+"""Stochastic modules."""
 
 from jax import lax
 from jax import random
 import jax.numpy as jnp
 
 
-
 def dropout(scope, inputs, rate, deterministic=False, rng=None):
   """Applies a random dropout mask to the input.
   Args:
     inputs: the inputs that should be randomly masked.
     rate: the probablity of masking out a value.
     deterministic: if false the inputs are scaled by `1 / (1 - rate)` and
       masked, whereas if true, no mask is applied and the inputs are returned as
       is.
     rng: an optional `jax.random.PRNGKey`. By default `nn.make_rng()` will
       be used.
   Returns:
     The masked inputs.
   """
-  if rate == 0.:
+  if rate == 0.0:
     return inputs
-  keep_prob = 1. - rate
+  keep_prob = 1.0 - rate
 
   if deterministic:
     return inputs
   else:
     if rng is None:
       rng = scope.make_rng('dropout')
     mask = random.bernoulli(rng, p=keep_prob, shape=inputs.shape)
```

### Comparing `flax-0.7.0/flax/core/partial_eval.py` & `flax-0.7.1/flax/core/partial_eval.py`

 * *Files 0% similar despite different names*

```diff
@@ -44,14 +44,15 @@
 
   Args:
     fn: the function to be lazily evaluated.
   Returns:
     A new function that accepts a mix of concrete values and
     ``jax.ShapeDtypeStruct`` instances.
   """
+
   @functools.wraps(fn)
   def wrapper(*args, **kwargs):
     # TODO(mattjj,jheek): use a public JAX API
     # flatten fn and prepare for internal JAX transform
     inputs_flat, in_tree = jax.tree_util.tree_flatten((args, kwargs))
     f_flat, out_tree = jax.api_util.flatten_fun(lu.wrap_init(fn), in_tree)
     # map inputs to PartialVal known/unknown
```

### Comparing `flax-0.7.0/flax/core/scope.py` & `flax-0.7.1/flax/core/scope.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,36 +16,49 @@
 
 import collections
 import contextlib
 import dataclasses
 import functools
 import hashlib
 import typing
-from typing import (Any, Callable, Dict, Generic, Iterable, Mapping, Optional,
-                    Sequence, Set, Tuple, TypeVar, Union)
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Generic,
+    Iterable,
+    Mapping,
+    Optional,
+    Sequence,
+    Set,
+    Tuple,
+    TypeVar,
+    Union,
+)
 
-from flax.ids import uuid
 from flax import config as config
 from flax import configurations as legacy_config  # only for flax_lazy_rng
 from flax import errors
 from flax import struct
 from flax import traceback_util
-from .frozen_dict import freeze
-from .frozen_dict import FrozenDict
-from .frozen_dict import unfreeze
-from . import partial_eval
-from . import tracers
-from . import meta
+from flax.ids import uuid
 import jax
 from jax import config as jax_config
 from jax import numpy as jnp
 from jax import random
 from jax import tree_util
 import numpy as np
 
+from . import meta
+from . import partial_eval
+from . import tracers
+from .frozen_dict import freeze
+from .frozen_dict import FrozenDict
+from .frozen_dict import unfreeze
+
 traceback_util.register_exclusion(__file__)
 
 T = TypeVar('T')
 
 PRNGKey = Any
 Array = Any
 
@@ -53,28 +66,30 @@
 
 
 Filter = Union[bool, str, typing.Collection[str], 'DenyList']
 
 # When conditioning on filters we require explicit boolean comparisons.
 # pylint: disable=g-bool-id-comparison
 
+
 @dataclasses.dataclass(frozen=True, eq=True)
 class DenyList:
   """DenyList represents an opt-out based mutability filter.
 
   DenyList can be used to make every collection mutable except the ones
   defined in the given filter.
   To for example make everything but the params collection mutable::
 
     nn.apply(fn, mutable=nn.DenyList(["params"]))
 
   Attributes:
     deny: The filter representing the collections that are not mutable.
 
   """
+
   deny: Filter
 
 
 CollectionFilter = Filter
 PRNGSequenceFilter = Filter
 
 Collection = Mapping[str, Any]
@@ -85,23 +100,25 @@
 MutableVariableDict = Dict[str, MutableCollection]
 
 PRNGFoldable = Union[int, str]
 
 
 class LazyRng(struct.PyTreeNode):
   """Wrapper around JAX PRNGKey that lazily maintains a tuple of static data to be folded into the rng."""
+
   rng: PRNGKey
   suffix: Tuple[PRNGFoldable, ...] = struct.field(pytree_node=False)
 
   def as_jax_rng(self) -> PRNGKey:
     return _fold_in_static(self.rng, self.suffix)
 
   @staticmethod
-  def create(rng: Union['LazyRng', PRNGKey],
-             *suffix: PRNGFoldable) -> 'LazyRng':
+  def create(
+      rng: Union['LazyRng', PRNGKey], *suffix: PRNGFoldable
+  ) -> 'LazyRng':
     if not legacy_config.flax_lazy_rng:
       if isinstance(rng, LazyRng):
         assert not rng.suffix
         rng = rng.rng
       return LazyRng(_legacy_rng_fold_in(rng, suffix), ())
     if isinstance(rng, LazyRng):
       return LazyRng(rng.rng, rng.suffix + suffix)
@@ -121,16 +138,17 @@
     elif isinstance(x, int):
       rng = random.fold_in(rng, x)
     else:
       raise ValueError(f'Expected int or string, got: {x}')
   return rng
 
 
-def _fold_in_static(rng: PRNGKey,
-                    data: typing.Collection[PRNGFoldable]) -> PRNGKey:
+def _fold_in_static(
+    rng: PRNGKey, data: typing.Collection[PRNGFoldable]
+) -> PRNGKey:
   """Folds static data (strings & ints) into a jax.random.PRNGKey using its SHA-1 hash.
 
   This is faster than splitting an PRNGKey because it allows generating new PRNG
   keys in parallel that are independent of each other.
 
   Args:
    rng: the rng to fold the string into.
@@ -150,15 +168,15 @@
       m.update(x.encode('utf-8'))
     elif isinstance(x, int):
       m.update(x.to_bytes((x.bit_length() + 7) // 8, byteorder='big'))
     else:
       raise ValueError(f'Expected int or string, got: {x}')
   d = m.digest()
   hash_int = int.from_bytes(d[:4], byteorder='big')
-  return random.fold_in(rng, jnp.uint32(hash_int)) # type: ignore
+  return random.fold_in(rng, jnp.uint32(hash_int))  # type: ignore
 
 
 def is_filter_empty(filter_like: Filter) -> bool:
   """Returns True if `filter_like` is an empty filter.
 
   Args:
     filter_like: The filter to test.
@@ -299,16 +317,16 @@
     return subtract_filters(b, a.deny)
   a = filter_to_set(a)
   b = filter_to_set(b)
   return a.intersection(b)
 
 
 def group_collections(
-    xs: VariableDict,
-    col_filters: Sequence[CollectionFilter]) -> Sequence[MutableVariableDict]:
+    xs: VariableDict, col_filters: Sequence[CollectionFilter]
+) -> Sequence[MutableVariableDict]:
   """Groups variables by collection filters.
 
   Iteratively applies the filters in `col_filters` to `xs`, and adds the result
   of applying each filter to the output sequence. Each key in `xs` is only added
   to the output once.
 
   Args:
@@ -339,16 +357,15 @@
   """A Variable object allows mutable access to a variable in a VariableDict.
 
   Variables are identified by a collection (e.g., "batch_stats") and a name
   (e.g., "moving_mean"). The value property gives access to the variable's
   content and can be assigned to for mutation.
   """
 
-  def __init__(
-      self, scope: 'Scope', collection: str, name: str, unbox: bool):
+  def __init__(self, scope: 'Scope', collection: str, name: str, unbox: bool):
     """Initializes a variable.
 
     Args:
       scope: The scope in which the variable is stored.
       collection: The collection of the variable (e.g., "params").
       name: The name of the variable (e.g., "dense").
       unbox: Whether to unbox boxed values with metadata.
@@ -366,38 +383,41 @@
     return meta.unbox(v) if self.unbox else v
 
   @value.setter
   def value(self, value: T):
     """Updates the value of this Variable."""
     if self.unbox:
       cur = self.scope.get_variable(self.collection, self.name)
-      cur_struct = tree_util.tree_structure(cur,
-                                            is_leaf=meta.is_axis_metadata)
-      value_struct = tree_util.tree_structure(value,
-                                              is_leaf=meta.is_axis_metadata)
-      has_meta = any(map(meta.is_axis_metadata,
-                         cur_struct.flatten_up_to(cur)))
+      cur_struct = tree_util.tree_structure(cur, is_leaf=meta.is_axis_metadata)
+      value_struct = tree_util.tree_structure(
+          value, is_leaf=meta.is_axis_metadata
+      )
+      has_meta = any(map(meta.is_axis_metadata, cur_struct.flatten_up_to(cur)))
       if cur_struct == value_struct and has_meta:
         value = meta.replace_boxed(cur, value)
 
     self.scope.put_variable(self.collection, self.name, value)
 
   def is_mutable(self) -> bool:
     """Checks if this Variable is mutable."""
     return self.scope.is_mutable_collection(self.collection)
 
 
 class _ChildRNGSentinel:
   pass
+
+
 # used to identify that an rng counter is meant for a child scope
 child_rng_token = _ChildRNGSentinel()
 
 
 class _DefaultSentinel:
   pass
+
+
 # used to denote no default flag value on scope
 no_flag = _DefaultSentinel()
 
 
 class Scope:
   """A Scope allows easy access to variables and manages RNGS of a neural network layer.
 
@@ -405,24 +425,27 @@
   :class:`flax.linen.module.Module`, so users writing neural network code
   usually generally do not interact with ``Scopes`` directly.
 
   See `core design tests
   <https://github.com/google/flax/tree/main/tests/core/design>`_
   for a number of examples using ``Scopes``.
   """
+
   reservations: Dict[str, Set[Optional[str]]]
 
-  def __init__(self,
-               variables: MutableVariableDict,
-               rngs: Optional[Dict[str, Union[PRNGKey, LazyRng]]] = None,
-               name: Optional[str] = None,
-               mutable: CollectionFilter = False,
-               parent: Optional['Scope'] = None,
-               path: Iterable[str] = (),
-               flags: Optional[Mapping] = None):
+  def __init__(
+      self,
+      variables: MutableVariableDict,
+      rngs: Optional[Dict[str, Union[PRNGKey, LazyRng]]] = None,
+      name: Optional[str] = None,
+      mutable: CollectionFilter = False,
+      parent: Optional['Scope'] = None,
+      path: Iterable[str] = (),
+      flags: Optional[Mapping] = None,
+  ):
     """Initializes a Scope.
 
     Args:
       variables: VariableDict to initialize the Scope with.
       rngs: RNGs used in this scope or one of the child scopes.
       name: name of this scope.
       mutable: A CollectionFilter determining which variables are mutable.
@@ -451,15 +474,19 @@
     # If the root variable dict and path are the same, then two scopes behave
     # identically. Effectively, a scope is nothing more than a cursor into a
     # variable dict and an rng counter dict.
     if not isinstance(other, Scope):
       return False
     if self is other:
       return True
-    return self.root._variables is other.root._variables and self.path == other.path and self.rng_counters is other.rng_counters
+    return (
+        self.root._variables is other.root._variables
+        and self.path == other.path
+        and self.rng_counters is other.rng_counters
+    )
 
   def __hash__(self) -> int:
     # see __eq__
     return hash((id(self.root._variables), self.path, id(self.rng_counters)))
 
   @property
   def root(self) -> 'Scope':
@@ -490,16 +517,17 @@
   def invalidate(self):
     """Invalidates the Scope."""
     self._invalid = True
 
   def mutable_variables(self) -> Union[VariableDict, Dict[str, Any]]:
     """Returns an immutable copy of the mutable variables belonging to this Scope."""
     self._populate_collections()
-    xs = {k: v for k, v in self._variables.items()
-          if in_filter(self.mutable, k)}
+    xs = {
+        k: v for k, v in self._variables.items() if in_filter(self.mutable, k)
+    }
     if config.flax_return_frozendict:
       return freeze(xs)
     return xs
 
   def variables(self) -> Union[VariableDict, Dict[str, Any]]:
     """Returns an immutable copy of the variables belonging to this Scope."""
     self._populate_collections()
@@ -517,47 +545,59 @@
       rewind_rngs: if true, reset the RNG counter of this scope.
 
     Returns:
       A rewound version of this scope, which means reservations are
       emptied, and the rng counter is optionally rewound.
     """
     self._check_valid()
-    scope = Scope(self._variables, self.rngs, self.name, self.mutable,
-                  self.parent, path=self.path, flags=self.flags)
+    scope = Scope(
+        self._variables,
+        self.rngs,
+        self.name,
+        self.mutable,
+        self.parent,
+        path=self.path,
+        flags=self.flags,
+    )
     if not rewind_rngs:
       scope.rng_counters = self.rng_counters
     return scope
 
   def name_reserved(self, name: str, col: Optional[str] = None) -> bool:
     """Checks whether a name for a child Scope or Variable is taken.
 
     Args:
       name: the name to check for collision.
       col: if a variable, the collection used.
     """
     if name in self.reservations:
       # allow the same name for two variables in
       # different collections, otherwise raise error.
-      if (None in self.reservations[name] or col is None
-          or col in self.reservations[name]):
-          return True
+      if (
+          None in self.reservations[name]
+          or col is None
+          or col in self.reservations[name]
+      ):
+        return True
     return False
 
   def reserve(self, name: str, col: Optional[str] = None):
     """Reserves a name for a child Scope or Variable.
 
     Throws an error if the name exists already.
 
     Args:
       name: the name to reserve.
       col: if a variable, the collection used.
     """
     if not isinstance(name, str):
-      raise TypeError('The type of scope "{name}" should be string but '
-                      f'it is {type(name)}')
+      raise TypeError(
+          'The type of scope "{name}" should be string but '
+          f'it is {type(name)}'
+      )
     if self.name_reserved(name, col):
       raise ValueError(f'Duplicate use of scope name: "{name}"')
     self.reservations[name].add(col)
 
   def default_name(self, prefix: str) -> str:
     """Generates an unreserved name with the given prefix.
 
@@ -570,18 +610,17 @@
     i = 0
     while True:
       name = f'{prefix}{i}'
       if name not in self.reservations:
         return name
       i += 1
 
-  def push(self,
-           name: Optional[str] = None,
-           prefix: str = '',
-           reuse=False) -> 'Scope':
+  def push(
+      self, name: Optional[str] = None, prefix: str = '', reuse=False
+  ) -> 'Scope':
     """Creates a child Scope.
 
     Args:
       name: optional name of the child.
       prefix: prefix used for generating the name if `name` is `None`.
       reuse: if True will return a pre-existing child scope with the given name
         instead of throwing an error.
@@ -594,34 +633,38 @@
     if name is None:
       name = self.default_name(prefix)
     if not reuse or name not in self.reservations:
       self.reserve(name)
     rngs = {key: LazyRng.create(rng, name) for key, rng in self.rngs.items()}
     rng_key = (child_rng_token, name)
     if rng_key in self.rng_counters:
-      rng_counters = self.rng_counters.get(rng_key) # type: ignore
+      rng_counters = self.rng_counters.get(rng_key)  # type: ignore
     else:
       rng_counters = {key: 0 for key in rngs}
-      self.rng_counters[rng_key] = rng_counters # type: ignore
-    scope = Scope({},
-                  name=name,
-                  rngs=rngs,
-                  parent=self,
-                  mutable=self.mutable,
-                  path=self.path + (name,),
-                  flags=self.flags)
+      self.rng_counters[rng_key] = rng_counters  # type: ignore
+    scope = Scope(
+        {},
+        name=name,
+        rngs=rngs,
+        parent=self,
+        mutable=self.mutable,
+        path=self.path + (name,),
+        flags=self.flags,
+    )
     scope.rng_counters = rng_counters
     return scope
 
-  def child(self,
-            fn: Callable[..., Any],
-            name: Optional[str] = None,
-            prefix: Optional[str] = None,
-            named_call: bool = True,
-            **partial_kwargs) -> Callable[..., Any]:
+  def child(
+      self,
+      fn: Callable[..., Any],
+      name: Optional[str] = None,
+      prefix: Optional[str] = None,
+      named_call: bool = True,
+      **partial_kwargs,
+  ) -> Callable[..., Any]:
     """Partially applies a child scope to fn.
 
     When calling the returned function multiple times variables will be reused.
 
     Args:
       fn: the function to partially apply the child Scope to.
       name: optional name of the child.
@@ -757,26 +800,34 @@
     if not self.is_mutable_collection(col):
       raise errors.ModifyScopeVariableError(col, name, self.path_text)
     variables = self._mutable_collection(col)
 
     # Make sure reference sharing of child variable dictionaries isn't broken.
     # See https://github.com/google/flax/issues/2022 for more details.
     def put(target, key, val):
-      if (key in target and isinstance(target[key], dict) and
-          isinstance(val, Mapping)):
+      if (
+          key in target
+          and isinstance(target[key], dict)
+          and isinstance(val, Mapping)
+      ):
         for k, v in val.items():
           put(target[key], k, v)
       else:
         target[key] = val
 
     put(variables, name, value)
 
-  def variable(self, col: str, name: str,  # pylint: disable=keyword-arg-before-vararg
-               init_fn: Optional[Callable[..., T]] = None,
-               *init_args, unbox: bool = True) -> Variable[T]:
+  def variable(
+      self,
+      col: str,
+      name: str,  # pylint: disable=keyword-arg-before-vararg
+      init_fn: Optional[Callable[..., T]] = None,
+      *init_args,
+      unbox: bool = True,
+  ) -> Variable[T]:
     """Creates a variable if it doesn't exist yet in this scope and returns it.
 
     Args:
       col: the collection of the variable.
       name: the name of the variable.
       init_fn: a function taking a PRNGKey plus any other number of positional
         arguments. If None, the variable must already be initialized otherwise
@@ -794,16 +845,17 @@
         if self.is_collection_empty(col):
           raise errors.ScopeCollectionNotFound(col, name, self.path_text)
         raise errors.ScopeVariableNotFoundError(name, col, self.path_text)
       init_value = init_fn(*init_args)
       self.put_variable(col, name, init_value)
     return Variable(self, col, name, unbox=unbox)
 
-  def param(self, name: str, init_fn: Callable[..., T], *init_args,
-            unbox: bool = True) -> T:
+  def param(
+      self, name: str, init_fn: Callable[..., T], *init_args, unbox: bool = True
+  ) -> T:
     """Creates a parameter if it doesn't exist yet in this scope and returns it.
 
     If the parameter exists already, the existing value is simply returned.
 
     Args:
       name: the name of the parameter.
       init_fn: a function taking a PRNGKey plus any other number of positional
@@ -813,32 +865,34 @@
         value, see ``flax.nn.meta.unbox`` (default: True).
 
     Returns:
       The parameters. Throws an error if the params exist already.
     """
     self.reserve(name, 'params')
     if self.has_variable('params', name):
-      abs_rng = jax.ShapeDtypeStruct(random.default_prng_impl().key_shape,
-                                     jnp.uint32)
+      abs_rng = jax.ShapeDtypeStruct(
+          random.default_prng_impl().key_shape, jnp.uint32
+      )
       value = self.get_variable('params', name)
       # Validate that the shape of the init_fn output is the same as the shape
       # of the existing parameter. This is to make sure that the hparams set up
       # in a Flax Module match the shapes coming in during apply, and if not,
       # catch it with an error message.
       # NOTE: We could consider moving this to `self.`
       abs_value = jax.eval_shape(lambda rng: init_fn(rng, *init_args), abs_rng)
       abs_value_flat = jax.tree_util.tree_leaves(abs_value)
       value_flat = jax.tree_util.tree_leaves(value)
       for val, abs_val in zip(value_flat, abs_value_flat):
         # NOTE: We could check dtype consistency here as well but it's
         # usefuleness is less obvious. We might intentionally change the dtype
         # for inference to a half float type for example.
         if jnp.shape(val) != jnp.shape(abs_val):
-          raise errors.ScopeParamShapeError(name, self.path_text,
-                                            jnp.shape(abs_val), jnp.shape(val))
+          raise errors.ScopeParamShapeError(
+              name, self.path_text, jnp.shape(abs_val), jnp.shape(val)
+          )
     else:
       if not self.is_mutable_collection('params'):
         if self.is_collection_empty('params'):
           raise errors.ScopeCollectionNotFound('params', name, self.path_text)
         raise errors.ScopeParamNotFoundError(name, self.path_text)
       value = init_fn(self.make_rng('params'), *init_args)
       self.put_variable('params', name, value)
@@ -866,18 +920,20 @@
     if in_filter(mutable, key):
       new_variables[key] = unfreeze(value)
     else:
       new_variables[key] = freeze(value)
   return new_variables
 
 
-def bind(variables: VariableDict,
-         rngs: Optional[RNGSequences] = None,
-         mutable: CollectionFilter = False,
-         flags: Optional[Mapping] = None):
+def bind(
+    variables: VariableDict,
+    rngs: Optional[RNGSequences] = None,
+    mutable: CollectionFilter = False,
+    flags: Optional[Mapping] = None,
+):
   """Binds variables and rngs to a new ``Scope``.
 
   bind provides a ``Scope`` instance without transforming a function with
   ``apply``. This is particularly useful for debugging and interactive use cases
   like notebooks where a function would limit the ability split up code into
   different cells.
 
@@ -895,87 +951,102 @@
   Returns:
     A new scope with the variables and rngs bound to it.
   """
   if not _is_valid_variables(variables):
     raise errors.ApplyScopeInvalidVariablesTypeError()
   if rngs is not None and not _is_valid_rngs(rngs):
     raise errors.InvalidRngError(
-        'rngs should be a dictionary mapping strings to `jax.PRNGKey`.')
+        'rngs should be a dictionary mapping strings to `jax.PRNGKey`.'
+    )
   new_variables = _unfreeze_variables(variables, mutable)
   return Scope(new_variables, rngs=rngs, mutable=mutable, flags=flags)
 
 
-def apply(fn: Callable[..., Any],
-          mutable: CollectionFilter = False,
-          flags: Optional[Mapping] = None) -> Callable[..., Any]:
+def apply(
+    fn: Callable[..., Any],
+    mutable: CollectionFilter = False,
+    flags: Optional[Mapping] = None,
+) -> Callable[..., Any]:
   """Functionalize a `Scope` function.
 
   Args:
     fn: a function taking a `Scope` as its first argument.
     mutable: the filter determining which variable collections are mutable.
     flags: internal flags.
 
   Returns:
     `fn` with the scope partially applied.
   """
 
   @functools.wraps(fn)
-  def wrapper(variables: VariableDict,
-              *args,
-              rngs: Optional[RNGSequences] = None,
-              **kwargs) -> Union[Any, Tuple[Any, Union[VariableDict, Dict[str, Any]]]]:
+  def wrapper(
+      variables: VariableDict,
+      *args,
+      rngs: Optional[RNGSequences] = None,
+      **kwargs,
+  ) -> Union[Any, Tuple[Any, Union[VariableDict, Dict[str, Any]]]]:
     # Try to detect if user accidentally passed {'params': {'params': ...}.
-    if 'params' in variables and isinstance(
-        variables['params'],
-        (dict, FrozenDict)) and 'params' in variables['params']:
+    if (
+        'params' in variables
+        and isinstance(variables['params'], (dict, FrozenDict))
+        and 'params' in variables['params']
+    ):
       raise errors.ApplyScopeInvalidVariablesStructureError(variables)
 
-    with bind(variables, rngs=rngs, mutable=mutable,
-              flags=flags).temporary() as root:
+    with bind(
+        variables, rngs=rngs, mutable=mutable, flags=flags
+    ).temporary() as root:
       y = fn(root, *args, **kwargs)
     if mutable is not False:
       return y, root.mutable_variables()
     else:
       return y
 
   return wrapper
 
 
-def init(fn: Callable[..., Any],
-         mutable: CollectionFilter = True,
-         flags: Optional[Mapping] = None) -> Callable[..., Any]:
+def init(
+    fn: Callable[..., Any],
+    mutable: CollectionFilter = True,
+    flags: Optional[Mapping] = None,
+) -> Callable[..., Any]:
   """Functionalize a `Scope` function for initialization.
 
   Args:
     fn: a function taking a `Scope` as its first argument.
     mutable: the filter determining which variable collections are mutable.
     flags: internal flags.
 
   Returns:
     `fn` with the scope partially applied.
   """
 
   @functools.wraps(fn)
   def wrapper(rngs, *args, **kwargs) -> Tuple[Any, VariableDict]:
     if not _is_valid_rng(rngs) and not _is_valid_rngs(rngs):
-      raise ValueError('First argument passed to an init function should be a '
-                       '`jax.PRNGKey` or a dictionary mapping strings to '
-                       '`jax.PRNGKey`.')
+      raise ValueError(
+          'First argument passed to an init function should be a '
+          '`jax.PRNGKey` or a dictionary mapping strings to '
+          '`jax.PRNGKey`.'
+      )
     if not isinstance(rngs, dict):
       rngs = {'params': rngs}
     init_flags = {**(flags if flags is not None else {}), 'initializing': True}
-    return apply(fn, mutable=mutable, flags=init_flags)({}, *args, rngs=rngs,
-                                                        **kwargs)
+    return apply(fn, mutable=mutable, flags=init_flags)(
+        {}, *args, rngs=rngs, **kwargs
+    )
 
   return wrapper
 
 
-def lazy_init(fn: Callable[..., Any],
-         mutable: CollectionFilter = True,
-         flags: Optional[Mapping] = None) -> Callable[..., Any]:
+def lazy_init(
+    fn: Callable[..., Any],
+    mutable: CollectionFilter = True,
+    flags: Optional[Mapping] = None,
+) -> Callable[..., Any]:
   """Functionalizes a `Scope` function for lazy initialization.
 
   Similair to ``init`` except that the init function now accepts
   ``jax.ShapeDtypeStruct`` instances for arguments that do not
   affect the variable initialization (typically this is all the input data).
 
   Example::
@@ -994,15 +1065,17 @@
     mutable: the filter determining which variable collections are mutable.
     flags: internal flags.
 
   Returns:
     `fn` with the scope partially applied. Unlike ``init`` which returns a tuple of function
     output and variables, the lazy init function only returns the variables.
   """
-  return partial_eval.lazy_init(lambda *args, **kwargs: init(fn, mutable, flags)(*args, **kwargs)[1])
+  return partial_eval.lazy_init(
+      lambda *args, **kwargs: init(fn, mutable, flags)(*args, **kwargs)[1]
+  )
 
 
 def _is_valid_collection(col: VariableDict):
   if not isinstance(col, (FrozenDict, dict)):
     return False
   for name in col.keys():
     # Any value can be stored in a collection so only keys can be verified.
@@ -1027,23 +1100,25 @@
       return False
   return True
 
 
 def _is_valid_rng(rng: Array):
   """Checks whether rng is a valid JAX PRNGKey, also handling custom prngs."""
   # New-style JAX KeyArrays have a base type.
-  if jax_config.jax_enable_custom_prng: # type: ignore[attr-defined]
+  if jax_config.jax_enable_custom_prng:  # type: ignore[attr-defined]
     if not isinstance(rng, jax.random.KeyArray):
       return False
   # Old-style JAX PRNGKeys are plain uint32 arrays.
   else:
     if not isinstance(rng, (np.ndarray, jnp.ndarray)):
       return False
-    if (rng.shape != random.default_prng_impl().key_shape or
-        rng.dtype != jnp.uint32):
+    if (
+        rng.shape != random.default_prng_impl().key_shape
+        or rng.dtype != jnp.uint32
+    ):
       return False
   return True
 
 
 def _is_valid_rngs(rngs: RNGSequences):
   if not isinstance(rngs, (FrozenDict, dict)):
     return False
```

### Comparing `flax-0.7.0/flax/core/tracers.py` & `flax-0.7.1/flax/core/tracers.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/flax/core/variables.py` & `flax-0.7.1/flax/core/variables.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/flax/errors.py` & `flax-0.7.1/flax/errors.py`

 * *Files 3% similar despite different names*

```diff
@@ -48,15 +48,17 @@
     super().__init__(f'')
 """
 
 
 class FlaxError(Exception):
 
   def __init__(self, message):
-    error_page = 'https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html'
+    error_page = (
+        'https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html'
+    )
     module_name = self.__class__.__module__
     class_name = self.__class__.__name__
     error_msg = f'{message} ({error_page}#{module_name}.{class_name})'
     super().__init__(error_msg)
 
 
 #################################################
@@ -82,16 +84,17 @@
         k = self.param("kernel", lambda _: x)
         return x * k
     Foo().lazy_init(random.PRNGKey(0), jax.ShapeDtypeStruct((8, 4), jnp.float32))
   """
 
   def __init__(self, partial_val):
     super().__init__(
-        f'Lazy init encountered a value that could with '
-        f'the given inputs (shape: {partial_val}).')
+        'Lazy init encountered a value that could with '
+        f'the given inputs (shape: {partial_val}).'
+    )
 
 
 #################################################
 # scope.py errors                               #
 #################################################
 
 
@@ -160,32 +163,35 @@
   """When calling :meth:`Module.apply() <flax.linen.Module.apply>`, the first
 
   argument should be a variable dict. For more explanation on variable dicts,
   please see :mod:`flax.core.variables`.
   """
 
   def __init__(self):
-    super().__init__('The first argument passed to an apply function should be '
-                     'a dictionary of collections. Each collection should be a '
-                     'dictionary with string keys.')
+    super().__init__(
+        'The first argument passed to an apply function should be '
+        'a dictionary of collections. Each collection should be a '
+        'dictionary with string keys.'
+    )
 
 
 class ApplyScopeInvalidVariablesStructureError(FlaxError):
   """This error is thrown when the dict passed as `variables` to apply() has an
 
   extra 'params' layer, i.e. {'params': {'params': ...}}.
   For more explanation on variable dicts, please see :mod:`flax.core.variables`.
   """
 
   def __init__(self, variables):
     super().__init__(
-        'Expect the `variables` (first argument) passed to apply() '
-        'to be a dict with the structure {"params": ...}, but got a dict '
-        'with an extra params layer, i.e.  {"params": {"params": ... } }. '
-        f'You should instead pass in your dict\'s ["params"].')
+        f'Expect the `variables` (first argument) passed to apply() '
+        f'to be a dict with the structure {{"params": ...}}, but got a dict '
+        f'with an extra params layer, i.e.  {{"params": {{"params": ... }} }}. '
+        f'You should instead pass in your dict\'s ["params"].'
+    )
 
 
 class ScopeParamNotFoundError(FlaxError):
   """This error is thrown when trying to access a parameter that does not exist.
 
   For instance, in the code below, the initialized embedding name 'embedding'
   does not match the apply name 'embed'::
@@ -204,16 +210,18 @@
 
     model = Embed(4, 8)
     variables = model.init(random.PRNGKey(0), jnp.ones((5, 5, 1)))
     _ = model.apply(variables, jnp.ones((5, 5, 1)), 'embed')
   """
 
   def __init__(self, param_name, scope_path):
-    super().__init__(f'Could not find parameter named "{param_name}" in scope '
-                     f'"{scope_path}".')
+    super().__init__(
+        f'Could not find parameter named "{param_name}" in scope '
+        f'"{scope_path}".'
+    )
 
 
 class ScopeCollectionNotFound(FlaxError):
   """This error is thrown when trying to access a variable from an empty
   collection.
 
   There are two common causes:
@@ -224,15 +232,16 @@
      | In this case, you should have made the collection mutable during
      | apply (e.g.: ``module.apply(variables, ..., mutable=['state'])``.
   """
 
   def __init__(self, col_name, var_name, scope_path):
     super().__init__(
         f'Tried to access "{var_name}" from collection "{col_name}" in '
-        f'"{scope_path}" but the collection is empty.')
+        f'"{scope_path}" but the collection is empty.'
+    )
 
 
 class ScopeParamShapeError(FlaxError):
   """This error is thrown when the shape of an existing parameter is different from
 
   the shape of the return value of the ``init_fn``. This can happen when the
   shape provided during :meth:`Module.apply() <flax.linen.Module.apply>` is
@@ -256,46 +265,50 @@
         return y
 
     variables = NoBiasDense().init(random.PRNGKey(0), jnp.ones((5, 5, 1)))
     _ = NoBiasDense().apply(variables, jnp.ones((5, 5)))
   """
 
   def __init__(self, param_name, scope_path, value_shape, init_shape):
-    super().__init__(f'Initializer expected to generate shape {init_shape} '
-                     f'but got shape {value_shape} instead for parameter '
-                     f'"{param_name}" in "{scope_path}".')
+    super().__init__(
+        f'Initializer expected to generate shape {init_shape} '
+        f'but got shape {value_shape} instead for parameter '
+        f'"{param_name}" in "{scope_path}".'
+    )
 
 
 class ScopeVariableNotFoundError(FlaxError):
   """This error is thrown when trying to use a variable in a Scope in a collection
 
   that is immutable. In order to create this variable, mark the collection as
   mutable explicitly using the ``mutable`` keyword in
   :meth:`Module.apply() <flax.linen.Module.apply>`.
   """
 
   def __init__(self, name, col, scope_path):
-    super().__init__(f'No Variable named "{name}" for collection "{col}" '
-                     f'exists in "{scope_path}".')
+    super().__init__(
+        f'No Variable named "{name}" for collection "{col}" '
+        f'exists in "{scope_path}".'
+    )
 
 
 class InvalidFilterError(FlaxError):
   """A filter should be either a boolean, a string or a container object."""
 
   def __init__(self, filter_like):
     super().__init__(f'Invalid Filter: "{filter_like}"')
 
 
 class InvalidScopeError(FlaxError):
   """A temporary Scope is only valid within the context in which it is created::
 
-    with Scope(variables, rngs=rngs).temporary() as root:
-      y = fn(root, *args, **kwargs)
-      # Here root is valid.
-    # Here root is invalid.
+  with Scope(variables, rngs=rngs).temporary() as root:
+    y = fn(root, *args, **kwargs)
+    # Here root is valid.
+  # Here root is invalid.
   """
 
   def __init__(self, scope_name):
     super().__init__(f'The scope "{scope_name}" is no longer valid.')
 
 
 class ModifyScopeVariableError(FlaxError):
@@ -315,16 +328,18 @@
     v = MyModule.init(...)
     ...
     logits = MyModule.apply(v, batch)  # This throws an error.
     logits = MyModule.apply(v, batch, mutable=['batch_stats'])  # This works.
   """
 
   def __init__(self, col, variable_name, scope_path):
-    super().__init__(f'Cannot update variable "{variable_name}" in '
-                     f'"{scope_path}" because collection "{col}" is immutable.')
+    super().__init__(
+        f'Cannot update variable "{variable_name}" in '
+        f'"{scope_path}" because collection "{col}" is immutable.'
+    )
 
 
 class JaxTransformError(FlaxError):
   """JAX transforms and Flax modules cannot be mixed.
 
   JAX's functional transformations expect pure function.
   When you want to use JAX transformations **inside** Flax models,
@@ -346,16 +361,17 @@
 
   using a transformation (e.g.: ``scan``, ``vmap``) without specifying the
   "partition_name" in the ``metadata_params`` dict.
   """
 
   def __init__(self, target):
     super().__init__(
-        f'Trying to transform a Partitioned variable but "partition_name"'
-        f' is not specified in metadata_params: {target}')
+        'Trying to transform a Partitioned variable but "partition_name"'
+        f' is not specified in metadata_params: {target}'
+    )
 
 
 #################################################
 # module.py errors                              #
 #################################################
 
 
@@ -406,16 +422,18 @@
       def __call__(self, inputs):
         _ = self.param('mean', initializers.lecun_normal(), (2, 2))
         _ = self.variable('stats', 'mean', initializers.zeros_init(), (2, 2))
   """
 
   def __init__(self, key_type, value, module_name):
     # key_type is in {param, variable, submodule}.
-    super().__init__(f'Could not create {key_type} "{value}" in Module '
-                     f'{module_name}: Name in use.')
+    super().__init__(
+        f'Could not create {key_type} "{value}" in Module '
+        f'{module_name}: Name in use.'
+    )
 
 
 class AssignSubModuleError(FlaxError):
   """You are only allowed to create submodules in two places:
 
   1.  If your Module is noncompact: inside
       :meth:`Module.setup() <flax.linen.Module.setup>`.
@@ -449,16 +467,18 @@
 
   In this case, ``self.conv(kernel_size=4)`` is called from ``__call__``, which
   is disallowed because it's neither within ``setup`` nor a method wrapped in
   x``nn.compact``.
   """
 
   def __init__(self, cls):
-    super().__init__(f'Submodule {cls} must be defined in `setup()` or in a '
-                     'method wrapped in `@compact`')
+    super().__init__(
+        f'Submodule {cls} must be defined in `setup()` or in a '
+        'method wrapped in `@compact`'
+    )
 
 
 class SetAttributeInModuleSetupError(FlaxError):
   """You are not allowed to modify Module class attributes in
 
   :meth:`Module.setup() <flax.linen.Module.setup>`::
 
@@ -521,17 +541,19 @@
           self.dense.features = 20  # <--- This is not allowed
 
         def __call__(self, x):
           return self.dense(x)
   """
 
   def __init__(self, module_cls, attr_name, attr_val):
-    super().__init__(f'Can\'t set {attr_name}={attr_val} for Module of type '
-                     f'{module_cls}: Module instance is frozen outside of '
-                     'setup method.')
+    super().__init__(
+        f"Can't set {attr_name}={attr_val} for Module of type "
+        f'{module_cls}: Module instance is frozen outside of '
+        'setup method.'
+    )
 
 
 class MultipleMethodsCompactError(FlaxError):
   """The ``@compact`` decorator may only be added to at most one method in a Flax
 
   module. In order to resolve this, you can:
 
@@ -554,32 +576,34 @@
   The following attributes are reserved:
 
   * ``parent``: The parent Module of this Module.
   * ``name``: The name of this Module.
   """
 
   def __init__(self, annotations):
-    super().__init__(f'properties `parent` and `name` are reserved: '
-                     f'{annotations}')
+    super().__init__(
+        f'properties `parent` and `name` are reserved: {annotations}'
+    )
 
 
 class ApplyModuleInvalidMethodError(FlaxError):
   """When calling :meth:`Module.apply() <flax.linen.Module.apply>`, you can specify
 
   the method to apply using parameter ``method``. This error is thrown if the
   provided parameter is not a method in the Module and not a function with at
   least one argument.
 
   Learn more on the reference docs for
   :meth:`Module.apply() <flax.linen.Module.apply>`.
   """
 
   def __init__(self, method):
-    super().__init__(f'Cannot call apply(): {method} is not a valid function '
-                     'for apply().')
+    super().__init__(
+        f'Cannot call apply(): {method} is not a valid function for apply().'
+    )
 
 
 class CallCompactUnboundModuleError(FlaxError):
   """This error occurs when you are trying to call a Module directly, rather than
 
   through :meth:`Module.apply() <flax.linen.Module.apply>`. For instance, the
   error will be raised when trying to run this code::
@@ -597,15 +621,15 @@
     from jax import random
     variables = test_dense.init(random.PRNGKey(0), jnp.ones((5,5)))
 
     y = test_dense.apply(variables, jnp.ones((5,5)))
   """
 
   def __init__(self):
-    super().__init__('Can\'t call compact methods on unbound modules')
+    super().__init__("Can't call compact methods on unbound modules")
 
 
 class CallSetupUnboundModuleError(FlaxError):
   """This error occurs when you are trying to call `.setup()` directly.
 
   For instance, the error will be raised when trying to run this code::
 
@@ -630,15 +654,16 @@
       return module.submodule.clone() # avoid leaking the Scope
 
     empty_variables = {} # you can also use the real variables
     submodule = nn.apply(get_submodule, module)(empty_variables)
   """
 
   def __init__(self):
-    super().__init__('Can\'t call compact methods on unbound modules')
+    super().__init__("Can't call compact methods on unbound modules")
+
 
 class CallUnbindOnUnboundModuleError(FlaxError):
   """This error occurs when you are trying to call ``.unbind()`` on an unbound
   Module. For instance, when you try running the following example,
   an error will be raised::
 
     from flax import linen as nn
@@ -654,16 +679,18 @@
   Instead, you should ``bind`` the Module to a variable collection before calling
   ``.unbind()``::
 
     bound_module = module.bind(variables)
     ... # do something with bound_module
     module = bound_module.unbind() # <-- OK!
   """
+
   def __init__(self):
-    super().__init__('Can\'t call `unbind()` on unbound modules')
+    super().__init__("Can't call `unbind()` on unbound modules")
+
 
 class InvalidInstanceModuleError(FlaxError):
   """This error occurs when you are trying to call `.init()`, `.init_with_output()`, `.apply() or `.bind()`
 
   on the Module class itself, instead of an instance of the Module class.
   For example, the error will be raised when trying to run this code::
 
@@ -677,15 +704,16 @@
     B.init(k, x)   # B is module class, not B() a module instance
     B.apply(vs, x)   # similar issue with apply called on class instead of
     instance.
   """
 
   def __init__(self):
     super().__init__(
-        'Can only call init, init_with_output or apply methods on an instance of the Module class, not the Module class itself'
+        'Can only call init, init_with_output or apply methods on an instance'
+        ' of the Module class, not the Module class itself'
     )
 
 
 class IncorrectPostInitOverrideError(FlaxError):
   """This error occurs when you overrode `.__post_init__()` without calling `super().__post_init__()`.
 
   For example, the error will be raised when trying to run this code::
@@ -704,15 +732,16 @@
 
     r = A(x=3)
     r.init(jax.random.PRNGKey(2), jnp.ones(3))
   """
 
   def __init__(self):
     super().__init__(
-        'Overrode `.__post_init__()` without calling `super().__post_init__()`')
+        'Overrode `.__post_init__()` without calling `super().__post_init__()`'
+    )
 
 
 class DescriptorAttributeError(FlaxError):
   """This error occurs when you are trying to access a property that is accessing a non-existent attribute.
 
   For example, the error will be raised when trying to run this code::
 
@@ -725,30 +754,32 @@
 
     foo = Foo()
     variables = foo.init(jax.random.PRNGKey(0), jnp.ones(shape=(1, 8)))
   """
 
   def __init__(self):
     super().__init__(
-        'Trying to access a property that is accessing a non-existent attribute.'
+        'Trying to access a property that is accessing a non-existent'
+        ' attribute.'
     )
 
 
 class InvalidCheckpointError(FlaxError):
   """A checkpoint cannot be stored in a directory that already has
 
   a checkpoint at the current or a later step.
 
   You can pass ``overwrite=True`` to disable this behavior and
   overwrite existing checkpoints in the target directory.
   """
 
   def __init__(self, path, step):
     super().__init__(
-        f'Trying to save an outdated checkpoint at step: "{step}" and path: "{path}".'
+        f'Trying to save an outdated checkpoint at step: "{step}" and path:'
+        f' "{path}".'
     )
 
 
 class MPACheckpointingRequiredError(FlaxError):
   """To optimally save and restore a multiprocess array (GDA or jax Array outputted from pjit), use GlobalAsyncCheckpointManager.
 
   You can create an GlobalAsyncCheckpointManager at top-level and pass it as
@@ -759,15 +790,16 @@
     save_checkpoint(..., gda_manager=gda_manager)
   """
 
   def __init__(self, path, step):
     super().__init__(
         f'Checkpoint failed at step: "{step}" and path: "{path}": Target '
         'contains a multiprocess array should be saved/restored with a '
-        'GlobalAsyncCheckpointManager.')
+        'GlobalAsyncCheckpointManager.'
+    )
 
 
 class MPARestoreTargetRequiredError(FlaxError):
   """Provide a valid target when restoring a checkpoint with a multiprocess array.
 
   Multiprocess arrays need a sharding (global meshes and partition specs) to be
   initialized. Therefore, to restore a checkpoint that contains a multiprocess
@@ -777,15 +809,16 @@
   """
 
   def __init__(self, path, step, key=None):
     error_msg = (
         f'Restore checkpoint failed at step: "{step}" and path: "{path}": '
         'Checkpoints containing a multiprocess array need to be restored with '
         'a target with pre-created arrays. If you cannot provide a full valid '
-        'target, consider ``allow_partial_mpa_restoration=True``. ')
+        'target, consider ``allow_partial_mpa_restoration=True``. '
+    )
     if key:
       error_msg += f'This error fired when trying to restore array at {key}.'
     super().__init__(error_msg)
 
 
 class MPARestoreDataCorruptedError(FlaxError):
   """A multiprocess array stored in Google Cloud Storage doesn't contain a "commit_success.txt" file, which should be written at the end of the save.
@@ -793,44 +826,30 @@
   Failure of finding it could indicate a corruption of your saved GDA data.
   """
 
   def __init__(self, step, path):
     super().__init__(
         f'Restore checkpoint failed at step: "{step}" on multiprocess array at '
         f' "{path}": No "commit_success.txt" found on this "_gda" directory. '
-        'Was its save halted before completion?')
-
-
-class MPARestoreTypeNotMatchError(FlaxError):
-  """Make sure the multiprocess array type you use matches your configuration in jax.config.jax_array.
-
-  If you turned `jax.config.jax_array` on, you should use
-  `jax.experimental.array.Array` everywhere, instead of using
-  `GlobalDeviceArray`. Otherwise, avoid using jax.experimental.array
-  to restore your checkpoint.
-  """
-
-  def __init__(self, step, gda_path):
-    super().__init__(
-        f'Restore checkpoint failed at step: "{step}" on multiprocess array at '
-        f' "{gda_path}": The array type provided by the target does not match '
-        'the JAX global configuration, namely the jax.config.jax_array.')
+        'Was its save halted before completion?'
+    )
 
 
 #################################################
 # transforms.py errors                          #
 #################################################
 
 
 class TransformedMethodReturnValueError(FlaxError):
   """Transformed Module methods cannot return other Modules or Variables."""
 
   def __init__(self, name):
     super().__init__(
-        f'Transformed module method {name} cannot return Modules or Variables.')
+        f'Transformed module method {name} cannot return Modules or Variables.'
+    )
 
 
 class TransformTargetError(FlaxError):
   """Linen transformations must be applied to Modules classes or functions taking a Module instance as the first argument.
 
   This error occurs when passing an invalid target to a linen transform
   (nn.vmap, nn.scan, etc.). This occurs for example when trying to
@@ -854,15 +873,16 @@
             True})(nn.Dense(3), x)
   """
 
   def __init__(self, target):
     super().__init__(
         'Linen transformations must be applied to Modules classes or'
         ' functions taking a Module instance as the first argument.'
-        f' The provided target is not a Module class or callable: {target}')
+        f' The provided target is not a Module class or callable: {target}'
+    )
 
 
 #################################################
 # io.py errors                                  #
 #################################################
```

### Comparing `flax-0.7.0/flax/ids.py` & `flax-0.7.1/flax/ids.py`

 * *Files 5% similar despite different names*

```diff
@@ -24,34 +24,42 @@
   to preserve and recreate sharing-by-reference relationship when lifting
   transforms and adopting outside Modules.
   - Use of id() is unacceptable because these identifiers are literally
     pointers which can be recycled, so we rely on a globally unique counter id
     instead.
   - We need to handle copy/deepcopy uniqueness via a wrapped type.
   """
+
   def __init__(self):
     self._lock = threading.Lock()
     self._id = 0
 
   def __call__(self):
     with self._lock:
       self._id += 1
       return FlaxId(self._id)
 
+
 uuid = UUIDManager()
 
 
 class FlaxId:
   """Hashable wrapper for ids that handles uniqueness of copies."""
+
   def __init__(self, rawid):
     self.id = rawid
+
   def __eq__(self, other):
     return isinstance(other, FlaxId) and other.id == self.id
+
   def __hash__(self):
     return hash(self.id)
+
   def __repr__(self):
     return f"FlaxId({self.id})"
+
   def __deepcopy__(self, memo):
     del memo
     return uuid()
+
   def __copy__(self):
     return uuid()
```

### Comparing `flax-0.7.0/flax/io.py` & `flax-0.7.1/flax/io.py`

 * *Files 6% similar despite different names*

```diff
@@ -30,32 +30,37 @@
 # Global Modes and selective import of tensorflow.io gfile.
 
 
 class BackendMode(Enum):
   DEFAULT = 0
   TF = 1
 
+
 io_mode = None
 gfile = None
 
-if importlib.util.find_spec('tensorflow'):
+if importlib.util.find_spec("tensorflow"):
   from tensorflow.io import gfile  # type: ignore
+
   io_mode = BackendMode.TF
 else:
-  logging.warning("Tensorflow library not found, tensorflow.io.gfile "
-                  "operations will use native shim calls. "
-                  "GCS paths (i.e. 'gs://...') cannot be accessed.")
+  logging.warning(
+      "Tensorflow library not found, tensorflow.io.gfile "
+      "operations will use native shim calls. "
+      "GCS paths (i.e. 'gs://...') cannot be accessed."
+  )
   io_mode = BackendMode.DEFAULT
 
 
 # Constants and Exceptions
 
 
 if io_mode == BackendMode.TF:
   from tensorflow import errors as tf_errors  # type: ignore
+
   NotFoundError = tf_errors.NotFoundError
 else:
   NotFoundError = FileNotFoundError
 
 
 # Overrides for testing.
 
@@ -87,18 +92,18 @@
 
 
 # tensorflow.io.gfile API shim functions.
 
 
 def GFile(name, mode):  # pylint: disable=invalid-name
   if io_mode == BackendMode.DEFAULT:
-    if 'b' in mode:
+    if "b" in mode:
       return open(name, mode)  # pylint: disable=unspecified-encoding
     else:
-      return open(name, mode, encoding='utf-8')
+      return open(name, mode, encoding="utf-8")
   elif io_mode == BackendMode.TF:
     return gfile.GFile(name, mode)
   else:
     raise ValueError("Unknown IO Backend Mode.")
 
 
 def listdir(path):
@@ -158,15 +163,17 @@
     return gfile.makedirs(path)
   else:
     raise ValueError("Unknown IO Backend Mode.")
 
 
 def glob(pattern):
   if io_mode == BackendMode.DEFAULT:
-    return [ path.rstrip('/') for path in glob_module.glob(pattern, recursive=False) ]
+    return [
+        path.rstrip("/") for path in glob_module.glob(pattern, recursive=False)
+    ]
   elif io_mode == BackendMode.TF:
     return gfile.glob(pattern)
   else:
     raise ValueError("Unknown IO Backend Mode.")
 
 
 def remove(path):
```

### Comparing `flax-0.7.0/flax/jax_utils.py` & `flax-0.7.1/flax/jax_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,16 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Utilities we could consider upstreaming to Jax.
-"""
+"""Utilities we could consider upstreaming to Jax."""
 
 import collections
 from collections.abc import Iterable  # pylint: disable=g-importing-member
 import itertools
 import warnings
 
 import jax
@@ -49,16 +48,15 @@
 
 def unreplicate(tree):
   """Returns a single instance of a replicated array."""
   return jax.tree_util.tree_map(lambda x: x[0], tree)
 
 
 def pmean(xs, axis_name):
-  warnings.warn('use jax.lax.pmean instead',
-                DeprecationWarning)
+  warnings.warn("use jax.lax.pmean instead", DeprecationWarning)
   return lax.pmean(xs, axis_name)
 
 
 def partial_eval_by_shape(fn, input_spec, *args, **kwargs):
   """Lazily evaluate a function by using the shapes of the inputs.
 
   This function is similar to `jax.eval_shape` with the key difference that
@@ -79,19 +77,23 @@
   # output cannot be returned in lazy_create because jax.eval_shape will only
   # return the shape and dtype.
   # TODO(mattjj,jheek): use a public JAX API
   f = lambda *inputs: fn(*inputs, *args, **kwargs)
   input_structs = [_parse_spec(spec) for spec in input_spec]
   inputs_flat, in_tree = jax.tree_util.tree_flatten(input_structs)
   f_flat, out_tree = jax.api_util.flatten_fun_nokwargs(lu.wrap_init(f), in_tree)
-  in_pvals = [pe.PartialVal.unknown(core.ShapedArray(x.shape, x.dtype))
-              for x in inputs_flat]
+  in_pvals = [
+      pe.PartialVal.unknown(core.ShapedArray(x.shape, x.dtype))
+      for x in inputs_flat
+  ]
   _, out_pvals, _ = pe.trace_to_jaxpr_nounits(f_flat, in_pvals)
-  out_flat = [const if pv is None else jax.ShapeDtypeStruct(pv.shape, pv.dtype)
-              for pv, const in out_pvals]
+  out_flat = [
+      const if pv is None else jax.ShapeDtypeStruct(pv.shape, pv.dtype)
+      for pv, const in out_pvals
+  ]
   return jax.tree_util.tree_unflatten(out_tree(), out_flat)
 
 
 def _parse_spec(spec):
   """Parse an input spec of the form (shape, dtype) or shape into a jax.ShapeDtypeStruct."""
   spec = tuple(spec)
   if len(spec) == 2 and isinstance(spec[0], Iterable):
@@ -156,16 +158,18 @@
     n: number of dimensions to scan over (default: 1)
   Returns:
     A tuple of the final carry and the values returned by the body.
   """
   if n == 1:
     return lax.scan(body_fn, init, xs, unroll=unroll[0])
   else:
+
     def scan_body(c, x):
-      return _scan_nd(body_fn, c, x, n=n-1, unroll=unroll[1:])
+      return _scan_nd(body_fn, c, x, n=n - 1, unroll=unroll[1:])
+
     return lax.scan(scan_body, init, xs, unroll=unroll[0])
 
 
 def _invert_perm(perm):
   perm_inv = [0] * len(perm)
   for i, j in enumerate(perm):
     perm_inv[j] = i
@@ -203,37 +207,41 @@
   # Pad unroll with ones so we start unrolling from the innermost loop
   len_diff = len(axis) - len(unroll)
   unroll = (1,) * len_diff + unroll
 
   def transpose_in(x):
     perm = axis + tuple(np.delete(np.arange(x.ndim), axis))
     return x.transpose(perm)
+
   def transpose_out(x):
     perm = axis + tuple(np.delete(np.arange(x.ndim), axis))
     return x.transpose(_invert_perm(perm))
 
   def body_wrapper(c, xs):
     if keepdims:
-      xs = jax.tree_util.tree_map(lambda x: x.reshape((1,) * len(axis) + x.shape), xs)
+      xs = jax.tree_util.tree_map(
+          lambda x: x.reshape((1,) * len(axis) + x.shape), xs
+      )
       xs = jax.tree_util.tree_map(transpose_out, xs)
     c, ys = body_fn(c, xs)
     if keepdims:
       ys = jax.tree_util.tree_map(transpose_in, ys)
-      ys = jax.tree_util.tree_map(lambda x: x.reshape(x.shape[len(axis):]), ys)
+      ys = jax.tree_util.tree_map(lambda x: x.reshape(x.shape[len(axis) :]), ys)
     return c, ys
 
   xs = jax.tree_util.tree_map(transpose_in, xs)
   c, ys = _scan_nd(body_wrapper, init, xs, n=len(axis), unroll=unroll)
   ys = jax.tree_util.tree_map(transpose_out, ys)
   return c, ys
 
 
 # Copied from https://github.com/google-research/big_vision
-def pad_shard_unpad(wrapped, static_argnums=(0,), static_argnames=(),
-                    static_return=False):
+def pad_shard_unpad(
+    wrapped, static_argnums=(0,), static_argnames=(), static_return=False
+):
   """Wraps a function with code that pads, shards, then un-shards, un-pads.
 
   Args:
     wrapped: the function to be wrapped. Signature is `params, *args, *kwargs`.
     static_argnums: indices of arguments to `wrapped` that should _not_ be
       padded and sharded, but instead be forwarded as-is. The default is (0,)
       because by far the most common use-case is to pass `params` first.
@@ -280,25 +288,28 @@
       _, *shape = x.shape
       db, rest = divmod(b, d)
       if rest:
         x = np.concatenate([x, np.zeros((d - rest, *shape), x.dtype)], axis=0)
         db += 1
       if min_device_batch and db < min_device_batch:
         x = np.concatenate(
-            [x, np.zeros((d * (min_device_batch - db), *shape), x.dtype)])
+            [x, np.zeros((d * (min_device_batch - db), *shape), x.dtype)]
+        )
         db = min_device_batch
       return x.reshape(d, db, *shape)
 
     def maybe_pad(tree, actually_pad=True):
-      if not actually_pad: return tree  # For call-site convenience below.
+      if not actually_pad:
+        return tree  # For call-site convenience below.
       return jax.tree_util.tree_map(pad, tree)
 
     args = [maybe_pad(a, i not in static_argnums) for i, a in enumerate(args)]
     kw = {k: maybe_pad(v, k not in static_argnames) for k, v in kw.items()}
     out = wrapped(*args, **kw)
 
     def unpad(x):
       # Transfer back before cutting, to reduce on-device shape diversity.
       return jax.device_get(x).reshape([np.prod(x.shape[:2]), *x.shape[2:]])[:b]
+
     return out if static_return else jax.tree_util.tree_map(unpad, out)
 
   return pad_shard_unpad_wrapper
```

### Comparing `flax-0.7.0/flax/linen/README.md` & `flax-0.7.1/flax/linen/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/flax/linen/__init__.py` & `flax-0.7.1/flax/linen/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -13,137 +13,133 @@
 # limitations under the License.
 
 """The Flax Module system."""
 
 
 # pylint: disable=g-multiple-import,useless-import-alias
 # re-export commonly used modules and functions
-from .activation import (
-  PReLU as PReLU,
-  celu as celu,
-  elu as elu,
-  gelu as gelu,
-  glu as glu,
-  hard_sigmoid as hard_sigmoid,
-  hard_silu as hard_silu,
-  hard_swish as hard_swish,
-  hard_tanh as hard_tanh,
-  leaky_relu as leaky_relu,
-  log_sigmoid as log_sigmoid,
-  log_softmax as log_softmax,
-  logsumexp as logsumexp,
-  normalize as normalize,
-  one_hot as one_hot,
-  relu as relu,
-  relu6 as relu6,
-  selu as selu,
-  sigmoid as sigmoid,
-  silu as silu,
-  soft_sign as soft_sign,
-  softmax as softmax,
-  softplus as softplus,
-  standardize as standardize,
-  swish as swish,
-  tanh as tanh
-)
-from .attention import (
-  MultiHeadDotProductAttention as MultiHeadDotProductAttention,
-  SelfAttention as SelfAttention,
-  combine_masks as combine_masks,
-  dot_product_attention as dot_product_attention,
-  dot_product_attention_weights as dot_product_attention_weights,
-  make_attention_mask as make_attention_mask,
-  make_causal_mask as make_causal_mask
-)
-from .combinators import Sequential as Sequential
 from ..core import (
-  DenyList as DenyList,
-  FrozenDict as FrozenDict,
-  broadcast as broadcast,
-  meta as meta,
+    DenyList as DenyList,
+    FrozenDict as FrozenDict,
+    broadcast as broadcast,
+    meta as meta,
 )
 from ..core.meta import (
+    PARTITION_NAME as PARTITION_NAME,
     Partitioned as Partitioned,
-    with_partitioning as with_partitioning,
     get_partition_spec as get_partition_spec,
     get_sharding as get_sharding,
     unbox as unbox,
-    PARTITION_NAME as PARTITION_NAME,
+    with_partitioning as with_partitioning,
 )
-from .spmd import (
-    logical_axis_rules as logical_axis_rules,
-    set_logical_axis_rules as set_logical_axis_rules,
-    get_logical_axis_rules as get_logical_axis_rules,
-    logical_to_mesh_axes,
-    logical_to_mesh,
-    logical_to_mesh_sharding,
-    with_logical_constraint,
-    LogicallyPartitioned as LogicallyPartitioned,
-    with_logical_partitioning as with_logical_partitioning,
+from .activation import (
+    PReLU as PReLU,
+    celu as celu,
+    elu as elu,
+    gelu as gelu,
+    glu as glu,
+    hard_sigmoid as hard_sigmoid,
+    hard_silu as hard_silu,
+    hard_swish as hard_swish,
+    hard_tanh as hard_tanh,
+    leaky_relu as leaky_relu,
+    log_sigmoid as log_sigmoid,
+    log_softmax as log_softmax,
+    logsumexp as logsumexp,
+    normalize as normalize,
+    one_hot as one_hot,
+    relu6 as relu6,
+    relu as relu,
+    selu as selu,
+    sigmoid as sigmoid,
+    silu as silu,
+    soft_sign as soft_sign,
+    softmax as softmax,
+    softplus as softplus,
+    standardize as standardize,
+    swish as swish,
+    tanh as tanh,
 )
+from .attention import (
+    MultiHeadDotProductAttention as MultiHeadDotProductAttention,
+    SelfAttention as SelfAttention,
+    combine_masks as combine_masks,
+    dot_product_attention_weights as dot_product_attention_weights,
+    dot_product_attention as dot_product_attention,
+    make_attention_mask as make_attention_mask,
+    make_causal_mask as make_causal_mask,
+)
+from .combinators import Sequential as Sequential
 from .initializers import (
-  ones as ones,
-  ones_init as ones_init,
-  zeros as zeros,
-  zeros_init as zeros_init
+    ones_init as ones_init,
+    ones as ones,
+    zeros_init as zeros_init,
+    zeros as zeros,
 )
 from .linear import (
-  Conv as Conv,
-  ConvLocal as ConvLocal,
-  ConvTranspose as ConvTranspose,
-  Dense as Dense,
-  DenseGeneral as DenseGeneral,
-  Embed as Embed
+    ConvLocal as ConvLocal,
+    ConvTranspose as ConvTranspose,
+    Conv as Conv,
+    DenseGeneral as DenseGeneral,
+    Dense as Dense,
+    Embed as Embed,
 )
 from .module import (
-  Module as Module,
-  Variable as Variable,
-  apply as apply,
-  compact as compact,
-  disable_named_call as disable_named_call,
-  enable_named_call as enable_named_call,
-  init as init,
-  init_with_output as init_with_output,
-  merge_param as merge_param,
-  nowrap as nowrap,
-  override_named_call as override_named_call
+    Module as Module,
+    Variable as Variable,
+    apply as apply,
+    compact as compact,
+    disable_named_call as disable_named_call,
+    enable_named_call as enable_named_call,
+    init_with_output as init_with_output,
+    init as init,
+    merge_param as merge_param,
+    nowrap as nowrap,
+    override_named_call as override_named_call,
 )
 from .normalization import (
-  BatchNorm as BatchNorm,
-  GroupNorm as GroupNorm,
-  LayerNorm as LayerNorm,
-  RMSNorm as RMSNorm
-)
-from .pooling import (
-  avg_pool as avg_pool,
-  max_pool as max_pool,
-  pool as pool
+    BatchNorm as BatchNorm,
+    GroupNorm as GroupNorm,
+    LayerNorm as LayerNorm,
+    RMSNorm as RMSNorm,
 )
+from .pooling import (avg_pool as avg_pool, max_pool as max_pool, pool as pool)
 from .recurrent import (
-  ConvLSTMCell as ConvLSTMCell,
-  GRUCell as GRUCell,
-  LSTMCell as LSTMCell,
-  OptimizedLSTMCell as OptimizedLSTMCell,
-  RNN as RNN,
-  RNNCellBase as RNNCellBase,
-  Bidirectional as Bidirectional,
+    Bidirectional as Bidirectional,
+    ConvLSTMCell as ConvLSTMCell,
+    GRUCell as GRUCell,
+    LSTMCell as LSTMCell,
+    OptimizedLSTMCell as OptimizedLSTMCell,
+    RNNCellBase as RNNCellBase,
+    RNN as RNN,
+)
+from .spmd import (
+    LogicallyPartitioned as LogicallyPartitioned,
+    get_logical_axis_rules as get_logical_axis_rules,
+    logical_axis_rules as logical_axis_rules,
+    logical_to_mesh,
+    logical_to_mesh_axes,
+    logical_to_mesh_sharding,
+    set_logical_axis_rules as set_logical_axis_rules,
+    with_logical_constraint,
+    with_logical_partitioning as with_logical_partitioning,
 )
 from .stochastic import Dropout as Dropout
+from .summary import tabulate
 from .transforms import (
-  checkpoint as checkpoint,
-  custom_vjp as custom_vjp,
-  jit as jit,
-  jvp as jvp,
-  map_variables as map_variables,
-  named_call as named_call,
-  remat as remat,
-  remat_scan as remat_scan,
-  scan as scan,
-  vjp as vjp,
-  vmap as vmap,
-  while_loop as while_loop,
-  cond as cond,
-  switch as switch,
-  add_metadata_axis,
+    add_metadata_axis,
+    checkpoint as checkpoint,
+    cond as cond,
+    custom_vjp as custom_vjp,
+    jit as jit,
+    jvp as jvp,
+    map_variables as map_variables,
+    named_call as named_call,
+    remat_scan as remat_scan,
+    remat as remat,
+    scan as scan,
+    switch as switch,
+    vjp as vjp,
+    vmap as vmap,
+    while_loop as while_loop,
 )
-from .summary import tabulate
 # pylint: enable=g-multiple-import
```

### Comparing `flax-0.7.0/flax/linen/activation.py` & `flax-0.7.1/flax/linen/activation.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,16 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Activation functions.
-"""
+"""Activation functions."""
 
 # pylint: disable=unused-import
 # re-export activation functions from jax.nn
 from typing import Any, Optional
 
 from flax.linen.module import compact
 from flax.linen.module import Module
@@ -66,14 +65,15 @@
     x = nn.PReLU()(x)
 
   Attributes:
     param_dtype: the dtype passed to parameter initializers (default: float32).
     negative_slope_init: the value to initialize the negative slope
       (default 0.01).
   """
+
   param_dtype: Dtype = jnp.float32
   negative_slope_init: float = 0.01
 
   @compact
   def __call__(self, inputs: Array) -> Array:
     """Applies an activation to the inputs.
 
@@ -81,10 +81,12 @@
       inputs: the nd-array to apply the activation function to.
 
     Returns:
       The transformed input.
     """
     negative_slope = self.param(
         'negative_slope',
-        lambda k: jnp.asarray(self.negative_slope_init, self.param_dtype))
-    return jnp.where(inputs >= 0, inputs,
-                     jnp.asarray(negative_slope, inputs.dtype) * inputs)
+        lambda k: jnp.asarray(self.negative_slope_init, self.param_dtype),
+    )
+    return jnp.where(
+        inputs >= 0, inputs, jnp.asarray(negative_slope, inputs.dtype) * inputs
+    )
```

### Comparing `flax-0.7.0/flax/linen/attention.py` & `flax-0.7.1/flax/linen/attention.py`

 * *Files 3% similar despite different names*

```diff
@@ -34,24 +34,26 @@
 
 PRNGKey = Any
 Shape = Tuple[int, ...]
 Dtype = Any
 Array = Any
 
 
-def dot_product_attention_weights(query: Array,
-                                  key: Array,
-                                  bias: Optional[Array] = None,
-                                  mask: Optional[Array] = None,
-                                  broadcast_dropout: bool = True,
-                                  dropout_rng: Optional[PRNGKey] = None,
-                                  dropout_rate: float = 0.,
-                                  deterministic: bool = False,
-                                  dtype: Optional[Dtype] = None,
-                                  precision: PrecisionLike = None):
+def dot_product_attention_weights(
+    query: Array,
+    key: Array,
+    bias: Optional[Array] = None,
+    mask: Optional[Array] = None,
+    broadcast_dropout: bool = True,
+    dropout_rng: Optional[PRNGKey] = None,
+    dropout_rate: float = 0.0,
+    deterministic: bool = False,
+    dtype: Optional[Dtype] = None,
+    precision: PrecisionLike = None,
+):
   """Computes dot-product attention weights given query and key.
 
   Used by :func:`dot_product_attention`, which is what you'll most likely use.
   But if you want access to the attention weights for introspection, then
   you can directly call this function and call einsum yourself.
 
   Args:
@@ -79,65 +81,65 @@
   Returns:
     Output of shape `[batch..., num_heads, q_length, kv_length]`.
   """
   query, key = promote_dtype(query, key, dtype=dtype)
   dtype = query.dtype
 
   assert query.ndim == key.ndim, 'q, k must have same rank.'
-  assert query.shape[:-3] == key.shape[:-3], (
-      'q, k batch dims must match.')
-  assert query.shape[-2] == key.shape[-2], (
-      'q, k num_heads must match.')
+  assert query.shape[:-3] == key.shape[:-3], 'q, k batch dims must match.'
+  assert query.shape[-2] == key.shape[-2], 'q, k num_heads must match.'
   assert query.shape[-1] == key.shape[-1], 'q, k depths must match.'
 
   # calculate attention matrix
   depth = query.shape[-1]
   query = query / jnp.sqrt(depth).astype(dtype)
   # attn weight shape is (batch..., num_heads, q_length, kv_length)
-  attn_weights = jnp.einsum('...qhd,...khd->...hqk', query, key,
-                            precision=precision)
+  attn_weights = jnp.einsum(
+      '...qhd,...khd->...hqk', query, key, precision=precision
+  )
 
   # apply attention bias: masking, dropout, proximity bias, etc.
   if bias is not None:
     attn_weights = attn_weights + bias
   # apply attention mask
   if mask is not None:
     big_neg = jnp.finfo(dtype).min
     attn_weights = jnp.where(mask, attn_weights, big_neg)
 
   # normalize the attention weights
   attn_weights = jax.nn.softmax(attn_weights).astype(dtype)
 
   # apply attention dropout
-  if not deterministic and dropout_rate > 0.:
+  if not deterministic and dropout_rate > 0.0:
     keep_prob = 1.0 - dropout_rate
     if broadcast_dropout:
       # dropout is broadcast across the batch + head dimensions
       dropout_shape = tuple([1] * (key.ndim - 2)) + attn_weights.shape[-2:]
-      keep = random.bernoulli(dropout_rng, keep_prob, dropout_shape) # type: ignore
+      keep = random.bernoulli(dropout_rng, keep_prob, dropout_shape)  # type: ignore
     else:
-      keep = random.bernoulli(dropout_rng, keep_prob, attn_weights.shape) # type: ignore
-    multiplier = (keep.astype(dtype) /
-                  jnp.asarray(keep_prob, dtype=dtype))
+      keep = random.bernoulli(dropout_rng, keep_prob, attn_weights.shape)  # type: ignore
+    multiplier = keep.astype(dtype) / jnp.asarray(keep_prob, dtype=dtype)
     attn_weights = attn_weights * multiplier
 
   return attn_weights
 
 
-def dot_product_attention(query: Array,
-                          key: Array,
-                          value: Array,
-                          bias: Optional[Array] = None,
-                          mask: Optional[Array] = None,
-                          broadcast_dropout: bool = True,
-                          dropout_rng: Optional[PRNGKey] = None,
-                          dropout_rate: float = 0.,
-                          deterministic: bool = False,
-                          dtype: Optional[Dtype] = None,
-                          precision: PrecisionLike = None):
+def dot_product_attention(
+    query: Array,
+    key: Array,
+    value: Array,
+    bias: Optional[Array] = None,
+    mask: Optional[Array] = None,
+    broadcast_dropout: bool = True,
+    dropout_rng: Optional[PRNGKey] = None,
+    dropout_rate: float = 0.0,
+    deterministic: bool = False,
+    dtype: Optional[Dtype] = None,
+    precision: PrecisionLike = None,
+):
   """Computes dot-product attention given query, key, and value.
 
   This is the core function for applying attention based on
   https://arxiv.org/abs/1706.03762. It calculates the attention weights given
   query and key and combines the values using the attention weights.
 
   Note: query, key, value needn't have any batch dimensions.
@@ -168,79 +170,94 @@
 
   Returns:
     Output of shape `[batch..., q_length, num_heads, v_depth_per_head]`.
   """
   query, key, value = promote_dtype(query, key, value, dtype=dtype)
   dtype = query.dtype
   assert key.ndim == query.ndim == value.ndim, 'q, k, v must have same rank.'
-  assert query.shape[:-3] == key.shape[:-3] == value.shape[:-3], (
-      'q, k, v batch dims must match.')
-  assert query.shape[-2] == key.shape[-2] == value.shape[-2], (
-      'q, k, v num_heads must match.')
+  assert (
+      query.shape[:-3] == key.shape[:-3] == value.shape[:-3]
+  ), 'q, k, v batch dims must match.'
+  assert (
+      query.shape[-2] == key.shape[-2] == value.shape[-2]
+  ), 'q, k, v num_heads must match.'
   assert key.shape[-3] == value.shape[-3], 'k, v lengths must match.'
 
   # compute attention weights
   attn_weights = dot_product_attention_weights(
-      query, key, bias, mask, broadcast_dropout, dropout_rng, dropout_rate,
-      deterministic, dtype, precision)
+      query,
+      key,
+      bias,
+      mask,
+      broadcast_dropout,
+      dropout_rng,
+      dropout_rate,
+      deterministic,
+      dtype,
+      precision,
+  )
 
   # return weighted sum over values for each query position
-  return jnp.einsum('...hqk,...khd->...qhd', attn_weights, value,
-                    precision=precision)
+  return jnp.einsum(
+      '...hqk,...khd->...qhd', attn_weights, value, precision=precision
+  )
 
 
 class MultiHeadDotProductAttention(Module):
   """Multi-head dot-product attention.
 
-    Attributes:
-      num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])
-        should be divisible by the number of heads.
-      dtype: the dtype of the computation
-        (default: infer from inputs and params)
-      param_dtype: the dtype passed to parameter initializers (default: float32)
-      qkv_features: dimension of the key, query, and value.
-      out_features: dimension of the last projection
-      broadcast_dropout: bool: use a broadcasted dropout along batch dims.
-      dropout_rate: dropout rate
-      deterministic: if false, the attention weight is masked randomly
-        using dropout, whereas if true, the attention weights
-        are deterministic.
-      precision: numerical precision of the computation see `jax.lax.Precision`
-        for details.
-      kernel_init: initializer for the kernel of the Dense layers.
-      bias_init: initializer for the bias of the Dense layers.
-      use_bias: bool: whether pointwise QKVO dense transforms use bias.
-      attention_fn: dot_product_attention or compatible function. Accepts
-        query, key, value, and returns output of shape
-        `[bs, dim1, dim2, ..., dimN,, num_heads, value_channels]``
-      decode: whether to prepare and use an autoregressive cache.
+  Attributes:
+    num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])
+      should be divisible by the number of heads.
+    dtype: the dtype of the computation (default: infer from inputs and params)
+    param_dtype: the dtype passed to parameter initializers (default: float32)
+    qkv_features: dimension of the key, query, and value.
+    out_features: dimension of the last projection
+    broadcast_dropout: bool: use a broadcasted dropout along batch dims.
+    dropout_rate: dropout rate
+    deterministic: if false, the attention weight is masked randomly using
+      dropout, whereas if true, the attention weights are deterministic.
+    precision: numerical precision of the computation see `jax.lax.Precision`
+      for details.
+    kernel_init: initializer for the kernel of the Dense layers.
+    bias_init: initializer for the bias of the Dense layers.
+    use_bias: bool: whether pointwise QKVO dense transforms use bias.
+    attention_fn: dot_product_attention or compatible function. Accepts query,
+      key, value, and returns output of shape `[bs, dim1, dim2, ..., dimN,,
+      num_heads, value_channels]``
+    decode: whether to prepare and use an autoregressive cache.
   """
+
   num_heads: int
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   qkv_features: Optional[int] = None
   out_features: Optional[int] = None
   broadcast_dropout: bool = True
-  dropout_rate: float = 0.
+  dropout_rate: float = 0.0
   deterministic: Optional[bool] = None
   precision: PrecisionLike = None
   kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
+      initializers.zeros_init()
+  )
   use_bias: bool = True
   attention_fn: Callable[..., Array] = dot_product_attention
   decode: bool = False
   qkv_dot_general: DotGeneralT = lax.dot_general
   out_dot_general: DotGeneralT = lax.dot_general
 
   @compact
-  def __call__(self,
-               inputs_q: Array,
-               inputs_kv: Array,
-               mask: Optional[Array] = None,
-               deterministic: Optional[bool] = None):
+  def __call__(
+      self,
+      inputs_q: Array,
+      inputs_kv: Array,
+      mask: Optional[Array] = None,
+      deterministic: Optional[bool] = None,
+  ):
     """Applies multi-head dot product attention on the input data.
 
     Projects the inputs into multi-headed query, key, and value vectors,
     applies dot-product attention and project the results to an output vector.
 
     Args:
       inputs_q: input queries of shape
@@ -276,59 +293,76 @@
         bias_init=self.bias_init,
         use_bias=self.use_bias,
         precision=self.precision,
         dot_general=self.qkv_dot_general,
     )
     # project inputs_q to multi-headed q/k/v
     # dimensions are then [batch..., length, n_heads, n_features_per_head]
-    query, key, value = (dense(name='query')(inputs_q),
-                         dense(name='key')(inputs_kv),
-                         dense(name='value')(inputs_kv))
+    query, key, value = (
+        dense(name='query')(inputs_q),
+        dense(name='key')(inputs_kv),
+        dense(name='value')(inputs_kv),
+    )
 
     # During fast autoregressive decoding, we feed one position at a time,
     # and cache the keys and values step by step.
     if self.decode:
       # detect if we're initializing by absence of existing cache data.
       is_initialized = self.has_variable('cache', 'cached_key')
-      cached_key = self.variable('cache', 'cached_key',
-                                 jnp.zeros, key.shape, key.dtype)
-      cached_value = self.variable('cache', 'cached_value',
-                                   jnp.zeros, value.shape, value.dtype)
-      cache_index = self.variable('cache', 'cache_index',
-                                  lambda: jnp.array(0, dtype=jnp.int32))
+      cached_key = self.variable(
+          'cache', 'cached_key', jnp.zeros, key.shape, key.dtype
+      )
+      cached_value = self.variable(
+          'cache', 'cached_value', jnp.zeros, value.shape, value.dtype
+      )
+      cache_index = self.variable(
+          'cache', 'cache_index', lambda: jnp.array(0, dtype=jnp.int32)
+      )
       if is_initialized:
-        *batch_dims, max_length, num_heads, depth_per_head = (
-            cached_key.value.shape)
+        (
+            *batch_dims,
+            max_length,
+            num_heads,
+            depth_per_head,
+        ) = cached_key.value.shape
         # shape check of cached keys against query input
         expected_shape = tuple(batch_dims) + (1, num_heads, depth_per_head)
         if expected_shape != query.shape:
-          raise ValueError('Autoregressive cache shape error, '
-                           'expected query shape %s instead got %s.' %
-                           (expected_shape, query.shape))
+          raise ValueError(
+              'Autoregressive cache shape error, '
+              'expected query shape %s instead got %s.'
+              % (expected_shape, query.shape)
+          )
         # update key, value caches with our new 1d spatial slices
         cur_index = cache_index.value
         indices = (0,) * len(batch_dims) + (cur_index, 0, 0)
         key = lax.dynamic_update_slice(cached_key.value, key, indices)
         value = lax.dynamic_update_slice(cached_value.value, value, indices)
         cached_key.value = key
         cached_value.value = value
         cache_index.value = cache_index.value + 1
         # causal mask for cached decoder self-attention:
         # our single query position should only attend to those key
         # positions that have already been generated and cached,
         # not the remaining zero elements.
         mask = combine_masks(
             mask,
-            jnp.broadcast_to(jnp.arange(max_length) <= cur_index,
-                             tuple(batch_dims) + (1, 1, max_length)))
+            jnp.broadcast_to(
+                jnp.arange(max_length) <= cur_index,
+                tuple(batch_dims) + (1, 1, max_length),
+            ),
+        )
 
     dropout_rng = None
-    if self.dropout_rate > 0.:  # Require `deterministic` only if using dropout.
-      m_deterministic = merge_param('deterministic', self.deterministic,
-                                    deterministic)
+    if (
+        self.dropout_rate > 0.0
+    ):  # Require `deterministic` only if using dropout.
+      m_deterministic = merge_param(
+          'deterministic', self.deterministic, deterministic
+      )
       if not m_deterministic:
         dropout_rng = self.make_rng('dropout')
     else:
       m_deterministic = True
 
     # apply attention
     x = self.attention_fn(
@@ -337,37 +371,42 @@
         value,
         mask=mask,
         dropout_rng=dropout_rng,
         dropout_rate=self.dropout_rate,
         broadcast_dropout=self.broadcast_dropout,
         deterministic=m_deterministic,
         dtype=self.dtype,
-        precision=self.precision)  # pytype: disable=wrong-keyword-args
+        precision=self.precision,
+    )  # pytype: disable=wrong-keyword-args
     # back to the original inputs dimensions
     out = DenseGeneral(
         features=features,
         axis=(-2, -1),
         kernel_init=self.kernel_init,
         bias_init=self.bias_init,
         use_bias=self.use_bias,
         dtype=self.dtype,
         param_dtype=self.param_dtype,
         precision=self.precision,
         dot_general=self.out_dot_general,
-        name='out', # type: ignore[call-arg]
+        name='out',  # type: ignore[call-arg]
     )(x)
     return out
 
 
 class SelfAttention(MultiHeadDotProductAttention):
   """Self-attention special case of multi-head dot-product attention."""
 
   @compact
-  def __call__(self, inputs_q: Array, mask: Optional[Array] = None, # type: ignore
-               deterministic: Optional[bool] = None):
+  def __call__(  # type: ignore
+      self,
+      inputs_q: Array,
+      mask: Optional[Array] = None,
+      deterministic: Optional[bool] = None,
+  ):
     """Applies multi-head dot product self-attention on the input data.
 
     Projects the inputs into multi-headed query, key, and value vectors,
     applies dot-product attention and project the results to an output vector.
 
     Args:
       inputs_q: input queries of shape
@@ -379,26 +418,29 @@
       deterministic: if false, the attention weight is masked randomly
         using dropout, whereas if true, the attention weights
         are deterministic.
 
     Returns:
       output of shape `[batch_sizes..., length, features]`.
     """
-    return super().__call__(inputs_q, inputs_q, mask,
-                            deterministic=deterministic)
+    return super().__call__(
+        inputs_q, inputs_q, mask, deterministic=deterministic
+    )
 
 
 # mask-making utility functions
 
 
-def make_attention_mask(query_input: Array,
-                        key_input: Array,
-                        pairwise_fn: Callable[..., Any] = jnp.multiply,
-                        extra_batch_dims: int = 0,
-                        dtype: Dtype = jnp.float32):
+def make_attention_mask(
+    query_input: Array,
+    key_input: Array,
+    pairwise_fn: Callable[..., Any] = jnp.multiply,
+    extra_batch_dims: int = 0,
+    dtype: Dtype = jnp.float32,
+):
   """Mask-making helper for attention weights.
 
   In case of 1d inputs (i.e., `[batch..., len_q]`, `[batch..., len_kv]`, the
   attention weights will be `[batch..., heads, len_q, len_kv]` and this
   function will produce `[batch..., 1, len_q, len_kv]`.
 
   Args:
@@ -408,24 +450,25 @@
     extra_batch_dims: number of extra batch dims to add singleton
       axes for, none by default
     dtype: mask return dtype
 
   Returns:
     A `[batch..., 1, len_q, len_kv]` shaped mask for 1d attention.
   """
-  mask = pairwise_fn(jnp.expand_dims(query_input, axis=-1),
-                     jnp.expand_dims(key_input, axis=-2))
+  mask = pairwise_fn(
+      jnp.expand_dims(query_input, axis=-1), jnp.expand_dims(key_input, axis=-2)
+  )
   mask = jnp.expand_dims(mask, axis=-3)
   mask = jnp.expand_dims(mask, axis=tuple(range(extra_batch_dims)))
   return mask.astype(dtype)
 
 
-def make_causal_mask(x: Array,
-                     extra_batch_dims: int = 0,
-                     dtype: Dtype = jnp.float32) -> Array:
+def make_causal_mask(
+    x: Array, extra_batch_dims: int = 0, dtype: Dtype = jnp.float32
+) -> Array:
   """Make a causal mask for self-attention.
 
   In case of 1d inputs (i.e., `[batch..., len]`, the self-attention weights
   will be `[batch..., heads, len, len]` and this function will produce a
   causal mask of shape `[batch..., 1, len, len]`.
 
   Args:
@@ -434,31 +477,36 @@
       none by default
     dtype: mask return dtype
 
   Returns:
     A `[batch..., 1, len, len]` shaped causal mask for 1d attention.
   """
   idxs = jnp.broadcast_to(jnp.arange(x.shape[-1], dtype=jnp.int32), x.shape)
-  return make_attention_mask(idxs, idxs, jnp.greater_equal,
-                             extra_batch_dims=extra_batch_dims, dtype=dtype)
+  return make_attention_mask(
+      idxs,
+      idxs,
+      jnp.greater_equal,
+      extra_batch_dims=extra_batch_dims,
+      dtype=dtype,
+  )
 
 
-def combine_masks(*masks: Optional[Array],
-                  dtype: Dtype = jnp.float32) -> Array:
+def combine_masks(*masks: Optional[Array], dtype: Dtype = jnp.float32) -> Array:
   """Combine attention masks.
 
   Args:
     *masks: set of attention mask arguments to combine, some can be None.
     dtype: dtype for the returned mask.
 
   Returns:
     Combined mask, reduced by logical and, returns None if no masks given.
   """
   masks_list = [m for m in masks if m is not None]
   if not masks_list:
     return None
-  assert all(map(lambda x: x.ndim == masks_list[0].ndim, masks_list)), (
-      f'masks must have same rank: {tuple(map(lambda x: x.ndim, masks_list))}')
+  assert all(
+      map(lambda x: x.ndim == masks_list[0].ndim, masks_list)
+  ), f'masks must have same rank: {tuple(map(lambda x: x.ndim, masks_list))}'
   mask, *other_masks = masks_list
   for other_mask in other_masks:
     mask = jnp.logical_and(mask, other_mask)
   return mask.astype(dtype)
```

### Comparing `flax-0.7.0/flax/linen/combinators.py` & `flax-0.7.1/flax/linen/combinators.py`

 * *Files 3% similar despite different names*

```diff
@@ -65,25 +65,27 @@
       num_layers: Sequence[int]
 
       @nn.compact
       def __call__(self, x):
         return nn.Sequential([CrossAttentionBlock() for _ in
                               range(self.num_layers)])(query, key_value)
   """
+
   layers: Sequence[Callable[..., Any]]
 
   def __post_init__(self):
     if not isinstance(self.layers, Sequence):
-      raise ValueError('\'layers\' must be a sequence, '
-                       f'got \'{type(self.layers).__name__}\'.')
+      raise ValueError(
+          f"'layers' must be a sequence, got '{type(self.layers).__name__}'."
+      )
     super().__post_init__()
 
   def __call__(self, *args, **kwargs):
     if not self.layers:
-      raise ValueError(f'Empty Sequential module {self.name}.')
+      raise ValueError(f"Empty Sequential module {self.name}.")
 
     outputs = self.layers[0](*args, **kwargs)
     for layer in self.layers[1:]:
       if isinstance(outputs, tuple):
         outputs = layer(*outputs)
       elif isinstance(outputs, Dict):
         outputs = layer(**outputs)
```

### Comparing `flax-0.7.0/flax/linen/dtypes.py` & `flax-0.7.1/flax/linen/dtypes.py`

 * *Files 4% similar despite different names*

```diff
@@ -33,17 +33,17 @@
 import jax
 
 
 Dtype = Any
 Array = Any
 
 
-def canonicalize_dtype(*args,
-                       dtype: Optional[Dtype] = None,
-                       inexact: bool = True) -> Dtype:
+def canonicalize_dtype(
+    *args, dtype: Optional[Dtype] = None, inexact: bool = True
+) -> Dtype:
   """Canonicalize an optional dtype to the definitive dtype.
 
   If the ``dtype`` is None this function will infer the dtype. If it is not
   None it will be returned unmodified or an exceptions is raised if the dtype
   is invalid.
   from the input arguments using ``jnp.result_type``.
 
@@ -66,33 +66,32 @@
       dtype = jnp.promote_types(jnp.float32, dtype)
   if inexact and not jnp.issubdtype(dtype, jnp.inexact):
     raise ValueError(f'Dtype must be inexact: {dtype}')
   return dtype
 
 
 def promote_dtype(*args, dtype=None, inexact=True) -> List[Array]:
-  """"Promotes input arguments to a specified or inferred dtype.
+  """ "Promotes input arguments to a specified or inferred dtype.
 
   All args are cast to the same dtype. See ``canonicalize_dtype`` for how
   this dtype is determined.
 
   The behavior of promote_dtype is mostly a convinience wrapper around
   ``jax.numpy.promote_types``. The differences being that it automatically casts
   all input to the inferred dtypes, allows inference to be overridden by a
   forced dtype, and has an optional check to garantuee the resulting dtype is
   inexact.
 
   Args:
-    *args: JAX array compatible values. None values
-      are returned as is.
-    dtype: Optional dtype override. If specified the arguments are cast to
-      the specified dtype instead and dtype inference is disabled.
-    inexact: When True, the output dtype must be a subdtype
-    of `jnp.inexact`. Inexact dtypes are real or complex floating points. This
-    is useful when you want to apply operations that don't work directly on
-    integers like taking a mean for example.
+    *args: JAX array compatible values. None values are returned as is.
+    dtype: Optional dtype override. If specified the arguments are cast to the
+      specified dtype instead and dtype inference is disabled.
+    inexact: When True, the output dtype must be a subdtype of `jnp.inexact`.
+      Inexact dtypes are real or complex floating points. This is useful when
+      you want to apply operations that don't work directly on integers like
+      taking a mean for example.
+
   Returns:
     The arguments cast to arrays of the same dtype.
   """
   dtype = canonicalize_dtype(*args, dtype=dtype, inexact=inexact)
-  return [jnp.asarray(x, dtype) if x is not None else None
-          for x in args]
+  return [jnp.asarray(x, dtype) if x is not None else None for x in args]
```

### Comparing `flax-0.7.0/flax/linen/experimental/layers_with_named_axes.py` & `flax-0.7.1/flax/linen/experimental/layers_with_named_axes.py`

 * *Files 5% similar despite different names*

```diff
@@ -35,15 +35,17 @@
 Shape = Any
 Activation = Callable[..., Array]
 # Parameter initializers.
 Initializer = Callable[[PRNGKey, Shape, DType], Array]
 
 
 default_kernel_init = initializers.lecun_normal()
-default_embed_init = initializers.variance_scaling(1.0, 'fan_in', 'normal', out_axis=0)
+default_embed_init = initializers.variance_scaling(
+    1.0, 'fan_in', 'normal', out_axis=0
+)
 
 
 class Dense(nn.Module):
   """A Dense layer with named axes for :meth:`jax.experimental.pjit.pjit`.
 
   .. warning:: This class is hightly EXPERIMENTAL and the API is likely to
       change. For regular (non-pjit) use, please use
@@ -55,53 +57,60 @@
     dtype: the dtype of the computation (default: float32).
     param_dtype: the dtype passed to parameter initializers (default: float32).
     precision: numerical precision of the computation see `jax.lax.Precision`
       for details.
     kernel_init: initializer function for the weight matrix.
     bias_init: initializer function for the bias.
   """
+
   features: int
   use_bias: bool = True
   dtype: DType = jnp.float32
   param_dtype: DType = jnp.float32
   precision: PrecisionLike = None
   kernel_init: Callable[[PRNGKey, Shape, DType], Array] = default_kernel_init
-  bias_init: Callable[[PRNGKey, Shape, DType], Array] = initializers.zeros_init()
+  bias_init: Callable[[PRNGKey, Shape, DType], Array] = (
+      initializers.zeros_init()
+  )
   kernel_axes: Tuple[str, ...] = ()
   dot_general: DotGeneralT = lax.dot_general
 
   @nn.compact
   def __call__(self, inputs: Array) -> Array:
     """Applies a linear transformation to the inputs along the last dimension.
 
     Args:
       inputs: The nd-array to be transformed.
 
     Returns:
       The transformed input.
     """
     inputs = jnp.asarray(inputs, self.dtype)
-    kernel = param_with_axes('kernel',
-                             self.kernel_init,
-                             (inputs.shape[-1], self.features),
-                             self.param_dtype,
-                             axes=self.kernel_axes)
+    kernel = param_with_axes(
+        'kernel',
+        self.kernel_init,
+        (inputs.shape[-1], self.features),
+        self.param_dtype,
+        axes=self.kernel_axes,
+    )
     kernel = jnp.asarray(kernel, self.dtype)
     y = self.dot_general(
         inputs,
         kernel,
         (((inputs.ndim - 1,), (0,)), ((), ())),
         precision=self.precision,
     )
     if self.use_bias:
-      bias = param_with_axes('bias',
-                             self.bias_init,
-                             (self.features,),
-                             self.param_dtype,
-                             axes=(self.kernel_axes[-1],))
+      bias = param_with_axes(
+          'bias',
+          self.bias_init,
+          (self.features,),
+          self.param_dtype,
+          axes=(self.kernel_axes[-1],),
+      )
       bias = jnp.asarray(bias, self.dtype)
       y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))
     return y
 
 
 class Embed(nn.Module):
   """An embedding layer with named axes for :meth:`jax.experimental.pjit.pjit`.
@@ -115,30 +124,33 @@
     features: number of feature dimensions for each embedding.
     dtype: the dtype of the embedding vectors (default: float32).
     param_dtype: the dtype passed to parameter initializers (default: float32).
     embedding_init: embedding initializer.
     one_hot: performs the gather with a one-hot contraction rather than a true
       gather. This is currently needed for SPMD partitioning.
   """
+
   num_embeddings: int
   features: int
   cast_input_dtype: Optional[DType] = None
   dtype: DType = jnp.float32
   param_dtype: DType = jnp.float32
   attend_dtype: Optional[DType] = None
   embedding_init: Initializer = default_embed_init
   one_hot: bool = False
   embedding: Array = dataclasses.field(init=False)
 
   def setup(self):
     self.embedding = param_with_axes(
         'embedding',
-        self.embedding_init, (self.num_embeddings, self.features),
+        self.embedding_init,
+        (self.num_embeddings, self.features),
         self.param_dtype,
-        axes=('vocab', 'embed'))
+        axes=('vocab', 'embed'),
+    )
 
   def __call__(self, inputs: Array) -> Array:
     """Embeds the inputs along the last dimension.
 
     Args:
       inputs: input data, all dimensions are considered batch dimensions.
 
@@ -206,26 +218,34 @@
   # promote x to at least float32, this avoids half precision computation
   # but preserves double or complex floating points
   x = jnp.asarray(x, jnp.promote_types(jnp.float32, jnp.result_type(x)))
   mean = jnp.mean(x, axes)
   mean2 = jnp.mean(_abs_sq(x), axes)
   # mean2 - _abs_sq(mean) is not guaranteed to be non-negative due
   # to floating point round-off errors.
-  var = jnp.maximum(0., mean2 - _abs_sq(mean))
+  var = jnp.maximum(0.0, mean2 - _abs_sq(mean))
   return mean, var
 
 
-def _normalize(mdl: nn.Module, x: Array, mean: Array, var: Array,
-               reduction_axes: Axes, feature_axes: Axes,
-               dtype: DType, param_dtype: DType,
-               epsilon: float,
-               use_bias: bool, use_scale: bool,
-               bias_init: Callable[[PRNGKey, Shape, DType], Array],
-               scale_init: Callable[[PRNGKey, Shape, DType], Array]):
-  """"Normalizes the input of a normalization layer and optionally applies a learned scale and bias.
+def _normalize(
+    mdl: nn.Module,
+    x: Array,
+    mean: Array,
+    var: Array,
+    reduction_axes: Axes,
+    feature_axes: Axes,
+    dtype: DType,
+    param_dtype: DType,
+    epsilon: float,
+    use_bias: bool,
+    use_scale: bool,
+    bias_init: Callable[[PRNGKey, Shape, DType], Array],
+    scale_init: Callable[[PRNGKey, Shape, DType], Array],
+):
+  """ "Normalizes the input of a normalization layer and optionally applies a learned scale and bias.
 
   A seperate bias and scale is learned for each feature as specified by
   feature_axes.
   """
   reduction_axes = _canonicalize_axes(x.ndim, reduction_axes)
   feature_axes = _canonicalize_axes(x.ndim, feature_axes)
   stats_shape = list(x.shape)
@@ -238,25 +258,22 @@
   for ax in feature_axes:
     feature_shape[ax] = x.shape[ax]
     reduced_feature_shape.append(x.shape[ax])
   y = x - mean
   mul = lax.rsqrt(var + epsilon)
   if use_scale:
     scale = mdl.param_with_axes(
-        'scale',
-        scale_init,
-        reduced_feature_shape,
-        param_dtype,
-        axes=('embed',)).reshape(feature_shape)
+        'scale', scale_init, reduced_feature_shape, param_dtype, axes=('embed',)
+    ).reshape(feature_shape)
     mul *= scale
   y *= mul
   if use_bias:
     bias = mdl.param_with_axes(
-        'bias', bias_init, reduced_feature_shape, param_dtype,
-        axes=('embed',)).reshape(feature_shape)
+        'bias', bias_init, reduced_feature_shape, param_dtype, axes=('embed',)
+    ).reshape(feature_shape)
     y += bias
   return jnp.asarray(y, dtype)
 
 
 class LayerNorm(nn.Module):
   """Layer normalization (https://arxiv.org/abs/1607.06450) with named axes for :meth:`jax.experimental.pjit.pjit`.
 
@@ -278,21 +295,26 @@
     use_bias:  If True, bias (beta) is added.
     use_scale: If True, multiply by scale (gamma). When the next layer is linear
       (also e.g. nn.relu), this can be disabled since the scaling will be done
       by the next layer.
     bias_init: Initializer for bias, by default, zero.
     scale_init: Initializer for scale, by default, one.
   """
+
   epsilon: float = 1e-6
   dtype: Any = jnp.float32
   param_dtype: DType = jnp.float32
   use_bias: bool = True
   use_scale: bool = True
-  bias_init: Callable[[PRNGKey, Shape, DType], Array] = initializers.zeros_init()
-  scale_init: Callable[[PRNGKey, Shape, DType], Array] = initializers.ones_init()
+  bias_init: Callable[[PRNGKey, Shape, DType], Array] = (
+      initializers.zeros_init()
+  )
+  scale_init: Callable[[PRNGKey, Shape, DType], Array] = (
+      initializers.ones_init()
+  )
 
   @nn.compact
   def __call__(self, x):
     """Applies layer normalization on the input.
 
     Args:
       x: the inputs
@@ -302,11 +324,21 @@
     """
     reduction_axes = (-1,)
     feature_axes = (-1,)
 
     mean, var = _compute_stats(x, reduction_axes)
 
     return _normalize(
-        self, x, mean, var, reduction_axes, feature_axes,
-        self.dtype, self.param_dtype, self.epsilon,
-        self.use_bias, self.use_scale,
-        self.bias_init, self.scale_init)
+        self,
+        x,
+        mean,
+        var,
+        reduction_axes,
+        feature_axes,
+        self.dtype,
+        self.param_dtype,
+        self.epsilon,
+        self.use_bias,
+        self.use_scale,
+        self.bias_init,
+        self.scale_init,
+    )
```

### Comparing `flax-0.7.0/flax/linen/initializers.py` & `flax-0.7.1/flax/linen/initializers.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,31 +33,33 @@
 from jax.nn.initializers import variance_scaling as variance_scaling
 from jax.nn.initializers import xavier_normal as xavier_normal
 from jax.nn.initializers import xavier_uniform as xavier_uniform
 from jax.nn.initializers import zeros as zeros
 from jax.nn.initializers import Initializer as Initializer
 # pylint: enable=unused-import
 
+
 def zeros_init() -> Initializer:
   """Builds an initializer that returns a constant array full of zeros.
 
   >>> import jax, jax.numpy as jnp
   >>> from flax.linen.initializers import zeros_init
   >>> zeros_initializer = zeros_init()
   >>> zeros_initializer(jax.random.PRNGKey(42), (2, 3), jnp.float32)
   Array([[0., 0., 0.],
          [0., 0., 0.]], dtype=float32)
   """
   return zeros
 
+
 def ones_init() -> Initializer:
   """Builds an initializer that returns a constant array full of ones.
 
   >>> import jax, jax.numpy as jnp
   >>> from flax.linen.initializers import ones_init
   >>> ones_initializer = ones_init()
   >>> ones_initializer(jax.random.PRNGKey(42), (3, 2), jnp.float32)
   Array([[1., 1.],
          [1., 1.],
          [1., 1.]], dtype=float32)
   """
-  return ones
+  return ones
```

### Comparing `flax-0.7.0/flax/linen/kw_only_dataclasses.py` & `flax-0.7.1/flax/linen/kw_only_dataclasses.py`

 * *Files 2% similar despite different names*

```diff
@@ -72,17 +72,19 @@
       parameter list.
     **kwargs: Keyword arguments forwarded to `dataclasses.field`
 
   Returns:
     A `dataclasses.Field` object.
   """
   if kw_only is not dataclasses.MISSING and kw_only:
-    if (kwargs.get('default', dataclasses.MISSING) is dataclasses.MISSING and
-        kwargs.get('default_factory',
-                   dataclasses.MISSING) is dataclasses.MISSING):
+    if (
+        kwargs.get('default', dataclasses.MISSING) is dataclasses.MISSING
+        and kwargs.get('default_factory', dataclasses.MISSING)
+        is dataclasses.MISSING
+    ):
       raise ValueError('Keyword-only fields with no default are not supported.')
     if metadata is None:
       metadata = {}
     metadata[KW_ONLY] = True
   return dataclasses.field(metadata=metadata, **kwargs)
 
 
@@ -128,65 +130,72 @@
     if annotation is KW_ONLY:
       if kw_only_name is not None:
         raise TypeError('Multiple KW_ONLY markers')
       kw_only_name = name
     elif kw_only_name is not None:
       if not hasattr(cls, name):
         raise ValueError(
-            'Keyword-only fields with no default are not supported.')
+            'Keyword-only fields with no default are not supported.'
+        )
       default = getattr(cls, name)
       if isinstance(default, dataclasses.Field):
         default.metadata = {**default.metadata, **{KW_ONLY: True}}
       else:
         default = field(default=default, kw_only=True)
       setattr(cls, name, default)
   if kw_only_name:
     del cls.__annotations__[kw_only_name]
 
   # Inject extra fields.
   if extra_fields:
     for name, annotation, default in extra_fields:
       if not (isinstance(name, str) and isinstance(default, dataclasses.Field)):
-        raise ValueError('Expected extra_fields to a be a list of '
-                         '(name, type, Field) tuples.')
+        raise ValueError(
+            'Expected extra_fields to a be a list of '
+            '(name, type, Field) tuples.'
+        )
       setattr(cls, name, default)
       cls.__annotations__[name] = annotation
 
   # Extract kw_only fields from base classes' __dataclass_fields__.
   for base in reversed(cls.__mro__[1:]):
     if not dataclasses.is_dataclass(base):
       continue
     base_annotations = base.__dict__.get('__annotations__', {})
     base_dataclass_fields[base] = dict(
-        getattr(base, '__dataclass_fields__', {}))
+        getattr(base, '__dataclass_fields__', {})
+    )
     for base_field in list(dataclasses.fields(base)):
       field_name = base_field.name
       if base_field.metadata.get(KW_ONLY) or field_name in kw_only_fields:
-        kw_only_fields[field_name] = (base_annotations.get(field_name),
-                                      base_field)
+        kw_only_fields[field_name] = (
+            base_annotations.get(field_name),
+            base_field,
+        )
         del base.__dataclass_fields__[field_name]
 
   # Remove any keyword-only fields from this class.
   cls_annotations = cls.__dict__['__annotations__']
   for name, annotation in list(cls_annotations.items()):
     value = getattr(cls, name, None)
-    if ((isinstance(value, dataclasses.Field) and value.metadata.get(KW_ONLY))
-        or name in kw_only_fields):
+    if (
+        isinstance(value, dataclasses.Field) and value.metadata.get(KW_ONLY)
+    ) or name in kw_only_fields:
       del cls_annotations[name]
       kw_only_fields[name] = (annotation, value)
 
   # Add keyword-only fields at the end of __annotations__, in the order they
   # were found in the base classes and in this class.
   for name, (annotation, default) in kw_only_fields.items():
     setattr(cls, name, default)
     cls_annotations.pop(name, None)
     cls_annotations[name] = annotation
 
   # Apply the dataclass transform.
   transformed_cls = dataclasses.dataclass(cls, **kwargs)
 
   # Restore the base classes' __dataclass_fields__.
-  for (cls, dataclass_fields) in base_dataclass_fields.items():
+  for cls, dataclass_fields in base_dataclass_fields.items():
     cls.__dataclass_fields__ = dataclass_fields
 
   # Return the transformed dataclass
   return transformed_cls
```

### Comparing `flax-0.7.0/flax/linen/linear.py` & `flax-0.7.1/flax/linen/linear.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,37 +11,50 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Linear modules."""
 
 import dataclasses
-from typing import (Any, Callable, Iterable, List, Optional, Sequence, Tuple,
-                    Union)
+from typing import (
+    Any,
+    Callable,
+    Iterable,
+    List,
+    Optional,
+    Sequence,
+    Tuple,
+    Union,
+)
 
 from flax.core import meta
 from flax.linen import initializers
+from flax.linen.dtypes import promote_dtype
 from flax.linen.module import compact
 from flax.linen.module import Module
-from flax.linen.dtypes import promote_dtype
+import jax
 from jax import eval_shape
 from jax import lax
 from jax import random
 from jax.core import ShapedArray
 import jax.numpy as jnp
 import numpy as np
-import jax
 
 
 PRNGKey = Any
 Shape = Tuple[int, ...]
 Dtype = Any  # this could be a real type?
 Array = Any
-PrecisionLike = Union[None, str, lax.Precision, Tuple[str, str],
-                      Tuple[lax.Precision, lax.Precision]]
+PrecisionLike = Union[
+    None,
+    str,
+    lax.Precision,
+    Tuple[str, str],
+    Tuple[lax.Precision, lax.Precision],
+]
 DotGeneralT = Callable[..., Array]
 ConvGeneralDilatedT = Callable[..., Array]
 
 default_kernel_init = initializers.lecun_normal()
 
 
 def _normalize_axes(axes: Tuple[int, ...], ndim: int) -> Tuple[int, ...]:
@@ -68,22 +81,25 @@
     dtype: the dtype of the computation (default: infer from input and params).
     param_dtype: the dtype passed to parameter initializers (default: float32).
     kernel_init: initializer function for the weight matrix.
     bias_init: initializer function for the bias.
     precision: numerical precision of the computation see `jax.lax.Precision`
       for details.
   """
+
   features: Union[int, Sequence[int]]
   axis: Union[int, Sequence[int]] = -1
   batch_dims: Sequence[int] = ()
   use_bias: bool = True
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
+      initializers.zeros_init()
+  )
   precision: PrecisionLike = None
   dot_general: DotGeneralT = lax.dot_general
 
   @compact
   def __call__(self, inputs: Array) -> Array:
     """Applies a linear transformation to the inputs along multiple dimensions.
 
@@ -95,57 +111,68 @@
     """
     features = _canonicalize_tuple(self.features)
     axis = _canonicalize_tuple(self.axis)
     batch_dims = _canonicalize_tuple(self.batch_dims)
     if batch_dims:
       max_dim = np.max(batch_dims)
       if set(batch_dims) != set(range(max_dim + 1)):
-        raise ValueError('batch_dims %s must be consecutive leading '
-                         'dimensions starting from 0.' % str(batch_dims))
+        raise ValueError(
+            'batch_dims %s must be consecutive leading '
+            'dimensions starting from 0.'
+            % str(batch_dims)
+        )
 
     ndim = inputs.ndim
     n_batch_dims = len(batch_dims)
     axis = _normalize_axes(axis, ndim)
     batch_dims = _normalize_axes(batch_dims, ndim)
     n_axis, n_features = len(axis), len(features)
 
     def kernel_init_wrap(rng, shape, dtype=jnp.float32):
-      flat_shape = (np.prod(shape[:n_batch_dims]) *
-                    np.prod(shape[n_batch_dims:n_axis + n_batch_dims]),
-                    np.prod(shape[-n_features:]),)
+      flat_shape = (
+          np.prod(shape[:n_batch_dims])
+          * np.prod(shape[n_batch_dims : n_axis + n_batch_dims]),
+          np.prod(shape[-n_features:]),
+      )
       flat_shape = jax.tree_map(int, flat_shape)
       kernel = self.kernel_init(rng, flat_shape, dtype)
       if isinstance(kernel, meta.AxisMetadata):
         return meta.replace_boxed(kernel, jnp.reshape(kernel.unbox(), shape))
       return jnp.reshape(kernel, shape)
 
     batch_shape = tuple(inputs.shape[ax] for ax in batch_dims)
     # batch and non-contracting dims of input with 1s for batch dims.
     expanded_batch_shape = tuple(
         inputs.shape[ax] if ax in batch_dims else 1
-        for ax in range(inputs.ndim) if ax not in axis)
+        for ax in range(inputs.ndim)
+        if ax not in axis
+    )
     kernel_shape = tuple(inputs.shape[ax] for ax in axis) + features
-    kernel = self.param('kernel', kernel_init_wrap, batch_shape + kernel_shape,
-                        self.param_dtype)
+    kernel = self.param(
+        'kernel', kernel_init_wrap, batch_shape + kernel_shape, self.param_dtype
+    )
 
     batch_ind = tuple(range(n_batch_dims))
     contract_ind = tuple(range(n_batch_dims, n_axis + n_batch_dims))
 
     if self.use_bias:
+
       def bias_init_wrap(rng, shape, dtype=jnp.float32):
-        flat_shape = (np.prod(shape[:n_batch_dims]) *
-                      np.prod(shape[-n_features:]),)
+        flat_shape = (
+            np.prod(shape[:n_batch_dims]) * np.prod(shape[-n_features:]),
+        )
         flat_shape = jax.tree_map(int, flat_shape)
         bias = self.bias_init(rng, flat_shape, dtype)
         if isinstance(bias, meta.AxisMetadata):
           return meta.replace_boxed(bias, jnp.reshape(bias.unbox(), shape))
         return jnp.reshape(bias, shape)
 
-      bias = self.param('bias', bias_init_wrap, batch_shape + features,
-                        self.param_dtype)
+      bias = self.param(
+          'bias', bias_init_wrap, batch_shape + features, self.param_dtype
+      )
     else:
       bias = None
 
     inputs, kernel, bias = promote_dtype(inputs, kernel, bias, dtype=self.dtype)
 
     out = self.dot_general(
         inputs,
@@ -170,40 +197,46 @@
     dtype: the dtype of the computation (default: infer from input and params).
     param_dtype: the dtype passed to parameter initializers (default: float32).
     precision: numerical precision of the computation see `jax.lax.Precision`
       for details.
     kernel_init: initializer function for the weight matrix.
     bias_init: initializer function for the bias.
   """
+
   features: int
   use_bias: bool = True
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   precision: PrecisionLike = None
   kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
+      initializers.zeros_init()
+  )
   dot_general: DotGeneralT = lax.dot_general
 
   @compact
   def __call__(self, inputs: Array) -> Array:
     """Applies a linear transformation to the inputs along the last dimension.
 
     Args:
       inputs: The nd-array to be transformed.
 
     Returns:
       The transformed input.
     """
-    kernel = self.param('kernel',
-                        self.kernel_init,
-                        (jnp.shape(inputs)[-1], self.features),
-                        self.param_dtype)
+    kernel = self.param(
+        'kernel',
+        self.kernel_init,
+        (jnp.shape(inputs)[-1], self.features),
+        self.param_dtype,
+    )
     if self.use_bias:
-      bias = self.param('bias', self.bias_init, (self.features,),
-                        self.param_dtype)
+      bias = self.param(
+          'bias', self.bias_init, (self.features,), self.param_dtype
+      )
     else:
       bias = None
     inputs, kernel, bias = promote_dtype(inputs, kernel, bias, dtype=self.dtype)
     y = self.dot_general(
         inputs,
         kernel,
         (((inputs.ndim - 1,), (0,)), ((), ())),
@@ -224,15 +257,15 @@
 
 
 PaddingLike = Union[str, int, Sequence[Union[int, Tuple[int, int]]]]
 LaxPadding = Union[str, Sequence[Tuple[int, int]]]
 
 
 def canonicalize_padding(padding: PaddingLike, rank: int) -> LaxPadding:
-  """"Canonicalizes conv padding to a jax.lax supported format."""
+  """ "Canonicalizes conv padding to a jax.lax supported format."""
   if isinstance(padding, str):
     return padding
   if isinstance(padding, int):
     return [(padding, padding)] * rank
   if isinstance(padding, Sequence) and len(padding) == rank:
     new_pad = []
     for p in padding:
@@ -241,17 +274,18 @@
       elif isinstance(p, tuple) and len(p) == 2:
         new_pad.append(p)
       else:
         break
     if len(new_pad) == rank:
       return new_pad
   raise ValueError(
-    f'Invalid padding format: {padding}, should be str, int,'
-    f' or a sequence of len {rank} where each element is an'
-    f' int or pair of ints.')
+      f'Invalid padding format: {padding}, should be str, int,'
+      f' or a sequence of len {rank} where each element is an'
+      ' int or pair of ints.'
+  )
 
 
 class _Conv(Module):
   """Convolution Module wrapping `lax.conv_general_dilated[_local]`.
 
   Attributes:
     features: number of convolution filters.
@@ -283,28 +317,31 @@
     dtype: the dtype of the computation (default: infer from input and params).
     param_dtype: the dtype passed to parameter initializers (default: float32).
     precision: numerical precision of the computation see `jax.lax.Precision`
       for details.
     kernel_init: initializer for the convolutional kernel.
     bias_init: initializer for the bias.
   """
+
   features: int
   kernel_size: Sequence[int]
   strides: Union[None, int, Sequence[int]] = 1
   padding: PaddingLike = 'SAME'
   input_dilation: Union[None, int, Sequence[int]] = 1
   kernel_dilation: Union[None, int, Sequence[int]] = 1
   feature_group_count: int = 1
   use_bias: bool = True
   mask: Optional[Array] = None
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   precision: PrecisionLike = None
   kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
+      initializers.zeros_init()
+  )
   conv_general_dilated: ConvGeneralDilatedT = lax.conv_general_dilated
 
   @property
   def shared_weights(self) -> bool:  # type: ignore
     """Defines whether weights are shared or not between different pixels.
 
     Returns:
@@ -333,76 +370,86 @@
         n return, an allowance made to enable writing single-example code.
 
     Returns:
       The convolved data.
     """
 
     if isinstance(self.kernel_size, int):
-      raise TypeError('Expected Conv kernel_size to be a'
-                      ' tuple/list of integers (eg.: [3, 3]) but got'
-                      f' {self.kernel_size}.')
+      raise TypeError(
+          'Expected Conv kernel_size to be a'
+          ' tuple/list of integers (eg.: [3, 3]) but got'
+          f' {self.kernel_size}.'
+      )
     else:
       kernel_size = tuple(self.kernel_size)
 
-    def maybe_broadcast(x: Optional[Union[int, Sequence[int]]]) -> (
-        Tuple[int, ...]):
+    def maybe_broadcast(
+        x: Optional[Union[int, Sequence[int]]]
+    ) -> Tuple[int, ...]:
       if x is None:
         # backward compatibility with using None as sentinel for
         # broadcast 1
         x = 1
       if isinstance(x, int):
         return (x,) * len(kernel_size)
       return tuple(x)
 
     # Combine all input batch dimensions into a single leading batch axis.
     num_batch_dimensions = inputs.ndim - (len(kernel_size) + 1)
     if num_batch_dimensions != 1:
       input_batch_shape = inputs.shape[:num_batch_dimensions]
       total_batch_size = int(np.prod(input_batch_shape))
-      flat_input_shape = (
-          (total_batch_size,) + inputs.shape[num_batch_dimensions:])
+      flat_input_shape = (total_batch_size,) + inputs.shape[
+          num_batch_dimensions:
+      ]
       inputs = jnp.reshape(inputs, flat_input_shape)
 
     # self.strides or (1,) * (inputs.ndim - 2)
     strides = maybe_broadcast(self.strides)
     input_dilation = maybe_broadcast(self.input_dilation)
     kernel_dilation = maybe_broadcast(self.kernel_dilation)
 
     padding_lax = canonicalize_padding(self.padding, len(kernel_size))
     if padding_lax == 'CIRCULAR':
       kernel_size_dilated = [
           (k - 1) * d + 1 for k, d in zip(kernel_size, kernel_dilation)
       ]
       zero_pad: List[Tuple[int, int]] = [(0, 0)]
-      pads = (zero_pad + [((k - 1) // 2, k // 2) for k in kernel_size_dilated] +
-              [(0, 0)])
+      pads = (
+          zero_pad
+          + [((k - 1) // 2, k // 2) for k in kernel_size_dilated]
+          + [(0, 0)]
+      )
       inputs = jnp.pad(inputs, pads, mode='wrap')
       padding_lax = 'VALID'
     elif padding_lax == 'CAUSAL':
       if len(kernel_size) != 1:
         raise ValueError(
-            'Causal padding is only implemented for 1D convolutions.')
+            'Causal padding is only implemented for 1D convolutions.'
+        )
       left_pad = kernel_dilation[0] * (kernel_size[0] - 1)
       pads = [(0, 0), (left_pad, 0), (0, 0)]
       inputs = jnp.pad(inputs, pads)
       padding_lax = 'VALID'
 
     dimension_numbers = _conv_dimension_numbers(inputs.shape)
     in_features = jnp.shape(inputs)[-1]
 
     if self.shared_weights:
       # One shared convolutional kernel for all pixels in the output.
       assert in_features % self.feature_group_count == 0
       kernel_shape = kernel_size + (
-          in_features // self.feature_group_count, self.features)
+          in_features // self.feature_group_count,
+          self.features,
+      )
 
     else:
       if self.feature_group_count != 1:
         raise NotImplementedError(
-            f'`lax.conv_general_dilated_local` does not support '
+            '`lax.conv_general_dilated_local` does not support '
             f'`feature_group_count != 1`, got `{self.feature_group_count}`.'
         )
 
       # Need to know the spatial output shape of a standard convolution to
       # create the unshared convolution kernel.
       conv_output_shape = eval_shape(
           lambda lhs, rhs: self.conv_general_dilated(  # pylint: disable=g-long-lambda
@@ -415,23 +462,28 @@
               rhs_dilation=kernel_dilation,
           ),
           inputs,
           ShapedArray(kernel_size + (in_features, self.features), inputs.dtype),
       ).shape
 
       # One (unshared) convolutional kernel per each pixel in the output.
-      kernel_shape = conv_output_shape[1:-1] + (np.prod(kernel_size) *
-                                                in_features, self.features)
+      kernel_shape = conv_output_shape[1:-1] + (
+          np.prod(kernel_size) * in_features,
+          self.features,
+      )
 
     if self.mask is not None and self.mask.shape != kernel_shape:
-      raise ValueError('Mask needs to have the same shape as weights. '
-                       f'Shapes are: {self.mask.shape}, {kernel_shape}')
+      raise ValueError(
+          'Mask needs to have the same shape as weights. '
+          f'Shapes are: {self.mask.shape}, {kernel_shape}'
+      )
 
-    kernel = self.param('kernel', self.kernel_init, kernel_shape,
-                        self.param_dtype)
+    kernel = self.param(
+        'kernel', self.kernel_init, kernel_shape, self.param_dtype
+    )
 
     if self.mask is not None:
       kernel *= self.mask
 
     if self.use_bias:
       if self.shared_weights:
         # One bias weight per output channel, shared between pixels.
@@ -463,15 +515,15 @@
           rhs=kernel,
           window_strides=strides,
           padding=padding_lax,
           filter_shape=kernel_size,
           lhs_dilation=input_dilation,
           rhs_dilation=kernel_dilation,
           dimension_numbers=dimension_numbers,
-          precision=self.precision
+          precision=self.precision,
       )
 
     if self.use_bias:
       bias = bias.reshape((1,) * (y.ndim - bias.ndim) + bias.shape)
       y += bias
 
     if num_batch_dimensions != 1:
@@ -593,26 +645,29 @@
     precision: numerical precision of the computation see `jax.lax.Precision`
       for details.
     kernel_init: initializer for the convolutional kernel.
     bias_init: initializer for the bias.
     transpose_kernel: if True flips spatial axes and swaps the input/output
       channel axes of the kernel.
   """
+
   features: int
   kernel_size: Union[int, Sequence[int]]
   strides: Optional[Sequence[int]] = None
   padding: PaddingLike = 'SAME'
   kernel_dilation: Optional[Sequence[int]] = None
   use_bias: bool = True
   mask: Optional[Array] = None
   dtype: Dtype = None
   param_dtype: Dtype = jnp.float32
   precision: PrecisionLike = None
   kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
+      initializers.zeros_init()
+  )
   transpose_kernel: bool = False
 
   @compact
   def __call__(self, inputs: Array) -> Array:
     """Applies a transposed convolution to the inputs.
 
     Behaviour mirrors of `jax.lax.conv_transpose`.
@@ -640,16 +695,17 @@
       kernel_size = tuple(self.kernel_size)
 
     # Combine all input batch dimensions into a single leading batch axis.
     num_batch_dimensions = inputs.ndim - (len(kernel_size) + 1)
     if num_batch_dimensions != 1:
       input_batch_shape = inputs.shape[:num_batch_dimensions]
       total_batch_size = int(np.prod(input_batch_shape))
-      flat_input_shape = (
-          (total_batch_size,) + inputs.shape[num_batch_dimensions:])
+      flat_input_shape = (total_batch_size,) + inputs.shape[
+          num_batch_dimensions:
+      ]
       inputs = jnp.reshape(inputs, flat_input_shape)
 
     strides: Tuple[int, ...]
     if self.strides is None:
       strides = (1,) * (inputs.ndim - 2)
     else:
       strides = tuple(self.strides)
@@ -657,57 +713,62 @@
     in_features = jnp.shape(inputs)[-1]
     if self.transpose_kernel:
       kernel_shape = kernel_size + (self.features, in_features)
     else:
       kernel_shape = kernel_size + (in_features, self.features)
 
     if self.mask is not None and self.mask.shape != kernel_shape:
-      raise ValueError('Mask needs to have the same shape as weights. '
-                       f'Shapes are: {self.mask.shape}, {kernel_shape}')
+      raise ValueError(
+          'Mask needs to have the same shape as weights. '
+          f'Shapes are: {self.mask.shape}, {kernel_shape}'
+      )
 
-    kernel = self.param('kernel', self.kernel_init, kernel_shape,
-                        self.param_dtype)
+    kernel = self.param(
+        'kernel', self.kernel_init, kernel_shape, self.param_dtype
+    )
 
     if self.mask is not None:
       kernel *= self.mask
 
     padding_lax = canonicalize_padding(self.padding, len(kernel_size))
     if padding_lax == 'CIRCULAR':
       padding_lax = 'VALID'
 
     if self.use_bias:
-      bias = self.param('bias', self.bias_init, (self.features,),
-                        self.param_dtype)
+      bias = self.param(
+          'bias', self.bias_init, (self.features,), self.param_dtype
+      )
     else:
       bias = None
 
-    inputs, kernel, bias = promote_dtype(inputs, kernel, bias,
-                                         dtype=self.dtype)
+    inputs, kernel, bias = promote_dtype(inputs, kernel, bias, dtype=self.dtype)
 
     y = lax.conv_transpose(
         inputs,
         kernel,
         strides,
         padding_lax,
         rhs_dilation=self.kernel_dilation,
         transpose_kernel=self.transpose_kernel,
-        precision=self.precision)
+        precision=self.precision,
+    )
 
     if self.padding == 'CIRCULAR':
       # For circular padding, we need to identify the size of the final output
       # ("period") along each spatial dimension, pad each dimension to an
       # integer number of periods, and wrap the array periodically around each
       # dimension. Padding should be done in such a way that the start of the
       # original input data inside the padded array is located at integer
       # number of periods - otherwise the result would be circularly shifted.
 
       # Compute period along each spatial dimension - it's input size scaled
       # by the stride.
       scaled_x_dims = [
-          x_dim * stride for x_dim, stride in zip(jnp.shape(inputs)[1:-1], strides)
+          x_dim * stride
+          for x_dim, stride in zip(jnp.shape(inputs)[1:-1], strides)
       ]
       # Compute difference between the current size of y and the final output
       # size, and complement this difference to 2 * period - that gives how
       # much we need to pad.
       size_diffs = [
           -(y_dim - x_dim) % (2 * x_dim)
           for y_dim, x_dim in zip(y.shape[1:-1], scaled_x_dims)
@@ -726,56 +787,62 @@
         total_pad = [
             ((size_diff + 1) // 2, size_diff // 2) for size_diff in size_diffs
         ]
       y = jnp.pad(y, [(0, 0)] + total_pad + [(0, 0)])
       # Wrap the result periodically around each spatial dimension,
       # one by one.
       for i in range(1, y.ndim - 1):
-        y = y.reshape(y.shape[:i] + (-1, scaled_x_dims[i - 1]) +
-                      y.shape[i + 1:])
+        y = y.reshape(
+            y.shape[:i] + (-1, scaled_x_dims[i - 1]) + y.shape[i + 1 :]
+        )
         y = y.sum(axis=i)
 
     if self.use_bias:
       y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))
 
     if num_batch_dimensions != 1:
       output_shape = input_batch_shape + y.shape[1:]
       y = jnp.reshape(y, output_shape)
 
     return y
 
 
-default_embed_init = initializers.variance_scaling(1.0, 'fan_in', 'normal', out_axis=0)
+default_embed_init = initializers.variance_scaling(
+    1.0, 'fan_in', 'normal', out_axis=0
+)
 
 
 class Embed(Module):
   """Embedding Module.
 
   A parameterized function from integers [0, n) to d-dimensional vectors.
 
   Attributes:
     num_embeddings: number of embeddings.
     features: number of feature dimensions for each embedding.
     dtype: the dtype of the embedding vectors (default: same as embedding).
     param_dtype: the dtype passed to parameter initializers (default: float32).
     embedding_init: embedding initializer.
   """
+
   num_embeddings: int
   features: int
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   embedding_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_embed_init
 
   embedding: Array = dataclasses.field(init=False)
 
   def setup(self):
-    self.embedding = self.param('embedding',
-                                self.embedding_init,
-                                (self.num_embeddings, self.features),
-                                self.param_dtype)
+    self.embedding = self.param(
+        'embedding',
+        self.embedding_init,
+        (self.num_embeddings, self.features),
+        self.param_dtype,
+    )
 
   def __call__(self, inputs: Array) -> Array:
     """Embeds the inputs along the last dimension.
 
     Args:
       inputs: input data, all dimensions are considered batch dimensions.
 
@@ -783,15 +850,17 @@
       Output which is embedded input data.  The output shape follows the input,
       with an additional `features` dimension appended.
     """
     if not jnp.issubdtype(inputs.dtype, jnp.integer):
       raise ValueError('Input type must be an integer or unsigned integer.')
     # Use take because fancy indexing numpy arrays with JAX indices does not
     # work correctly.
-    embedding, = promote_dtype(self.embedding, dtype=self.dtype, inexact=False)
+    (embedding,) = promote_dtype(
+        self.embedding, dtype=self.dtype, inexact=False
+    )
     return jnp.take(embedding, inputs, axis=0)
 
   def attend(self, query: Array) -> Array:
     """Attend over the embedding using a query array.
 
     Args:
       query: array with last dimension equal the feature depth `features` of the
```

### Comparing `flax-0.7.0/flax/linen/module.py` & `flax-0.7.1/flax/linen/module.py`

 * *Files 2% similar despite different names*

```diff
@@ -39,5444 +39,5509 @@
 00000260: 636f 6e74 6578 746c 6962 0a69 6d70 6f72  contextlib.impor
 00000270: 7420 6461 7461 636c 6173 7365 730a 696d  t dataclasses.im
 00000280: 706f 7274 2065 6e75 6d0a 696d 706f 7274  port enum.import
 00000290: 2066 756e 6374 6f6f 6c73 0a69 6d70 6f72   functools.impor
 000002a0: 7420 696e 7370 6563 740a 696d 706f 7274  t inspect.import
 000002b0: 2072 650a 696d 706f 7274 2073 7973 0a69   re.import sys.i
 000002c0: 6d70 6f72 7420 7468 7265 6164 696e 670a  mport threading.
-000002d0: 696d 706f 7274 2074 7970 696e 670a 696d  import typing.im
-000002e0: 706f 7274 2077 6561 6b72 6566 0a66 726f  port weakref.fro
-000002f0: 6d20 7479 7069 6e67 2069 6d70 6f72 7420  m typing import 
-00000300: 2841 6e79 2c20 4361 6c6c 6162 6c65 2c20  (Any, Callable, 
-00000310: 4469 6374 2c20 4974 6572 6162 6c65 2c20  Dict, Iterable, 
-00000320: 4c69 7374 2c20 5365 7175 656e 6365 2c20  List, Sequence, 
-00000330: 4e61 6d65 6454 7570 6c65 2c20 4d61 7070  NamedTuple, Mapp
-00000340: 696e 672c 0a20 2020 2020 2020 2020 2020  ing,.           
-00000350: 2020 2020 2020 2020 204f 7074 696f 6e61           Optiona
-00000360: 6c2c 2053 6574 2c20 5475 706c 652c 2054  l, Set, Tuple, T
-00000370: 7970 652c 2054 7970 6556 6172 2c20 556e  ype, TypeVar, Un
-00000380: 696f 6e2c 206f 7665 726c 6f61 6429 0a0a  ion, overload)..
-00000390: 696d 706f 7274 206a 6178 0a69 6d70 6f72  import jax.impor
-000003a0: 7420 6e75 6d70 7920 6173 206e 700a 696d  t numpy as np.im
-000003b0: 706f 7274 206a 6178 2e6e 756d 7079 2061  port jax.numpy a
-000003c0: 7320 6a6e 700a 6672 6f6d 2074 7970 696e  s jnp.from typin
-000003d0: 675f 6578 7465 6e73 696f 6e73 2069 6d70  g_extensions imp
-000003e0: 6f72 7420 5072 6f74 6f63 6f6c 2c20 5c0a  ort Protocol, \.
-000003f0: 2020 6461 7461 636c 6173 735f 7472 616e    dataclass_tran
-00000400: 7366 6f72 6d20 2023 2070 7974 7970 653a  sform  # pytype:
-00000410: 2064 6973 6162 6c65 3d6e 6f74 2d73 7570   disable=not-sup
-00000420: 706f 7274 6564 2d79 6574 0a0a 696d 706f  ported-yet..impo
-00000430: 7274 2066 6c61 780a 696d 706f 7274 2066  rt flax.import f
-00000440: 6c61 782e 6c69 6e65 6e20 6173 206e 6e0a  lax.linen as nn.
-00000450: 6672 6f6d 2066 6c61 7820 696d 706f 7274  from flax import
-00000460: 2028 636f 6e66 6967 2c20 636f 7265 2c20   (config, core, 
-00000470: 6572 726f 7273 2c20 7365 7269 616c 697a  errors, serializ
-00000480: 6174 696f 6e2c 2074 7261 6365 6261 636b  ation, traceback
-00000490: 5f75 7469 6c2c 0a20 2020 2020 2020 2020  _util,.         
-000004a0: 2020 2020 2020 2020 2074 7261 7665 7273           travers
-000004b0: 655f 7574 696c 290a 6672 6f6d 2066 6c61  e_util).from fla
-000004c0: 782e 636f 7265 2069 6d70 6f72 7420 5363  x.core import Sc
-000004d0: 6f70 650a 6672 6f6d 2066 6c61 782e 636f  ope.from flax.co
-000004e0: 7265 2069 6d70 6f72 7420 7061 7274 6961  re import partia
-000004f0: 6c5f 6576 616c 0a66 726f 6d20 666c 6178  l_eval.from flax
-00000500: 2e63 6f72 652e 6672 6f7a 656e 5f64 6963  .core.frozen_dic
-00000510: 7420 696d 706f 7274 2046 726f 7a65 6e44  t import FrozenD
-00000520: 6963 740a 6672 6f6d 2066 6c61 782e 636f  ict.from flax.co
-00000530: 7265 2e73 636f 7065 2069 6d70 6f72 7420  re.scope import 
-00000540: 2820 2023 2070 796c 696e 743a 2064 6973  (  # pylint: dis
-00000550: 6162 6c65 3d67 2d6d 756c 7469 706c 652d  able=g-multiple-
-00000560: 696d 706f 7274 0a20 2020 2043 6f6c 6c65  import.    Colle
-00000570: 6374 696f 6e46 696c 7465 722c 2044 656e  ctionFilter, Den
-00000580: 794c 6973 742c 2046 726f 7a65 6e56 6172  yList, FrozenVar
-00000590: 6961 626c 6544 6963 742c 2056 6172 6961  iableDict, Varia
-000005a0: 626c 652c 2056 6172 6961 626c 6544 6963  ble, VariableDic
-000005b0: 742c 0a20 2020 2075 6e69 6f6e 5f66 696c  t,.    union_fil
-000005c0: 7465 7273 290a 6672 6f6d 2066 6c61 782e  ters).from flax.
-000005d0: 6964 7320 696d 706f 7274 2046 6c61 7849  ids import FlaxI
-000005e0: 640a 6672 6f6d 2066 6c61 782e 6964 7320  d.from flax.ids 
-000005f0: 696d 706f 7274 2075 7569 640a 6672 6f6d  import uuid.from
-00000600: 2066 6c61 782e 6c69 6e65 6e20 696d 706f   flax.linen impo
-00000610: 7274 206b 775f 6f6e 6c79 5f64 6174 6163  rt kw_only_datac
-00000620: 6c61 7373 6573 0a0a 0a74 7261 6365 6261  lasses...traceba
-00000630: 636b 5f75 7469 6c2e 7265 6769 7374 6572  ck_util.register
-00000640: 5f65 7863 6c75 7369 6f6e 285f 5f66 696c  _exclusion(__fil
-00000650: 655f 5f29 0a0a 4b65 7941 7272 6179 203d  e__)..KeyArray =
-00000660: 2055 6e69 6f6e 5b6a 6178 2e41 7272 6179   Union[jax.Array
-00000670: 2c20 6a61 782e 7261 6e64 6f6d 2e4b 6579  , jax.random.Key
-00000680: 4172 7261 795d 2020 2320 7079 6c69 6e74  Array]  # pylint
-00000690: 3a20 6469 7361 626c 653d 696e 7661 6c69  : disable=invali
-000006a0: 642d 6e61 6d65 0a52 4e47 5365 7175 656e  d-name.RNGSequen
-000006b0: 6365 7320 3d20 4469 6374 5b73 7472 2c20  ces = Dict[str, 
-000006c0: 4b65 7941 7272 6179 5d0a 4172 7261 7920  KeyArray].Array 
-000006d0: 3d20 416e 7920 2020 2023 2070 796c 696e  = Any    # pylin
-000006e0: 743a 2064 6973 6162 6c65 3d69 6e76 616c  t: disable=inval
-000006f0: 6964 2d6e 616d 650a 0a0a 5420 3d20 5479  id-name...T = Ty
-00000700: 7065 5661 7228 2754 2729 0a4b 203d 2054  peVar('T').K = T
-00000710: 7970 6556 6172 2827 4b27 290a 4d20 3d20  ypeVar('K').M = 
-00000720: 5479 7065 5661 7228 274d 272c 2062 6f75  TypeVar('M', bou
-00000730: 6e64 3d27 4d6f 6475 6c65 2729 0a5f 4361  nd='Module')._Ca
-00000740: 6c6c 6162 6c65 5420 3d20 5479 7065 5661  llableT = TypeVa
-00000750: 7228 275f 4361 6c6c 6162 6c65 5427 2c20  r('_CallableT', 
-00000760: 626f 756e 643d 4361 6c6c 6162 6c65 290a  bound=Callable).
-00000770: 0a0a 2320 5573 6564 2066 6f72 2061 6273  ..# Used for abs
-00000780: 7472 6163 746c 7920 7465 7374 696e 6720  tractly testing 
-00000790: 6d6f 6475 6c65 2062 6568 6176 696f 722e  module behavior.
-000007a0: 0a54 6573 7453 636f 7065 203d 2074 7970  .TestScope = typ
-000007b0: 6528 2754 6573 7453 636f 7065 272c 0a20  e('TestScope',. 
-000007c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000007d0: 2853 636f 7065 2c29 2c0a 2020 2020 2020  (Scope,),.      
-000007e0: 2020 2020 2020 2020 2020 207b 276d 616b             {'mak
-000007f0: 655f 726e 6727 3a20 6c61 6d62 6461 2073  e_rng': lambda s
-00000800: 656c 662c 206e 616d 653a 206a 6178 2e72  elf, name: jax.r
-00000810: 616e 646f 6d2e 5052 4e47 4b65 7928 3029  andom.PRNGKey(0)
-00000820: 7d29 0a0a 0a23 2070 796c 696e 743a 2064  })...# pylint: d
-00000830: 6973 6162 6c65 3d70 726f 7465 6374 6564  isable=protected
-00000840: 2d61 6363 6573 732c 6174 7472 6962 7574  -access,attribut
-00000850: 652d 6465 6669 6e65 642d 6f75 7473 6964  e-defined-outsid
-00000860: 652d 696e 6974 0a0a 6465 6620 5f69 6e64  e-init..def _ind
-00000870: 656e 7428 783a 2073 7472 2c20 6e75 6d5f  ent(x: str, num_
-00000880: 7370 6163 6573 3a20 696e 7429 3a0a 2020  spaces: int):.  
-00000890: 696e 6465 6e74 5f73 7472 203d 2027 2027  indent_str = ' '
-000008a0: 202a 206e 756d 5f73 7061 6365 730a 2020   * num_spaces.  
-000008b0: 6c69 6e65 7320 3d20 782e 7370 6c69 7428  lines = x.split(
-000008c0: 275c 6e27 290a 2020 2320 736b 6970 206c  '\n').  # skip l
-000008d0: 6173 7420 6c69 6e65 2062 6563 6175 7365  ast line because
-000008e0: 2069 7420 6973 2061 6c77 6179 7320 656d   it is always em
-000008f0: 7074 7920 616e 6420 7368 6f75 6c64 206e  pty and should n
-00000900: 6f74 2062 6520 696e 6465 6e74 6564 2e0a  ot be indented..
-00000910: 2020 6173 7365 7274 206e 6f74 206c 696e    assert not lin
-00000920: 6573 5b2d 315d 0a20 2072 6574 7572 6e20  es[-1].  return 
-00000930: 275c 6e27 2e6a 6f69 6e28 696e 6465 6e74  '\n'.join(indent
-00000940: 5f73 7472 202b 206c 696e 6520 666f 7220  _str + line for 
-00000950: 6c69 6e65 2069 6e20 6c69 6e65 735b 3a2d  line in lines[:-
-00000960: 315d 2920 2b20 275c 6e27 0a0a 0a64 6566  1]) + '\n'...def
-00000970: 205f 6174 7472 5f72 6570 7228 7661 6c75   _attr_repr(valu
-00000980: 653a 2041 6e79 293a 0a20 2069 6620 6361  e: Any):.  if ca
-00000990: 6c6c 6162 6c65 2876 616c 7565 2920 616e  llable(value) an
-000009a0: 6420 280a 2020 2020 2020 2869 7369 6e73  d (.      (isins
-000009b0: 7461 6e63 6528 7661 6c75 652c 206e 6e2e  tance(value, nn.
-000009c0: 4d6f 6475 6c65 2920 616e 6420 7661 6c75  Module) and valu
-000009d0: 652e 5f5f 6469 6374 5f5f 2e67 6574 2827  e.__dict__.get('
-000009e0: 5f5f 6e61 6d65 5f5f 272c 204e 6f6e 6529  __name__', None)
-000009f0: 290a 2020 2020 2020 6f72 2028 6e6f 7420  ).      or (not 
-00000a00: 6973 696e 7374 616e 6365 2876 616c 7565  isinstance(value
-00000a10: 2c20 6e6e 2e4d 6f64 756c 6529 2061 6e64  , nn.Module) and
-00000a20: 2067 6574 6174 7472 2876 616c 7565 2c20   getattr(value, 
-00000a30: 275f 5f6e 616d 655f 5f27 2c20 4e6f 6e65  '__name__', None
-00000a40: 2929 0a20 2029 3a0a 2020 2020 7661 6c75  )).  ):.    valu
-00000a50: 655f 7265 7020 3d20 7661 6c75 652e 5f5f  e_rep = value.__
-00000a60: 6e61 6d65 5f5f 0a20 2065 6c73 653a 0a20  name__.  else:. 
-00000a70: 2020 2076 616c 7565 5f72 6570 203d 2072     value_rep = r
-00000a80: 6570 7228 7661 6c75 6529 0a20 2072 6574  epr(value).  ret
-00000a90: 7572 6e20 7661 6c75 655f 7265 700a 0a0a  urn value_rep...
-00000aa0: 6465 6620 5f6d 6f64 756c 655f 7265 7072  def _module_repr
-00000ab0: 286d 6f64 756c 653a 2027 4d6f 6475 6c65  (module: 'Module
-00000ac0: 272c 206e 756d 5f73 7061 6365 733a 2069  ', num_spaces: i
-00000ad0: 6e74 203d 2034 293a 0a20 2022 2222 5265  nt = 4):.  """Re
-00000ae0: 7475 726e 7320 6120 7072 6574 7479 2070  turns a pretty p
-00000af0: 7269 6e74 6564 2072 6570 7265 7365 6e74  rinted represent
-00000b00: 6174 696f 6e20 6f66 2074 6865 206d 6f64  ation of the mod
-00000b10: 756c 652e 2222 220a 2020 636c 7320 3d20  ule.""".  cls = 
-00000b20: 7479 7065 286d 6f64 756c 6529 0a20 2063  type(module).  c
-00000b30: 6c73 5f6e 616d 6520 3d20 636c 732e 5f5f  ls_name = cls.__
-00000b40: 6e61 6d65 5f5f 0a20 2072 6570 203d 2027  name__.  rep = '
-00000b50: 270a 0a20 2061 7474 7269 6275 7465 7320  '..  attributes 
-00000b60: 3d20 7b0a 2020 2020 2020 662e 6e61 6d65  = {.      f.name
-00000b70: 3a20 662e 7479 7065 0a20 2020 2020 2066  : f.type.      f
-00000b80: 6f72 2066 2069 6e20 6461 7461 636c 6173  or f in dataclas
-00000b90: 7365 732e 6669 656c 6473 2863 6c73 290a  ses.fields(cls).
-00000ba0: 2020 2020 2020 6966 2066 2e6e 616d 6520        if f.name 
-00000bb0: 6e6f 7420 696e 2028 2770 6172 656e 7427  not in ('parent'
-00000bc0: 2c20 276e 616d 6527 2920 616e 6420 662e  , 'name') and f.
-00000bd0: 7265 7072 0a20 207d 0a20 2063 6869 6c64  repr.  }.  child
-00000be0: 5f6d 6f64 756c 6573 203d 207b 6b3a 2076  _modules = {k: v
-00000bf0: 2066 6f72 206b 2c20 7620 696e 206d 6f64   for k, v in mod
-00000c00: 756c 652e 5f73 7461 7465 2e63 6869 6c64  ule._state.child
-00000c10: 7265 6e2e 6974 656d 7328 2920 2023 2070  ren.items()  # p
-00000c20: 7974 7970 653a 2064 6973 6162 6c65 3d61  ytype: disable=a
-00000c30: 7474 7269 6275 7465 2d65 7272 6f72 0a20  ttribute-error. 
-00000c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c50: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
-00000c60: 762c 204d 6f64 756c 6529 7d0a 2020 6966  v, Module)}.  if
-00000c70: 2061 7474 7269 6275 7465 733a 0a20 2020   attributes:.   
-00000c80: 2072 6570 202b 3d20 2723 2061 7474 7269   rep += '# attri
-00000c90: 6275 7465 735c 6e27 0a20 2020 2066 6f72  butes\n'.    for
-00000ca0: 2061 7474 7220 696e 2061 7474 7269 6275   attr in attribu
-00000cb0: 7465 732e 6b65 7973 2829 3a0a 2020 2020  tes.keys():.    
-00000cc0: 2020 2320 544f 444f 286a 6865 656b 293a    # TODO(jheek):
-00000cd0: 2063 616e 2077 6520 6765 7420 6120 6e69   can we get a ni
-00000ce0: 6365 2073 7472 696e 6720 7265 7072 6573  ce string repres
-00000cf0: 656e 7461 7469 6f6e 206f 6620 6174 7472  entation of attr
-00000d00: 6962 7574 6520 7479 7065 733f 0a20 2020  ibute types?.   
-00000d10: 2020 2076 616c 7565 203d 206d 6f64 756c     value = modul
-00000d20: 652e 5f5f 6469 6374 5f5f 2e67 6574 2861  e.__dict__.get(a
-00000d30: 7474 722c 204e 6f6e 6529 0a20 2020 2020  ttr, None).     
-00000d40: 2076 616c 7565 5f72 6570 203d 205f 6174   value_rep = _at
-00000d50: 7472 5f72 6570 7228 7661 6c75 6529 0a20  tr_repr(value). 
-00000d60: 2020 2020 2072 6570 202b 3d20 6627 7b61       rep += f'{a
-00000d70: 7474 727d 203d 207b 7661 6c75 655f 7265  ttr} = {value_re
-00000d80: 707d 5c6e 270a 2020 6966 2063 6869 6c64  p}\n'.  if child
-00000d90: 5f6d 6f64 756c 6573 3a0a 2020 2020 7265  _modules:.    re
-00000da0: 7020 2b3d 2027 2320 6368 696c 6472 656e  p += '# children
-00000db0: 5c6e 270a 2020 2020 666f 7220 6e61 6d65  \n'.    for name
-00000dc0: 2c20 6368 696c 6420 696e 2063 6869 6c64  , child in child
-00000dd0: 5f6d 6f64 756c 6573 2e69 7465 6d73 2829  _modules.items()
-00000de0: 3a0a 2020 2020 2020 6368 696c 645f 7265  :.      child_re
-00000df0: 7020 3d20 5f6d 6f64 756c 655f 7265 7072  p = _module_repr
-00000e00: 2863 6869 6c64 2c20 6e75 6d5f 7370 6163  (child, num_spac
-00000e10: 6573 290a 2020 2020 2020 7265 7020 2b3d  es).      rep +=
-00000e20: 2066 277b 6e61 6d65 7d20 3d20 7b63 6869   f'{name} = {chi
-00000e30: 6c64 5f72 6570 7d5c 6e27 0a20 2069 6620  ld_rep}\n'.  if 
-00000e40: 7265 703a 0a20 2020 2072 6574 7572 6e20  rep:.    return 
-00000e50: 6627 7b63 6c73 5f6e 616d 657d 285c 6e7b  f'{cls_name}(\n{
-00000e60: 5f69 6e64 656e 7428 7265 702c 206e 756d  _indent(rep, num
-00000e70: 5f73 7061 6365 7329 7d29 270a 2020 656c  _spaces)})'.  el
-00000e80: 7365 3a0a 2020 2020 7265 7475 726e 2066  se:.    return f
-00000e90: 277b 636c 735f 6e61 6d65 7d28 2927 0a0a  '{cls_name}()'..
-00000ea0: 2320 5461 6275 6c61 7469 6f6e 2075 7469  # Tabulation uti
-00000eb0: 6c69 7469 6573 2e0a 2320 2d2d 2d2d 2d2d  lities..# ------
-00000ec0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ed0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ee0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ef0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f00: 2d2d 2d2d 2d2d 2d0a 0a5f 6669 6e64 5f6e  -------.._find_n
-00000f10: 6f6e 5f6c 6966 7465 645f 6d6f 6475 6c65  on_lifted_module
-00000f20: 203d 2072 652e 636f 6d70 696c 6528 7227   = re.compile(r'
-00000f30: 2e2a 5c28 282e 2a29 5c29 2729 0a0a 6465  .*\((.*)\)')..de
-00000f40: 6620 5f66 6978 5f70 6174 685f 7061 7274  f _fix_path_part
-00000f50: 2870 6172 743a 2073 7472 293a 0a20 2022  (part: str):.  "
-00000f60: 2222 4669 7865 7320 6120 7061 7468 2070  ""Fixes a path p
-00000f70: 6172 7420 6279 2072 656d 6f76 696e 6720  art by removing 
-00000f80: 7472 616e 7366 6f72 6d61 7469 6f6e 206e  transformation n
-00000f90: 616d 6520 616e 6420 7061 7265 6e74 6865  ame and parenthe
-00000fa0: 7369 7320 736f 6d65 7469 6d65 730a 2020  sis sometimes.  
-00000fb0: 696e 7365 7274 6564 2062 7920 6c69 6674  inserted by lift
-00000fc0: 6564 2074 7261 6e73 666f 726d 6174 696f  ed transformatio
-00000fd0: 6e73 2222 220a 2020 6d61 7463 6820 3d20  ns""".  match = 
-00000fe0: 5f66 696e 645f 6e6f 6e5f 6c69 6674 6564  _find_non_lifted
-00000ff0: 5f6d 6f64 756c 652e 6d61 7463 6828 7061  _module.match(pa
-00001000: 7274 290a 2020 6966 206d 6174 6368 3a0a  rt).  if match:.
-00001010: 2020 2020 7265 7475 726e 206d 6174 6368      return match
-00001020: 2e67 726f 7570 2831 290a 2020 7265 7475  .group(1).  retu
-00001030: 726e 2070 6172 740a 0a40 6461 7461 636c  rn part..@datacl
-00001040: 6173 7365 732e 6461 7461 636c 6173 730a  asses.dataclass.
-00001050: 636c 6173 7320 5f43 616c 6c49 6e66 6f3a  class _CallInfo:
-00001060: 0a20 2069 6e64 6578 3a20 696e 740a 2020  .  index: int.  
-00001070: 7061 7468 3a20 5475 706c 655b 7374 722c  path: Tuple[str,
-00001080: 202e 2e2e 5d0a 2020 6d6f 6475 6c65 5f74   ...].  module_t
-00001090: 7970 653a 2054 7970 655b 274d 6f64 756c  ype: Type['Modul
-000010a0: 6527 5d0a 2020 6d65 7468 6f64 3a20 7374  e'].  method: st
-000010b0: 720a 2020 6172 6773 3a20 5475 706c 655b  r.  args: Tuple[
-000010c0: 416e 792c 202e 2e2e 5d0a 2020 6b77 6172  Any, ...].  kwar
-000010d0: 6773 3a20 4469 6374 5b73 7472 2c20 416e  gs: Dict[str, An
-000010e0: 795d 0a20 206f 7574 7075 7473 3a20 416e  y].  outputs: An
-000010f0: 790a 0a40 6461 7461 636c 6173 7365 732e  y..@dataclasses.
-00001100: 6461 7461 636c 6173 730a 636c 6173 7320  dataclass.class 
-00001110: 5f43 616c 6c49 6e66 6f43 6f6e 7465 7874  _CallInfoContext
-00001120: 2874 6872 6561 6469 6e67 2e6c 6f63 616c  (threading.local
-00001130: 293a 0a20 2069 6e64 6578 3a20 696e 740a  ):.  index: int.
-00001140: 2020 6361 6c6c 733a 204c 6973 745b 5f43    calls: List[_C
-00001150: 616c 6c49 6e66 6f5d 0a0a 2020 6465 6620  allInfo]..  def 
-00001160: 6765 745f 6361 6c6c 5f69 6e64 6578 2873  get_call_index(s
-00001170: 656c 662c 206d 6f64 756c 653a 2027 4d6f  elf, module: 'Mo
-00001180: 6475 6c65 2729 202d 3e20 696e 743a 0a20  dule') -> int:. 
-00001190: 2020 2069 6e64 6578 203d 2073 656c 662e     index = self.
-000011a0: 696e 6465 780a 2020 2020 7365 6c66 2e69  index.    self.i
-000011b0: 6e64 6578 202b 3d20 310a 2020 2020 7265  ndex += 1.    re
-000011c0: 7475 726e 2069 6e64 6578 0a0a 4063 6f6e  turn index..@con
-000011d0: 7465 7874 6c69 622e 636f 6e74 6578 746d  textlib.contextm
-000011e0: 616e 6167 6572 0a64 6566 205f 7461 6275  anager.def _tabu
-000011f0: 6c61 7465 5f63 6f6e 7465 7874 2829 3a0a  late_context():.
-00001200: 2020 5f63 6f6e 7465 7874 2e63 616c 6c5f    _context.call_
-00001210: 696e 666f 5f73 7461 636b 2e61 7070 656e  info_stack.appen
-00001220: 6428 5f43 616c 6c49 6e66 6f43 6f6e 7465  d(_CallInfoConte
-00001230: 7874 2830 2c20 5b5d 2929 0a20 2074 7279  xt(0, [])).  try
-00001240: 3a0a 2020 2020 7969 656c 640a 2020 6669  :.    yield.  fi
-00001250: 6e61 6c6c 793a 0a20 2020 205f 636f 6e74  nally:.    _cont
-00001260: 6578 742e 6361 6c6c 5f69 6e66 6f5f 7374  ext.call_info_st
-00001270: 6163 6b2e 706f 7028 290a 0a23 2054 7261  ack.pop()..# Tra
-00001280: 636b 2070 6172 656e 7420 7265 6c61 7469  ck parent relati
-00001290: 6f6e 7368 6970 2061 6372 6f73 7320 4d6f  onship across Mo
-000012a0: 6475 6c65 732e 0a23 202d 2d2d 2d2d 2d2d  dules..# -------
-000012b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012f0: 2d2d 2d2d 2d2d 0a63 6c61 7373 205f 4479  ------.class _Dy
-00001300: 6e61 6d69 6343 6f6e 7465 7874 2874 6872  namicContext(thr
-00001310: 6561 6469 6e67 2e6c 6f63 616c 293a 0a20  eading.local):. 
-00001320: 2022 2222 4479 6e61 6d69 6320 636f 6e74   """Dynamic cont
-00001330: 6578 742e 2222 220a 2020 2320 544f 444f  ext.""".  # TODO
-00001340: 286d 6172 6376 616e 7a65 6529 3a20 7377  (marcvanzee): sw
-00001350: 6974 6368 2074 6f20 7573 696e 6720 636f  itch to using co
-00001360: 6e74 6578 7476 6172 7320 6f6e 6365 206d  ntextvars once m
-00001370: 696e 696d 756d 2070 7974 686f 6e20 7665  inimum python ve
-00001380: 7273 696f 6e20 6973 0a20 2023 2033 2e37  rsion is.  # 3.7
-00001390: 0a0a 2020 6465 6620 5f5f 696e 6974 5f5f  ..  def __init__
-000013a0: 2873 656c 6629 3a0a 2020 2020 7365 6c66  (self):.    self
-000013b0: 2e6d 6f64 756c 655f 7374 6163 6b20 3d20  .module_stack = 
-000013c0: 5b4e 6f6e 652c 5d0a 2020 2020 7365 6c66  [None,].    self
-000013d0: 2e63 6170 7475 7265 5f73 7461 636b 203d  .capture_stack =
-000013e0: 205b 5d0a 2020 2020 7365 6c66 2e63 616c   [].    self.cal
-000013f0: 6c5f 696e 666f 5f73 7461 636b 203d 205b  l_info_stack = [
-00001400: 5d0a 0a23 2054 6865 2067 6c6f 6261 6c20  ]..# The global 
-00001410: 636f 6e74 6578 740a 5f63 6f6e 7465 7874  context._context
-00001420: 203d 205f 4479 6e61 6d69 6343 6f6e 7465   = _DynamicConte
-00001430: 7874 2829 0a0a 0a63 6c61 7373 205f 5365  xt()...class _Se
-00001440: 6e74 696e 656c 3a0a 0a20 2064 6566 205f  ntinel:..  def _
-00001450: 5f63 6f70 795f 5f28 7365 6c66 293a 0a20  _copy__(self):. 
-00001460: 2020 2072 6574 7572 6e20 7365 6c66 2020     return self  
-00001470: 2320 446f 206e 6f74 2063 6f70 7920 7369  # Do not copy si
-00001480: 6e67 6c65 746f 6e20 7365 6e74 696e 656c  ngleton sentinel
-00001490: 2e0a 0a20 2064 6566 205f 5f64 6565 7063  ...  def __deepc
-000014a0: 6f70 795f 5f28 7365 6c66 2c20 6d65 6d6f  opy__(self, memo
-000014b0: 293a 0a20 2020 2064 656c 206d 656d 6f0a  ):.    del memo.
-000014c0: 2020 2020 7265 7475 726e 2073 656c 6620      return self 
-000014d0: 2023 2044 6f20 6e6f 7420 636f 7079 2073   # Do not copy s
-000014e0: 696e 676c 6574 6f6e 2073 656e 7469 6e65  ingleton sentine
-000014f0: 6c2e 0a0a 0a5f 756e 7370 6563 6966 6965  l...._unspecifie
-00001500: 645f 7061 7265 6e74 203d 205f 5365 6e74  d_parent = _Sent
-00001510: 696e 656c 2829 0a0a 0a23 2045 6e61 626c  inel()...# Enabl
-00001520: 6520 6175 746f 6d61 7469 6320 6e61 6d65  e automatic name
-00001530: 645f 6361 6c6c 2077 7261 7070 696e 6720  d_call wrapping 
-00001540: 666f 7220 6c61 6265 6c6c 696e 6720 7072  for labelling pr
-00001550: 6f66 696c 6520 7472 6163 6573 2e0a 2320  ofile traces..# 
-00001560: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001570: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001580: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001590: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000015a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 5f75  -------------._u
-000015b0: 7365 5f6e 616d 6564 5f63 616c 6c20 3d20  se_named_call = 
-000015c0: 636f 6e66 6967 2e66 6c61 785f 7072 6f66  config.flax_prof
-000015d0: 696c 650a 0a0a 6465 6620 5f64 6572 6976  ile...def _deriv
-000015e0: 655f 7072 6f66 696c 696e 675f 6e61 6d65  e_profiling_name
-000015f0: 286d 6f64 756c 652c 2066 6e29 3a0a 2020  (module, fn):.  
-00001600: 6465 6620 5f67 6574 5f66 6e5f 6e61 6d65  def _get_fn_name
-00001610: 2866 6e29 3a0a 2020 2020 6966 2069 7369  (fn):.    if isi
-00001620: 6e73 7461 6e63 6528 666e 2c20 6675 6e63  nstance(fn, func
-00001630: 746f 6f6c 732e 7061 7274 6961 6c29 3a0a  tools.partial):.
-00001640: 2020 2020 2020 7265 7475 726e 205f 6765        return _ge
-00001650: 745f 666e 5f6e 616d 6528 666e 2e66 756e  t_fn_name(fn.fun
-00001660: 6329 0a20 2020 2072 6574 7572 6e20 666e  c).    return fn
-00001670: 2e5f 5f6e 616d 655f 5f0a 2020 666e 5f6e  .__name__.  fn_n
-00001680: 616d 6520 3d20 5f67 6574 5f66 6e5f 6e61  ame = _get_fn_na
-00001690: 6d65 2866 6e29 0a20 206d 6574 686f 645f  me(fn).  method_
-000016a0: 7375 6666 6978 203d 2066 272e 7b66 6e5f  suffix = f'.{fn_
-000016b0: 6e61 6d65 7d27 2069 6620 666e 5f6e 616d  name}' if fn_nam
-000016c0: 6520 213d 2027 5f5f 6361 6c6c 5f5f 2720  e != '__call__' 
-000016d0: 656c 7365 2027 270a 2020 6d6f 6475 6c65  else ''.  module
-000016e0: 5f6e 616d 6520 3d20 6d6f 6475 6c65 2e6e  _name = module.n
-000016f0: 616d 6520 6f72 206d 6f64 756c 652e 5f5f  ame or module.__
-00001700: 636c 6173 735f 5f2e 5f5f 6e61 6d65 5f5f  class__.__name__
-00001710: 0a20 2072 6574 7572 6e20 6627 7b6d 6f64  .  return f'{mod
-00001720: 756c 655f 6e61 6d65 7d7b 6d65 7468 6f64  ule_name}{method
-00001730: 5f73 7566 6669 787d 270a 0a0a 6465 6620  _suffix}'...def 
-00001740: 656e 6162 6c65 5f6e 616d 6564 5f63 616c  enable_named_cal
-00001750: 6c28 293a 0a20 2022 2222 456e 6162 6c65  l():.  """Enable
-00001760: 7320 6e61 6d65 6420 6361 6c6c 2077 7261  s named call wra
-00001770: 7070 696e 6720 666f 7220 6c61 6265 6c6c  pping for labell
-00001780: 696e 6720 7072 6f66 696c 6520 7472 6163  ing profile trac
-00001790: 6573 2e0a 0a20 2057 6865 6e20 6e61 6d65  es...  When name
-000017a0: 6420 6361 6c6c 2077 7261 7070 696e 6720  d call wrapping 
-000017b0: 6973 2065 6e61 626c 6564 2061 6c6c 204a  is enabled all J
-000017c0: 4158 206f 7073 2065 7865 6375 7465 6420  AX ops executed 
-000017d0: 696e 2061 204d 6f64 756c 650a 2020 7769  in a Module.  wi
-000017e0: 6c6c 2062 6520 7275 6e20 756e 6465 7220  ll be run under 
-000017f0: 6060 6a61 782e 6e61 6d65 645f 7363 6f70  ``jax.named_scop
-00001800: 6560 602e 2054 6865 2060 604d 6f64 756c  e``. The ``Modul
-00001810: 6560 6020 636c 6173 7320 6e61 6d65 2077  e`` class name w
-00001820: 696c 6c0a 2020 7368 6f77 2075 7020 6172  ill.  show up ar
-00001830: 6f75 6e64 2074 6865 206f 7065 7261 7469  ound the operati
-00001840: 6f6e 7320 6265 6c6f 6e67 696e 6720 746f  ons belonging to
-00001850: 2074 6861 7420 4d6f 6475 6c65 2069 6e20   that Module in 
-00001860: 7468 650a 2020 5465 6e73 6f72 626f 6172  the.  Tensorboar
-00001870: 6420 7072 6f66 696c 696e 6720 5549 2c20  d profiling UI, 
-00001880: 7369 6d70 6c69 6679 696e 6720 7468 6520  simplifying the 
-00001890: 7072 6f66 696c 696e 6720 7072 6f63 6573  profiling proces
-000018a0: 732e 0a0a 2020 4e6f 7465 2074 6861 7420  s...  Note that 
-000018b0: 6060 6a61 782e 6e61 6d65 645f 7363 6f70  ``jax.named_scop
-000018c0: 6560 6020 6f6e 6c79 2077 6f72 6b73 2066  e`` only works f
-000018d0: 6f72 0a20 2063 6f6d 7069 6c65 6420 6675  or.  compiled fu
-000018e0: 6e63 7469 6f6e 7320 2865 2e67 2e3a 2075  nctions (e.g.: u
-000018f0: 7369 6e67 206a 6178 2e6a 6974 206f 7220  sing jax.jit or 
-00001900: 6a61 782e 706d 6170 292e 0a20 2022 2222  jax.pmap)..  """
-00001910: 0a20 2067 6c6f 6261 6c20 5f75 7365 5f6e  .  global _use_n
-00001920: 616d 6564 5f63 616c 6c0a 2020 5f75 7365  amed_call.  _use
-00001930: 5f6e 616d 6564 5f63 616c 6c20 3d20 5472  _named_call = Tr
-00001940: 7565 0a0a 0a64 6566 2064 6973 6162 6c65  ue...def disable
-00001950: 5f6e 616d 6564 5f63 616c 6c28 293a 0a20  _named_call():. 
-00001960: 2022 2222 4469 7361 626c 6573 206e 616d   """Disables nam
-00001970: 6564 2063 616c 6c20 7772 6170 7069 6e67  ed call wrapping
-00001980: 2e0a 0a20 2053 6565 2060 6065 6e61 626c  ...  See ``enabl
-00001990: 655f 6e61 6d65 645f 6361 6c6c 6060 0a20  e_named_call``. 
-000019a0: 2022 2222 0a20 2067 6c6f 6261 6c20 5f75   """.  global _u
-000019b0: 7365 5f6e 616d 6564 5f63 616c 6c0a 2020  se_named_call.  
-000019c0: 5f75 7365 5f6e 616d 6564 5f63 616c 6c20  _use_named_call 
-000019d0: 3d20 4661 6c73 650a 0a0a 4063 6f6e 7465  = False...@conte
-000019e0: 7874 6c69 622e 636f 6e74 6578 746d 616e  xtlib.contextman
-000019f0: 6167 6572 0a64 6566 206f 7665 7272 6964  ager.def overrid
-00001a00: 655f 6e61 6d65 645f 6361 6c6c 2865 6e61  e_named_call(ena
-00001a10: 626c 653a 2062 6f6f 6c20 3d20 5472 7565  ble: bool = True
-00001a20: 293a 0a20 2023 2070 796c 696e 743a 2064  ):.  # pylint: d
-00001a30: 6973 6162 6c65 3d67 2d64 6f63 2d72 6574  isable=g-doc-ret
-00001a40: 7572 6e2d 6f72 2d79 6965 6c64 0a20 2022  urn-or-yield.  "
-00001a50: 2222 5265 7475 726e 7320 6120 636f 6e74  ""Returns a cont
-00001a60: 6578 7420 6d61 6e61 6765 7220 7468 6174  ext manager that
-00001a70: 2065 6e61 626c 6573 2f64 6973 6162 6c65   enables/disable
-00001a80: 7320 6e61 6d65 6420 6361 6c6c 2077 7261  s named call wra
-00001a90: 7070 696e 672e 0a0a 2020 4172 6773 3a0a  pping...  Args:.
-00001aa0: 2020 2020 656e 6162 6c65 3a20 4966 2074      enable: If t
-00001ab0: 7275 652c 2065 6e61 626c 6573 206e 616d  rue, enables nam
-00001ac0: 6564 2063 616c 6c20 7772 6170 7069 6e67  ed call wrapping
-00001ad0: 2066 6f72 206c 6162 656c 6c69 6e67 2070   for labelling p
-00001ae0: 726f 6669 6c65 2074 7261 6365 732e 0a20  rofile traces.. 
-00001af0: 2020 2020 2028 7365 6520 6060 656e 6162       (see ``enab
-00001b00: 6c65 645f 6e61 6d65 645f 6361 6c6c 6060  led_named_call``
-00001b10: 292e 0a20 2022 2222 0a20 2023 2070 796c  )..  """.  # pyl
-00001b20: 696e 743a 2065 6e61 626c 653d 672d 646f  int: enable=g-do
-00001b30: 632d 7265 7475 726e 2d6f 722d 7969 656c  c-return-or-yiel
-00001b40: 640a 2020 676c 6f62 616c 205f 7573 655f  d.  global _use_
-00001b50: 6e61 6d65 645f 6361 6c6c 0a20 2075 7365  named_call.  use
-00001b60: 5f6e 616d 6564 5f63 616c 6c5f 7072 6576  _named_call_prev
-00001b70: 203d 205f 7573 655f 6e61 6d65 645f 6361   = _use_named_ca
-00001b80: 6c6c 0a20 205f 7573 655f 6e61 6d65 645f  ll.  _use_named_
-00001b90: 6361 6c6c 203d 2065 6e61 626c 650a 2020  call = enable.  
-00001ba0: 7472 793a 0a20 2020 2079 6965 6c64 0a20  try:.    yield. 
-00001bb0: 2066 696e 616c 6c79 3a0a 2020 2020 5f75   finally:.    _u
-00001bc0: 7365 5f6e 616d 6564 5f63 616c 6c20 3d20  se_named_call = 
-00001bd0: 7573 655f 6e61 6d65 645f 6361 6c6c 5f70  use_named_call_p
-00001be0: 7265 760a 0a0a 2320 5574 696c 6974 6965  rev...# Utilitie
-00001bf0: 7320 666f 7220 7079 7472 6565 7320 6f66  s for pytrees of
-00001c00: 204d 6f64 756c 6573 2064 6566 696e 6564   Modules defined
-00001c10: 2069 6e73 6964 6520 7365 7475 7028 290a   inside setup().
-00001c20: 2320 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  # --------------
-00001c30: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001c40: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001c50: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001c60: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a  ---------------.
-00001c70: 0a0a 6465 6620 5f73 6f72 7465 645f 6974  ..def _sorted_it
-00001c80: 656d 7328 7829 3a0a 2020 2222 2252 6574  ems(x):.  """Ret
-00001c90: 7572 6e73 2069 7465 6d73 206f 6620 6120  urns items of a 
-00001ca0: 6469 6374 206f 7264 6572 6564 2062 7920  dict ordered by 
-00001cb0: 6b65 7973 2e22 2222 0a20 2072 6574 7572  keys.""".  retur
-00001cc0: 6e20 736f 7274 6564 2878 2e69 7465 6d73  n sorted(x.items
-00001cd0: 2829 2c20 6b65 793d 6c61 6d62 6461 2078  (), key=lambda x
-00001ce0: 3a20 785b 305d 290a 0a0a 6465 6620 5f67  : x[0])...def _g
-00001cf0: 6574 5f73 7566 6669 785f 7661 6c75 655f  et_suffix_value_
-00001d00: 7061 6972 7328 0a20 2020 2074 7265 655f  pairs(.    tree_
-00001d10: 6f72 5f6c 6561 663a 2041 6e79 2920 2d3e  or_leaf: Any) ->
-00001d20: 204c 6973 745b 5475 706c 655b 7374 722c   List[Tuple[str,
-00001d30: 2054 7970 655b 274d 6f64 756c 6527 5d5d   Type['Module']]
-00001d40: 5d3a 0a20 2022 2222 4865 6c70 6572 2066  ]:.  """Helper f
-00001d50: 6f72 206e 616d 696e 6720 7079 7472 6565  or naming pytree
-00001d60: 7320 6f66 2073 7562 6d6f 6475 6c65 732e  s of submodules.
-00001d70: 2222 220a 2020 6469 6374 5f6f 725f 6c65  """.  dict_or_le
-00001d80: 6166 203d 2073 6572 6961 6c69 7a61 7469  af = serializati
-00001d90: 6f6e 2e74 6f5f 7374 6174 655f 6469 6374  on.to_state_dict
-00001da0: 2874 7265 655f 6f72 5f6c 6561 6629 0a20  (tree_or_leaf). 
-00001db0: 2069 6620 6e6f 7420 6973 696e 7374 616e   if not isinstan
-00001dc0: 6365 2864 6963 745f 6f72 5f6c 6561 662c  ce(dict_or_leaf,
-00001dd0: 2064 6963 7429 206f 7220 6e6f 7420 6469   dict) or not di
-00001de0: 6374 5f6f 725f 6c65 6166 3a0a 2020 2020  ct_or_leaf:.    
-00001df0: 7265 7475 726e 205b 2827 272c 2074 7265  return [('', tre
-00001e00: 655f 6f72 5f6c 6561 6629 5d0a 2020 656c  e_or_leaf)].  el
-00001e10: 7365 3a0a 2020 2020 666c 6174 5f64 6963  se:.    flat_dic
-00001e20: 7420 3d20 7472 6176 6572 7365 5f75 7469  t = traverse_uti
-00001e30: 6c2e 666c 6174 7465 6e5f 6469 6374 2864  l.flatten_dict(d
-00001e40: 6963 745f 6f72 5f6c 6561 6629 0a20 2020  ict_or_leaf).   
-00001e50: 2072 6574 7572 6e20 5b28 275f 2720 2b20   return [('_' + 
-00001e60: 275f 272e 6a6f 696e 286b 292c 2076 2920  '_'.join(k), v) 
-00001e70: 666f 7220 6b2c 2076 2069 6e20 5f73 6f72  for k, v in _sor
-00001e80: 7465 645f 6974 656d 7328 666c 6174 5f64  ted_items(flat_d
-00001e90: 6963 7429 5d0a 0a0a 6465 6620 5f6d 6170  ict)]...def _map
-00001ea0: 5f6f 7665 725f 6d6f 6475 6c65 735f 696e  _over_modules_in
-00001eb0: 5f74 7265 6528 666e 2c20 7472 6565 5f6f  _tree(fn, tree_o
-00001ec0: 725f 6c65 6166 293a 0a20 2022 2222 4865  r_leaf):.  """He
-00001ed0: 6c70 6572 2066 6f72 206d 6170 7069 6e67  lper for mapping
-00001ee0: 2066 756e 6374 696f 6e20 6f76 6572 2073   function over s
-00001ef0: 7562 6d6f 6475 6c65 732e 2222 220a 2020  ubmodules.""".  
-00001f00: 6469 6374 5f6f 725f 6c65 6166 203d 2073  dict_or_leaf = s
-00001f10: 6572 6961 6c69 7a61 7469 6f6e 2e74 6f5f  erialization.to_
-00001f20: 7374 6174 655f 6469 6374 2874 7265 655f  state_dict(tree_
-00001f30: 6f72 5f6c 6561 6629 0a20 2069 6620 6e6f  or_leaf).  if no
-00001f40: 7420 6973 696e 7374 616e 6365 2864 6963  t isinstance(dic
-00001f50: 745f 6f72 5f6c 6561 662c 2064 6963 7429  t_or_leaf, dict)
-00001f60: 206f 7220 6e6f 7420 6469 6374 5f6f 725f   or not dict_or_
-00001f70: 6c65 6166 3a0a 2020 2020 7265 7475 726e  leaf:.    return
-00001f80: 2066 6e28 2727 2c20 7472 6565 5f6f 725f   fn('', tree_or_
-00001f90: 6c65 6166 290a 2020 656c 7365 3a0a 2020  leaf).  else:.  
-00001fa0: 2020 666c 6174 5f64 6963 7420 3d20 7472    flat_dict = tr
-00001fb0: 6176 6572 7365 5f75 7469 6c2e 666c 6174  averse_util.flat
-00001fc0: 7465 6e5f 6469 6374 2864 6963 745f 6f72  ten_dict(dict_or
-00001fd0: 5f6c 6561 662c 206b 6565 705f 656d 7074  _leaf, keep_empt
-00001fe0: 795f 6e6f 6465 733d 5472 7565 290a 2020  y_nodes=True).  
-00001ff0: 2020 6d61 7070 6564 5f66 6c61 745f 6469    mapped_flat_di
-00002000: 6374 203d 207b 6b3a 2066 6e28 275f 2720  ct = {k: fn('_' 
-00002010: 2b20 275f 272e 6a6f 696e 286b 292c 2076  + '_'.join(k), v
-00002020: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00002030: 2020 2020 2020 2020 2020 666f 7220 6b2c            for k,
-00002040: 2076 2069 6e20 5f73 6f72 7465 645f 6974   v in _sorted_it
-00002050: 656d 7328 666c 6174 5f64 6963 7429 7d0a  ems(flat_dict)}.
-00002060: 2020 2020 7265 7475 726e 2073 6572 6961      return seria
-00002070: 6c69 7a61 7469 6f6e 2e66 726f 6d5f 7374  lization.from_st
-00002080: 6174 655f 6469 6374 280a 2020 2020 2020  ate_dict(.      
-00002090: 2020 7472 6565 5f6f 725f 6c65 6166 2c20    tree_or_leaf, 
-000020a0: 7472 6176 6572 7365 5f75 7469 6c2e 756e  traverse_util.un
-000020b0: 666c 6174 7465 6e5f 6469 6374 286d 6170  flatten_dict(map
-000020c0: 7065 645f 666c 6174 5f64 6963 7429 290a  ped_flat_dict)).
-000020d0: 0a0a 6465 6620 5f66 7265 657a 655f 6174  ..def _freeze_at
-000020e0: 7472 2876 616c 3a20 416e 7929 202d 3e20  tr(val: Any) -> 
-000020f0: 416e 793a 0a20 2022 2222 5265 6375 7273  Any:.  """Recurs
-00002100: 6976 656c 7920 7772 6170 2074 6865 2067  ively wrap the g
-00002110: 6976 656e 2061 7474 7269 6275 7465 2060  iven attribute `
-00002120: 7661 7260 2069 6e20 6060 4672 6f7a 656e  var` in ``Frozen
-00002130: 4469 6374 6060 2e22 2222 0a20 2069 6620  Dict``.""".  if 
-00002140: 6973 696e 7374 616e 6365 2876 616c 2c20  isinstance(val, 
-00002150: 2864 6963 742c 2046 726f 7a65 6e44 6963  (dict, FrozenDic
-00002160: 7429 293a 0a20 2020 2072 6574 7572 6e20  t)):.    return 
-00002170: 4672 6f7a 656e 4469 6374 287b 6b3a 205f  FrozenDict({k: _
-00002180: 6672 6565 7a65 5f61 7474 7228 7629 2066  freeze_attr(v) f
-00002190: 6f72 206b 2c20 7620 696e 2076 616c 2e69  or k, v in val.i
-000021a0: 7465 6d73 2829 7d29 0a20 2065 6c69 6620  tems()}).  elif 
-000021b0: 6973 696e 7374 616e 6365 2876 616c 2c20  isinstance(val, 
-000021c0: 7475 706c 6529 3a0a 2020 2020 2320 5370  tuple):.    # Sp
-000021d0: 6563 6961 6c20 6361 7365 206e 616d 6564  ecial case named
-000021e0: 7475 706c 6573 2061 6e64 2073 7065 6369  tuples and speci
-000021f0: 616c 204a 4158 2074 7570 6c65 2073 7472  al JAX tuple str
-00002200: 7563 7475 7265 7320 6f74 6865 7277 6973  uctures otherwis
-00002210: 6520 7468 6579 0a20 2020 2023 2077 6f75  e they.    # wou
-00002220: 6c64 2062 6520 646f 776e 6772 6164 6564  ld be downgraded
-00002230: 2074 6f20 6e6f 726d 616c 2074 7570 6c65   to normal tuple
-00002240: 732e 0a20 2020 2069 6620 6861 7361 7474  s..    if hasatt
-00002250: 7228 7661 6c2c 2027 5f66 6965 6c64 7327  r(val, '_fields'
-00002260: 2920 6f72 2074 7970 6528 7661 6c29 2e5f  ) or type(val)._
-00002270: 5f6e 616d 655f 5f20 3d3d 2027 5061 7274  _name__ == 'Part
-00002280: 6974 696f 6e53 7065 6327 3a0a 2020 2020  itionSpec':.    
-00002290: 2020 7265 7475 726e 2074 7970 6528 7661    return type(va
-000022a0: 6c29 282a 5b5f 6672 6565 7a65 5f61 7474  l)(*[_freeze_att
-000022b0: 7228 7629 2066 6f72 2076 2069 6e20 7661  r(v) for v in va
-000022c0: 6c5d 290a 2020 2020 656c 7365 3a0a 2020  l]).    else:.  
-000022d0: 2020 2020 7265 7475 726e 2074 7570 6c65      return tuple
-000022e0: 285f 6672 6565 7a65 5f61 7474 7228 7629  (_freeze_attr(v)
-000022f0: 2066 6f72 2076 2069 6e20 7661 6c29 0a20   for v in val). 
-00002300: 2065 6c69 6620 6973 696e 7374 616e 6365   elif isinstance
-00002310: 2876 616c 2c20 6c69 7374 293a 0a20 2020  (val, list):.   
-00002320: 2072 6574 7572 6e20 7475 706c 6528 5f66   return tuple(_f
-00002330: 7265 657a 655f 6174 7472 2876 2920 666f  reeze_attr(v) fo
-00002340: 7220 7620 696e 2076 616c 290a 2020 656c  r v in val).  el
-00002350: 7365 3a0a 2020 2020 7265 7475 726e 2076  se:.    return v
-00002360: 616c 0a0a 0a23 204d 6574 686f 6420 7772  al...# Method wr
-00002370: 6170 7069 6e67 206f 6620 2263 6f6d 7061  apping of "compa
-00002380: 6374 206d 6574 686f 6473 2220 616e 6420  ct methods" and 
-00002390: 7365 7475 7028 290a 2320 2d2d 2d2d 2d2d  setup().# ------
-000023a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000023b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000023c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000023d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000023e0: 2d2d 2d2d 2d2d 2d0a 6465 6620 636f 6d70  -------.def comp
-000023f0: 6163 7428 6675 6e3a 205f 4361 6c6c 6162  act(fun: _Callab
-00002400: 6c65 5429 202d 3e20 5f43 616c 6c61 626c  leT) -> _Callabl
-00002410: 6554 3a0a 2020 2222 224d 6172 6b73 2074  eT:.  """Marks t
-00002420: 6865 2067 6976 656e 206d 6f64 756c 6520  he given module 
-00002430: 6d65 7468 6f64 2061 6c6c 6f77 696e 6720  method allowing 
-00002440: 696e 6c69 6e65 6420 7375 626d 6f64 756c  inlined submodul
-00002450: 6573 2e0a 0a20 204d 6574 686f 6473 2077  es...  Methods w
-00002460: 7261 7070 6564 2069 6e20 4063 6f6d 7061  rapped in @compa
-00002470: 6374 2063 616e 2064 6566 696e 6520 7375  ct can define su
-00002480: 626d 6f64 756c 6573 2064 6972 6563 746c  bmodules directl
-00002490: 7920 7769 7468 696e 2074 6865 206d 6574  y within the met
-000024a0: 686f 642e 0a0a 2020 466f 7220 696e 7374  hod...  For inst
-000024b0: 616e 6365 3a3a 0a0a 2020 2020 4063 6f6d  ance::..    @com
-000024c0: 7061 6374 0a20 2020 205f 5f63 616c 6c5f  pact.    __call_
-000024d0: 5f28 7365 6c66 2c20 782c 2066 6561 7475  _(self, x, featu
-000024e0: 7265 7329 3a0a 2020 2020 2020 7820 3d20  res):.      x = 
-000024f0: 6e6e 2e44 656e 7365 2866 6561 7475 7265  nn.Dense(feature
-00002500: 7329 2878 290a 2020 2020 2020 2e2e 2e0a  s)(x).      ....
-00002510: 0a20 2041 7420 6d6f 7374 206f 6e65 206d  .  At most one m
-00002520: 6574 686f 6420 696e 2065 6163 6820 4d6f  ethod in each Mo
-00002530: 6475 6c65 206d 6179 2062 6520 7772 6170  dule may be wrap
-00002540: 7065 6420 7769 7468 2040 636f 6d70 6163  ped with @compac
-00002550: 742e 0a0a 2020 4172 6773 3a0a 2020 2020  t...  Args:.    
-00002560: 6675 6e3a 2054 6865 204d 6f64 756c 6520  fun: The Module 
-00002570: 6d65 7468 6f64 2074 6f20 6d61 726b 2061  method to mark a
-00002580: 7320 636f 6d70 6163 742e 0a20 2052 6574  s compact..  Ret
-00002590: 7572 6e73 3a0a 2020 2020 5468 6520 6769  urns:.    The gi
-000025a0: 7665 6e20 6675 6e63 7469 6f6e 2060 6675  ven function `fu
-000025b0: 6e60 206d 6172 6b65 6420 6173 2063 6f6d  n` marked as com
-000025c0: 7061 6374 2e0a 2020 2222 220a 2020 6675  pact..  """.  fu
-000025d0: 6e2e 636f 6d70 6163 7420 3d20 5472 7565  n.compact = True
-000025e0: 2020 2320 7479 7065 3a20 6967 6e6f 7265    # type: ignore
-000025f0: 5b61 7474 722d 6465 6669 6e65 645d 0a20  [attr-defined]. 
-00002600: 2072 6574 7572 6e20 6675 6e0a 0a0a 6465   return fun...de
-00002610: 6620 6e6f 7772 6170 2866 756e 3a20 5f43  f nowrap(fun: _C
-00002620: 616c 6c61 626c 6554 2920 2d3e 205f 4361  allableT) -> _Ca
-00002630: 6c6c 6162 6c65 543a 0a20 2022 2222 4d61  llableT:.  """Ma
-00002640: 726b 7320 7468 6520 6769 7665 6e20 6d6f  rks the given mo
-00002650: 6475 6c65 206d 6574 686f 6420 6173 2061  dule method as a
-00002660: 2068 656c 7065 7220 6d65 7468 6f64 2074   helper method t
-00002670: 6861 7420 6e65 6564 6e27 7420 6265 2077  hat needn't be w
-00002680: 7261 7070 6564 2e0a 0a20 204d 6574 686f  rapped...  Metho
-00002690: 6473 2077 7261 7070 6564 2069 6e20 406e  ds wrapped in @n
-000026a0: 6f77 7261 7020 6172 6520 7072 6976 6174  owrap are privat
-000026b0: 6520 6865 6c70 6572 206d 6574 686f 6473  e helper methods
-000026c0: 2074 6861 7420 6e65 6564 6e27 7420 6265   that needn't be
-000026d0: 2077 7261 7070 6564 0a20 2077 6974 6820   wrapped.  with 
-000026e0: 7468 6520 7374 6174 6520 6861 6e64 6c65  the state handle
-000026f0: 7220 6f72 2061 2073 6570 6172 6174 6520  r or a separate 
-00002700: 6e61 6d65 645f 6361 6c6c 2074 7261 6e73  named_call trans
-00002710: 666f 726d 2e0a 0a20 2054 6869 7320 6973  form...  This is
-00002720: 206e 6565 6465 6420 696e 2073 6576 6572   needed in sever
-00002730: 616c 2063 6f6e 6372 6574 6520 696e 7374  al concrete inst
-00002740: 616e 6365 733a 0a20 2020 2d20 6966 2079  ances:.   - if y
-00002750: 6f75 2772 6520 7375 6263 6c61 7373 696e  ou're subclassin
-00002760: 6720 6120 6d65 7468 6f64 206c 696b 6520  g a method like 
-00002770: 4d6f 6475 6c65 2e70 6172 616d 2061 6e64  Module.param and
-00002780: 2064 6f6e 2774 2077 616e 7420 7468 6973   don't want this
-00002790: 0a20 2020 2020 6f76 6572 7269 6465 6e20  .     overriden 
-000027a0: 636f 7265 2066 756e 6374 696f 6e20 6465  core function de
-000027b0: 636f 7261 7465 6420 7769 7468 2074 6865  corated with the
-000027c0: 2073 7461 7465 206d 616e 6167 656d 656e   state managemen
-000027d0: 7420 7772 6170 7065 722e 0a20 2020 2d20  t wrapper..   - 
-000027e0: 4966 2079 6f75 2077 616e 7420 6120 6d65  If you want a me
-000027f0: 7468 6f64 2074 6f20 6265 2063 616c 6c61  thod to be calla
-00002800: 626c 6520 6672 6f6d 2061 6e20 756e 626f  ble from an unbo
-00002810: 756e 6420 4d6f 6475 6c65 2028 652e 672e  und Module (e.g.
-00002820: 3a20 610a 2020 2020 2066 756e 6374 696f  : a.     functio
-00002830: 6e20 6f66 2063 6f6e 7374 7275 6374 696f  n of constructio
-00002840: 6e20 6f66 2061 7267 756d 656e 7473 2074  n of arguments t
-00002850: 6861 7420 646f 6573 6e27 7420 6465 7065  hat doesn't depe
-00002860: 6e64 206f 6e20 7061 7261 6d73 2f52 4e47  nd on params/RNG
-00002870: 7329 0a0a 2020 466f 7220 696e 7374 616e  s)..  For instan
-00002880: 6365 3a3a 0a0a 2020 2020 406e 6f77 7261  ce::..    @nowra
-00002890: 700a 2020 2020 6465 6620 5f6d 616b 655f  p.    def _make_
-000028a0: 6465 6e73 6528 7365 6c66 2c20 6e75 6d5f  dense(self, num_
-000028b0: 6665 6174 7572 6573 293a 0a20 2020 2020  features):.     
-000028c0: 2072 6574 7572 6e20 6e6e 2e44 656e 7365   return nn.Dense
-000028d0: 286e 756d 5f66 6561 7475 7265 7329 0a0a  (num_features)..
-000028e0: 2020 2020 4063 6f6d 7061 6374 0a20 2020      @compact.   
-000028f0: 2064 6566 205f 5f63 616c 6c5f 5f28 7365   def __call__(se
-00002900: 6c66 2c20 7829 3a0a 2020 2020 2020 2320  lf, x):.      # 
-00002910: 6e6f 7720 7361 6665 2074 6f20 7573 6520  now safe to use 
-00002920: 636f 6e73 7472 7563 746f 7220 6865 6c70  constructor help
-00002930: 6572 2065 7665 6e20 6966 2075 7369 6e67  er even if using
-00002940: 206e 616d 6564 5f63 616c 6c0a 2020 2020   named_call.    
-00002950: 2020 6465 6e73 6520 3d20 7365 6c66 2e5f    dense = self._
-00002960: 6d61 6b65 5f64 656e 7365 2873 656c 662e  make_dense(self.
-00002970: 6e75 6d5f 6665 6174 7572 6573 290a 2020  num_features).  
-00002980: 2020 2020 7265 7475 726e 2064 656e 7365      return dense
-00002990: 2878 290a 0a20 2041 7267 733a 0a20 2020  (x)..  Args:.   
-000029a0: 2066 756e 3a20 5468 6520 4d6f 6475 6c65   fun: The Module
-000029b0: 206d 6574 686f 6420 746f 206d 6172 6b20   method to mark 
-000029c0: 6173 206e 6f77 7261 702e 0a20 2052 6574  as nowrap..  Ret
-000029d0: 7572 6e73 3a0a 2020 2020 5468 6520 6769  urns:.    The gi
-000029e0: 7665 6e20 6675 6e63 7469 6f6e 2060 6675  ven function `fu
-000029f0: 6e60 206d 6172 6b65 6420 6173 206e 6f77  n` marked as now
-00002a00: 7261 702e 0a20 2022 2222 0a20 2066 756e  rap..  """.  fun
-00002a10: 2e6e 6f77 7261 7020 3d20 5472 7565 2020  .nowrap = True  
-00002a20: 2320 7479 7065 3a20 6967 6e6f 7265 5b61  # type: ignore[a
-00002a30: 7474 722d 6465 6669 6e65 645d 0a20 2072  ttr-defined].  r
-00002a40: 6574 7572 6e20 6675 6e0a 0a0a 6465 6620  eturn fun...def 
-00002a50: 5f67 6574 5f6c 6f63 616c 5f6d 6574 686f  _get_local_metho
-00002a60: 645f 6e61 6d65 7328 636c 733a 2041 6e79  d_names(cls: Any
-00002a70: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00002a80: 2020 2020 2020 2020 2020 2020 2020 6578                ex
-00002a90: 636c 7564 653a 2049 7465 7261 626c 655b  clude: Iterable[
-00002aa0: 7374 725d 203d 2028 2929 202d 3e20 5475  str] = ()) -> Tu
-00002ab0: 706c 655b 7374 722c 202e 2e2e 5d3a 0a20  ple[str, ...]:. 
-00002ac0: 2022 2222 4765 7473 206d 6574 686f 6420   """Gets method 
-00002ad0: 6e61 6d65 7320 6f66 2061 2063 6c61 7373  names of a class
-00002ae0: 2c20 6578 636c 7564 696e 6720 636c 6173  , excluding clas
-00002af0: 7320 616e 6420 7374 6174 6963 206d 6574  s and static met
-00002b00: 686f 6473 2e0a 0a20 2041 7267 733a 0a20  hods...  Args:. 
-00002b10: 2020 2063 6c73 3a20 5468 6520 636c 6173     cls: The clas
-00002b20: 7320 746f 2067 6574 206d 6574 686f 6420  s to get method 
-00002b30: 6e61 6d65 7320 666f 722e 0a20 2020 2065  names for..    e
-00002b40: 7863 6c75 6465 3a20 4e61 6d65 7320 746f  xclude: Names to
-00002b50: 2065 7863 6c75 6465 2066 726f 6d20 6f75   exclude from ou
-00002b60: 7470 7574 2e0a 2020 5265 7475 726e 733a  tput..  Returns:
-00002b70: 0a20 2020 2041 206c 6973 7420 6f66 206d  .    A list of m
-00002b80: 6574 686f 6420 6e61 6d65 732e 0a20 2022  ethod names..  "
-00002b90: 2222 0a20 2074 7275 655f 6d65 7468 6f64  "".  true_method
-00002ba0: 7320 3d20 7365 7428 290a 2020 666f 7220  s = set().  for 
-00002bb0: 6d20 696e 2063 6c73 2e5f 5f64 6963 745f  m in cls.__dict_
-00002bc0: 5f3a 0a20 2020 2069 6620 6361 6c6c 6162  _:.    if callab
-00002bd0: 6c65 2863 6c73 2e5f 5f64 6963 745f 5f5b  le(cls.__dict__[
-00002be0: 6d5d 2920 616e 6420 6e6f 7420 696e 7370  m]) and not insp
-00002bf0: 6563 742e 6973 636c 6173 7328 636c 732e  ect.isclass(cls.
-00002c00: 5f5f 6469 6374 5f5f 5b6d 5d29 3a20 2023  __dict__[m]):  #
-00002c10: 2070 7974 7970 653a 2064 6973 6162 6c65   pytype: disable
-00002c20: 3d6e 6f74 2d73 7570 706f 7274 6564 2d79  =not-supported-y
-00002c30: 6574 0a20 2020 2020 206d 7479 7065 203d  et.      mtype =
-00002c40: 2074 7970 6528 636c 732e 5f5f 6469 6374   type(cls.__dict
-00002c50: 5f5f 5b6d 5d29 0a20 2020 2020 2069 6620  __[m]).      if 
-00002c60: 6d74 7970 6520 213d 2073 7461 7469 636d  mtype != staticm
-00002c70: 6574 686f 6420 616e 6420 6d74 7970 6520  ethod and mtype 
-00002c80: 213d 2063 6c61 7373 6d65 7468 6f64 3a0a  != classmethod:.
-00002c90: 2020 2020 2020 2020 7472 7565 5f6d 6574          true_met
-00002ca0: 686f 6473 2e61 6464 286d 290a 2020 7265  hods.add(m).  re
-00002cb0: 7475 726e 2074 7570 6c65 2874 7275 655f  turn tuple(true_
-00002cc0: 6d65 7468 6f64 732e 6469 6666 6572 656e  methods.differen
-00002cd0: 6365 2873 6574 2865 7863 6c75 6465 2929  ce(set(exclude))
-00002ce0: 290a 0a64 6566 205f 6765 745f 6c6f 6361  )..def _get_loca
-00002cf0: 6c5f 6465 7363 7269 7074 6f72 5f6e 616d  l_descriptor_nam
-00002d00: 6573 2863 6c73 3a20 416e 792c 0a20 2020  es(cls: Any,.   
-00002d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002d20: 2020 2020 2020 2020 2020 2020 2065 7863               exc
-00002d30: 6c75 6465 3a20 4974 6572 6162 6c65 5b73  lude: Iterable[s
-00002d40: 7472 5d20 3d20 2829 2920 2d3e 2054 7570  tr] = ()) -> Tup
-00002d50: 6c65 5b73 7472 2c20 2e2e 2e5d 3a0a 2020  le[str, ...]:.  
-00002d60: 2222 2247 6574 7320 6465 7363 7269 7074  """Gets descript
-00002d70: 6f72 206e 616d 6573 206f 6620 6120 636c  or names of a cl
-00002d80: 6173 732e 0a0a 2020 4172 6773 3a0a 2020  ass...  Args:.  
-00002d90: 2020 636c 733a 2054 6865 2063 6c61 7373    cls: The class
-00002da0: 2074 6f20 6765 7420 7072 6f70 6572 7479   to get property
-00002db0: 206e 616d 6573 2066 6f72 2e0a 2020 2020   names for..    
-00002dc0: 6578 636c 7564 653a 204e 616d 6573 2074  exclude: Names t
-00002dd0: 6f20 6578 636c 7564 6520 6672 6f6d 206f  o exclude from o
-00002de0: 7574 7075 742e 0a20 2052 6574 7572 6e73  utput..  Returns
-00002df0: 3a0a 2020 2020 4120 6c69 7374 206f 6620  :.    A list of 
-00002e00: 7072 6f70 6572 7479 206e 616d 6573 2e0a  property names..
-00002e10: 2020 2222 220a 2020 7472 7565 5f70 726f    """.  true_pro
-00002e20: 7065 7274 6965 7320 3d20 7365 7428 290a  perties = set().
-00002e30: 2020 666f 7220 6d2c 2061 7474 7220 696e    for m, attr in
-00002e40: 2063 6c73 2e5f 5f64 6963 745f 5f2e 6974   cls.__dict__.it
-00002e50: 656d 7328 293a 0a20 2020 2069 6620 6e6f  ems():.    if no
-00002e60: 7420 6361 6c6c 6162 6c65 2861 7474 7229  t callable(attr)
-00002e70: 2061 6e64 2028 0a20 2020 2020 2068 6173   and (.      has
-00002e80: 6174 7472 2861 7474 722c 2027 5f5f 6765  attr(attr, '__ge
-00002e90: 745f 5f27 2920 6f72 2068 6173 6174 7472  t__') or hasattr
-00002ea0: 2861 7474 722c 2027 5f5f 7365 745f 5f27  (attr, '__set__'
-00002eb0: 2920 6f72 0a20 2020 2020 2068 6173 6174  ) or.      hasat
-00002ec0: 7472 2861 7474 722c 2027 5f5f 6465 6c65  tr(attr, '__dele
-00002ed0: 7465 5f5f 2729 0a20 2020 2029 3a0a 2020  te__').    ):.  
-00002ee0: 2020 2020 6d74 7970 6520 3d20 7479 7065      mtype = type
-00002ef0: 2861 7474 7229 0a20 2020 2020 2069 6620  (attr).      if 
-00002f00: 6d74 7970 6520 213d 2073 7461 7469 636d  mtype != staticm
-00002f10: 6574 686f 6420 616e 6420 6d74 7970 6520  ethod and mtype 
-00002f20: 213d 2063 6c61 7373 6d65 7468 6f64 3a0a  != classmethod:.
-00002f30: 2020 2020 2020 2020 7472 7565 5f70 726f          true_pro
-00002f40: 7065 7274 6965 732e 6164 6428 6d29 0a20  perties.add(m). 
-00002f50: 2072 6574 7572 6e20 7475 706c 6528 7472   return tuple(tr
-00002f60: 7565 5f70 726f 7065 7274 6965 732e 6469  ue_properties.di
-00002f70: 6666 6572 656e 6365 2873 6574 2865 7863  fference(set(exc
-00002f80: 6c75 6465 2929 290a 0a0a 6465 6620 7772  lude)))...def wr
-00002f90: 6170 5f6d 6574 686f 645f 6f6e 6365 2866  ap_method_once(f
-00002fa0: 756e 3a20 4361 6c6c 6162 6c65 5b2e 2e2e  un: Callable[...
-00002fb0: 2c20 416e 795d 2920 2d3e 2043 616c 6c61  , Any]) -> Calla
-00002fc0: 626c 655b 2e2e 2e2c 2041 6e79 5d3a 0a20  ble[..., Any]:. 
-00002fd0: 2022 2222 4d61 6e61 6765 7320 4d6f 6475   """Manages Modu
-00002fe0: 6c65 2073 7461 7465 2066 6f72 2061 2067  le state for a g
-00002ff0: 6976 656e 2075 7365 722d 6465 6669 6e65  iven user-define
-00003000: 6420 6d65 7468 6f64 2e0a 0a20 2041 7267  d method...  Arg
-00003010: 733a 0a20 2020 2066 756e 3a20 5573 6572  s:.    fun: User
-00003020: 2d64 6566 696e 6564 204d 6f64 756c 6520  -defined Module 
-00003030: 6d65 7468 6f64 2074 6f20 6d61 6e61 6765  method to manage
-00003040: 2073 7461 7465 2066 6f72 2e0a 2020 5265   state for..  Re
-00003050: 7475 726e 733a 0a20 2020 2057 7261 7070  turns:.    Wrapp
-00003060: 6564 206d 6574 686f 642e 0a20 2022 2222  ed method..  """
-00003070: 0a20 2023 2044 6f6e 2774 2072 6577 7261  .  # Don't rewra
-00003080: 7020 6d65 7468 6f64 7320 7468 6174 2068  p methods that h
-00003090: 6176 6520 616c 7265 6164 7920 6861 6420  ave already had 
-000030a0: 7468 6520 7374 6174 6520 6d61 6e61 6765  the state manage
-000030b0: 6d65 6e74 2077 7261 7070 6572 0a20 2023  ment wrapper.  #
-000030c0: 2061 7070 6c69 6564 2069 6e20 7468 6520   applied in the 
-000030d0: 6465 636f 7261 746f 7220 7374 6163 6b2e  decorator stack.
-000030e0: 2020 5468 6973 2077 7261 7070 6572 2073    This wrapper s
-000030f0: 686f 756c 6420 616c 7761 7973 2062 6520  hould always be 
-00003100: 6170 706c 6965 640a 2020 2320 6265 666f  applied.  # befo
-00003110: 7265 2074 7261 6e73 666f 726d 6174 696f  re transformatio
-00003120: 6e20 7772 6170 7065 7273 2e0a 2020 6966  n wrappers..  if
-00003130: 2068 6173 6174 7472 2866 756e 2c20 276d   hasattr(fun, 'm
-00003140: 6574 686f 645f 6861 6e64 6c65 725f 7772  ethod_handler_wr
-00003150: 6170 7065 6427 293a 0a20 2020 2072 6574  apped'):.    ret
-00003160: 7572 6e20 6675 6e0a 0a20 2040 6675 6e63  urn fun..  @func
-00003170: 746f 6f6c 732e 7772 6170 7328 6675 6e29  tools.wraps(fun)
-00003180: 0a20 2064 6566 2077 7261 7070 6564 5f6d  .  def wrapped_m
-00003190: 6f64 756c 655f 6d65 7468 6f64 282a 6172  odule_method(*ar
-000031a0: 6773 2c20 2a2a 6b77 6172 6773 293a 0a20  gs, **kwargs):. 
-000031b0: 2020 2023 2057 6520 6d69 6768 7420 6861     # We might ha
-000031c0: 7665 2069 6e63 6f72 7265 6374 6c79 2077  ve incorrectly w
-000031d0: 7261 7070 7065 6420 6120 6361 6c6c 6162  rappped a callab
-000031e0: 6c65 0a20 2020 2023 2074 6861 7420 6973  le.    # that is
-000031f0: 206e 6f74 2061 206d 6574 686f 642e 2043   not a method. C
-00003200: 6865 636b 2077 6865 7468 6572 2074 6865  heck whether the
-00003210: 2066 6972 7374 2061 7267 2069 7320 7365   first arg is se
-00003220: 6c66 2c0a 2020 2020 2320 6f74 6865 7277  lf,.    # otherw
-00003230: 6973 6520 6361 6c6c 2074 6865 2077 7261  ise call the wra
-00003240: 7070 6564 2066 756e 6374 696f 6e20 6173  pped function as
-00003250: 2069 732e 0a20 2020 2069 6620 6172 6773   is..    if args
-00003260: 2061 6e64 2069 7369 6e73 7461 6e63 6528   and isinstance(
-00003270: 6172 6773 5b30 5d2c 204d 6f64 756c 6529  args[0], Module)
-00003280: 3a0a 2020 2020 2020 7365 6c66 2c20 6172  :.      self, ar
-00003290: 6773 203d 2061 7267 735b 305d 2c20 6172  gs = args[0], ar
-000032a0: 6773 5b31 3a5d 0a20 2020 2020 2072 6574  gs[1:].      ret
-000032b0: 7572 6e20 7365 6c66 2e5f 6361 6c6c 5f77  urn self._call_w
-000032c0: 7261 7070 6564 5f6d 6574 686f 6428 6675  rapped_method(fu
-000032d0: 6e2c 2061 7267 732c 206b 7761 7267 7329  n, args, kwargs)
-000032e0: 0a20 2020 2065 6c73 653a 0a20 2020 2020  .    else:.     
-000032f0: 2072 6574 7572 6e20 6675 6e28 2a61 7267   return fun(*arg
-00003300: 732c 202a 2a6b 7761 7267 7329 0a20 2077  s, **kwargs).  w
-00003310: 7261 7070 6564 5f6d 6f64 756c 655f 6d65  rapped_module_me
-00003320: 7468 6f64 2e6d 6574 686f 645f 6861 6e64  thod.method_hand
-00003330: 6c65 725f 7772 6170 7065 6420 3d20 5472  ler_wrapped = Tr
-00003340: 7565 2020 2320 7479 7065 3a20 6967 6e6f  ue  # type: igno
-00003350: 7265 5b61 7474 722d 6465 6669 6e65 645d  re[attr-defined]
-00003360: 0a20 2072 6574 7572 6e20 7772 6170 7065  .  return wrappe
-00003370: 645f 6d6f 6475 6c65 5f6d 6574 686f 640a  d_module_method.
-00003380: 0a64 6566 2077 7261 705f 6465 7363 7269  .def wrap_descri
-00003390: 7074 6f72 5f6f 6e63 6528 6465 7363 7269  ptor_once(descri
-000033a0: 7074 6f72 2920 2d3e 2022 4465 7363 7269  ptor) -> "Descri
-000033b0: 7074 6f72 5772 6170 7065 7222 3a0a 2020  ptorWrapper":.  
-000033c0: 2222 2257 7261 7073 2061 2064 6573 6372  """Wraps a descr
-000033d0: 6970 746f 7220 746f 2067 6976 6520 6265  iptor to give be
-000033e0: 7474 6572 2065 7272 6f72 206d 6573 7361  tter error messa
-000033f0: 6765 732e 0a0a 2020 4172 6773 3a0a 2020  ges...  Args:.  
-00003400: 2020 7072 6f70 3a20 5573 6572 2d64 6566    prop: User-def
-00003410: 696e 6564 204d 6f64 756c 6520 6174 7472  ined Module attr
-00003420: 6962 7574 6520 6465 7363 7269 7074 6f72  ibute descriptor
-00003430: 2e0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
-00003440: 2057 7261 7070 6564 2064 6573 6372 6970   Wrapped descrip
-00003450: 746f 722e 0a20 2022 2222 0a20 2023 2044  tor..  """.  # D
-00003460: 6f6e 2774 2072 6577 7261 7020 6465 7363  on't rewrap desc
-00003470: 7269 7074 6f72 732e 0a20 2069 6620 6973  riptors..  if is
-00003480: 696e 7374 616e 6365 2864 6573 6372 6970  instance(descrip
-00003490: 746f 722c 2044 6573 6372 6970 746f 7257  tor, DescriptorW
-000034a0: 7261 7070 6572 293a 0a20 2020 2072 6574  rapper):.    ret
-000034b0: 7572 6e20 6465 7363 7269 7074 6f72 0a0a  urn descriptor..
-000034c0: 2020 7265 7475 726e 2063 7265 6174 655f    return create_
-000034d0: 6465 7363 7269 7074 6f72 5f77 7261 7070  descriptor_wrapp
-000034e0: 6572 2864 6573 6372 6970 746f 7229 0a0a  er(descriptor)..
-000034f0: 0a64 6566 205f 7772 6170 5f68 6173 6828  .def _wrap_hash(
-00003500: 6861 7368 5f66 6e3a 2043 616c 6c61 626c  hash_fn: Callabl
-00003510: 655b 2e2e 2e2c 2041 6e79 5d29 202d 3e20  e[..., Any]) -> 
-00003520: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
-00003530: 795d 3a0a 2020 2222 2257 7261 7073 2061  y]:.  """Wraps a
-00003540: 2068 6173 6820 6675 6e63 7469 6f6e 2077   hash function w
-00003550: 6974 6820 736f 6d65 2063 6865 636b 2066  ith some check f
-00003560: 6f72 2046 6c61 7820 4d6f 6475 6c65 732e  or Flax Modules.
-00003570: 2222 220a 2020 4066 756e 6374 6f6f 6c73  """.  @functools
-00003580: 2e77 7261 7073 2868 6173 685f 666e 290a  .wraps(hash_fn).
-00003590: 2020 6465 6620 7772 6170 7065 6428 7365    def wrapped(se
-000035a0: 6c66 293a 0a20 2020 2069 6620 7365 6c66  lf):.    if self
-000035b0: 2e73 636f 7065 2069 7320 6e6f 7420 4e6f  .scope is not No
-000035c0: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
-000035d0: 5479 7065 4572 726f 7228 2743 616e 5c27  TypeError('Can\'
-000035e0: 7420 6361 6c6c 205f 5f68 6173 685f 5f20  t call __hash__ 
-000035f0: 6f6e 206d 6f64 756c 6573 2074 6861 7420  on modules that 
-00003600: 686f 6c64 2076 6172 6961 626c 6573 2e27  hold variables.'
-00003610: 290a 2020 2020 7472 793a 0a20 2020 2020  ).    try:.     
-00003620: 2068 6173 685f 7661 6c75 6520 3d20 6861   hash_value = ha
-00003630: 7368 5f66 6e28 7365 6c66 290a 2020 2020  sh_fn(self).    
-00003640: 6578 6365 7074 2054 7970 6545 7272 6f72  except TypeError
-00003650: 2061 7320 6578 633a 0a20 2020 2020 2072   as exc:.      r
-00003660: 6169 7365 2054 7970 6545 7272 6f72 2827  aise TypeError('
-00003670: 4661 696c 6564 2074 6f20 6861 7368 2046  Failed to hash F
-00003680: 6c61 7820 4d6f 6475 6c65 2e20 2027 0a20  lax Module.  '. 
-00003690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000036a0: 2020 2020 2027 5468 6520 6d6f 6475 6c65       'The module
-000036b0: 2070 726f 6261 626c 7920 636f 6e74 6169   probably contai
-000036c0: 6e73 2075 6e68 6173 6861 626c 6520 6174  ns unhashable at
-000036d0: 7472 6962 7574 6573 2e20 2027 0a20 2020  tributes.  '.   
-000036e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000036f0: 2020 2066 274d 6f64 756c 653d 7b73 656c     f'Module={sel
-00003700: 667d 2729 2066 726f 6d20 6578 630a 2020  f}') from exc.  
-00003710: 2020 7265 7475 726e 2068 6173 685f 7661    return hash_va
-00003720: 6c75 650a 2020 7265 7475 726e 2077 7261  lue.  return wra
-00003730: 7070 6564 0a0a 0a64 6566 205f 6765 745f  pped...def _get_
-00003740: 756e 626f 756e 645f 666e 286d 6574 686f  unbound_fn(metho
-00003750: 645f 6f72 5f66 6e3a 2043 616c 6c61 626c  d_or_fn: Callabl
-00003760: 655b 2e2e 2e2c 2041 6e79 5d29 202d 3e20  e[..., Any]) -> 
-00003770: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
-00003780: 795d 3a0a 2020 2222 2252 6574 7572 6e73  y]:.  """Returns
-00003790: 2061 6e20 756e 626f 756e 6420 6675 6e63   an unbound func
-000037a0: 7469 6f6e 2066 726f 6d20 6120 6d65 7468  tion from a meth
-000037b0: 6f64 2074 6861 7420 6973 2070 6f73 7369  od that is possi
-000037c0: 626c 7920 626f 756e 642e 0a0a 2020 5468  bly bound...  Th
-000037d0: 6973 206d 6561 6e73 2074 6861 7420 6966  is means that if
-000037e0: 2074 6865 2070 6173 7365 6420 6675 6e63   the passed func
-000037f0: 7469 6f6e 2062 656c 6f6e 6773 206f 6620  tion belongs of 
-00003800: 616e 2069 6e73 7461 6e63 6520 6f66 2061  an instance of a
-00003810: 2063 6c61 7373 2c20 7468 656e 0a20 2074   class, then.  t
-00003820: 6865 2072 6574 7572 6e65 6420 6675 6e63  he returned func
-00003830: 7469 6f6e 2064 6f65 7320 6e6f 206c 6f6e  tion does no lon
-00003840: 6765 7220 6465 7065 6e64 206f 6e20 7468  ger depend on th
-00003850: 6520 696e 7374 616e 6365 2c20 7768 6963  e instance, whic
-00003860: 6820 6973 2070 6173 7365 640a 2020 6173  h is passed.  as
-00003870: 2074 6865 2066 6972 7374 2061 7267 756d   the first argum
-00003880: 656e 7420 746f 2074 6865 2066 756e 6374  ent to the funct
-00003890: 696f 6e2e 0a0a 2020 4172 6773 3a0a 2020  ion...  Args:.  
-000038a0: 2020 6d65 7468 6f64 5f6f 725f 666e 3a20    method_or_fn: 
-000038b0: 4120 636c 6173 7320 6d65 7468 6f64 206f  A class method o
-000038c0: 7220 6675 6e63 7469 6f6e 2e0a 2020 5265  r function..  Re
-000038d0: 7475 726e 733a 0a20 2020 2041 6e20 756e  turns:.    An un
-000038e0: 626f 756e 6420 7665 7273 696f 6e20 6f66  bound version of
-000038f0: 2069 6e70 7574 2066 756e 6374 696f 6e2e   input function.
-00003900: 0a20 2022 2222 0a20 2069 6620 2869 6e73  .  """.  if (ins
-00003910: 7065 6374 2e69 736d 6574 686f 6428 6d65  pect.ismethod(me
-00003920: 7468 6f64 5f6f 725f 666e 2920 616e 640a  thod_or_fn) and.
-00003930: 2020 2020 2020 6973 696e 7374 616e 6365        isinstance
-00003940: 286d 6574 686f 645f 6f72 5f66 6e2e 5f5f  (method_or_fn.__
-00003950: 7365 6c66 5f5f 2c20 4d6f 6475 6c65 2929  self__, Module))
-00003960: 3a20 2023 2070 7974 7970 653a 2064 6973  :  # pytype: dis
-00003970: 6162 6c65 3d61 7474 7269 6275 7465 2d65  able=attribute-e
-00003980: 7272 6f72 0a20 2020 206d 6574 686f 645f  rror.    method_
-00003990: 6f72 5f66 6e20 3d20 6d65 7468 6f64 5f6f  or_fn = method_o
-000039a0: 725f 666e 2e5f 5f66 756e 635f 5f20 2023  r_fn.__func__  #
-000039b0: 2070 7974 7970 653a 2064 6973 6162 6c65   pytype: disable
-000039c0: 3d61 7474 7269 6275 7465 2d65 7272 6f72  =attribute-error
-000039d0: 0a0a 2020 2320 5468 6520 6d65 7468 6f64  ..  # The method
-000039e0: 2073 686f 756c 6420 6265 2063 616c 6c61   should be calla
-000039f0: 626c 652c 2061 6e64 2069 7420 7368 6f75  ble, and it shou
-00003a00: 6c64 2068 6176 6520 6174 206c 6561 7374  ld have at least
-00003a10: 206f 6e65 2061 7267 756d 656e 740a 2020   one argument.  
-00003a20: 2320 7265 7072 6573 656e 7469 6e67 2074  # representing t
-00003a30: 6865 2063 6c61 7373 2074 6861 7420 6973  he class that is
-00003a40: 2070 6173 7365 6420 696e 2e0a 2020 6966   passed in..  if
-00003a50: 2028 6e6f 7420 6361 6c6c 6162 6c65 286d   (not callable(m
-00003a60: 6574 686f 645f 6f72 5f66 6e29 206f 720a  ethod_or_fn) or.
-00003a70: 2020 2020 2020 6c65 6e28 696e 7370 6563        len(inspec
-00003a80: 742e 7369 676e 6174 7572 6528 6d65 7468  t.signature(meth
-00003a90: 6f64 5f6f 725f 666e 292e 7061 7261 6d65  od_or_fn).parame
-00003aa0: 7465 7273 2920 3c20 3129 3a0a 2020 2020  ters) < 1):.    
-00003ab0: 7261 6973 6520 6572 726f 7273 2e41 7070  raise errors.App
-00003ac0: 6c79 4d6f 6475 6c65 496e 7661 6c69 644d  lyModuleInvalidM
-00003ad0: 6574 686f 6445 7272 6f72 286d 6574 686f  ethodError(metho
-00003ae0: 645f 6f72 5f66 6e29 0a0a 2020 7265 7475  d_or_fn)..  retu
-00003af0: 726e 206d 6574 686f 645f 6f72 5f66 6e0a  rn method_or_fn.
-00003b00: 0a64 6566 205f 6d61 705f 7375 626d 6f64  .def _map_submod
-00003b10: 756c 6573 2866 6e3a 2043 616c 6c61 626c  ules(fn: Callabl
-00003b20: 655b 5b27 4d6f 6475 6c65 275d 2c20 416e  e[['Module'], An
-00003b30: 795d 2c20 7472 6565 293a 0a20 2022 2222  y], tree):.  """
-00003b40: 4d61 7020 6120 6675 6e63 7469 6f6e 206f  Map a function o
-00003b50: 7665 7220 616c 6c20 7375 626d 6f64 756c  ver all submodul
-00003b60: 6573 2069 6e20 6120 7472 6565 2e22 2222  es in a tree."""
-00003b70: 0a20 2067 203d 206c 616d 6264 6120 5f2c  .  g = lambda _,
-00003b80: 2078 3a20 666e 2878 2920 6966 2069 7369   x: fn(x) if isi
-00003b90: 6e73 7461 6e63 6528 782c 204d 6f64 756c  nstance(x, Modul
-00003ba0: 6529 2065 6c73 6520 780a 2020 7265 7475  e) else x.  retu
-00003bb0: 726e 205f 6672 6565 7a65 5f61 7474 7228  rn _freeze_attr(
-00003bc0: 5f6d 6170 5f6f 7665 725f 6d6f 6475 6c65  _map_over_module
-00003bd0: 735f 696e 5f74 7265 6528 672c 2074 7265  s_in_tree(g, tre
-00003be0: 6529 290a 0a63 6c61 7373 2053 6574 7570  e))..class Setup
-00003bf0: 5374 6174 6528 656e 756d 2e49 6e74 456e  State(enum.IntEn
-00003c00: 756d 293a 0a20 2023 2073 6574 7570 2829  um):.  # setup()
-00003c10: 2068 6173 206e 6f74 2062 6565 6e20 6361   has not been ca
-00003c20: 6c6c 6564 2e0a 2020 4e45 5720 3d20 300a  lled..  NEW = 0.
-00003c30: 2020 2320 7365 7475 7028 2920 6861 7320    # setup() has 
-00003c40: 6265 656e 2063 616c 6c65 6420 6f75 7473  been called outs
-00003c50: 6964 6520 6120 7472 616e 7366 6f72 6d20  ide a transform 
-00003c60: 626f 756e 6461 7279 2e0a 2020 5452 414e  boundary..  TRAN
-00003c70: 5346 4f52 4d45 4420 3d20 310a 2020 2320  SFORMED = 1.  # 
-00003c80: 7365 7475 7028 2920 6861 7320 6265 656e  setup() has been
-00003c90: 2063 616c 6c65 642e 0a20 2044 4f4e 4520   called..  DONE 
-00003ca0: 3d20 320a 0a0a 4064 6174 6163 6c61 7373  = 2...@dataclass
-00003cb0: 6573 2e64 6174 6163 6c61 7373 0a63 6c61  es.dataclass.cla
-00003cc0: 7373 205f 4d6f 6475 6c65 496e 7465 726e  ss _ModuleIntern
-00003cd0: 616c 5374 6174 653a 0a20 2022 2222 4570  alState:.  """Ep
-00003ce0: 6865 6d65 7261 6c20 4d6f 6475 6c65 2045  hemeral Module E
-00003cf0: 7661 6c75 6174 696f 6e20 5374 6174 652e  valuation State.
-00003d00: 0a0a 2020 466f 7220 636c 6172 6974 792c  ..  For clarity,
-00003d10: 2077 6520 636f 6c6c 6563 7420 616c 6c20   we collect all 
-00003d20: 6f66 2074 6865 2074 656d 706f 7261 7279  of the temporary
-00003d30: 2066 6c61 6773 2061 6e64 2065 7068 656d   flags and ephem
-00003d40: 6572 616c 2073 7461 7465 2075 7365 6420  eral state used 
-00003d50: 6279 0a20 204d 6f64 756c 6573 2066 6f72  by.  Modules for
-00003d60: 2061 7574 6f6e 616d 696e 6720 616e 6420   autonaming and 
-00003d70: 6572 726f 7220 6d65 7373 6167 6573 2068  error messages h
-00003d80: 6572 652c 2061 6c6f 6e67 7369 6465 2074  ere, alongside t
-00003d90: 6865 2072 756c 6573 2075 7365 640a 2020  he rules used.  
-00003da0: 746f 2070 6173 7320 7468 6973 2065 7068  to pass this eph
-00003db0: 656d 6572 616c 2073 7461 7465 2061 6372  emeral state acr
-00003dc0: 6f73 7320 7472 616e 7366 6f72 6d20 626f  oss transform bo
-00003dd0: 756e 6461 7269 6573 2e0a 2020 2222 220a  undaries..  """.
-00003de0: 2020 696e 5f63 6f6d 7061 6374 5f6d 6574    in_compact_met
-00003df0: 686f 643a 2062 6f6f 6c20 3d20 4661 6c73  hod: bool = Fals
-00003e00: 650a 2020 696e 5f73 6574 7570 3a20 626f  e.  in_setup: bo
-00003e10: 6f6c 203d 2046 616c 7365 0a20 2073 6574  ol = False.  set
-00003e20: 7570 5f63 616c 6c65 643a 2053 6574 7570  up_called: Setup
-00003e30: 5374 6174 6520 3d20 5365 7475 7053 7461  State = SetupSta
-00003e40: 7465 2e4e 4557 0a20 2069 735f 696e 6974  te.NEW.  is_init
-00003e50: 6961 6c69 7a65 643a 2062 6f6f 6c20 3d20  ialized: bool = 
-00003e60: 4661 6c73 650a 2020 6175 746f 6e61 6d65  False.  autoname
-00003e70: 5f63 7572 736f 723a 2044 6963 745b 7374  _cursor: Dict[st
-00003e80: 722c 2069 6e74 5d20 3d20 6461 7461 636c  r, int] = datacl
-00003e90: 6173 7365 732e 6669 656c 6428 6465 6661  asses.field(defa
-00003ea0: 756c 745f 6661 6374 6f72 793d 6469 6374  ult_factory=dict
-00003eb0: 290a 2020 6368 696c 6472 656e 3a20 4469  ).  children: Di
-00003ec0: 6374 5b73 7472 2c20 556e 696f 6e5b 7374  ct[str, Union[st
-00003ed0: 722c 2027 4d6f 6475 6c65 275d 5d20 3d20  r, 'Module']] = 
-00003ee0: 6461 7461 636c 6173 7365 732e 6669 656c  dataclasses.fiel
-00003ef0: 6428 0a20 2020 2020 2064 6566 6175 6c74  d(.      default
-00003f00: 5f66 6163 746f 7279 3d64 6963 7429 0a0a  _factory=dict)..
-00003f10: 2020 6465 6620 7265 7365 7428 7365 6c66    def reset(self
-00003f20: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    "
-00003f30: 2222 5265 7365 7473 2074 7261 6e73 6965  ""Resets transie
-00003f40: 6e74 2073 7461 7465 2e0a 0a20 2020 2054  nt state...    T
-00003f50: 6869 7320 6675 6e63 7469 6f6e 2069 7320  his function is 
-00003f60: 6361 6c6c 6564 2061 6674 6572 2065 6163  called after eac
-00003f70: 6820 6d6f 6475 6c65 206d 6574 686f 642c  h module method,
-00003f80: 2073 6f20 6f6e 6c79 2061 7474 7269 6275   so only attribu
-00003f90: 7465 7320 7468 6174 0a20 2020 2061 7265  tes that.    are
-00003fa0: 206d 6574 686f 642d 6465 7065 6e64 656e   method-dependen
-00003fb0: 7420 6172 6520 7265 7365 742e 0a20 2020  t are reset..   
-00003fc0: 2022 2222 0a20 2020 2073 656c 662e 696e   """.    self.in
-00003fd0: 5f63 6f6d 7061 6374 5f6d 6574 686f 6420  _compact_method 
-00003fe0: 3d20 4661 6c73 650a 2020 2020 7365 6c66  = False.    self
-00003ff0: 2e69 6e5f 7365 7475 7020 3d20 4661 6c73  .in_setup = Fals
-00004000: 650a 2020 2020 7365 6c66 2e61 7574 6f6e  e.    self.auton
-00004010: 616d 655f 6375 7273 6f72 203d 2064 6963  ame_cursor = dic
-00004020: 7428 290a 0a20 2064 6566 2065 7870 6f72  t()..  def expor
-00004030: 7428 7365 6c66 2920 2d3e 2027 5f4d 6f64  t(self) -> '_Mod
-00004040: 756c 6549 6e74 6572 6e61 6c53 7461 7465  uleInternalState
-00004050: 273a 0a20 2020 2022 2222 4578 706f 7274  ':.    """Export
-00004060: 7320 7472 616e 7366 6f72 6d2d 7072 6573  s transform-pres
-00004070: 6572 7665 6420 7374 6174 6520 6163 726f  erved state acro
-00004080: 7373 2074 7261 6e73 666f 726d 2062 6f75  ss transform bou
-00004090: 6e64 6172 792e 2222 220a 2020 2020 7365  ndary.""".    se
-000040a0: 7475 705f 7374 6174 6520 3d20 5365 7475  tup_state = Setu
-000040b0: 7053 7461 7465 2e54 5241 4e53 464f 524d  pState.TRANSFORM
-000040c0: 4544 2069 6620 7365 6c66 2e73 6574 7570  ED if self.setup
-000040d0: 5f63 616c 6c65 6420 656c 7365 2053 6574  _called else Set
-000040e0: 7570 5374 6174 652e 4e45 570a 2020 2020  upState.NEW.    
-000040f0: 636c 6f6e 6564 203d 205f 4d6f 6475 6c65  cloned = _Module
-00004100: 496e 7465 726e 616c 5374 6174 6528 0a20  InternalState(. 
-00004110: 2020 2020 2020 2069 6e5f 636f 6d70 6163         in_compac
-00004120: 745f 6d65 7468 6f64 3d73 656c 662e 696e  t_method=self.in
-00004130: 5f63 6f6d 7061 6374 5f6d 6574 686f 642c  _compact_method,
-00004140: 0a20 2020 2020 2020 2069 6e5f 7365 7475  .        in_setu
-00004150: 703d 7365 6c66 2e69 6e5f 7365 7475 702c  p=self.in_setup,
-00004160: 0a20 2020 2020 2020 2073 6574 7570 5f63  .        setup_c
-00004170: 616c 6c65 643d 7365 7475 705f 7374 6174  alled=setup_stat
-00004180: 652c 0a20 2020 2020 2020 2069 735f 696e  e,.        is_in
-00004190: 6974 6961 6c69 7a65 643d 7365 6c66 2e69  itialized=self.i
-000041a0: 735f 696e 6974 6961 6c69 7a65 642c 0a20  s_initialized,. 
-000041b0: 2020 2020 2020 2061 7574 6f6e 616d 655f         autoname_
-000041c0: 6375 7273 6f72 3d64 6963 7428 7365 6c66  cursor=dict(self
-000041d0: 2e61 7574 6f6e 616d 655f 6375 7273 6f72  .autoname_cursor
-000041e0: 2929 0a20 2020 2072 6574 7572 6e20 636c  )).    return cl
-000041f0: 6f6e 6564 0a0a 2020 6465 6620 7265 696d  oned..  def reim
-00004200: 706f 7274 2873 656c 662c 206f 7468 6572  port(self, other
-00004210: 3a20 275f 4d6f 6475 6c65 496e 7465 726e  : '_ModuleIntern
-00004220: 616c 5374 6174 6527 2920 2d3e 204e 6f6e  alState') -> Non
-00004230: 653a 0a20 2020 2022 2222 5265 2d69 6d70  e:.    """Re-imp
-00004240: 6f72 7473 2074 7261 6e73 666f 726d 2d70  orts transform-p
-00004250: 7265 7365 7276 6564 2073 7461 7465 2066  reserved state f
-00004260: 726f 6d20 6163 726f 7373 2074 7261 6e73  rom across trans
-00004270: 666f 726d 2062 6f75 6e64 6172 792e 2222  form boundary.""
-00004280: 220a 2020 2020 7365 6c66 2e69 6e5f 636f  ".    self.in_co
-00004290: 6d70 6163 745f 6d65 7468 6f64 203d 206f  mpact_method = o
-000042a0: 7468 6572 2e69 6e5f 636f 6d70 6163 745f  ther.in_compact_
-000042b0: 6d65 7468 6f64 0a20 2020 2073 656c 662e  method.    self.
-000042c0: 696e 5f73 6574 7570 203d 206f 7468 6572  in_setup = other
-000042d0: 2e69 6e5f 7365 7475 700a 2020 2020 7365  .in_setup.    se
-000042e0: 6c66 2e69 735f 696e 6974 6961 6c69 7a65  lf.is_initialize
-000042f0: 6420 3d20 6f74 6865 722e 6973 5f69 6e69  d = other.is_ini
-00004300: 7469 616c 697a 6564 0a20 2020 2073 656c  tialized.    sel
-00004310: 662e 6175 746f 6e61 6d65 5f63 7572 736f  f.autoname_curso
-00004320: 7220 3d20 6469 6374 286f 7468 6572 2e61  r = dict(other.a
-00004330: 7574 6f6e 616d 655f 6375 7273 6f72 290a  utoname_cursor).
-00004340: 0a5f 756e 696e 6974 6961 6c69 7a65 645f  ._uninitialized_
-00004350: 6d6f 6475 6c65 5f69 6e74 6572 6e61 6c5f  module_internal_
-00004360: 7374 6174 6520 3d20 5f4d 6f64 756c 6549  state = _ModuleI
-00004370: 6e74 6572 6e61 6c53 7461 7465 2829 0a0a  nternalState()..
-00004380: 0a5f 554e 4445 4649 4e45 445f 434f 5059  ._UNDEFINED_COPY
-00004390: 5f50 4943 4b4c 455f 4d45 5448 4f44 5320  _PICKLE_METHODS 
-000043a0: 3d20 280a 2020 2020 275f 5f67 6574 7374  = (.    '__getst
-000043b0: 6174 655f 5f27 2c20 275f 5f73 6574 7374  ate__', '__setst
-000043c0: 6174 655f 5f27 2c20 275f 5f67 6574 6e65  ate__', '__getne
-000043d0: 7761 7267 735f 6578 5f5f 272c 0a20 2020  wargs_ex__',.   
-000043e0: 2027 5f5f 7265 6475 6365 5f5f 272c 2027   '__reduce__', '
-000043f0: 5f5f 7265 6475 6365 5f65 785f 5f27 2c20  __reduce_ex__', 
-00004400: 275f 5f63 6f70 795f 5f27 2c20 275f 5f64  '__copy__', '__d
-00004410: 6565 7063 6f70 795f 5f27 290a 0a0a 5f63  eepcopy__')..._c
-00004420: 6163 6865 733a 2027 7765 616b 7265 662e  aches: 'weakref.
-00004430: 5765 616b 4b65 7944 6963 7469 6f6e 6172  WeakKeyDictionar
-00004440: 795b 5363 6f70 652c 2077 6561 6b72 6566  y[Scope, weakref
-00004450: 2e57 6561 6b56 616c 7565 4469 6374 696f  .WeakValueDictio
-00004460: 6e61 7279 5b46 6c61 7849 642c 204d 6f64  nary[FlaxId, Mod
-00004470: 756c 655d 5d27 203d 2028 0a20 2020 2077  ule]]' = (.    w
-00004480: 6561 6b72 6566 2e57 6561 6b4b 6579 4469  eakref.WeakKeyDi
-00004490: 6374 696f 6e61 7279 2829 290a 0a0a 7475  ctionary())...tu
-000044a0: 706c 655f 7265 6475 6365 203d 206c 616d  ple_reduce = lam
-000044b0: 6264 6120 7873 2c20 783a 2078 7320 2b20  bda xs, x: xs + 
-000044c0: 2878 2c29 0a74 7570 6c65 5f69 6e69 7420  (x,).tuple_init 
-000044d0: 3d20 6c61 6d62 6461 3a20 2829 0a0a 0a63  = lambda: ()...c
-000044e0: 6170 7475 7265 5f63 616c 6c5f 696e 7465  apture_call_inte
-000044f0: 726d 6564 6961 7465 7320 3d20 6c61 6d62  rmediates = lamb
-00004500: 6461 205f 2c20 6d65 7468 6f64 5f6e 616d  da _, method_nam
-00004510: 653a 206d 6574 686f 645f 6e61 6d65 203d  e: method_name =
-00004520: 3d20 275f 5f63 616c 6c5f 5f27 0a0a 0a63  = '__call__'...c
-00004530: 6c61 7373 2050 6172 656e 7444 6573 6372  lass ParentDescr
-00004540: 6970 746f 723a 0a20 2022 2222 5772 6170  iptor:.  """Wrap
-00004550: 7320 7061 7265 6e74 206d 6f64 756c 6520  s parent module 
-00004560: 7265 6665 7265 6e63 6573 2069 6e20 7765  references in we
-00004570: 616b 2072 6566 732e 0a0a 2020 5468 6973  ak refs...  This
-00004580: 2070 7265 7665 6e74 7320 7265 6665 7265   prevents refere
-00004590: 6e63 6520 6379 636c 6573 2066 726f 6d20  nce cycles from 
-000045a0: 666f 726d 696e 6720 7669 6120 7061 7265  forming via pare
-000045b0: 6e74 206c 696e 6b73 2077 6869 6368 2063  nt links which c
-000045c0: 616e 206c 6561 640a 2020 746f 2061 6363  an lead.  to acc
-000045d0: 6964 656e 7461 6c20 4f4f 4d73 2069 6e20  idental OOMs in 
-000045e0: 6561 6765 7220 6d6f 6465 2064 7565 2074  eager mode due t
-000045f0: 6f20 736c 6f77 2067 6172 6261 6765 2063  o slow garbage c
-00004600: 6f6c 6c65 6374 696f 6e20 6173 2077 656c  ollection as wel
-00004610: 6c20 6173 0a20 2073 7075 7269 6f75 7320  l as.  spurious 
-00004620: 7472 6163 6572 206c 6561 6b73 2064 7572  tracer leaks dur
-00004630: 696e 6720 6a69 7420 636f 6d70 696c 6174  ing jit compilat
-00004640: 696f 6e2e 0a0a 2020 4e6f 7465 3a20 2264  ion...  Note: "d
-00004650: 6573 6372 6970 746f 7273 2220 6172 6520  escriptors" are 
-00004660: 7468 6520 756e 6465 726c 7969 6e67 2070  the underlying p
-00004670: 7974 686f 6e20 6d65 6368 616e 6973 6d20  ython mechanism 
-00004680: 666f 7220 696d 706c 656d 656e 7469 6e67  for implementing
-00004690: 0a20 2064 796e 616d 6963 2040 7072 6f70  .  dynamic @prop
-000046a0: 6572 7479 2064 6563 6f72 6174 6f72 732e  erty decorators.
-000046b0: 2020 5765 206e 6565 6420 746f 2075 7365    We need to use
-000046c0: 2061 2072 6177 2064 6573 6372 6970 746f   a raw descripto
-000046d0: 7220 696e 7374 6561 6420 6f66 2074 6865  r instead of the
-000046e0: 0a20 206d 6f72 6520 636f 6d6d 6f6e 2064  .  more common d
-000046f0: 6563 6f72 6174 6f72 2069 6e20 6f72 6465  ecorator in orde
-00004700: 7220 746f 2066 6f72 6365 2074 6861 7420  r to force that 
-00004710: 7468 6520 6170 7072 6f70 7269 6174 6520  the appropriate 
-00004720: 6765 7474 6572 2f73 6574 7465 720a 2020  getter/setter.  
-00004730: 6c6f 6769 6320 6170 706c 6965 7320 696e  logic applies in
-00004740: 2073 7562 636c 6173 7365 7320 6576 656e   subclasses even
-00004750: 2061 6674 6572 2076 6172 696f 7573 2064   after various d
-00004760: 6174 6163 6c61 7373 2074 7261 6e73 666f  ataclass transfo
-00004770: 726d 732e 0a20 2022 2222 0a20 2064 6566  rms..  """.  def
-00004780: 205f 5f67 6574 5f5f 2873 656c 662c 206f   __get__(self, o
-00004790: 626a 2c20 6f62 6a74 7970 653d 4e6f 6e65  bj, objtype=None
-000047a0: 293a 0a20 2020 2023 2063 6865 636b 2069  ):.    # check i
-000047b0: 6620 6f62 6a20 6973 204e 6f6e 652c 2068  f obj is None, h
-000047c0: 6170 7065 6e73 2064 7572 696e 6720 2561  appens during %a
-000047d0: 7574 6f72 656c 6f61 640a 2020 2020 6966  utoreload.    if
-000047e0: 206f 626a 2069 7320 4e6f 6e65 3a0a 2020   obj is None:.  
-000047f0: 2020 2020 7265 7475 726e 204e 6f6e 650a      return None.
-00004800: 2020 2020 7061 7265 6e74 203d 206f 626a      parent = obj
-00004810: 6563 742e 5f5f 6765 7461 7474 7269 6275  ect.__getattribu
-00004820: 7465 5f5f 286f 626a 2c20 225f 7061 7265  te__(obj, "_pare
-00004830: 6e74 5f72 6566 2229 0a20 2020 2072 6574  nt_ref").    ret
-00004840: 7572 6e20 7061 7265 6e74 2829 2069 6620  urn parent() if 
-00004850: 6973 696e 7374 616e 6365 2870 6172 656e  isinstance(paren
-00004860: 742c 2077 6561 6b72 6566 2e52 6566 6572  t, weakref.Refer
-00004870: 656e 6365 5479 7065 2920 656c 7365 2070  enceType) else p
-00004880: 6172 656e 740a 0a20 2064 6566 205f 5f73  arent..  def __s
-00004890: 6574 5f5f 2873 656c 662c 206f 626a 2c20  et__(self, obj, 
-000048a0: 7661 6c75 6529 3a0a 2020 2020 6d61 7962  value):.    mayb
-000048b0: 655f 7765 616b 203d 2077 6561 6b72 6566  e_weak = weakref
-000048c0: 2e72 6566 2876 616c 7565 2920 6966 2069  .ref(value) if i
-000048d0: 7369 6e73 7461 6e63 6528 7661 6c75 652c  sinstance(value,
-000048e0: 204d 6f64 756c 6529 2065 6c73 6520 7661   Module) else va
-000048f0: 6c75 650a 2020 2020 6f62 6a65 6374 2e5f  lue.    object._
-00004900: 5f73 6574 6174 7472 5f5f 286f 626a 2c20  _setattr__(obj, 
-00004910: 225f 7061 7265 6e74 5f72 6566 222c 206d  "_parent_ref", m
-00004920: 6179 6265 5f77 6561 6b29 0a0a 0a63 6c61  aybe_weak)...cla
-00004930: 7373 2044 6573 6372 6970 746f 7228 5072  ss Descriptor(Pr
-00004940: 6f74 6f63 6f6c 293a 0a20 205f 5f69 7361  otocol):.  __isa
-00004950: 6273 7472 6163 746d 6574 686f 645f 5f3a  bstractmethod__:
-00004960: 2062 6f6f 6c0a 2020 6465 6620 5f5f 6765   bool.  def __ge
-00004970: 745f 5f28 7365 6c66 2c20 6f62 6a2c 206f  t__(self, obj, o
-00004980: 626a 7479 7065 3d4e 6f6e 6529 202d 3e20  bjtype=None) -> 
-00004990: 416e 793a 202e 2e2e 0a20 2064 6566 205f  Any: ....  def _
-000049a0: 5f73 6574 5f5f 2873 656c 662c 206f 626a  _set__(self, obj
-000049b0: 2c20 7661 6c75 6529 202d 3e20 4e6f 6e65  , value) -> None
-000049c0: 3a20 2e2e 2e0a 2020 6465 6620 5f5f 6465  : ....  def __de
-000049d0: 6c65 7465 5f5f 2873 656c 662c 206f 626a  lete__(self, obj
-000049e0: 2920 2d3e 204e 6f6e 653a 202e 2e2e 0a20  ) -> None: .... 
-000049f0: 2064 6566 205f 5f73 6574 5f6e 616d 655f   def __set_name_
-00004a00: 5f28 7365 6c66 2c20 6f77 6e65 722c 206e  _(self, owner, n
-00004a10: 616d 6529 202d 3e20 4e6f 6e65 3a20 2e2e  ame) -> None: ..
-00004a20: 2e0a 0a63 6c61 7373 2044 6573 6372 6970  ...class Descrip
-00004a30: 746f 7257 7261 7070 6572 3a0a 2020 7061  torWrapper:.  pa
-00004a40: 7373 0a0a 6465 6620 6372 6561 7465 5f64  ss..def create_d
-00004a50: 6573 6372 6970 746f 725f 7772 6170 7065  escriptor_wrappe
-00004a60: 7228 6465 7363 7269 7074 6f72 3a20 4465  r(descriptor: De
-00004a70: 7363 7269 7074 6f72 293a 0a20 2022 2222  scriptor):.  """
-00004a80: 4372 6561 7465 7320 6120 6465 7363 7269  Creates a descri
-00004a90: 7074 6f72 2077 7261 7070 6572 2074 6861  ptor wrapper tha
-00004aa0: 7420 6361 6c6c 7320 6120 6765 745f 666e  t calls a get_fn
-00004ab0: 206f 6e20 7468 6520 6465 7363 7269 7074   on the descript
-00004ac0: 6f72 2e22 2222 0a0a 2020 636c 6173 7320  or."""..  class 
-00004ad0: 5f44 6573 6372 6970 746f 7257 7261 7070  _DescriptorWrapp
-00004ae0: 6572 2844 6573 6372 6970 746f 7257 7261  er(DescriptorWra
-00004af0: 7070 6572 293a 0a20 2020 2022 2222 4120  pper):.    """A 
-00004b00: 6465 7363 7269 7074 6f72 2074 6861 7420  descriptor that 
-00004b10: 6361 6e20 7772 6170 2061 6e79 2064 6573  can wrap any des
-00004b20: 6372 6970 746f 7222 2222 0a0a 2020 2020  criptor"""..    
-00004b30: 6966 2068 6173 6174 7472 2864 6573 6372  if hasattr(descr
-00004b40: 6970 746f 722c 2027 5f5f 6973 6162 7374  iptor, '__isabst
-00004b50: 7261 6374 6d65 7468 6f64 5f5f 2729 3a0a  ractmethod__'):.
-00004b60: 2020 2020 2020 5f5f 6973 6162 7374 7261        __isabstra
-00004b70: 6374 6d65 7468 6f64 5f5f 203d 2064 6573  ctmethod__ = des
-00004b80: 6372 6970 746f 722e 5f5f 6973 6162 7374  criptor.__isabst
-00004b90: 7261 6374 6d65 7468 6f64 5f5f 0a0a 2020  ractmethod__..  
-00004ba0: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
-00004bb0: 656c 662c 2077 7261 7070 6564 3a20 4465  elf, wrapped: De
-00004bc0: 7363 7269 7074 6f72 293a 0a20 2020 2020  scriptor):.     
-00004bd0: 2073 656c 662e 7772 6170 7065 6420 3d20   self.wrapped = 
-00004be0: 7772 6170 7065 640a 0a20 2020 2023 2063  wrapped..    # c
-00004bf0: 6f6e 6469 7469 6f6e 616c 6c79 2064 6566  onditionally def
-00004c00: 696e 6520 6465 7363 7269 7074 6f72 206d  ine descriptor m
-00004c10: 6574 686f 6473 0a20 2020 2069 6620 6861  ethods.    if ha
-00004c20: 7361 7474 7228 6465 7363 7269 7074 6f72  sattr(descriptor
-00004c30: 2c20 275f 5f67 6574 5f5f 2729 3a0a 2020  , '__get__'):.  
-00004c40: 2020 2020 6465 6620 5f5f 6765 745f 5f28      def __get__(
-00004c50: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
-00004c60: 7761 7267 7329 3a0a 2020 2020 2020 2020  wargs):.        
-00004c70: 2320 6865 7265 2077 6520 7769 6c6c 2063  # here we will c
-00004c80: 6174 6368 2069 6e74 6572 6e61 6c20 4174  atch internal At
-00004c90: 7472 6962 7574 6545 7272 6f72 2061 6e64  tributeError and
-00004ca0: 2072 652d 7261 6973 6520 6974 2061 7320   re-raise it as 
-00004cb0: 610a 2020 2020 2020 2020 2320 6d6f 7265  a.        # more
-00004cc0: 2069 6e66 6f72 6d61 7469 7665 2061 6e64   informative and
-00004cd0: 2063 6f72 7265 6374 2065 7272 6f72 206d   correct error m
-00004ce0: 6573 7361 6765 2e0a 2020 2020 2020 2020  essage..        
-00004cf0: 7472 793a 0a20 2020 2020 2020 2020 2072  try:.          r
-00004d00: 6574 7572 6e20 7365 6c66 2e77 7261 7070  eturn self.wrapp
-00004d10: 6564 2e5f 5f67 6574 5f5f 282a 6172 6773  ed.__get__(*args
-00004d20: 2c20 2a2a 6b77 6172 6773 290a 2020 2020  , **kwargs).    
-00004d30: 2020 2020 6578 6365 7074 2041 7474 7269      except Attri
-00004d40: 6275 7465 4572 726f 7220 6173 2065 3a0a  buteError as e:.
-00004d50: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-00004d60: 6572 726f 7273 2e44 6573 6372 6970 746f  errors.Descripto
-00004d70: 7241 7474 7269 6275 7465 4572 726f 7228  rAttributeError(
-00004d80: 2920 6672 6f6d 2065 0a0a 2020 2020 6966  ) from e..    if
-00004d90: 2068 6173 6174 7472 2864 6573 6372 6970   hasattr(descrip
-00004da0: 746f 722c 2027 5f5f 7365 745f 5f27 293a  tor, '__set__'):
-00004db0: 0a20 2020 2020 2064 6566 205f 5f73 6574  .      def __set
-00004dc0: 5f5f 2873 656c 662c 202a 6172 6773 2c20  __(self, *args, 
-00004dd0: 2a2a 6b77 6172 6773 293a 0a20 2020 2020  **kwargs):.     
-00004de0: 2020 2072 6574 7572 6e20 7365 6c66 2e77     return self.w
-00004df0: 7261 7070 6564 2e5f 5f73 6574 5f5f 282a  rapped.__set__(*
-00004e00: 6172 6773 2c20 2a2a 6b77 6172 6773 290a  args, **kwargs).
-00004e10: 0a20 2020 2069 6620 6861 7361 7474 7228  .    if hasattr(
-00004e20: 6465 7363 7269 7074 6f72 2c20 275f 5f64  descriptor, '__d
-00004e30: 656c 6574 655f 5f27 293a 0a20 2020 2020  elete__'):.     
-00004e40: 2064 6566 205f 5f64 656c 6574 655f 5f28   def __delete__(
-00004e50: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
-00004e60: 7761 7267 7329 3a0a 2020 2020 2020 2020  wargs):.        
-00004e70: 7265 7475 726e 2073 656c 662e 7772 6170  return self.wrap
-00004e80: 7065 642e 5f5f 6465 6c65 7465 5f5f 282a  ped.__delete__(*
-00004e90: 6172 6773 2c20 2a2a 6b77 6172 6773 290a  args, **kwargs).
-00004ea0: 0a20 2020 2069 6620 6861 7361 7474 7228  .    if hasattr(
-00004eb0: 6465 7363 7269 7074 6f72 2c20 275f 5f73  descriptor, '__s
-00004ec0: 6574 5f6e 616d 655f 5f27 293a 0a20 2020  et_name__'):.   
-00004ed0: 2020 2064 6566 205f 5f73 6574 5f6e 616d     def __set_nam
-00004ee0: 655f 5f28 7365 6c66 2c20 2a61 7267 732c  e__(self, *args,
-00004ef0: 202a 2a6b 7761 7267 7329 3a0a 2020 2020   **kwargs):.    
-00004f00: 2020 2020 7365 6c66 2e77 7261 7070 6564      self.wrapped
-00004f10: 2e5f 5f73 6574 5f6e 616d 655f 5f28 2a61  .__set_name__(*a
-00004f20: 7267 732c 202a 2a6b 7761 7267 7329 0a0a  rgs, **kwargs)..
-00004f30: 2020 2020 6465 6620 5f5f 6765 7461 7474      def __getatt
-00004f40: 725f 5f28 7365 6c66 2c20 6e61 6d65 293a  r__(self, name):
-00004f50: 0a20 2020 2020 2072 6574 7572 6e20 6765  .      return ge
-00004f60: 7461 7474 7228 7365 6c66 2e77 7261 7070  tattr(self.wrapp
-00004f70: 6564 2c20 6e61 6d65 290a 0a20 2072 6574  ed, name)..  ret
-00004f80: 7572 6e20 5f44 6573 6372 6970 746f 7257  urn _DescriptorW
-00004f90: 7261 7070 6572 2864 6573 6372 6970 746f  rapper(descripto
-00004fa0: 7229 0a0a 2320 4261 7365 204d 6f64 756c  r)..# Base Modul
-00004fb0: 6520 6465 6669 6e69 7469 6f6e 2e0a 2320  e definition..# 
-00004fc0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00004fd0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00004fe0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00004ff0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00005000: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 0a23  -------------..#
-00005010: 2054 6865 204d 6f64 756c 6542 6173 6520   The ModuleBase 
-00005020: 636c 6173 7320 6973 2063 7265 6174 6564  class is created
-00005030: 206f 6e6c 7920 746f 206d 616b 6520 7374   only to make st
-00005040: 6174 6963 2061 6e61 6c79 7a65 7273 2068  atic analyzers h
-00005050: 6170 7079 0a23 206d 6169 6e6c 7920 7079  appy.# mainly py
-00005060: 7479 7065 2061 6e64 2070 7972 6967 6874  type and pyright
-00005070: 2e20 536f 6d65 206e 6f74 6573 3a0a 2320  . Some notes:.# 
-00005080: 2a20 7079 7269 6768 7420 2863 6f72 7265  * pyright (corre
-00005090: 6374 6c79 2920 636f 6d70 6c61 696e 7320  ctly) complains 
-000050a0: 7468 6174 204d 6f64 756c 6520 6974 7365  that Module itse
-000050b0: 6c66 2069 7320 6e6f 7420 6120 6461 7461  lf is not a data
-000050c0: 636c 6173 732c 2065 7665 6e0a 2320 2020  class, even.#   
-000050d0: 7468 6f75 6768 2061 6c6c 2069 7473 2073  though all its s
-000050e0: 7562 636c 6173 7365 7320 616e 6420 696e  ubclasses and in
-000050f0: 7461 6e63 6573 2041 5245 2064 6174 6163  tances ARE datac
-00005100: 6c61 7373 6573 2e20 4265 6361 7573 6520  lasses. Because 
-00005110: 7468 6572 6520 6973 206e 6f0a 2320 2020  there is no.#   
-00005120: 7761 7920 746f 2061 6e6e 6f74 6174 6520  way to annotate 
-00005130: 7468 6973 2069 6e20 6120 7761 7920 7468  this in a way th
-00005140: 6174 2070 7972 6967 6874 2075 6e64 6572  at pyright under
-00005150: 7374 616e 6473 2c20 7765 2063 7265 6174  stands, we creat
-00005160: 6520 610a 2320 2020 4d6f 6475 6c65 4261  e a.#   ModuleBa
-00005170: 7365 2063 6c61 7373 2064 6563 6f72 6174  se class decorat
-00005180: 6564 2077 6974 6820 6064 6174 6163 6c61  ed with `datacla
-00005190: 7373 5f74 7261 6e73 666f 726d 6020 7375  ss_transform` su
-000051a0: 6368 2074 6861 7420 7079 7269 6768 740a  ch that pyright.
-000051b0: 2320 2020 7468 696e 6b73 204d 6f64 756c  #   thinks Modul
-000051c0: 6520 6973 2061 2064 6174 6163 6c61 7373  e is a dataclass
-000051d0: 2028 696e 2072 6561 6c69 7479 206f 6e6c   (in reality onl
-000051e0: 7920 7375 6263 6c61 7373 6573 2061 7265  y subclasses are
-000051f0: 2069 6e73 7461 6e74 6961 7465 640a 2320   instantiated.# 
-00005200: 2020 736f 2074 6869 7320 6973 2066 696e    so this is fin
-00005210: 6529 2e0a 2320 2a20 5468 6520 605f 5f64  e)..# * The `__d
-00005220: 6174 6163 6c61 7373 5f66 6965 6c64 735f  ataclass_fields_
-00005230: 5f60 2061 7474 7269 6275 7465 2069 7320  _` attribute is 
-00005240: 6e65 6564 6564 2062 6563 6175 7365 2070  needed because p
-00005250: 7974 7970 6520 7365 656d 7320 746f 0a23  ytype seems to.#
-00005260: 2020 206e 6f74 2075 6e64 6572 7374 616e     not understan
-00005270: 6420 7468 6520 6064 6174 6163 6c61 7373  d the `dataclass
-00005280: 5f74 7261 6e73 666f 726d 6020 6465 636f  _transform` deco
-00005290: 7261 746f 722c 2074 6865 7265 666f 7265  rator, therefore
-000052a0: 2077 6520 6e65 6564 0a23 2020 2074 6f20   we need.#   to 
-000052b0: 6164 6420 7468 6520 6174 7472 6962 7574  add the attribut
-000052c0: 6520 6d61 6e75 616c 6c79 2e0a 2320 2a20  e manually..# * 
-000052d0: 4f74 6865 7220 6174 7472 6962 7574 6573  Other attributes
-000052e0: 2061 7265 2061 6e6e 6f74 6174 6564 2066   are annotated f
-000052f0: 6f72 2063 6f6d 706c 6574 656e 6573 732e  or completeness.
-00005300: 2042 6563 6175 7365 2077 6520 6172 6520   Because we are 
-00005310: 7573 696e 670a 2320 2020 7468 6520 6069  using.#   the `i
-00005320: 6620 7479 7069 6e67 2e54 5950 455f 4348  f typing.TYPE_CH
-00005330: 4543 4b49 4e47 6020 7061 7474 6572 6e2c  ECKING` pattern,
-00005340: 2074 6865 7365 2061 6e6e 6f74 6174 696f   these annotatio
-00005350: 6e73 2061 7265 206e 6f74 2070 7265 7365  ns are not prese
-00005360: 6e74 0a23 2020 2061 7420 7275 6e74 696d  nt.#   at runtim
-00005370: 6520 736f 2074 6865 7920 646f 6e27 7420  e so they don't 
-00005380: 6166 6665 6374 2074 6865 2064 6174 6163  affect the datac
-00005390: 6c61 7373 2062 6568 6176 696f 722e 0a40  lass behavior..@
-000053a0: 6461 7461 636c 6173 735f 7472 616e 7366  dataclass_transf
-000053b0: 6f72 6d28 290a 636c 6173 7320 4d6f 6475  orm().class Modu
-000053c0: 6c65 4261 7365 3a0a 2020 6966 2074 7970  leBase:.  if typ
-000053d0: 696e 672e 5459 5045 5f43 4845 434b 494e  ing.TYPE_CHECKIN
-000053e0: 473a 0a20 2020 2073 636f 7065 3a20 4f70  G:.    scope: Op
-000053f0: 7469 6f6e 616c 5b53 636f 7065 5d0a 2020  tional[Scope].  
-00005400: 2020 5f73 7461 7465 3a20 5f4d 6f64 756c    _state: _Modul
-00005410: 6549 6e74 6572 6e61 6c53 7461 7465 0a20  eInternalState. 
-00005420: 2020 205f 7061 7265 6e74 5f72 6566 3a20     _parent_ref: 
-00005430: 556e 696f 6e5b 274d 6f64 756c 6527 2c20  Union['Module', 
-00005440: 7765 616b 7265 662e 5265 6665 7265 6e63  weakref.Referenc
-00005450: 6554 7970 655b 274d 6f64 756c 6527 5d2c  eType['Module'],
-00005460: 204e 6f6e 655d 0a20 2020 2070 6172 656e   None].    paren
-00005470: 743a 2055 6e69 6f6e 5b27 4d6f 6475 6c65  t: Union['Module
-00005480: 272c 205f 5365 6e74 696e 656c 2c20 4e6f  ', _Sentinel, No
-00005490: 6e65 5d0a 2020 2020 5f5f 6461 7461 636c  ne].    __datacl
-000054a0: 6173 735f 6669 656c 6473 5f5f 3a20 4469  ass_fields__: Di
-000054b0: 6374 5b73 7472 2c20 6461 7461 636c 6173  ct[str, dataclas
-000054c0: 7365 732e 4669 656c 645d 0a0a 636c 6173  ses.Field]..clas
-000054d0: 7320 4d6f 6475 6c65 284d 6f64 756c 6542  s Module(ModuleB
-000054e0: 6173 6529 3a0a 2020 2222 2242 6173 6520  ase):.  """Base 
-000054f0: 636c 6173 7320 666f 7220 616c 6c20 6e65  class for all ne
-00005500: 7572 616c 206e 6574 776f 726b 206d 6f64  ural network mod
-00005510: 756c 6573 2e20 4c61 7965 7273 2061 6e64  ules. Layers and
-00005520: 206d 6f64 656c 7320 7368 6f75 6c64 2073   models should s
-00005530: 7562 636c 6173 7320 7468 6973 2063 6c61  ubclass this cla
-00005540: 7373 2e0a 0a20 2041 6c6c 2046 6c61 7820  ss...  All Flax 
-00005550: 4d6f 6475 6c65 7320 6172 6520 5079 7468  Modules are Pyth
-00005560: 6f6e 2033 2e37 0a20 2060 6461 7461 636c  on 3.7.  `datacl
-00005570: 6173 7365 7320 3c68 7474 7073 3a2f 2f64  asses <https://d
-00005580: 6f63 732e 7079 7468 6f6e 2e6f 7267 2f33  ocs.python.org/3
-00005590: 2f6c 6962 7261 7279 2f64 6174 6163 6c61  /library/datacla
-000055a0: 7373 6573 2e68 746d 6c3e 605f 2e20 5369  sses.html>`_. Si
-000055b0: 6e63 650a 2020 6461 7461 636c 6173 7365  nce.  dataclasse
-000055c0: 7320 7461 6b65 206f 7665 7220 6060 5f5f  s take over ``__
-000055d0: 696e 6974 5f5f 6060 2c20 796f 7520 7368  init__``, you sh
-000055e0: 6f75 6c64 2069 6e73 7465 6164 206f 7665  ould instead ove
-000055f0: 7272 6964 6520 3a6d 6574 683a 6073 6574  rride :meth:`set
-00005600: 7570 602c 0a20 2077 6869 6368 2069 7320  up`,.  which is 
-00005610: 6175 746f 6d61 7469 6361 6c6c 7920 6361  automatically ca
-00005620: 6c6c 6564 2074 6f20 696e 6974 6961 6c69  lled to initiali
-00005630: 7a65 2074 6865 206d 6f64 756c 652e 0a0a  ze the module...
-00005640: 2020 4d6f 6475 6c65 7320 6361 6e20 636f    Modules can co
-00005650: 6e74 6169 6e20 7375 626d 6f64 756c 6573  ntain submodules
-00005660: 2c20 616e 6420 696e 2074 6869 7320 7761  , and in this wa
-00005670: 7920 6361 6e20 6265 206e 6573 7465 6420  y can be nested 
-00005680: 696e 2061 2074 7265 650a 2020 7374 7275  in a tree.  stru
-00005690: 6374 7572 652e 2053 7562 6d6f 6465 6c73  cture. Submodels
-000056a0: 2063 616e 2062 6520 6173 7369 676e 6564   can be assigned
-000056b0: 2061 7320 7265 6775 6c61 7220 6174 7472   as regular attr
-000056c0: 6962 7574 6573 2069 6e73 6964 6520 7468  ibutes inside th
-000056d0: 650a 2020 3a6d 6574 683a 6073 6574 7570  e.  :meth:`setup
-000056e0: 6020 6d65 7468 6f64 2e0a 0a20 2059 6f75  ` method...  You
-000056f0: 2063 616e 2064 6566 696e 6520 6172 6269   can define arbi
-00005700: 7472 6172 7920 2266 6f72 7761 7264 2070  trary "forward p
-00005710: 6173 7322 206d 6574 686f 6473 206f 6e20  ass" methods on 
-00005720: 796f 7572 204d 6f64 756c 6520 7375 6263  your Module subc
-00005730: 6c61 7373 2e0a 2020 5768 696c 6520 6e6f  lass..  While no
-00005740: 206d 6574 686f 6473 2061 7265 2073 7065   methods are spe
-00005750: 6369 616c 2d63 6173 6564 2c20 6060 5f5f  cial-cased, ``__
-00005760: 6361 6c6c 5f5f 6060 2069 7320 6120 706f  call__`` is a po
-00005770: 7075 6c61 7220 6368 6f69 6365 2062 6563  pular choice bec
-00005780: 6175 7365 0a20 2069 7420 616c 6c6f 7773  ause.  it allows
-00005790: 2079 6f75 2074 6f20 7573 6520 6d6f 6475   you to use modu
-000057a0: 6c65 2069 6e73 7461 6e63 6573 2061 7320  le instances as 
-000057b0: 6966 2074 6865 7920 6172 6520 6675 6e63  if they are func
-000057c0: 7469 6f6e 733a 3a0a 0a20 2020 2066 726f  tions::..    fro
-000057d0: 6d20 666c 6178 2069 6d70 6f72 7420 6c69  m flax import li
-000057e0: 6e65 6e20 6173 206e 6e0a 0a20 2020 2063  nen as nn..    c
-000057f0: 6c61 7373 204d 6f64 756c 6528 6e6e 2e4d  lass Module(nn.M
-00005800: 6f64 756c 6529 3a0a 2020 2020 2020 6665  odule):.      fe
-00005810: 6174 7572 6573 3a20 5475 706c 655b 696e  atures: Tuple[in
-00005820: 742c 202e 2e2e 5d20 3d20 2831 362c 2034  t, ...] = (16, 4
-00005830: 290a 0a20 2020 2020 2064 6566 2073 6574  )..      def set
-00005840: 7570 2873 656c 6629 3a0a 2020 2020 2020  up(self):.      
-00005850: 2020 7365 6c66 2e64 656e 7365 3120 3d20    self.dense1 = 
-00005860: 4465 6e73 6528 7365 6c66 2e66 6561 7475  Dense(self.featu
-00005870: 7265 735b 305d 290a 2020 2020 2020 2020  res[0]).        
-00005880: 7365 6c66 2e64 656e 7365 3220 3d20 4465  self.dense2 = De
-00005890: 6e73 6528 7365 6c66 2e66 6561 7475 7265  nse(self.feature
-000058a0: 735b 315d 290a 0a20 2020 2020 2064 6566  s[1])..      def
-000058b0: 205f 5f63 616c 6c5f 5f28 7365 6c66 2c20   __call__(self, 
-000058c0: 7829 3a0a 2020 2020 2020 2020 7265 7475  x):.        retu
-000058d0: 726e 2073 656c 662e 6465 6e73 6532 286e  rn self.dense2(n
-000058e0: 6e2e 7265 6c75 2873 656c 662e 6465 6e73  n.relu(self.dens
-000058f0: 6531 2878 2929 290a 0a20 204f 7074 696f  e1(x)))..  Optio
-00005900: 6e61 6c6c 792c 2066 6f72 206d 6f72 6520  nally, for more 
-00005910: 636f 6e63 6973 6520 6d6f 6475 6c65 2069  concise module i
-00005920: 6d70 6c65 6d65 6e74 6174 696f 6e73 2077  mplementations w
-00005930: 6865 7265 2073 7562 6d6f 6475 6c65 730a  here submodules.
-00005940: 2020 6465 6669 6e69 7469 6f6e 7320 6172    definitions ar
-00005950: 6520 636f 2d6c 6f63 6174 6564 2077 6974  e co-located wit
-00005960: 6820 7468 6569 7220 7573 6167 652c 2079  h their usage, y
-00005970: 6f75 2063 616e 2075 7365 2074 6865 0a20  ou can use the. 
-00005980: 203a 6d65 7468 3a60 636f 6d70 6163 7460   :meth:`compact`
-00005990: 2077 7261 7070 6572 2e0a 2020 2222 220a   wrapper..  """.
-000059a0: 0a20 2069 6620 7479 7069 6e67 2e54 5950  .  if typing.TYP
-000059b0: 455f 4348 4543 4b49 4e47 3a0a 2020 2020  E_CHECKING:.    
-000059c0: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
-000059d0: 662c 202a 6172 6773 2c20 2a2a 6b77 6172  f, *args, **kwar
-000059e0: 6773 293a 0a20 2020 2020 2023 2074 6869  gs):.      # thi
-000059f0: 7320 7374 7562 206d 616b 6573 2073 7572  s stub makes sur
-00005a00: 6520 7079 7479 7065 2061 6363 6570 7473  e pytype accepts
-00005a10: 2063 6f6e 7374 7275 6374 6f72 2061 7267   constructor arg
-00005a20: 756d 656e 7473 2e0a 2020 2020 2020 7061  uments..      pa
-00005a30: 7373 0a0a 2020 2020 6465 6620 5f5f 6361  ss..    def __ca
-00005a40: 6c6c 5f5f 2873 656c 662c 202a 6172 6773  ll__(self, *args
-00005a50: 2c20 2a2a 6b77 6172 6773 2920 2d3e 2041  , **kwargs) -> A
-00005a60: 6e79 3a0a 2020 2020 2020 2320 7468 6973  ny:.      # this
-00005a70: 2073 7475 6220 616c 6c6f 7773 2070 7974   stub allows pyt
-00005a80: 7970 6520 746f 2061 6363 6570 7420 4d6f  ype to accept Mo
-00005a90: 6475 6c65 7320 6173 2043 616c 6c61 626c  dules as Callabl
-00005aa0: 6573 2e0a 2020 2020 2020 7061 7373 0a0a  es..      pass..
-00005ab0: 2020 4063 6c61 7373 6d65 7468 6f64 0a20    @classmethod. 
-00005ac0: 2064 6566 205f 5f69 6e69 745f 7375 6263   def __init_subc
-00005ad0: 6c61 7373 5f5f 2863 6c73 2c20 6b77 5f6f  lass__(cls, kw_o
-00005ae0: 6e6c 793a 2062 6f6f 6c20 3d20 4661 6c73  nly: bool = Fals
-00005af0: 652c 202a 2a6b 7761 7267 733a 2041 6e79  e, **kwargs: Any
-00005b00: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    "
-00005b10: 2222 4175 746f 6d61 7469 6361 6c6c 7920  ""Automatically 
-00005b20: 696e 6974 6961 6c69 7a65 7320 616c 6c20  initializes all 
-00005b30: 7375 6263 6c61 7373 6573 2061 7320 6375  subclasses as cu
-00005b40: 7374 6f6d 2064 6174 6163 6c61 7373 6573  stom dataclasses
-00005b50: 2e22 2222 0a20 2020 2073 7570 6572 2829  .""".    super()
-00005b60: 2e5f 5f69 6e69 745f 7375 6263 6c61 7373  .__init_subclass
-00005b70: 5f5f 282a 2a6b 7761 7267 7329 0a20 2020  __(**kwargs).   
-00005b80: 2023 2041 6c6c 2046 6c61 7820 4d6f 6475   # All Flax Modu
-00005b90: 6c65 7320 6172 6520 6461 7461 636c 6173  les are dataclas
-00005ba0: 7365 732e 2020 5765 2066 6f72 6365 2074  ses.  We force t
-00005bb0: 6869 7320 636f 6e76 656e 7469 6f6e 2073  his convention s
-00005bc0: 696e 6365 0a20 2020 2023 2069 7420 656e  ince.    # it en
-00005bd0: 636f 7572 6167 6573 2074 6865 2073 7461  courages the sta
-00005be0: 7465 6c65 7373 2062 6568 6176 696f 7220  teless behavior 
-00005bf0: 6e65 6564 6564 2074 6f20 636c 6f6e 6520  needed to clone 
-00005c00: 6d6f 6475 6c65 2069 6e73 7461 6e63 6573  module instances
-00005c10: 2066 6f72 0a20 2020 2023 2066 756e 6374   for.    # funct
-00005c20: 696f 6e61 6c20 7472 616e 7366 6f72 6d61  ional transforma
-00005c30: 7469 6f6e 2e20 2049 6e73 7465 6164 206f  tion.  Instead o
-00005c40: 6620 7573 696e 6720 6120 7079 7468 6f6e  f using a python
-00005c50: 206d 6574 6163 6c61 7373 2c20 7765 0a20   metaclass, we. 
-00005c60: 2020 2023 2061 7574 6f6d 6174 6963 616c     # automatical
-00005c70: 6c79 2074 7261 6e73 666f 726d 204d 6f64  ly transform Mod
-00005c80: 756c 6573 2069 6e74 6f20 6461 7461 636c  ules into datacl
-00005c90: 6173 7365 7320 6174 2073 7562 636c 6173  asses at subclas
-00005ca0: 7320 6372 6561 7469 6f6e 0a20 2020 2023  s creation.    #
-00005cb0: 2074 696d 652c 2061 6e64 2077 6520 7365   time, and we se
-00005cc0: 7420 7468 6520 6c61 7374 2064 6174 6163  t the last datac
-00005cd0: 6c61 7373 2061 7267 756d 656e 7473 2074  lass arguments t
-00005ce0: 6f20 6070 6172 656e 7460 2061 6e64 2060  o `parent` and `
-00005cf0: 6e61 6d65 602e 0a20 2020 2063 6c73 2e5f  name`..    cls._
-00005d00: 6375 7374 6f6d 697a 6564 5f64 6174 6163  customized_datac
-00005d10: 6c61 7373 5f74 7261 6e73 666f 726d 286b  lass_transform(k
-00005d20: 775f 6f6e 6c79 290a 2020 2020 2320 5765  w_only).    # We
-00005d30: 2077 7261 7020 7573 6572 2d64 6566 696e   wrap user-defin
-00005d40: 6564 206d 6574 686f 6473 2069 6e63 6c75  ed methods inclu
-00005d50: 6469 6e67 2073 6574 7570 2061 6e64 205f  ding setup and _
-00005d60: 5f63 616c 6c5f 5f20 746f 2065 6e66 6f72  _call__ to enfor
-00005d70: 6365 0a20 2020 2023 2061 206e 756d 6265  ce.    # a numbe
-00005d80: 7220 6f66 2064 6966 6665 7265 6e74 2063  r of different c
-00005d90: 6865 636b 7320 616e 6420 746f 2070 726f  hecks and to pro
-00005da0: 7669 6465 2063 6c65 6172 2065 7272 6f72  vide clear error
-00005db0: 206d 6573 7361 6765 732e 0a20 2020 2063   messages..    c
-00005dc0: 6c73 2e5f 7665 7269 6679 5f73 696e 676c  ls._verify_singl
-00005dd0: 655f 6f72 5f6e 6f5f 636f 6d70 6163 7428  e_or_no_compact(
-00005de0: 290a 2020 2020 636c 732e 5f77 7261 705f  ).    cls._wrap_
-00005df0: 6d6f 6475 6c65 5f61 7474 7269 6275 7465  module_attribute
-00005e00: 7328 290a 2020 2020 2320 5365 7420 656d  s().    # Set em
-00005e10: 7074 7920 636c 6173 7320 6465 6661 756c  pty class defaul
-00005e20: 7473 2e0a 2020 2020 636c 732e 5f73 7461  ts..    cls._sta
-00005e30: 7465 203d 205f 756e 696e 6974 6961 6c69  te = _uninitiali
-00005e40: 7a65 645f 6d6f 6475 6c65 5f69 6e74 6572  zed_module_inter
-00005e50: 6e61 6c5f 7374 6174 6520 2320 7479 7065  nal_state # type
-00005e60: 3a20 6967 6e6f 7265 5b61 7474 722d 6465  : ignore[attr-de
-00005e70: 6669 6e65 645d 0a20 2020 2063 6c73 2e73  fined].    cls.s
-00005e80: 636f 7065 3a20 4f70 7469 6f6e 616c 5b53  cope: Optional[S
-00005e90: 636f 7065 5d20 3d20 4e6f 6e65 2023 2074  cope] = None # t
-00005ea0: 7970 653a 2069 676e 6f72 650a 2020 2020  ype: ignore.    
-00005eb0: 2320 4861 6e64 6c65 7320 7765 616b 2072  # Handles weak r
-00005ec0: 6566 6572 656e 6369 6e67 206f 6620 7061  eferencing of pa
-00005ed0: 7265 6e74 204d 6f64 756c 6573 2074 6f20  rent Modules to 
-00005ee0: 7072 6576 656e 7420 7265 6665 7265 6e63  prevent referenc
-00005ef0: 6520 6379 636c 6573 2e0a 2020 2020 636c  e cycles..    cl
-00005f00: 732e 5f70 6172 656e 745f 7265 6620 3d20  s._parent_ref = 
-00005f10: 4e6f 6e65 2023 2074 7970 653a 2069 676e  None # type: ign
-00005f20: 6f72 655b 6174 7472 2d64 6566 696e 6564  ore[attr-defined
-00005f30: 5d0a 2020 2020 636c 732e 7061 7265 6e74  ].    cls.parent
-00005f40: 203d 2050 6172 656e 7444 6573 6372 6970   = ParentDescrip
-00005f50: 746f 7228 2920 2320 7479 7065 3a20 6967  tor() # type: ig
-00005f60: 6e6f 7265 5b61 7373 6967 6e6d 656e 745d  nore[assignment]
-00005f70: 0a0a 2020 4063 6c61 7373 6d65 7468 6f64  ..  @classmethod
-00005f80: 0a20 2064 6566 205f 6375 7374 6f6d 697a  .  def _customiz
-00005f90: 6564 5f64 6174 6163 6c61 7373 5f74 7261  ed_dataclass_tra
-00005fa0: 6e73 666f 726d 2863 6c73 2c20 6b77 5f6f  nsform(cls, kw_o
-00005fb0: 6e6c 793a 2062 6f6f 6c29 3a0a 2020 2020  nly: bool):.    
-00005fc0: 2222 2254 7261 6e73 666f 726d 7320 6063  """Transforms `c
-00005fd0: 6c73 6020 696e 746f 2061 2064 6174 6163  ls` into a datac
-00005fe0: 6c61 7373 2c20 7769 7468 2063 7573 746f  lass, with custo
-00005ff0: 6d20 6164 6469 7469 6f6e 616c 2062 6568  m additional beh
-00006000: 6176 696f 722e 0a0a 2020 2020 312e 2049  avior...    1. I
-00006010: 6e6a 6563 7420 6070 6172 656e 7460 2061  nject `parent` a
-00006020: 6e64 2060 6e61 6d65 6020 6669 656c 6473  nd `name` fields
-00006030: 2e20 2028 4966 2074 6865 7920 6172 6520  .  (If they are 
-00006040: 616c 7265 6164 7920 7072 6573 656e 742c  already present,
-00006050: 0a20 2020 2020 2020 7468 656e 2063 6865  .       then che
-00006060: 636b 2074 6861 7420 7468 6579 2068 6176  ck that they hav
-00006070: 6520 7468 6520 6578 7065 6374 6564 2074  e the expected t
-00006080: 7970 6573 2e29 0a20 2020 2032 2e20 5365  ypes.).    2. Se
-00006090: 7420 636f 6d70 6172 652c 2068 6173 682c  t compare, hash,
-000060a0: 2061 6e64 2072 6570 7220 746f 2046 616c   and repr to Fal
-000060b0: 7365 2066 6f72 206e 6f6e 2d69 6e69 7420  se for non-init 
-000060c0: 6669 656c 6473 2e0a 2020 2020 332e 2047  fields..    3. G
-000060d0: 656e 6572 6174 6520 6120 6861 7368 2066  enerate a hash f
-000060e0: 756e 6374 696f 6e20 2869 6620 6e6f 7420  unction (if not 
-000060f0: 7072 6f76 6964 6564 2062 7920 636c 7329  provided by cls)
-00006100: 2e0a 2020 2020 2222 220a 2020 2020 2320  ..    """.    # 
-00006110: 4368 6563 6b20 7265 7365 7276 6564 2061  Check reserved a
-00006120: 7474 7269 6275 7465 7320 6861 7665 2065  ttributes have e
-00006130: 7870 6563 7465 6420 7479 7065 2061 6e6e  xpected type ann
-00006140: 6f74 6174 696f 6e73 2e0a 2020 2020 616e  otations..    an
-00006150: 6e6f 7461 7469 6f6e 7320 3d20 6469 6374  notations = dict
-00006160: 2863 6c73 2e5f 5f64 6963 745f 5f2e 6765  (cls.__dict__.ge
-00006170: 7428 275f 5f61 6e6e 6f74 6174 696f 6e73  t('__annotations
-00006180: 5f5f 272c 207b 7d29 290a 2020 2020 6966  __', {})).    if
-00006190: 2061 6e6e 6f74 6174 696f 6e73 2e67 6574   annotations.get
-000061a0: 2827 7061 7265 6e74 272c 205f 5061 7265  ('parent', _Pare
-000061b0: 6e74 5479 7065 2920 213d 205f 5061 7265  ntType) != _Pare
-000061c0: 6e74 5479 7065 3a0a 2020 2020 2020 7261  ntType:.      ra
-000061d0: 6973 6520 6572 726f 7273 2e52 6573 6572  ise errors.Reser
-000061e0: 7665 644d 6f64 756c 6541 7474 7269 6275  vedModuleAttribu
-000061f0: 7465 4572 726f 7228 616e 6e6f 7461 7469  teError(annotati
-00006200: 6f6e 7329 0a20 2020 2069 6620 616e 6e6f  ons).    if anno
-00006210: 7461 7469 6f6e 732e 6765 7428 276e 616d  tations.get('nam
-00006220: 6527 2c20 7374 7229 206e 6f74 2069 6e20  e', str) not in 
-00006230: 2827 7374 7227 2c20 7374 722c 204f 7074  ('str', str, Opt
-00006240: 696f 6e61 6c5b 7374 725d 293a 0a20 2020  ional[str]):.   
-00006250: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
-00006260: 5265 7365 7276 6564 4d6f 6475 6c65 4174  ReservedModuleAt
-00006270: 7472 6962 7574 6545 7272 6f72 2861 6e6e  tributeError(ann
-00006280: 6f74 6174 696f 6e73 290a 0a20 2020 2023  otations)..    #
-00006290: 2061 6e79 206e 6f6e 2d69 6e69 7420 6669   any non-init fi
-000062a0: 656c 6420 7769 6c6c 206f 6e6c 7920 6265  eld will only be
-000062b0: 2073 6574 2069 6e20 7365 7475 700a 2020   set in setup.  
-000062c0: 2020 2320 4475 7269 6e67 205f 5f68 6173    # During __has
-000062d0: 685f 5f20 616e 6420 5f5f 6571 5f5f 2074  h__ and __eq__ t
-000062e0: 6865 2066 6965 6c64 2069 7320 6e6f 7420  he field is not 
-000062f0: 7365 7420 7965 740a 2020 2020 2320 736f  set yet.    # so
-00006300: 2069 7420 7368 6f75 6c64 206e 6f74 2062   it should not b
-00006310: 6520 7573 6564 2069 6e20 636f 6d70 6172  e used in compar
-00006320: 652c 2068 6173 6820 6f72 2072 6570 722e  e, hash or repr.
-00006330: 0a20 2020 2066 6f72 2066 6965 6c64 2069  .    for field i
-00006340: 6e20 616e 6e6f 7461 7469 6f6e 733a 0a20  n annotations:. 
-00006350: 2020 2020 2066 6965 6c64 5f6d 6574 6120       field_meta 
-00006360: 3d20 6765 7461 7474 7228 636c 732c 2066  = getattr(cls, f
-00006370: 6965 6c64 2c20 4e6f 6e65 290a 2020 2020  ield, None).    
-00006380: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
-00006390: 6669 656c 645f 6d65 7461 2c20 6461 7461  field_meta, data
-000063a0: 636c 6173 7365 732e 4669 656c 6429 2061  classes.Field) a
-000063b0: 6e64 206e 6f74 2066 6965 6c64 5f6d 6574  nd not field_met
-000063c0: 612e 696e 6974 3a0a 2020 2020 2020 2020  a.init:.        
-000063d0: 6669 656c 645f 6d65 7461 2e63 6f6d 7061  field_meta.compa
-000063e0: 7265 203d 2046 616c 7365 0a20 2020 2020  re = False.     
-000063f0: 2020 2066 6965 6c64 5f6d 6574 612e 6861     field_meta.ha
-00006400: 7368 203d 2046 616c 7365 0a20 2020 2020  sh = False.     
-00006410: 2020 2066 6965 6c64 5f6d 6574 612e 7265     field_meta.re
-00006420: 7072 203d 2046 616c 7365 0a0a 2020 2020  pr = False..    
-00006430: 6578 7472 615f 6669 656c 6473 203d 205b  extra_fields = [
-00006440: 2827 7061 7265 6e74 272c 205f 5061 7265  ('parent', _Pare
-00006450: 6e74 5479 7065 2c0a 2020 2020 2020 2020  ntType,.        
-00006460: 2020 2020 2020 2020 2020 2020 206b 775f               kw_
-00006470: 6f6e 6c79 5f64 6174 6163 6c61 7373 6573  only_dataclasses
-00006480: 2e66 6965 6c64 280a 2020 2020 2020 2020  .field(.        
-00006490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000064a0: 2072 6570 723d 4661 6c73 652c 2064 6566   repr=False, def
-000064b0: 6175 6c74 3d5f 756e 7370 6563 6966 6965  ault=_unspecifie
-000064c0: 645f 7061 7265 6e74 2c0a 2020 2020 2020  d_parent,.      
-000064d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000064e0: 2020 206b 775f 6f6e 6c79 3d54 7275 6529     kw_only=True)
-000064f0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00006500: 2020 2020 2020 2028 276e 616d 6527 2c20         ('name', 
-00006510: 4f70 7469 6f6e 616c 5b73 7472 5d2c 0a20  Optional[str],. 
-00006520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006530: 2020 2020 6b77 5f6f 6e6c 795f 6461 7461      kw_only_data
-00006540: 636c 6173 7365 732e 6669 656c 6428 6465  classes.field(de
-00006550: 6661 756c 743d 4e6f 6e65 2c20 6b77 5f6f  fault=None, kw_o
-00006560: 6e6c 793d 5472 7565 2929 5d0a 0a20 2020  nly=True))]..   
-00006570: 2069 6620 6b77 5f6f 6e6c 793a 0a20 2020   if kw_only:.   
-00006580: 2020 2069 6620 7475 706c 6528 7379 732e     if tuple(sys.
-00006590: 7665 7273 696f 6e5f 696e 666f 295b 3a33  version_info)[:3
-000065a0: 5d20 3e3d 2028 332c 2031 302c 2030 293a  ] >= (3, 10, 0):
-000065b0: 0a20 2020 2020 2020 2066 6f72 206e 616d  .        for nam
-000065c0: 652c 2061 6e6e 6f74 6174 696f 6e2c 2064  e, annotation, d
-000065d0: 6566 6175 6c74 2069 6e20 6578 7472 615f  efault in extra_
-000065e0: 6669 656c 6473 3a20 2023 2070 7974 7970  fields:  # pytyp
-000065f0: 653a 2064 6973 6162 6c65 3d69 6e76 616c  e: disable=inval
-00006600: 6964 2d61 6e6e 6f74 6174 696f 6e0a 2020  id-annotation.  
-00006610: 2020 2020 2020 2020 7365 7461 7474 7228          setattr(
-00006620: 636c 732c 206e 616d 652c 2064 6566 6175  cls, name, defau
-00006630: 6c74 290a 2020 2020 2020 2020 2020 636c  lt).          cl
-00006640: 732e 5f5f 616e 6e6f 7461 7469 6f6e 735f  s.__annotations_
-00006650: 5f5b 6e61 6d65 5d20 3d20 616e 6e6f 7461  _[name] = annota
-00006660: 7469 6f6e 0a20 2020 2020 2020 2064 6174  tion.        dat
-00006670: 6163 6c61 7373 6573 2e64 6174 6163 6c61  aclasses.datacla
-00006680: 7373 280a 2020 2020 2020 2020 2020 2020  ss(.            
-00006690: 756e 7361 6665 5f68 6173 683d 275f 5f68  unsafe_hash='__h
-000066a0: 6173 685f 5f27 206e 6f74 2069 6e20 636c  ash__' not in cl
-000066b0: 732e 5f5f 6469 6374 5f5f 2c0a 2020 2020  s.__dict__,.    
-000066c0: 2020 2020 2020 2020 7265 7072 3d46 616c          repr=Fal
-000066d0: 7365 2c0a 2020 2020 2020 2020 2020 2020  se,.            
-000066e0: 6b77 5f6f 6e6c 793d 5472 7565 2c0a 2020  kw_only=True,.  
-000066f0: 2020 2020 2020 2928 636c 7329 2020 2320        )(cls)  # 
-00006700: 7479 7065 3a20 6967 6e6f 7265 5b63 616c  type: ignore[cal
-00006710: 6c2d 6f76 6572 6c6f 6164 5d0a 2020 2020  l-overload].    
-00006720: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-00006730: 7261 6973 6520 5479 7065 4572 726f 7228  raise TypeError(
-00006740: 2760 6b77 5f6f 6e6c 7960 2069 7320 6e6f  '`kw_only` is no
-00006750: 7420 6176 6169 6c61 626c 6520 6265 666f  t available befo
-00006760: 7265 2050 7920 332e 3130 2e27 290a 2020  re Py 3.10.').  
-00006770: 2020 656c 7365 3a0a 2020 2020 2020 2320    else:.      # 
-00006780: 4e6f 7720 6170 706c 7920 6461 7461 636c  Now apply datacl
-00006790: 6173 7320 7472 616e 7366 6f72 6d20 2877  ass transform (w
-000067a0: 6869 6368 206f 7065 7261 7465 7320 696e  hich operates in
-000067b0: 2d70 6c61 6365 292e 0a20 2020 2020 2023  -place)..      #
-000067c0: 2044 6f20 6765 6e65 7261 7465 2061 2068   Do generate a h
-000067d0: 6173 6820 6675 6e63 7469 6f6e 206f 6e6c  ash function onl
-000067e0: 7920 6966 206e 6f74 2070 726f 7669 6465  y if not provide
-000067f0: 6420 6279 2074 6865 2063 6c61 7373 2e0a  d by the class..
-00006800: 2020 2020 2020 6b77 5f6f 6e6c 795f 6461        kw_only_da
-00006810: 7461 636c 6173 7365 732e 6461 7461 636c  taclasses.datacl
-00006820: 6173 7328 0a20 2020 2020 2020 2020 2063  ass(.          c
-00006830: 6c73 2c0a 2020 2020 2020 2020 2020 756e  ls,.          un
-00006840: 7361 6665 5f68 6173 683d 275f 5f68 6173  safe_hash='__has
-00006850: 685f 5f27 206e 6f74 2069 6e20 636c 732e  h__' not in cls.
-00006860: 5f5f 6469 6374 5f5f 2c0a 2020 2020 2020  __dict__,.      
-00006870: 2020 2020 7265 7072 3d46 616c 7365 2c0a      repr=False,.
-00006880: 2020 2020 2020 2020 2020 6578 7472 615f            extra_
-00006890: 6669 656c 6473 3d65 7874 7261 5f66 6965  fields=extra_fie
-000068a0: 6c64 7329 2020 2320 7079 7479 7065 3a20  lds)  # pytype: 
-000068b0: 6469 7361 626c 653d 7772 6f6e 672d 6b65  disable=wrong-ke
-000068c0: 7977 6f72 642d 6172 6773 0a0a 2020 2020  yword-args..    
-000068d0: 636c 732e 5f5f 6861 7368 5f5f 203d 205f  cls.__hash__ = _
-000068e0: 7772 6170 5f68 6173 6828 636c 732e 5f5f  wrap_hash(cls.__
-000068f0: 6861 7368 5f5f 2920 2023 2074 7970 653a  hash__)  # type:
-00006900: 2069 676e 6f72 655b 6d65 7468 6f64 2d61   ignore[method-a
-00006910: 7373 6967 6e5d 0a0a 2020 4063 6c61 7373  ssign]..  @class
-00006920: 6d65 7468 6f64 0a20 2064 6566 205f 7665  method.  def _ve
-00006930: 7269 6679 5f73 696e 676c 655f 6f72 5f6e  rify_single_or_n
-00006940: 6f5f 636f 6d70 6163 7428 636c 7329 3a0a  o_compact(cls):.
-00006950: 2020 2020 2222 2253 7461 7469 6361 6c6c      """Staticall
-00006960: 7920 7665 7269 6669 6573 2074 6861 7420  y verifies that 
-00006970: 6174 206d 6f73 7420 6120 7369 6e67 6c65  at most a single
-00006980: 206d 6574 686f 6420 6973 206c 6162 656c   method is label
-00006990: 6c65 6420 636f 6d70 6163 742e 2222 220a  led compact.""".
-000069a0: 2020 2020 6d65 7468 6f64 7320 3d20 5b6d      methods = [m
-000069b0: 5b30 5d20 666f 7220 6d20 696e 2069 6e73  [0] for m in ins
-000069c0: 7065 6374 2e67 6574 6d65 6d62 6572 7328  pect.getmembers(
-000069d0: 636c 732c 2070 7265 6469 6361 7465 3d63  cls, predicate=c
-000069e0: 616c 6c61 626c 6529 5d0a 2020 2020 6e5f  allable)].    n_
-000069f0: 636f 6d70 6163 745f 666e 7320 3d20 6c65  compact_fns = le
-00006a00: 6e28 5b6d 6574 686f 645f 6e61 6d65 2066  n([method_name f
-00006a10: 6f72 206d 6574 686f 645f 6e61 6d65 2069  or method_name i
-00006a20: 6e20 6d65 7468 6f64 730a 2020 2020 2020  n methods.      
-00006a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006a40: 2020 2069 6620 6861 7361 7474 7228 6765     if hasattr(ge
-00006a50: 7461 7474 7228 636c 732c 206d 6574 686f  tattr(cls, metho
-00006a60: 645f 6e61 6d65 292c 2027 636f 6d70 6163  d_name), 'compac
-00006a70: 7427 295d 290a 2020 2020 6966 206e 5f63  t')]).    if n_c
-00006a80: 6f6d 7061 6374 5f66 6e73 203e 2031 3a0a  ompact_fns > 1:.
-00006a90: 2020 2020 2020 7261 6973 6520 6572 726f        raise erro
-00006aa0: 7273 2e4d 756c 7469 706c 654d 6574 686f  rs.MultipleMetho
-00006ab0: 6473 436f 6d70 6163 7445 7272 6f72 2829  dsCompactError()
-00006ac0: 0a0a 2020 4063 6c61 7373 6d65 7468 6f64  ..  @classmethod
-00006ad0: 0a20 2064 6566 205f 7772 6170 5f6d 6f64  .  def _wrap_mod
-00006ae0: 756c 655f 6174 7472 6962 7574 6573 2863  ule_attributes(c
-00006af0: 6c73 293a 0a20 2020 2022 2222 5772 6170  ls):.    """Wrap
-00006b00: 7320 7573 6572 2d64 6566 696e 6564 206e  s user-defined n
-00006b10: 6f6e 2d69 6e68 6572 6974 6564 206d 6574  on-inherited met
-00006b20: 686f 6473 2061 6e64 2064 6573 6372 6970  hods and descrip
-00006b30: 746f 7273 2077 6974 6820 7374 6174 650a  tors with state.
-00006b40: 2020 2020 6d61 6e61 6765 6d65 6e74 2066      management f
-00006b50: 756e 6374 696f 6e73 2e0a 2020 2020 2222  unctions..    ""
-00006b60: 220a 2020 2020 2320 7772 6170 206d 6574  ".    # wrap met
-00006b70: 686f 6473 0a20 2020 206d 6574 686f 645f  hods.    method_
-00006b80: 6578 636c 7573 696f 6e73 203d 2028 5b66  exclusions = ([f
-00006b90: 2e6e 616d 6520 666f 7220 6620 696e 2064  .name for f in d
-00006ba0: 6174 6163 6c61 7373 6573 2e66 6965 6c64  ataclasses.field
-00006bb0: 7328 636c 7329 5d20 2b0a 2020 2020 2020  s(cls)] +.      
-00006bc0: 2020 2020 2020 2020 2020 2020 5b27 5f5f              ['__
-00006bd0: 6571 5f5f 272c 2027 5f5f 7265 7072 5f5f  eq__', '__repr__
-00006be0: 272c 2027 5f5f 696e 6974 5f5f 272c 2027  ', '__init__', '
-00006bf0: 5f5f 6861 7368 5f5f 272c 0a20 2020 2020  __hash__',.     
-00006c00: 2020 2020 2020 2020 2020 2020 2020 275f                '_
-00006c10: 5f70 6f73 745f 696e 6974 5f5f 275d 290a  _post_init__']).
-00006c20: 2020 2020 666f 7220 6b65 7920 696e 205f      for key in _
-00006c30: 6765 745f 6c6f 6361 6c5f 6d65 7468 6f64  get_local_method
-00006c40: 5f6e 616d 6573 2863 6c73 2c20 6578 636c  _names(cls, excl
-00006c50: 7564 653d 6d65 7468 6f64 5f65 7863 6c75  ude=method_exclu
-00006c60: 7369 6f6e 7329 3a0a 2020 2020 2020 6d65  sions):.      me
-00006c70: 7468 6f64 203d 2067 6574 6174 7472 2863  thod = getattr(c
-00006c80: 6c73 2c20 6b65 7929 0a20 2020 2020 2069  ls, key).      i
-00006c90: 6620 6861 7361 7474 7228 6d65 7468 6f64  f hasattr(method
-00006ca0: 2c20 276e 6f77 7261 7027 293a 0a20 2020  , 'nowrap'):.   
-00006cb0: 2020 2020 2063 6f6e 7469 6e75 650a 2020       continue.  
-00006cc0: 2020 2020 7365 7461 7474 7228 636c 732c      setattr(cls,
-00006cd0: 206b 6579 2c20 7772 6170 5f6d 6574 686f   key, wrap_metho
-00006ce0: 645f 6f6e 6365 286d 6574 686f 6429 290a  d_once(method)).
-00006cf0: 0a20 2020 2023 2077 7261 7020 6465 7363  .    # wrap desc
-00006d00: 7269 7074 6f72 730a 2020 2020 6465 7363  riptors.    desc
-00006d10: 7269 7074 6f72 5f65 7863 6c75 7369 6f6e  riptor_exclusion
-00006d20: 7320 3d20 285b 662e 6e61 6d65 2066 6f72  s = ([f.name for
-00006d30: 2066 2069 6e20 6461 7461 636c 6173 7365   f in dataclasse
-00006d40: 732e 6669 656c 6473 2863 6c73 295d 202b  s.fields(cls)] +
-00006d50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00006d60: 2020 2020 2020 2020 2020 2020 2020 5b27                ['
-00006d70: 7061 7265 6e74 272c 2027 5f5f 6469 6374  parent', '__dict
-00006d80: 5f5f 275d 290a 2020 2020 666f 7220 6b65  __']).    for ke
-00006d90: 7920 696e 205f 6765 745f 6c6f 6361 6c5f  y in _get_local_
-00006da0: 6465 7363 7269 7074 6f72 5f6e 616d 6573  descriptor_names
-00006db0: 2863 6c73 2c20 6465 7363 7269 7074 6f72  (cls, descriptor
-00006dc0: 5f65 7863 6c75 7369 6f6e 7329 3a0a 2020  _exclusions):.  
-00006dd0: 2020 2020 2320 646f 6e27 7420 7573 6520      # don't use 
-00006de0: 6765 7461 7474 7220 6865 7265 2c20 7369  getattr here, si
-00006df0: 6e63 6520 6974 2077 696c 6c20 6361 6c6c  nce it will call
-00006e00: 2074 6865 2064 6573 6372 6970 746f 720a   the descriptor.
-00006e10: 2020 2020 2020 6465 7363 7269 7074 6f72        descriptor
-00006e20: 203d 2063 6c73 2e5f 5f64 6963 745f 5f5b   = cls.__dict__[
-00006e30: 6b65 795d 0a20 2020 2020 2069 6620 6861  key].      if ha
-00006e40: 7361 7474 7228 6465 7363 7269 7074 6f72  sattr(descriptor
-00006e50: 2c20 276e 6f77 7261 7027 293a 0a20 2020  , 'nowrap'):.   
-00006e60: 2020 2020 2063 6f6e 7469 6e75 650a 2020       continue.  
-00006e70: 2020 2020 7365 7461 7474 7228 636c 732c      setattr(cls,
-00006e80: 206b 6579 2c20 7772 6170 5f64 6573 6372   key, wrap_descr
-00006e90: 6970 746f 725f 6f6e 6365 2864 6573 6372  iptor_once(descr
-00006ea0: 6970 746f 7229 290a 2020 2020 7265 7475  iptor)).    retu
-00006eb0: 726e 2063 6c73 0a0a 2020 6465 6620 5f63  rn cls..  def _c
-00006ec0: 616c 6c5f 7772 6170 7065 645f 6d65 7468  all_wrapped_meth
-00006ed0: 6f64 2873 656c 662c 2066 756e 2c20 6172  od(self, fun, ar
-00006ee0: 6773 2c20 6b77 6172 6773 293a 0a20 2020  gs, kwargs):.   
-00006ef0: 2022 2222 2243 616c 6c73 2061 2077 7261   """"Calls a wra
-00006f00: 7070 6564 206d 6574 686f 642e 0a0a 2020  pped method...  
-00006f10: 2020 5468 6973 2066 756e 6374 696f 6e20    This function 
-00006f20: 6973 2072 6573 706f 6e73 6962 6c65 2066  is responsible f
-00006f30: 6f72 2073 6574 7469 6e67 2075 7020 7468  or setting up th
-00006f40: 6520 7468 7265 6164 206c 6f63 616c 2073  e thread local s
-00006f50: 7461 7465 0a20 2020 2063 6f72 7265 6374  tate.    correct
-00006f60: 6c79 2062 6566 6f72 6520 6361 6c6c 696e  ly before callin
-00006f70: 6720 7468 6520 6d65 7468 6f64 2061 6e64  g the method and
-00006f80: 2063 6c65 616e 696e 6720 7570 2061 6674   cleaning up aft
-00006f90: 6572 7761 7264 732e 0a20 2020 2054 6869  erwards..    Thi
-00006fa0: 7320 696e 636c 7564 6573 2073 746f 7269  s includes stori
-00006fb0: 6e67 2069 6e74 6572 6d65 6469 6174 6573  ng intermediates
-00006fc0: 2c20 7365 7475 7020 6f66 2074 6865 2063  , setup of the c
-00006fd0: 6f6d 7061 6374 2073 636f 7065 2c0a 2020  ompact scope,.  
-00006fe0: 2020 616e 6420 6d61 6b69 6e67 2073 7572    and making sur
-00006ff0: 6520 7365 7475 7020 6973 2063 616c 6c65  e setup is calle
-00007000: 6420 6265 666f 7265 2061 6e79 206f 7468  d before any oth
-00007010: 6572 206d 6574 686f 642e 0a0a 2020 2020  er method...    
-00007020: 4172 6773 3a0a 2020 2020 2020 6675 6e3a  Args:.      fun:
-00007030: 2054 6865 2077 7261 7070 6564 206d 6574   The wrapped met
-00007040: 686f 642e 0a20 2020 2020 2061 7267 733a  hod..      args:
-00007050: 204e 616d 6564 2061 7267 756d 656e 7473   Named arguments
-00007060: 2070 6173 7365 6420 746f 2060 6066 756e   passed to ``fun
-00007070: 6060 2e0a 2020 2020 2020 6b77 6172 6773  ``..      kwargs
-00007080: 3a20 4b65 7977 6f72 6420 6172 6775 6d65  : Keyword argume
-00007090: 6e74 7320 7061 7373 6564 2074 6f20 6060  nts passed to ``
-000070a0: 6675 6e60 602e 0a0a 2020 2020 5265 7475  fun``...    Retu
-000070b0: 726e 733a 0a20 2020 2020 2054 6865 2072  rns:.      The r
-000070c0: 6573 756c 7473 206f 6620 6361 6c6c 696e  esults of callin
-000070d0: 6720 6060 6675 6e60 602e 0a20 2020 2022  g ``fun``..    "
-000070e0: 2222 0a20 2020 2069 735f 636f 6d70 6163  "".    is_compac
-000070f0: 745f 6d65 7468 6f64 203d 2068 6173 6174  t_method = hasat
-00007100: 7472 2866 756e 2c20 2763 6f6d 7061 6374  tr(fun, 'compact
-00007110: 2729 0a20 2020 2066 756e 5f6e 616d 6520  ').    fun_name 
-00007120: 3d20 6765 7461 7474 7228 6675 6e2c 2027  = getattr(fun, '
-00007130: 5f5f 6e61 6d65 5f5f 272c 2027 756e 6e61  __name__', 'unna
-00007140: 6d65 645f 6675 6e63 7469 6f6e 2729 0a20  med_function'). 
-00007150: 2020 2069 735f 7365 7475 705f 6d65 7468     is_setup_meth
-00007160: 6f64 203d 2066 756e 5f6e 616d 6520 3d3d  od = fun_name ==
-00007170: 2027 7365 7475 7027 0a20 2020 2061 6464   'setup'.    add
-00007180: 5f63 616c 6c5f 696e 666f 203d 206e 6f74  _call_info = not
-00007190: 2069 735f 7365 7475 705f 6d65 7468 6f64   is_setup_method
-000071a0: 2061 6e64 206c 656e 285f 636f 6e74 6578   and len(_contex
-000071b0: 742e 6361 6c6c 5f69 6e66 6f5f 7374 6163  t.call_info_stac
-000071c0: 6b29 203e 2030 0a20 2020 2023 2057 6520  k) > 0.    # We 
-000071d0: 6c61 7a69 6c79 2063 616c 6c20 7365 7475  lazily call setu
-000071e0: 7028 2920 6f6e 6c79 2077 6865 6e20 6e65  p() only when ne
-000071f0: 6564 6564 2e0a 2020 2020 6966 2069 735f  eded..    if is_
-00007200: 7365 7475 705f 6d65 7468 6f64 3a0a 2020  setup_method:.  
-00007210: 2020 2020 6966 2073 656c 662e 7363 6f70      if self.scop
-00007220: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     
-00007230: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
-00007240: 4361 6c6c 5365 7475 7055 6e62 6f75 6e64  CallSetupUnbound
-00007250: 4d6f 6475 6c65 4572 726f 7228 290a 2020  ModuleError().  
-00007260: 2020 2020 6973 5f72 6563 7572 7265 6e74      is_recurrent
-00007270: 203d 2073 656c 662e 5f73 7461 7465 2e69   = self._state.i
-00007280: 6e5f 7365 7475 700a 2020 2020 2020 7365  n_setup.      se
-00007290: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
-000072a0: 7570 203d 2054 7275 650a 2020 2020 656c  up = True.    el
-000072b0: 7365 3a0a 2020 2020 2020 7365 6c66 2e5f  se:.      self._
-000072c0: 7472 795f 7365 7475 7028 290a 0a20 2020  try_setup()..   
-000072d0: 2069 6620 6973 5f63 6f6d 7061 6374 5f6d   if is_compact_m
-000072e0: 6574 686f 643a 0a20 2020 2020 2069 6620  ethod:.      if 
-000072f0: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
-00007300: 6e65 3a0a 2020 2020 2020 2020 7261 6973  ne:.        rais
-00007310: 6520 6572 726f 7273 2e43 616c 6c43 6f6d  e errors.CallCom
-00007320: 7061 6374 556e 626f 756e 644d 6f64 756c  pactUnboundModul
-00007330: 6545 7272 6f72 2829 0a20 2020 2020 2069  eError().      i
-00007340: 735f 7265 6375 7272 656e 7420 3d20 7365  s_recurrent = se
-00007350: 6c66 2e5f 7374 6174 652e 696e 5f63 6f6d  lf._state.in_com
-00007360: 7061 6374 5f6d 6574 686f 640a 2020 2020  pact_method.    
-00007370: 2020 7365 6c66 2e5f 7374 6174 652e 696e    self._state.in
-00007380: 5f63 6f6d 7061 6374 5f6d 6574 686f 6420  _compact_method 
-00007390: 3d20 5472 7565 0a20 2020 205f 636f 6e74  = True.    _cont
-000073a0: 6578 742e 6d6f 6475 6c65 5f73 7461 636b  ext.module_stack
-000073b0: 2e61 7070 656e 6428 7365 6c66 290a 2020  .append(self).  
-000073c0: 2020 7472 793a 0a20 2020 2020 2023 2067    try:.      # g
-000073d0: 6574 2063 616c 6c20 696e 666f 0a20 2020  et call info.   
-000073e0: 2020 2069 6620 6164 645f 6361 6c6c 5f69     if add_call_i
-000073f0: 6e66 6f3a 0a20 2020 2020 2020 2061 7373  nfo:.        ass
-00007400: 6572 7420 7365 6c66 2e73 636f 7065 2069  ert self.scope i
-00007410: 7320 6e6f 7420 4e6f 6e65 0a20 2020 2020  s not None.     
-00007420: 2020 2063 616c 6c5f 696e 6465 7820 3d20     call_index = 
-00007430: 5f63 6f6e 7465 7874 2e63 616c 6c5f 696e  _context.call_in
-00007440: 666f 5f73 7461 636b 5b2d 315d 2e67 6574  fo_stack[-1].get
-00007450: 5f63 616c 6c5f 696e 6465 7828 7365 6c66  _call_index(self
-00007460: 290a 2020 2020 2020 2020 7363 6f70 655f  ).        scope_
-00007470: 7061 7468 203d 206a 6178 2e74 7265 655f  path = jax.tree_
-00007480: 7574 696c 2e74 7265 655f 6d61 7028 5f66  util.tree_map(_f
-00007490: 6978 5f70 6174 685f 7061 7274 2c20 7365  ix_path_part, se
-000074a0: 6c66 2e73 636f 7065 2e70 6174 6829 0a0a  lf.scope.path)..
-000074b0: 2020 2020 2020 2320 6361 6c6c 206d 6574        # call met
-000074c0: 686f 640a 2020 2020 2020 6966 205f 7573  hod.      if _us
-000074d0: 655f 6e61 6d65 645f 6361 6c6c 3a0a 2020  e_named_call:.  
-000074e0: 2020 2020 2020 7769 7468 206a 6178 2e6e        with jax.n
-000074f0: 616d 6564 5f73 636f 7065 285f 6465 7269  amed_scope(_deri
-00007500: 7665 5f70 726f 6669 6c69 6e67 5f6e 616d  ve_profiling_nam
-00007510: 6528 7365 6c66 2c20 6675 6e29 293a 0a20  e(self, fun)):. 
-00007520: 2020 2020 2020 2020 2079 203d 2066 756e           y = fun
-00007530: 2873 656c 662c 202a 6172 6773 2c20 2a2a  (self, *args, **
-00007540: 6b77 6172 6773 290a 2020 2020 2020 656c  kwargs).      el
-00007550: 7365 3a0a 2020 2020 2020 2020 7920 3d20  se:.        y = 
-00007560: 6675 6e28 7365 6c66 2c20 2a61 7267 732c  fun(self, *args,
-00007570: 202a 2a6b 7761 7267 7329 0a0a 2020 2020   **kwargs)..    
-00007580: 2020 6966 205f 636f 6e74 6578 742e 6361    if _context.ca
-00007590: 7074 7572 655f 7374 6163 6b3a 0a20 2020  pture_stack:.   
-000075a0: 2020 2020 2066 696c 7465 725f 666e 203d       filter_fn =
-000075b0: 205f 636f 6e74 6578 742e 6361 7074 7572   _context.captur
-000075c0: 655f 7374 6163 6b5b 2d31 5d0a 2020 2020  e_stack[-1].    
-000075d0: 2020 2020 6966 2066 696c 7465 725f 666e      if filter_fn
-000075e0: 2061 6e64 2066 696c 7465 725f 666e 2873   and filter_fn(s
-000075f0: 656c 662c 2066 756e 5f6e 616d 6529 3a0a  elf, fun_name):.
-00007600: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
-00007610: 6f77 2827 696e 7465 726d 6564 6961 7465  ow('intermediate
-00007620: 7327 2c20 6675 6e5f 6e61 6d65 2c20 7929  s', fun_name, y)
-00007630: 0a20 2020 2020 2069 6620 6164 645f 6361  .      if add_ca
-00007640: 6c6c 5f69 6e66 6f3a 0a20 2020 2020 2020  ll_info:.       
-00007650: 205f 6172 6773 2c20 5f6b 7761 7267 732c   _args, _kwargs,
-00007660: 205f 7920 3d20 666c 6178 2e6c 696e 656e   _y = flax.linen
-00007670: 2e73 756d 6d61 7279 2e5f 7265 7072 6573  .summary._repres
-00007680: 656e 745f 7472 6565 2828 6172 6773 2c20  ent_tree((args, 
-00007690: 6b77 6172 6773 2c20 7929 290a 2020 2020  kwargs, y)).    
-000076a0: 2020 2020 5f63 6f6e 7465 7874 2e63 616c      _context.cal
-000076b0: 6c5f 696e 666f 5f73 7461 636b 5b2d 315d  l_info_stack[-1]
-000076c0: 2e63 616c 6c73 2e61 7070 656e 6428 0a20  .calls.append(. 
-000076d0: 2020 2020 2020 2020 205f 4361 6c6c 496e           _CallIn
-000076e0: 666f 2863 616c 6c5f 696e 6465 782c 2073  fo(call_index, s
-000076f0: 636f 7065 5f70 6174 682c 2074 7970 6528  cope_path, type(
-00007700: 7365 6c66 292c 2066 756e 2e5f 5f6e 616d  self), fun.__nam
-00007710: 655f 5f2c 205f 6172 6773 2c20 5f6b 7761  e__, _args, _kwa
-00007720: 7267 732c 205f 7929 290a 2020 2020 2020  rgs, _y)).      
-00007730: 7265 7475 726e 2079 0a20 2020 2066 696e  return y.    fin
-00007740: 616c 6c79 3a0a 2020 2020 2020 5f63 6f6e  ally:.      _con
-00007750: 7465 7874 2e6d 6f64 756c 655f 7374 6163  text.module_stac
-00007760: 6b2e 706f 7028 290a 2020 2020 2020 6966  k.pop().      if
-00007770: 2069 735f 636f 6d70 6163 745f 6d65 7468   is_compact_meth
-00007780: 6f64 3a0a 2020 2020 2020 2020 6f62 6a65  od:.        obje
-00007790: 6374 2e5f 5f73 6574 6174 7472 5f5f 2873  ct.__setattr__(s
-000077a0: 656c 662c 2027 7363 6f70 6527 2c20 7365  elf, 'scope', se
-000077b0: 6c66 2e73 636f 7065 2e72 6577 6f75 6e64  lf.scope.rewound
-000077c0: 2829 290a 2020 2020 2020 2320 7365 7475  ()).      # setu
-000077d0: 7020 6f72 2063 6f6d 7061 6374 2063 616c  p or compact cal
-000077e0: 6c73 2063 616e 2062 6520 7265 6375 7272  ls can be recurr
-000077f0: 656e 7420 666f 7220 6578 616d 706c 6520  ent for example 
-00007800: 6475 6520 746f 2073 7570 6572 2063 616c  due to super cal
-00007810: 6c73 0a20 2020 2020 2023 2072 6573 6574  ls.      # reset
-00007820: 7469 6e67 2074 6865 2073 7461 7465 2077  ting the state w
-00007830: 6f75 6c64 2063 6175 7365 2069 7320 636f  ould cause is co
-00007840: 6d70 6163 742f 7365 7475 7020 6d65 7468  mpact/setup meth
-00007850: 6f64 0a20 2020 2020 2023 2074 6f20 6265  od.      # to be
-00007860: 2073 6574 2074 6f20 4661 6c73 6520 7072   set to False pr
-00007870: 656d 6174 7572 656c 792e 0a20 2020 2020  ematurely..     
-00007880: 2069 6620 2869 735f 636f 6d70 6163 745f   if (is_compact_
-00007890: 6d65 7468 6f64 206f 7220 6973 5f73 6574  method or is_set
-000078a0: 7570 5f6d 6574 686f 6429 2061 6e64 206e  up_method) and n
-000078b0: 6f74 2069 735f 7265 6375 7272 656e 743a  ot is_recurrent:
-000078c0: 0a20 2020 2020 2020 2073 656c 662e 5f73  .        self._s
-000078d0: 7461 7465 2e72 6573 6574 2829 0a0a 2020  tate.reset()..  
-000078e0: 6465 6620 5f5f 7365 7461 7474 725f 5f28  def __setattr__(
-000078f0: 7365 6c66 2c20 6e61 6d65 3a20 7374 722c  self, name: str,
-00007900: 2076 616c 3a20 416e 7929 3a0a 2020 2020   val: Any):.    
-00007910: 2222 2253 6574 7320 616e 2061 7474 7269  """Sets an attri
-00007920: 6275 7465 206f 6e20 7468 6973 204d 6f64  bute on this Mod
-00007930: 756c 652e 0a0a 2020 2020 5765 206f 7665  ule...    We ove
-00007940: 726c 6f61 6420 7365 7461 7474 7220 736f  rload setattr so
-00007950: 6c65 6c79 2074 6f20 7375 7070 6f72 7420  lely to support 
-00007960: 7079 7468 6f6e 6963 206e 616d 696e 6720  pythonic naming 
-00007970: 7669 6120 6173 7369 676e 6d65 6e74 206f  via assignment o
-00007980: 660a 2020 2020 7375 626d 6f64 756c 6573  f.    submodules
-00007990: 2069 6e20 7468 6520 7370 6563 6961 6c20   in the special 
-000079a0: 3a6d 6574 683a 6073 6574 7570 6020 6675  :meth:`setup` fu
-000079b0: 6e63 7469 6f6e 3a3a 0a0a 2020 2020 2020  nction::..      
-000079c0: 7365 6c66 2e73 7562 6d6f 6475 6c65 5f6e  self.submodule_n
-000079d0: 616d 6520 3d20 4d79 4d6f 6475 6c65 282e  ame = MyModule(.
-000079e0: 2e2e 290a 0a20 2020 2057 6520 616c 736f  ..)..    We also
-000079f0: 2073 7570 706f 7274 206c 6973 7473 2061   support lists a
-00007a00: 6e64 206f 7468 6572 2067 656e 6572 616c  nd other general
-00007a10: 2070 7974 7265 6573 2c20 652e 672e 3a3a   pytrees, e.g.::
-00007a20: 0a0a 2020 2020 2020 7365 6c66 2e73 7562  ..      self.sub
-00007a30: 6d6f 6475 6c65 7320 3d20 5b4d 794d 6f64  modules = [MyMod
-00007a40: 756c 6530 282e 2e29 2c20 4d79 4d6f 6475  ule0(..), MyModu
-00007a50: 6c65 3128 2e2e 292c 202e 2e2e 5d0a 0a20  le1(..), ...].. 
-00007a60: 2020 2041 7267 733a 0a20 2020 2020 206e     Args:.      n
-00007a70: 616d 653a 2041 7474 7269 6275 7465 2074  ame: Attribute t
-00007a80: 6f20 7365 742e 0a20 2020 2020 2076 616c  o set..      val
-00007a90: 3a20 5661 6c75 6520 6f66 2074 6865 2061  : Value of the a
-00007aa0: 7474 7269 6275 7465 2e0a 2020 2020 2222  ttribute..    ""
-00007ab0: 220a 2020 2020 6669 656c 6473 203d 2073  ".    fields = s
-00007ac0: 656c 662e 5f5f 6461 7461 636c 6173 735f  elf.__dataclass_
-00007ad0: 6669 656c 6473 5f5f 2020 2320 7079 7479  fields__  # pyty
-00007ae0: 7065 3a20 6469 7361 626c 653d 6174 7472  pe: disable=attr
-00007af0: 6962 7574 652d 6572 726f 720a 2020 2020  ibute-error.    
-00007b00: 6973 5f64 6174 6163 6c61 7373 5f61 7474  is_dataclass_att
-00007b10: 7220 3d20 6e61 6d65 2069 6e20 6669 656c  r = name in fiel
-00007b20: 6473 2061 6e64 2066 6965 6c64 735b 6e61  ds and fields[na
-00007b30: 6d65 5d2e 696e 6974 0a0a 2020 2020 6966  me].init..    if
-00007b40: 206e 6f74 2073 656c 662e 5f73 7461 7465   not self._state
-00007b50: 2e69 6e5f 7365 7475 703a 0a20 2020 2020  .in_setup:.     
-00007b60: 2069 6620 6e6f 7420 7365 6c66 2e5f 7374   if not self._st
-00007b70: 6174 652e 6973 5f69 6e69 7469 616c 697a  ate.is_initializ
-00007b80: 6564 3a0a 2020 2020 2020 2020 2320 5365  ed:.        # Se
-00007b90: 7474 696e 6720 6174 7472 6962 7574 6573  tting attributes
-00007ba0: 2062 6566 6f72 6520 656e 6420 6f66 204d   before end of M
-00007bb0: 6f64 756c 652e 5f5f 706f 7374 5f69 6e69  odule.__post_ini
-00007bc0: 745f 5f28 290a 2020 2020 2020 2020 6f62  t__().        ob
-00007bd0: 6a65 6374 2e5f 5f73 6574 6174 7472 5f5f  ject.__setattr__
-00007be0: 2873 656c 662c 206e 616d 652c 2076 616c  (self, name, val
-00007bf0: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
-00007c00: 0a20 2020 2020 2065 6c73 653a 0a20 2020  .      else:.   
-00007c10: 2020 2020 2023 2057 6527 7265 2070 6173       # We're pas
-00007c20: 7420 616c 6c20 696e 6974 6961 6c69 7a61  t all initializa
-00007c30: 7469 6f6e 2061 6e64 2073 6574 7570 206c  tion and setup l
-00007c40: 6f67 6963 3a0a 2020 2020 2020 2020 2320  ogic:.        # 
-00007c50: 5261 6973 6573 2061 2054 7970 6545 7272  Raises a TypeErr
-00007c60: 6f72 206a 7573 7420 6c69 6b65 2066 726f  or just like fro
-00007c70: 7a65 6e20 7079 7468 6f6e 2064 6174 6163  zen python datac
-00007c80: 6c61 7373 6573 2e0a 2020 2020 2020 2020  lasses..        
-00007c90: 7261 6973 6520 6572 726f 7273 2e53 6574  raise errors.Set
-00007ca0: 4174 7472 6962 7574 6546 726f 7a65 6e4d  AttributeFrozenM
-00007cb0: 6f64 756c 6545 7272 6f72 280a 2020 2020  oduleError(.    
-00007cc0: 2020 2020 2020 2020 7365 6c66 2e5f 5f63          self.__c
-00007cd0: 6c61 7373 5f5f 2e5f 5f6e 616d 655f 5f2c  lass__.__name__,
-00007ce0: 206e 616d 652c 2076 616c 290a 0a20 2020   name, val)..   
-00007cf0: 2023 2057 6527 7265 2069 6e73 6964 6520   # We're inside 
-00007d00: 7468 6520 7365 7475 7028 2920 6d65 7468  the setup() meth
-00007d10: 6f64 3a0a 2020 2020 6966 2069 735f 6461  od:.    if is_da
-00007d20: 7461 636c 6173 735f 6174 7472 3a0a 2020  taclass_attr:.  
-00007d30: 2020 2020 2320 5468 6573 6520 6e61 6d65      # These name
-00007d40: 7320 6172 6520 7370 6563 6966 6965 6420  s are specified 
-00007d50: 6173 2064 6174 6163 6c61 7373 2066 6965  as dataclass fie
-00007d60: 6c64 732e 2054 6865 7920 7368 6f75 6c64  lds. They should
-00007d70: 206e 6f74 2062 650a 2020 2020 2020 2320   not be.      # 
-00007d80: 696e 6974 6961 6c69 7a65 6420 7769 7468  initialized with
-00007d90: 696e 2074 6865 2073 6574 7570 2829 206d  in the setup() m
-00007da0: 6574 686f 642c 2062 7574 2063 616e 2062  ethod, but can b
-00007db0: 6520 6d6f 6469 6669 6564 2066 7265 656c  e modified freel
-00007dc0: 790a 2020 2020 2020 2320 6265 666f 7265  y.      # before
-00007dd0: 2069 742e 0a20 2020 2020 2072 6169 7365   it..      raise
-00007de0: 2065 7272 6f72 732e 5365 7441 7474 7269   errors.SetAttri
-00007df0: 6275 7465 496e 4d6f 6475 6c65 5365 7475  buteInModuleSetu
-00007e00: 7045 7272 6f72 2829 0a0a 2020 2020 2320  pError()..    # 
-00007e10: 5661 6c75 6573 2028 7468 6174 206d 6179  Values (that may
-00007e20: 2062 6520 7661 7269 6162 6c65 7320 6f72   be variables or
-00007e30: 2073 7562 6d6f 6475 6c65 7329 2061 7265   submodules) are
-00007e40: 2062 6569 6e67 2064 6566 696e 6564 2061   being defined a
-00007e50: 6e64 0a20 2020 2023 2061 7474 6163 6865  nd.    # attache
-00007e60: 6420 696e 2073 6574 7570 2829 2c20 7765  d in setup(), we
-00007e70: 2072 756e 2073 6f6d 6520 6578 7472 6120   run some extra 
-00007e80: 6c6f 6769 6320 696e 2074 6861 7420 6361  logic in that ca
-00007e90: 7365 2e0a 2020 2020 7365 6c66 2e5f 7265  se..    self._re
-00007ea0: 6769 7374 6572 5f73 7562 6d6f 6475 6c65  gister_submodule
-00007eb0: 7328 6e61 6d65 2c20 7661 6c29 0a0a 2020  s(name, val)..  
-00007ec0: 6465 6620 5f5f 6765 7461 7474 725f 5f28  def __getattr__(
-00007ed0: 7365 6c66 2c20 6e61 6d65 3a20 7374 7229  self, name: str)
-00007ee0: 202d 3e20 416e 793a 0a20 2020 2022 2222   -> Any:.    """
-00007ef0: 4361 6c6c 2073 6574 7570 2829 2062 6566  Call setup() bef
-00007f00: 6f72 6520 6765 7474 696e 6720 616e 7920  ore getting any 
-00007f10: 7365 7475 702d 6465 6669 6e65 6420 6174  setup-defined at
-00007f20: 7472 6962 7574 6573 2e22 2222 0a20 2020  tributes.""".   
-00007f30: 2023 2057 6520 646f 6e27 7420 7761 6e74   # We don't want
-00007f40: 2074 6f20 7265 7475 726e 2061 6e79 7468   to return anyth
-00007f50: 696e 6720 666f 7220 7079 7468 6f6e 2063  ing for python c
-00007f60: 6f70 7920 2f20 7069 636b 6c65 206d 6574  opy / pickle met
-00007f70: 686f 6473 2e0a 2020 2020 6966 206e 616d  hods..    if nam
-00007f80: 6520 696e 205f 554e 4445 4649 4e45 445f  e in _UNDEFINED_
-00007f90: 434f 5059 5f50 4943 4b4c 455f 4d45 5448  COPY_PICKLE_METH
-00007fa0: 4f44 533a 0a20 2020 2020 2072 6169 7365  ODS:.      raise
-00007fb0: 2041 7474 7269 6275 7465 4572 726f 7228   AttributeError(
-00007fc0: 290a 2020 2020 7365 6c66 2e5f 7472 795f  ).    self._try_
-00007fd0: 7365 7475 7028 290a 2020 2020 6966 206e  setup().    if n
-00007fe0: 616d 6520 696e 2073 656c 662e 5f5f 6469  ame in self.__di
-00007ff0: 6374 5f5f 3a0a 2020 2020 2020 7265 7475  ct__:.      retu
-00008000: 726e 2073 656c 662e 5f5f 6469 6374 5f5f  rn self.__dict__
-00008010: 5b6e 616d 655d 0a20 2020 2065 6c73 653a  [name].    else:
-00008020: 0a20 2020 2020 206d 7367 203d 2066 2722  .      msg = f'"
-00008030: 7b73 656c 662e 5f5f 636c 6173 735f 5f2e  {self.__class__.
-00008040: 5f5f 6e61 6d65 5f5f 7d22 206f 626a 6563  __name__}" objec
-00008050: 7420 6861 7320 6e6f 2061 7474 7269 6275  t has no attribu
-00008060: 7465 2022 7b6e 616d 657d 222e 270a 2020  te "{name}".'.  
-00008070: 2020 2020 6966 2073 656c 662e 7363 6f70      if self.scop
-00008080: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     
-00008090: 2020 206d 7367 202b 3d20 2866 2720 4966     msg += (f' If
-000080a0: 2022 7b6e 616d 657d 2220 6973 2064 6566   "{name}" is def
-000080b0: 696e 6564 2069 6e20 5c27 2e73 6574 7570  ined in \'.setup
-000080c0: 2829 5c27 2c20 7265 6d65 6d62 6572 2074  ()\', remember t
-000080d0: 6865 7365 2066 6965 6c64 7320 270a 2020  hese fields '.  
-000080e0: 2020 2020 2020 2020 2761 7265 206f 6e6c          'are onl
-000080f0: 7920 6163 6365 7373 6962 6c65 2066 726f  y accessible fro
-00008100: 6d20 696e 7369 6465 205c 2769 6e69 745c  m inside \'init\
-00008110: 2720 6f72 205c 2761 7070 6c79 5c27 2e27  ' or \'apply\'.'
-00008120: 290a 2020 2020 2020 7261 6973 6520 4174  ).      raise At
-00008130: 7472 6962 7574 6545 7272 6f72 286d 7367  tributeError(msg
-00008140: 290a 0a20 2064 6566 205f 5f64 6972 5f5f  )..  def __dir__
-00008150: 2873 656c 6629 202d 3e20 4c69 7374 5b73  (self) -> List[s
-00008160: 7472 5d3a 0a20 2020 2022 2222 4361 6c6c  tr]:.    """Call
-00008170: 2073 6574 7570 2829 2062 6566 6f72 6520   setup() before 
-00008180: 6c69 7374 696e 6720 6174 7472 6962 7574  listing attribut
-00008190: 6573 2e22 2222 0a20 2020 2073 656c 662e  es.""".    self.
-000081a0: 5f74 7279 5f73 6574 7570 2829 0a20 2020  _try_setup().   
-000081b0: 2072 6574 7572 6e20 6f62 6a65 6374 2e5f   return object._
-000081c0: 5f64 6972 5f5f 2873 656c 6629 2020 2320  _dir__(self)  # 
-000081d0: 7479 7065 3a20 6967 6e6f 7265 0a0a 2020  type: ignore..  
-000081e0: 6465 6620 5f5f 706f 7374 5f69 6e69 745f  def __post_init_
-000081f0: 5f28 7365 6c66 2920 2d3e 204e 6f6e 653a  _(self) -> None:
-00008200: 0a20 2020 2023 2044 4f20 4e4f 5420 5245  .    # DO NOT RE
-00008210: 4d4f 5645 202d 204d 6172 6b65 7220 666f  MOVE - Marker fo
-00008220: 7220 696e 7465 726e 616c 206c 6f67 6769  r internal loggi
-00008230: 6e67 2e0a 2020 2020 2320 496e 2064 6174  ng..    # In dat
-00008240: 6163 6c61 7373 6573 2c20 5f5f 696e 6974  aclasses, __init
-00008250: 5f5f 2069 7320 6f76 6572 7269 6464 656e  __ is overridden
-00008260: 2074 6f20 7072 6f63 6573 7320 6461 7461   to process data
-00008270: 636c 6173 7320 6172 6775 6d65 6e74 732c  class arguments,
-00008280: 0a20 2020 2023 2061 6e64 205f 5f70 6f73  .    # and __pos
-00008290: 745f 696e 6974 5f5f 2069 7320 6361 6c6c  t_init__ is call
-000082a0: 6564 2069 6d6d 6564 6961 7465 6c79 2061  ed immediately a
-000082b0: 6674 6572 7761 7264 732e 2048 6572 652c  fterwards. Here,
-000082c0: 2064 6570 656e 6469 6e67 206f 6e20 7468   depending on th
-000082d0: 650a 2020 2020 2320 7479 7065 206f 6620  e.    # type of 
-000082e0: 6070 6172 656e 7460 2070 6173 7365 6420  `parent` passed 
-000082f0: 746f 2069 6e69 7469 616c 697a 6520 7468  to initialize th
-00008300: 6520 4d6f 6475 6c65 2c20 7765 2065 6974  e Module, we eit
-00008310: 6865 7220 6465 6665 720a 2020 2020 2320  her defer.    # 
-00008320: 696e 6974 6961 6c69 7a61 7469 6f6e 2c20  initialization, 
-00008330: 6174 7461 6368 2074 6869 7320 4d6f 6475  attach this Modu
-00008340: 6c65 2061 7320 6120 7375 626d 6f64 756c  le as a submodul
-00008350: 6520 6f66 2061 2070 6172 656e 742c 206f  e of a parent, o
-00008360: 7220 6269 6e64 0a20 2020 2023 2074 6869  r bind.    # thi
-00008370: 7320 4d6f 6475 6c65 2061 7420 7468 6520  s Module at the 
-00008380: 746f 702d 6c65 7665 6c20 746f 2076 6172  top-level to var
-00008390: 6961 626c 6573 2061 6e64 2072 6e67 732e  iables and rngs.
-000083a0: 0a0a 2020 2020 6f62 6a65 6374 2e5f 5f73  ..    object.__s
-000083b0: 6574 6174 7472 5f5f 2873 656c 662c 2027  etattr__(self, '
-000083c0: 5f69 6427 2c20 7575 6964 2829 290a 2020  _id', uuid()).  
-000083d0: 2020 6f62 6a65 6374 2e5f 5f73 6574 6174    object.__setat
-000083e0: 7472 5f5f 2873 656c 662c 2027 5f73 7461  tr__(self, '_sta
-000083f0: 7465 272c 205f 4d6f 6475 6c65 496e 7465  te', _ModuleInte
-00008400: 726e 616c 5374 6174 6528 2929 0a0a 2020  rnalState())..  
-00008410: 2020 2320 5479 7069 6361 6c6c 7920 7765    # Typically we
-00008420: 2073 6574 2074 6865 2070 6172 656e 7420   set the parent 
-00008430: 6261 7365 6420 6f6e 2074 6865 2064 796e  based on the dyn
-00008440: 616d 6963 206d 6f64 756c 6520 636f 6e74  amic module cont
-00008450: 6578 742e 0a20 2020 2069 6620 7365 6c66  ext..    if self
-00008460: 2e70 6172 656e 7420 6973 205f 756e 7370  .parent is _unsp
-00008470: 6563 6966 6965 645f 7061 7265 6e74 3a20  ecified_parent: 
-00008480: 2023 2070 7974 7970 653a 2064 6973 6162   # pytype: disab
-00008490: 6c65 3d61 7474 7269 6275 7465 2d65 7272  le=attribute-err
-000084a0: 6f72 0a20 2020 2020 206f 626a 6563 742e  or.      object.
-000084b0: 5f5f 7365 7461 7474 725f 5f28 7365 6c66  __setattr__(self
-000084c0: 2c20 2770 6172 656e 7427 2c20 5f63 6f6e  , 'parent', _con
-000084d0: 7465 7874 2e6d 6f64 756c 655f 7374 6163  text.module_stac
-000084e0: 6b5b 2d31 5d29 0a0a 2020 2020 2320 496e  k[-1])..    # In
-000084f0: 6974 6961 6c69 7a61 7469 6f6e 2069 7320  itialization is 
-00008500: 6465 6665 7272 6564 2066 6f72 2074 6f70  deferred for top
-00008510: 206c 6576 656c 204d 6f64 756c 6573 206f   level Modules o
-00008520: 7220 616e 7920 6f74 6865 7220 226f 7270  r any other "orp
-00008530: 6861 6e22 0a20 2020 2023 204d 6f64 756c  han".    # Modul
-00008540: 6573 2075 6e74 696c 2061 7474 6163 686d  es until attachm
-00008550: 656e 7420 6279 205f 5f73 6574 6174 7472  ent by __setattr
-00008560: 5f5f 2069 2e65 2e20 4d79 4d6f 6475 6c65  __ i.e. MyModule
-00008570: 282e 2e2e 2c20 7061 7265 6e74 3d4e 6f6e  (..., parent=Non
-00008580: 6529 0a20 2020 2069 6620 7365 6c66 2e70  e).    if self.p
-00008590: 6172 656e 7420 6973 204e 6f6e 653a 0a20  arent is None:. 
-000085a0: 2020 2020 2072 6574 7572 6e0a 0a20 2020       return..   
-000085b0: 2023 2052 6567 6973 7465 7220 7375 626d   # Register subm
-000085c0: 6f64 756c 6520 6f6e 2070 6172 656e 7420  odule on parent 
-000085d0: 4d6f 6475 6c65 2e0a 2020 2020 6966 2069  Module..    if i
-000085e0: 7369 6e73 7461 6e63 6528 7365 6c66 2e70  sinstance(self.p
-000085f0: 6172 656e 742c 204d 6f64 756c 6529 3a0a  arent, Module):.
-00008600: 2020 2020 2020 2320 5768 656e 2069 6e69        # When ini
-00008610: 7469 616c 697a 696e 6720 616e 2075 6e6e  tializing an unn
-00008620: 616d 6564 204d 6f64 756c 6520 696e 7369  amed Module insi
-00008630: 6465 2073 6574 7570 2829 0a20 2020 2020  de setup().     
-00008640: 2023 2069 6e69 7469 616c 697a 6174 696f   # initializatio
-00008650: 6e20 6973 2064 6566 6572 7265 6420 756e  n is deferred un
-00008660: 7469 6c20 6174 7461 6368 6d65 6e74 2062  til attachment b
-00008670: 7920 5f5f 7365 7461 7474 725f 5f0a 2020  y __setattr__.  
-00008680: 2020 2020 2320 692e 652e 2073 656c 662e      # i.e. self.
-00008690: 6d79 6d6f 6475 6c65 203d 204d 794d 6f64  mymodule = MyMod
-000086a0: 756c 6528 2e2e 2e29 0a20 2020 2020 2073  ule(...).      s
-000086b0: 656c 662e 6e61 6d65 3a20 4f70 7469 6f6e  elf.name: Option
-000086c0: 616c 5b73 7472 5d0a 2020 2020 2020 6966  al[str].      if
-000086d0: 2073 656c 662e 7061 7265 6e74 2e5f 7374   self.parent._st
-000086e0: 6174 652e 696e 5f73 6574 7570 2061 6e64  ate.in_setup and
-000086f0: 2073 656c 662e 6e61 6d65 2069 7320 4e6f   self.name is No
-00008700: 6e65 3a20 2023 2070 7974 7970 653a 2064  ne:  # pytype: d
-00008710: 6973 6162 6c65 3d61 7474 7269 6275 7465  isable=attribute
-00008720: 2d65 7272 6f72 0a20 2020 2020 2020 2072  -error.        r
-00008730: 6574 7572 6e0a 2020 2020 2020 6966 206e  eturn.      if n
-00008740: 6f74 2073 656c 662e 7061 7265 6e74 2e5f  ot self.parent._
-00008750: 696e 6974 6961 6c69 7a61 7469 6f6e 5f61  initialization_a
-00008760: 6c6c 6f77 6564 3a0a 2020 2020 2020 2020  llowed:.        
-00008770: 7261 6973 6520 6572 726f 7273 2e41 7373  raise errors.Ass
-00008780: 6967 6e53 7562 4d6f 6475 6c65 4572 726f  ignSubModuleErro
-00008790: 7228 7365 6c66 2e5f 5f63 6c61 7373 5f5f  r(self.__class__
-000087a0: 2e5f 5f6e 616d 655f 5f29 0a20 2020 2020  .__name__).     
-000087b0: 2023 2041 7574 6f6e 616d 696e 6720 6f66   # Autonaming of
-000087c0: 2073 7562 6d6f 6475 6c65 732e 0a20 2020   submodules..   
-000087d0: 2020 2069 6620 7365 6c66 2e6e 616d 6520     if self.name 
-000087e0: 6973 204e 6f6e 653a 2020 2320 7079 7479  is None:  # pyty
-000087f0: 7065 3a20 6469 7361 626c 653d 6174 7472  pe: disable=attr
-00008800: 6962 7574 652d 6572 726f 720a 2020 2020  ibute-error.    
-00008810: 2020 2020 7072 6566 6978 203d 2066 277b      prefix = f'{
-00008820: 7365 6c66 2e5f 5f63 6c61 7373 5f5f 2e5f  self.__class__._
-00008830: 5f6e 616d 655f 5f7d 270a 2020 2020 2020  _name__}'.      
-00008840: 2020 6375 7273 6f72 203d 2073 656c 662e    cursor = self.
-00008850: 7061 7265 6e74 2e5f 7374 6174 652e 6175  parent._state.au
-00008860: 746f 6e61 6d65 5f63 7572 736f 722e 6765  toname_cursor.ge
-00008870: 7428 7072 6566 6978 2c20 3029 0a20 2020  t(prefix, 0).   
-00008880: 2020 2020 2073 656c 662e 6e61 6d65 203d       self.name =
-00008890: 2066 277b 7072 6566 6978 7d5f 7b63 7572   f'{prefix}_{cur
-000088a0: 736f 727d 270a 2020 2020 2020 2020 7365  sor}'.        se
-000088b0: 6c66 2e70 6172 656e 742e 5f73 7461 7465  lf.parent._state
-000088c0: 2e61 7574 6f6e 616d 655f 6375 7273 6f72  .autoname_cursor
-000088d0: 5b70 7265 6669 785d 203d 2063 7572 736f  [prefix] = curso
-000088e0: 7220 2b20 310a 2020 2020 2020 2320 416c  r + 1.      # Al
-000088f0: 6c6f 7720 7363 6f70 6520 616c 6961 7369  low scope aliasi
-00008900: 6e67 2075 6e64 6572 2074 7261 6e73 666f  ng under transfo
-00008910: 726d 7320 666f 7220 7375 626d 6f64 756c  rms for submodul
-00008920: 6573 2064 6566 696e 6564 2069 6e20 7365  es defined in se
-00008930: 7475 702e 0a20 2020 2020 2072 6575 7365  tup..      reuse
-00008940: 5f73 636f 7065 7320 3d20 2873 656c 662e  _scopes = (self.
-00008950: 7061 7265 6e74 2e5f 7374 6174 652e 696e  parent._state.in
-00008960: 5f73 6574 7570 2061 6e64 0a20 2020 2020  _setup and.     
-00008970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008980: 2073 656c 662e 7061 7265 6e74 2e5f 7374   self.parent._st
-00008990: 6174 652e 7365 7475 705f 6361 6c6c 6564  ate.setup_called
-000089a0: 203d 3d20 5365 7475 7053 7461 7465 2e54   == SetupState.T
-000089b0: 5241 4e53 464f 524d 4544 290a 2020 2020  RANSFORMED).    
-000089c0: 2020 2320 5065 7266 6f72 6d20 6e61 6d65    # Perform name
-000089d0: 2d63 6f6c 6c69 7369 6f6e 2063 6865 636b  -collision check
-000089e0: 2e0a 2020 2020 2020 6966 2073 656c 662e  ..      if self.
-000089f0: 7061 7265 6e74 2e5f 6e61 6d65 5f74 616b  parent._name_tak
-00008a00: 656e 2873 656c 662e 6e61 6d65 2c20 7365  en(self.name, se
-00008a10: 6c66 2c20 7265 7573 655f 7363 6f70 6573  lf, reuse_scopes
-00008a20: 3d72 6575 7365 5f73 636f 7065 7329 3a0a  =reuse_scopes):.
-00008a30: 2020 2020 2020 2020 7061 7265 6e74 5f63          parent_c
-00008a40: 6c61 7373 203d 2073 656c 662e 7061 7265  lass = self.pare
-00008a50: 6e74 2e5f 5f63 6c61 7373 5f5f 2e5f 5f6e  nt.__class__.__n
-00008a60: 616d 655f 5f0a 2020 2020 2020 2020 7261  ame__.        ra
-00008a70: 6973 6520 6572 726f 7273 2e4e 616d 6549  ise errors.NameI
-00008a80: 6e55 7365 4572 726f 7228 2773 7562 6d6f  nUseError('submo
-00008a90: 6475 6c65 272c 2073 656c 662e 6e61 6d65  dule', self.name
-00008aa0: 2c20 7061 7265 6e74 5f63 6c61 7373 290a  , parent_class).
-00008ab0: 2020 2020 2020 2320 4669 6e61 6c69 7a65        # Finalize
-00008ac0: 2061 7474 6163 686d 656e 7420 746f 2070   attachment to p
-00008ad0: 6172 656e 7420 616e 6420 7363 6f70 6520  arent and scope 
-00008ae0: 696e 6974 6961 6c69 7a61 7469 6f6e 2e0a  initialization..
-00008af0: 2020 2020 2020 7365 6c66 2e70 6172 656e        self.paren
-00008b00: 742e 5f73 7461 7465 2e63 6869 6c64 7265  t._state.childre
-00008b10: 6e5b 7365 6c66 2e6e 616d 655d 203d 2073  n[self.name] = s
-00008b20: 656c 660a 2020 2020 2020 6173 7365 7274  elf.      assert
-00008b30: 2073 656c 662e 7061 7265 6e74 2e73 636f   self.parent.sco
-00008b40: 7065 2069 7320 6e6f 7420 4e6f 6e65 0a20  pe is not None. 
-00008b50: 2020 2020 206f 626a 6563 742e 5f5f 7365       object.__se
-00008b60: 7461 7474 725f 5f28 0a20 2020 2020 2020  tattr__(.       
-00008b70: 2020 2073 656c 662c 2027 7363 6f70 6527     self, 'scope'
-00008b80: 2c20 7365 6c66 2e70 6172 656e 742e 7363  , self.parent.sc
-00008b90: 6f70 652e 7075 7368 2873 656c 662e 6e61  ope.push(self.na
-00008ba0: 6d65 2c20 7265 7573 653d 7265 7573 655f  me, reuse=reuse_
-00008bb0: 7363 6f70 6573 2929 0a0a 2020 2020 2320  scopes))..    # 
-00008bc0: 546f 702d 6c65 7665 6c20 696e 766f 6361  Top-level invoca
-00008bd0: 7469 6f6e 2077 6974 6820 6120 6675 6e63  tion with a func
-00008be0: 7469 6f6e 616c 2053 636f 7065 2e0a 2020  tional Scope..  
-00008bf0: 2020 656c 6966 2069 7369 6e73 7461 6e63    elif isinstanc
-00008c00: 6528 7365 6c66 2e70 6172 656e 742c 2053  e(self.parent, S
-00008c10: 636f 7065 293a 0a20 2020 2020 206f 626a  cope):.      obj
-00008c20: 6563 742e 5f5f 7365 7461 7474 725f 5f28  ect.__setattr__(
-00008c30: 7365 6c66 2c20 2773 636f 7065 272c 2073  self, 'scope', s
-00008c40: 656c 662e 7061 7265 6e74 290a 2020 2020  elf.parent).    
-00008c50: 656c 7365 3a0a 2020 2020 2020 7261 6973  else:.      rais
-00008c60: 6520 5661 6c75 6545 7272 6f72 2827 7061  e ValueError('pa
-00008c70: 7265 6e74 206d 7573 7420 6265 204e 6f6e  rent must be Non
-00008c80: 652c 204d 6f64 756c 6520 6f72 2053 636f  e, Module or Sco
-00008c90: 7065 2729 0a0a 2020 2020 2320 6561 6765  pe')..    # eage
-00008ca0: 726c 7920 6269 6e64 2073 7562 6d6f 6475  rly bind submodu
-00008cb0: 6c65 7320 6966 2073 636f 7065 2069 7320  les if scope is 
-00008cc0: 6176 6169 6c61 626c 650a 2020 2020 6966  available.    if
-00008cd0: 2073 656c 662e 7363 6f70 6520 6973 206e   self.scope is n
-00008ce0: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
-00008cf0: 2066 6f72 2066 6965 6c64 2069 6e20 6461   for field in da
-00008d00: 7461 636c 6173 7365 732e 6669 656c 6473  taclasses.fields
-00008d10: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
-00008d20: 2020 6966 2066 6965 6c64 2e6e 616d 6520    if field.name 
-00008d30: 6e6f 7420 696e 2028 2770 6172 656e 7427  not in ('parent'
-00008d40: 2c20 276e 616d 6527 2920 616e 6420 6669  , 'name') and fi
-00008d50: 656c 642e 696e 6974 3a0a 2020 2020 2020  eld.init:.      
-00008d60: 2020 2020 2020 7365 6c66 2e5f 7265 6769        self._regi
-00008d70: 7374 6572 5f73 7562 6d6f 6475 6c65 7328  ster_submodules(
-00008d80: 6669 656c 642e 6e61 6d65 2c20 6765 7461  field.name, geta
-00008d90: 7474 7228 7365 6c66 2c20 6669 656c 642e  ttr(self, field.
-00008da0: 6e61 6d65 2929 0a0a 2020 2020 7365 6c66  name))..    self
-00008db0: 2e5f 7374 6174 652e 6973 5f69 6e69 7469  ._state.is_initi
-00008dc0: 616c 697a 6564 203d 2054 7275 650a 0a20  alized = True.. 
-00008dd0: 2064 6566 205f 5f72 6570 725f 5f28 7365   def __repr__(se
-00008de0: 6c66 2920 2d3e 2073 7472 3a0a 2020 2020  lf) -> str:.    
-00008df0: 7265 7475 726e 205f 6d6f 6475 6c65 5f72  return _module_r
-00008e00: 6570 7228 7365 6c66 290a 0a20 2064 6566  epr(self)..  def
-00008e10: 2073 6574 7570 2873 656c 6629 202d 3e20   setup(self) -> 
-00008e20: 4e6f 6e65 3a0a 2020 2020 2222 2249 6e69  None:.    """Ini
-00008e30: 7469 616c 697a 6573 2061 204d 6f64 756c  tializes a Modul
-00008e40: 6520 6c61 7a69 6c79 2028 7369 6d69 6c61  e lazily (simila
-00008e50: 7220 746f 2061 206c 617a 7920 6060 5f5f  r to a lazy ``__
-00008e60: 696e 6974 5f5f 6060 292e 0a0a 2020 2020  init__``)...    
-00008e70: 6060 7365 7475 7060 6020 6973 2063 616c  ``setup`` is cal
-00008e80: 6c65 6420 6f6e 6365 206c 617a 696c 7920  led once lazily 
-00008e90: 6f6e 2061 206d 6f64 756c 6520 696e 7374  on a module inst
-00008ea0: 616e 6365 2077 6865 6e20 6120 6d6f 6475  ance when a modu
-00008eb0: 6c65 0a20 2020 2069 7320 626f 756e 642c  le.    is bound,
-00008ec0: 2069 6d6d 6564 6961 7465 6c79 2062 6566   immediately bef
-00008ed0: 6f72 6520 616e 7920 6f74 6865 7220 6d65  ore any other me
-00008ee0: 7468 6f64 7320 6c69 6b65 2060 605f 5f63  thods like ``__c
-00008ef0: 616c 6c5f 5f60 6020 6172 650a 2020 2020  all__`` are.    
-00008f00: 696e 766f 6b65 642c 206f 7220 6265 666f  invoked, or befo
-00008f10: 7265 2061 2060 6073 6574 7570 6060 2d64  re a ``setup``-d
-00008f20: 6566 696e 6564 2061 7474 7269 6275 7465  efined attribute
-00008f30: 206f 6e20 6073 656c 6660 2069 7320 6163   on `self` is ac
-00008f40: 6365 7373 6564 2e0a 0a20 2020 2054 6869  cessed...    Thi
-00008f50: 7320 6361 6e20 6861 7070 656e 2069 6e20  s can happen in 
-00008f60: 7468 7265 6520 6361 7365 733a 0a0a 2020  three cases:..  
-00008f70: 2020 2020 312e 2049 6d6d 6564 6961 7465      1. Immediate
-00008f80: 6c79 2077 6865 6e20 696e 766f 6b69 6e67  ly when invoking
-00008f90: 203a 6d65 7468 3a60 6170 706c 7960 2c20   :meth:`apply`, 
-00008fa0: 3a6d 6574 683a 6069 6e69 7460 206f 720a  :meth:`init` or.
-00008fb0: 2020 2020 2020 2020 203a 6d65 7468 3a60           :meth:`
-00008fc0: 696e 6974 5f61 6e64 5f6f 7574 7075 7460  init_and_output`
-00008fd0: 2e0a 0a20 2020 2020 2032 2e20 4f6e 6365  ...      2. Once
-00008fe0: 2074 6865 206d 6f64 756c 6520 6973 2067   the module is g
-00008ff0: 6976 656e 2061 206e 616d 6520 6279 2062  iven a name by b
-00009000: 6569 6e67 2061 7373 6967 6e65 6420 746f  eing assigned to
-00009010: 2061 6e20 6174 7472 6962 7574 6520 6f66   an attribute of
-00009020: 0a20 2020 2020 2020 2020 616e 6f74 6865  .         anothe
-00009030: 7220 6d6f 6475 6c65 2069 6e73 6964 6520  r module inside 
-00009040: 7468 6520 6f74 6865 7220 6d6f 6475 6c65  the other module
-00009050: 2773 2060 6073 6574 7570 6060 206d 6574  's ``setup`` met
-00009060: 686f 640a 2020 2020 2020 2020 2028 7365  hod.         (se
-00009070: 6520 3a6d 6574 683a 605f 5f73 6574 6174  e :meth:`__setat
-00009080: 7472 5f5f 6029 3a3a 0a0a 2020 2020 2020  tr__`)::..      
-00009090: 2020 2020 2063 6c61 7373 204d 794d 6f64       class MyMod
-000090a0: 756c 6528 6e6e 2e4d 6f64 756c 6529 3a0a  ule(nn.Module):.
-000090b0: 2020 2020 2020 2020 2020 2020 2064 6566               def
-000090c0: 2073 6574 7570 2873 656c 6629 3a0a 2020   setup(self):.  
-000090d0: 2020 2020 2020 2020 2020 2020 2073 7562               sub
-000090e0: 6d6f 6475 6c65 203d 2043 6f6e 7628 2e2e  module = Conv(..
-000090f0: 2e29 0a0a 2020 2020 2020 2020 2020 2020  .)..            
-00009100: 2020 2023 2041 6363 6573 7369 6e67 2060     # Accessing `
-00009110: 7375 626d 6f64 756c 6560 2061 7474 7269  submodule` attri
-00009120: 6275 7465 7320 646f 6573 206e 6f74 2079  butes does not y
-00009130: 6574 2077 6f72 6b20 6865 7265 2e0a 0a20  et work here... 
-00009140: 2020 2020 2020 2020 2020 2020 2020 2320                # 
-00009150: 5468 6520 666f 6c6c 6f77 696e 6720 6c69  The following li
-00009160: 6e65 2069 6e76 6f6b 6573 2060 7365 6c66  ne invokes `self
-00009170: 2e5f 5f73 6574 6174 7472 5f5f 602c 2077  .__setattr__`, w
-00009180: 6869 6368 2067 6976 6573 0a20 2020 2020  hich gives.     
-00009190: 2020 2020 2020 2020 2020 2320 6073 7562            # `sub
-000091a0: 6d6f 6475 6c65 6020 7468 6520 6e61 6d65  module` the name
-000091b0: 2022 636f 6e76 3122 2e0a 2020 2020 2020   "conv1"..      
-000091c0: 2020 2020 2020 2020 2073 656c 662e 636f           self.co
-000091d0: 6e76 3120 3d20 7375 626d 6f64 756c 650a  nv1 = submodule.
-000091e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000091f0: 2320 4163 6365 7373 696e 6720 6073 7562  # Accessing `sub
-00009200: 6d6f 6475 6c65 6020 6174 7472 6962 7574  module` attribut
-00009210: 6573 206f 7220 6d65 7468 6f64 7320 6973  es or methods is
-00009220: 206e 6f77 2073 6166 6520 616e 640a 2020   now safe and.  
-00009230: 2020 2020 2020 2020 2020 2020 2023 2065               # e
-00009240: 6974 6865 7220 6361 7573 6573 2073 6574  ither causes set
-00009250: 7570 2829 2074 6f20 6265 2063 616c 6c65  up() to be calle
-00009260: 6420 6f6e 6365 2e0a 0a20 2020 2020 2033  d once...      3
-00009270: 2e20 4f6e 6365 2061 206d 6f64 756c 6520  . Once a module 
-00009280: 6973 2063 6f6e 7374 7275 6374 6564 2069  is constructed i
-00009290: 6e73 6964 6520 6120 6d65 7468 6f64 2077  nside a method w
-000092a0: 7261 7070 6564 2077 6974 680a 2020 2020  rapped with.    
-000092b0: 2020 2020 203a 6d65 7468 3a60 636f 6d70       :meth:`comp
-000092c0: 6163 7460 2c20 696d 6d65 6469 6174 656c  act`, immediatel
-000092d0: 7920 6265 666f 7265 2061 6e6f 7468 6572  y before another
-000092e0: 206d 6574 686f 6420 6973 2063 616c 6c65   method is calle
-000092f0: 6420 6f72 0a20 2020 2020 2020 2020 6060  d or.         ``
-00009300: 7365 7475 7060 6020 6465 6669 6e65 6420  setup`` defined 
-00009310: 6174 7472 6962 7574 6520 6973 2061 6363  attribute is acc
-00009320: 6573 7365 642e 0a20 2020 2022 2222 0a20  essed..    """. 
-00009330: 2020 2070 6173 730a 0a20 2064 6566 205f     pass..  def _
-00009340: 7265 6769 7374 6572 5f73 7562 6d6f 6475  register_submodu
-00009350: 6c65 7328 7365 6c66 2c20 6e61 6d65 2c20  les(self, name, 
-00009360: 7661 6c29 3a0a 2020 2020 2222 2252 6567  val):.    """Reg
-00009370: 6973 7465 7273 2061 2073 7562 6d6f 6475  isters a submodu
-00009380: 6c65 2e22 2222 0a20 2020 2061 7373 6572  le.""".    asser
-00009390: 7420 7365 6c66 2e73 636f 7065 2c20 2754  t self.scope, 'T
-000093a0: 7279 696e 6720 746f 2072 6567 6973 7465  rying to registe
-000093b0: 7220 7375 626d 6f64 756c 6573 206f 6e20  r submodules on 
-000093c0: 756e 626f 756e 6420 7363 6f70 652e 270a  unbound scope.'.
-000093d0: 2020 2020 726f 6f74 203d 2073 656c 662e      root = self.
-000093e0: 7363 6f70 652e 726f 6f74 0a20 2020 2063  scope.root.    c
-000093f0: 6163 6865 203d 205f 6361 6368 6573 2e67  ache = _caches.g
-00009400: 6574 2872 6f6f 742c 2077 6561 6b72 6566  et(root, weakref
-00009410: 2e57 6561 6b56 616c 7565 4469 6374 696f  .WeakValueDictio
-00009420: 6e61 7279 2829 290a 2020 2020 5f63 6163  nary()).    _cac
-00009430: 6865 735b 726f 6f74 5d20 3d20 6361 6368  hes[root] = cach
-00009440: 650a 2020 2020 7175 6575 6520 3d20 5b5d  e.    queue = []
-00009450: 0a20 2020 2070 7265 7365 7276 655f 6164  .    preserve_ad
-00009460: 6f70 7465 645f 6e61 6d65 7320 3d20 636f  opted_names = co
-00009470: 6e66 6967 2e66 6c61 785f 7072 6573 6572  nfig.flax_preser
-00009480: 7665 5f61 646f 7074 6564 5f6e 616d 6573  ve_adopted_names
-00009490: 0a20 2020 2069 6620 6861 7361 7474 7228  .    if hasattr(
-000094a0: 7479 7065 2873 656c 6629 2c20 2770 7265  type(self), 'pre
-000094b0: 7365 7276 655f 6164 6f70 7465 645f 6e61  serve_adopted_na
-000094c0: 6d65 7327 293a 0a20 2020 2020 2070 7265  mes'):.      pre
-000094d0: 7365 7276 655f 6164 6f70 7465 645f 6e61  serve_adopted_na
-000094e0: 6d65 7320 3d20 7479 7065 2873 656c 6629  mes = type(self)
-000094f0: 2e70 7265 7365 7276 655f 6164 6f70 7465  .preserve_adopte
-00009500: 645f 6e61 6d65 730a 2020 2020 6465 6620  d_names.    def 
-00009510: 6164 6f70 745f 6174 7472 5f6d 6f64 756c  adopt_attr_modul
-00009520: 6573 2863 6163 6865 2c20 7175 6575 652c  es(cache, queue,
-00009530: 2073 7566 6669 782c 2073 7562 7661 6c75   suffix, subvalu
-00009540: 6529 3a0a 2020 2020 2020 6966 2069 7369  e):.      if isi
-00009550: 6e73 7461 6e63 6528 7375 6276 616c 7565  nstance(subvalue
-00009560: 2c20 4d6f 6475 6c65 293a 0a20 2020 2020  , Module):.     
-00009570: 2020 2061 646f 7074 6564 5f6e 616d 6520     adopted_name 
-00009580: 3d20 4e6f 6e65 0a20 2020 2020 2020 2069  = None.        i
-00009590: 6620 7375 6276 616c 7565 2e70 6172 656e  f subvalue.paren
-000095a0: 7420 6973 204e 6f6e 653a 0a20 2020 2020  t is None:.     
-000095b0: 2020 2020 2023 2050 7265 7365 7276 6520       # Preserve 
-000095c0: 7368 6172 696e 672d 6279 2d72 6566 6572  sharing-by-refer
-000095d0: 656e 6365 2072 656c 6174 696f 6e73 6869  ence relationshi
-000095e0: 7073 2064 7572 696e 6720 6164 6f70 7469  ps during adopti
-000095f0: 6f6e 0a20 2020 2020 2020 2020 2023 2076  on.          # v
-00009600: 6961 2063 6163 6865 206b 6579 6564 206f  ia cache keyed o
-00009610: 6e20 756e 6971 7565 2069 6e73 7461 6e63  n unique instanc
-00009620: 6520 6964 732e 0a20 2020 2020 2020 2020  e ids..         
-00009630: 206b 6579 203d 2073 7562 7661 6c75 652e   key = subvalue.
-00009640: 5f69 640a 2020 2020 2020 2020 2020 2320  _id.          # 
-00009650: 4d6f 6475 6c65 2077 6173 2070 6173 7365  Module was passe
-00009660: 6420 6672 6f6d 206f 7574 7369 6465 2e20  d from outside. 
-00009670: 4974 206e 6565 6473 2074 6f20 6265 2063  It needs to be c
-00009680: 6c6f 6e65 642e 0a20 2020 2020 2020 2020  loned..         
-00009690: 2023 204f 7574 7369 6465 206d 6f64 756c   # Outside modul
-000096a0: 6573 2061 7265 206e 616d 6564 2062 7920  es are named by 
-000096b0: 6174 7461 6368 6d65 6e74 2c20 6e6f 7420  attachment, not 
-000096c0: 616e 206f 7574 6572 206e 616d 652c 0a20  an outer name,. 
-000096d0: 2020 2020 2020 2020 2023 2055 4e4c 4553           # UNLES
-000096e0: 5320 7765 2772 6520 7573 696e 6720 6e65  S we're using ne
-000096f0: 7720 6164 6f70 7465 6420 6e61 6d65 2070  w adopted name p
-00009700: 6f6c 6963 792c 2069 6e20 7768 6963 6820  olicy, in which 
-00009710: 6361 7365 2061 6e20 6578 6973 7469 6e67  case an existing
-00009720: 0a20 2020 2020 2020 2020 2023 206e 616d  .          # nam
-00009730: 6520 7769 6c6c 2062 6520 7573 6564 2c20  e will be used, 
-00009740: 6173 2069 7320 6f66 7465 6e20 7375 7070  as is often supp
-00009750: 6c69 6564 2062 7920 636f 6e66 6967 2073  lied by config s
-00009760: 7973 7465 6d73 2e0a 2020 2020 2020 2020  ystems..        
-00009770: 2020 6966 2070 7265 7365 7276 655f 6164    if preserve_ad
-00009780: 6f70 7465 645f 6e61 6d65 733a 0a20 2020  opted_names:.   
-00009790: 2020 2020 2020 2020 2061 646f 7074 6564           adopted
-000097a0: 5f6e 616d 6520 3d20 6f62 6a65 6374 2e5f  _name = object._
-000097b0: 5f67 6574 6174 7472 6962 7574 655f 5f28  _getattribute__(
-000097c0: 7375 6276 616c 7565 2c20 276e 616d 6527  subvalue, 'name'
-000097d0: 290a 2020 2020 2020 2020 2020 6966 206b  ).          if k
-000097e0: 6579 2069 6e20 6361 6368 653a 0a20 2020  ey in cache:.   
-000097f0: 2020 2020 2020 2020 2073 7562 7661 6c75           subvalu
-00009800: 6520 3d20 6361 6368 655b 6b65 795d 0a20  e = cache[key]. 
-00009810: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
-00009820: 2020 2020 2020 2020 2020 2073 7562 7661             subva
-00009830: 6c75 6520 3d20 7375 6276 616c 7565 2e63  lue = subvalue.c
-00009840: 6c6f 6e65 286e 616d 653d 4e6f 6e65 290a  lone(name=None).
-00009850: 2020 2020 2020 2020 2020 2020 6361 6368              cach
-00009860: 655b 6b65 795d 203d 2073 7562 7661 6c75  e[key] = subvalu
-00009870: 650a 2020 2020 2020 2020 6966 2073 7562  e.        if sub
-00009880: 7661 6c75 652e 6e61 6d65 2069 7320 4e6f  value.name is No
-00009890: 6e65 3a0a 2020 2020 2020 2020 2020 6f62  ne:.          ob
-000098a0: 6a65 6374 2e5f 5f73 6574 6174 7472 5f5f  ject.__setattr__
-000098b0: 2873 7562 7661 6c75 652c 2027 7061 7265  (subvalue, 'pare
-000098c0: 6e74 272c 2073 656c 6629 0a20 2020 2020  nt', self).     
-000098d0: 2020 2020 2069 6620 6164 6f70 7465 645f       if adopted_
-000098e0: 6e61 6d65 2069 7320 4e6f 6e65 3a0a 2020  name is None:.  
-000098f0: 2020 2020 2020 2020 2020 6164 6f70 7465            adopte
-00009900: 645f 6e61 6d65 203d 2066 277b 6e61 6d65  d_name = f'{name
-00009910: 7d7b 7375 6666 6978 7d27 0a20 2020 2020  }{suffix}'.     
-00009920: 2020 2020 206f 626a 6563 742e 5f5f 7365       object.__se
-00009930: 7461 7474 725f 5f28 7375 6276 616c 7565  tattr__(subvalue
-00009940: 2c20 276e 616d 6527 2c20 6164 6f70 7465  , 'name', adopte
-00009950: 645f 6e61 6d65 290a 2020 2020 2020 2020  d_name).        
-00009960: 2020 7175 6575 652e 6170 7065 6e64 2873    queue.append(s
-00009970: 7562 7661 6c75 6529 0a20 2020 2020 2072  ubvalue).      r
-00009980: 6574 7572 6e20 7375 6276 616c 7565 0a20  eturn subvalue. 
-00009990: 2020 2076 616c 203d 205f 6672 6565 7a65     val = _freeze
-000099a0: 5f61 7474 7228 5f6d 6170 5f6f 7665 725f  _attr(_map_over_
-000099b0: 6d6f 6475 6c65 735f 696e 5f74 7265 6528  modules_in_tree(
-000099c0: 0a20 2020 2020 2020 2066 756e 6374 6f6f  .        functoo
-000099d0: 6c73 2e70 6172 7469 616c 2861 646f 7074  ls.partial(adopt
-000099e0: 5f61 7474 725f 6d6f 6475 6c65 732c 2063  _attr_modules, c
-000099f0: 6163 6865 2c20 7175 6575 6529 2c20 7661  ache, queue), va
-00009a00: 6c29 290a 2020 2020 6f62 6a65 6374 2e5f  l)).    object._
-00009a10: 5f73 6574 6174 7472 5f5f 2873 656c 662c  _setattr__(self,
-00009a20: 206e 616d 652c 2076 616c 290a 2020 2020   name, val).    
-00009a30: 666f 7220 7820 696e 2071 7565 7565 3a0a  for x in queue:.
-00009a40: 2020 2020 2020 782e 5f5f 706f 7374 5f69        x.__post_i
-00009a50: 6e69 745f 5f28 290a 0a20 2064 6566 205f  nit__()..  def _
-00009a60: 7472 795f 7365 7475 7028 7365 6c66 2c20  try_setup(self, 
-00009a70: 7368 616c 6c6f 773a 2062 6f6f 6c20 3d20  shallow: bool = 
-00009a80: 4661 6c73 6529 202d 3e20 4e6f 6e65 3a0a  False) -> None:.
-00009a90: 2020 2020 2222 2254 7269 6573 2074 6f20      """Tries to 
-00009aa0: 7365 7475 7020 6d6f 6475 6c65 2069 6620  setup module if 
-00009ab0: 7363 6f70 6520 6973 2061 7661 696c 6162  scope is availab
-00009ac0: 6c65 2061 6e64 2073 6574 7570 2068 6173  le and setup has
-00009ad0: 206e 6f74 2062 6565 6e20 6361 6c6c 6564   not been called
-00009ae0: 2079 6574 2e22 2222 0a20 2020 2069 6620   yet.""".    if 
-00009af0: 2873 656c 662e 7363 6f70 650a 2020 2020  (self.scope.    
-00009b00: 2020 2020 616e 6420 6e6f 7420 7365 6c66      and not self
-00009b10: 2e5f 7374 6174 652e 696e 5f73 6574 7570  ._state.in_setup
-00009b20: 0a20 2020 2020 2020 2061 6e64 2073 656c  .        and sel
-00009b30: 662e 5f73 7461 7465 2e73 6574 7570 5f63  f._state.setup_c
-00009b40: 616c 6c65 6420 213d 2053 6574 7570 5374  alled != SetupSt
-00009b50: 6174 652e 444f 4e45 293a 0a20 2020 2020  ate.DONE):.     
-00009b60: 2074 7279 3a0a 2020 2020 2020 2020 7365   try:.        se
-00009b70: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
-00009b80: 7570 203d 2054 7275 650a 2020 2020 2020  up = True.      
-00009b90: 2020 2320 4120 7368 616c 6c6f 7720 7365    # A shallow se
-00009ba0: 7475 7020 7769 6c6c 206f 6e6c 7920 7265  tup will only re
-00009bb0: 6769 7374 6572 2061 7474 7269 6275 7465  gister attribute
-00009bc0: 2073 7562 6d6f 6475 6c65 7320 6275 7420   submodules but 
-00009bd0: 6974 2064 6f65 730a 2020 2020 2020 2020  it does.        
-00009be0: 2320 6e6f 7420 6361 6c6c 2074 6865 2075  # not call the u
-00009bf0: 7365 7227 7320 7365 7475 702e 2054 6869  ser's setup. Thi
-00009c00: 7320 6176 6f69 6473 2072 756e 6e69 6e67  s avoids running
-00009c10: 2062 6566 6f72 6520 610a 2020 2020 2020   before a.      
-00009c20: 2020 2320 7472 616e 7366 6f72 6d61 7469    # transformati
-00009c30: 6f6e 2e0a 2020 2020 2020 2020 666f 7220  on..        for 
-00009c40: 6669 656c 6420 696e 2064 6174 6163 6c61  field in datacla
-00009c50: 7373 6573 2e66 6965 6c64 7328 7365 6c66  sses.fields(self
-00009c60: 293a 0a20 2020 2020 2020 2020 2069 6620  ):.          if 
-00009c70: 6669 656c 642e 6e61 6d65 206e 6f74 2069  field.name not i
-00009c80: 6e20 2827 7061 7265 6e74 272c 2027 6e61  n ('parent', 'na
-00009c90: 6d65 2729 2061 6e64 2066 6965 6c64 2e69  me') and field.i
-00009ca0: 6e69 743a 0a20 2020 2020 2020 2020 2020  nit:.           
-00009cb0: 2073 656c 662e 5f72 6567 6973 7465 725f   self._register_
-00009cc0: 7375 626d 6f64 756c 6573 2866 6965 6c64  submodules(field
-00009cd0: 2e6e 616d 652c 2067 6574 6174 7472 2873  .name, getattr(s
-00009ce0: 656c 662c 2066 6965 6c64 2e6e 616d 6529  elf, field.name)
-00009cf0: 290a 2020 2020 2020 2020 6966 206e 6f74  ).        if not
-00009d00: 2073 6861 6c6c 6f77 3a0a 2020 2020 2020   shallow:.      
-00009d10: 2020 2020 7365 6c66 2e73 6574 7570 2829      self.setup()
-00009d20: 0a20 2020 2020 2020 2023 2057 6520 7275  .        # We ru
-00009d30: 6e20 7374 6174 6963 2063 6865 636b 7320  n static checks 
-00009d40: 6162 7374 7261 6374 6c79 206f 6e63 6520  abstractly once 
-00009d50: 666f 7220 7365 7475 7020 6265 666f 7265  for setup before
-00009d60: 2061 6e79 2074 7261 6e73 666f 726d 730a   any transforms.
-00009d70: 2020 2020 2020 2020 2320 746f 2064 6574          # to det
-00009d80: 6563 7420 6e61 6d65 2063 6f6c 6c69 7369  ect name collisi
-00009d90: 6f6e 7320 616e 6420 6f74 6865 7220 7079  ons and other py
-00009da0: 7468 6f6e 2065 7272 6f72 732e 0a20 2020  thon errors..   
-00009db0: 2020 2020 2065 6c69 6620 7365 6c66 2e5f       elif self._
-00009dc0: 7374 6174 652e 7365 7475 705f 6361 6c6c  state.setup_call
-00009dd0: 6564 203d 3d20 5365 7475 7053 7461 7465  ed == SetupState
-00009de0: 2e4e 4557 3a0a 2020 2020 2020 2020 2020  .NEW:.          
-00009df0: 7365 6c66 2e5f 7661 6c69 6461 7465 5f73  self._validate_s
-00009e00: 6574 7570 2829 0a20 2020 2020 2066 696e  etup().      fin
-00009e10: 616c 6c79 3a0a 2020 2020 2020 2020 7365  ally:.        se
-00009e20: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
-00009e30: 7570 203d 2046 616c 7365 0a20 2020 2020  up = False.     
-00009e40: 2020 2069 6620 6e6f 7420 7368 616c 6c6f     if not shallo
-00009e50: 773a 0a20 2020 2020 2020 2020 2073 656c  w:.          sel
-00009e60: 662e 5f73 7461 7465 2e73 6574 7570 5f63  f._state.setup_c
-00009e70: 616c 6c65 6420 3d20 5365 7475 7053 7461  alled = SetupSta
-00009e80: 7465 2e44 4f4e 450a 0a20 2064 6566 205f  te.DONE..  def _
-00009e90: 7661 6c69 6461 7465 5f73 6574 7570 2873  validate_setup(s
-00009ea0: 656c 6629 202d 3e20 4e6f 6e65 3a0a 2020  elf) -> None:.  
-00009eb0: 2020 2222 2241 6273 7472 6163 746c 7920    """Abstractly 
-00009ec0: 6576 616c 7561 7465 7320 7365 7475 7020  evaluates setup 
-00009ed0: 6f6e 6c79 2074 6f20 7275 6e20 7374 6174  only to run stat
-00009ee0: 6963 2063 6865 636b 732e 2222 220a 2020  ic checks.""".  
-00009ef0: 2020 6465 6620 7275 6e5f 7365 7475 705f    def run_setup_
-00009f00: 6f6e 6c79 2878 293a 0a20 2020 2020 2077  only(x):.      w
-00009f10: 7261 7070 6564 5f69 6420 3d20 7772 6170  rapped_id = wrap
-00009f20: 5f6d 6574 686f 645f 6f6e 6365 286c 616d  _method_once(lam
-00009f30: 6264 6120 6d2c 2078 3a20 7829 0a20 2020  bda m, x: x).   
-00009f40: 2020 2077 6974 6820 5465 7374 5363 6f70     with TestScop
-00009f50: 6528 7b7d 2c20 726e 6773 3d7b 7d2c 206d  e({}, rngs={}, m
-00009f60: 7574 6162 6c65 3d54 7275 6529 2e74 656d  utable=True).tem
-00009f70: 706f 7261 7279 2829 2061 7320 726f 6f74  porary() as root
-00009f80: 3a0a 2020 2020 2020 2020 7265 7475 726e  :.        return
-00009f90: 2077 7261 7070 6564 5f69 6428 7365 6c66   wrapped_id(self
-00009fa0: 2e63 6c6f 6e65 2870 6172 656e 743d 726f  .clone(parent=ro
-00009fb0: 6f74 292c 2078 290a 2020 2020 5f20 3d20  ot), x).    _ = 
-00009fc0: 6a61 782e 6576 616c 5f73 6861 7065 2872  jax.eval_shape(r
-00009fd0: 756e 5f73 6574 7570 5f6f 6e6c 792c 2030  un_setup_only, 0
-00009fe0: 290a 0a20 2064 6566 205f 6e61 6d65 5f74  )..  def _name_t
-00009ff0: 616b 656e 2873 656c 662c 0a20 2020 2020  aken(self,.     
-0000a000: 2020 2020 2020 2020 2020 2020 206e 616d               nam
-0000a010: 653a 2073 7472 2c0a 2020 2020 2020 2020  e: str,.        
-0000a020: 2020 2020 2020 2020 2020 6d6f 6475 6c65            module
-0000a030: 3a20 4f70 7469 6f6e 616c 5b27 4d6f 6475  : Optional['Modu
-0000a040: 6c65 275d 203d 204e 6f6e 652c 0a20 2020  le'] = None,.   
-0000a050: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-0000a060: 6575 7365 5f73 636f 7065 733a 2062 6f6f  euse_scopes: boo
-0000a070: 6c20 3d20 4661 6c73 652c 0a20 2020 2020  l = False,.     
-0000a080: 2020 2020 2020 2020 2020 2020 2063 6f6c               col
-0000a090: 6c65 6374 696f 6e3a 204f 7074 696f 6e61  lection: Optiona
-0000a0a0: 6c5b 7374 725d 203d 204e 6f6e 6529 202d  l[str] = None) -
-0000a0b0: 3e20 626f 6f6c 3a0a 2020 2020 6173 7365  > bool:.    asse
-0000a0c0: 7274 2073 656c 662e 7363 6f70 6520 6973  rt self.scope is
-0000a0d0: 206e 6f74 204e 6f6e 650a 2020 2020 6966   not None.    if
-0000a0e0: 2072 6575 7365 5f73 636f 7065 733a 0a20   reuse_scopes:. 
-0000a0f0: 2020 2020 2072 6574 7572 6e20 4661 6c73       return Fals
-0000a100: 650a 2020 2020 7265 7475 726e 2073 656c  e.    return sel
-0000a110: 662e 7363 6f70 652e 6e61 6d65 5f72 6573  f.scope.name_res
-0000a120: 6572 7665 6428 6e61 6d65 2c20 636f 6c6c  erved(name, coll
-0000a130: 6563 7469 6f6e 290a 0a20 2040 7072 6f70  ection)..  @prop
-0000a140: 6572 7479 0a20 2064 6566 205f 696e 6974  erty.  def _init
-0000a150: 6961 6c69 7a61 7469 6f6e 5f61 6c6c 6f77  ialization_allow
-0000a160: 6564 2873 656c 6629 3a0a 2020 2020 7265  ed(self):.    re
-0000a170: 7475 726e 2028 6e6f 7420 7365 6c66 2e5f  turn (not self._
-0000a180: 7374 6174 652e 6973 5f69 6e69 7469 616c  state.is_initial
-0000a190: 697a 6564 2020 2320 616c 6c6f 7720 6561  ized  # allow ea
-0000a1a0: 6765 7220 6174 7461 6368 6d65 6e74 2069  ger attachment i
-0000a1b0: 6e20 706f 7374 2d69 6e69 740a 2020 2020  n post-init.    
-0000a1c0: 2020 2020 2020 2020 6f72 2073 656c 662e          or self.
-0000a1d0: 5f73 7461 7465 2e69 6e5f 7365 7475 700a  _state.in_setup.
-0000a1e0: 2020 2020 2020 2020 2020 2020 6f72 2073              or s
-0000a1f0: 656c 662e 5f73 7461 7465 2e69 6e5f 636f  elf._state.in_co
-0000a200: 6d70 6163 745f 6d65 7468 6f64 290a 0a20  mpact_method).. 
-0000a210: 2064 6566 2063 6c6f 6e65 2873 656c 663a   def clone(self:
-0000a220: 204d 2c20 2a2c 0a20 2020 2020 2020 2020   M, *,.         
-0000a230: 2020 2070 6172 656e 743a 204f 7074 696f     parent: Optio
-0000a240: 6e61 6c5b 556e 696f 6e5b 5363 6f70 652c  nal[Union[Scope,
-0000a250: 2027 4d6f 6475 6c65 275d 5d20 3d20 4e6f   'Module']] = No
-0000a260: 6e65 2c0a 2020 2020 2020 2020 2020 2020  ne,.            
-0000a270: 5f64 6565 705f 636c 6f6e 653a 2055 6e69  _deep_clone: Uni
-0000a280: 6f6e 5b62 6f6f 6c2c 2077 6561 6b72 6566  on[bool, weakref
-0000a290: 2e57 6561 6b56 616c 7565 4469 6374 696f  .WeakValueDictio
-0000a2a0: 6e61 7279 5d20 3d20 4661 6c73 652c 0a20  nary] = False,. 
-0000a2b0: 2020 2020 2020 2020 2020 202a 2a75 7064             **upd
-0000a2c0: 6174 6573 2920 2d3e 204d 3a0a 2020 2020  ates) -> M:.    
-0000a2d0: 2222 2243 7265 6174 6573 2061 2063 6c6f  """Creates a clo
-0000a2e0: 6e65 206f 6620 7468 6973 204d 6f64 756c  ne of this Modul
-0000a2f0: 652c 2077 6974 6820 6f70 7469 6f6e 616c  e, with optional
-0000a300: 6c79 2075 7064 6174 6564 2061 7267 756d  ly updated argum
-0000a310: 656e 7473 2e0a 0a20 2020 2041 7267 733a  ents...    Args:
-0000a320: 0a20 2020 2020 2070 6172 656e 743a 2054  .      parent: T
-0000a330: 6865 2070 6172 656e 7420 6f66 2074 6865  he parent of the
-0000a340: 2063 6c6f 6e65 2e20 5468 6520 636c 6f6e   clone. The clon
-0000a350: 6520 7769 6c6c 2068 6176 6520 6e6f 2070  e will have no p
-0000a360: 6172 656e 7420 6966 206e 6f0a 2020 2020  arent if no.    
-0000a370: 2020 2020 6578 706c 6963 6974 2070 6172      explicit par
-0000a380: 656e 7420 6973 2073 7065 6369 6669 6564  ent is specified
-0000a390: 2e0a 2020 2020 2020 5f64 6565 705f 636c  ..      _deep_cl
-0000a3a0: 6f6e 653a 2041 2062 6f6f 6c65 616e 206f  one: A boolean o
-0000a3b0: 7220 6120 7765 616b 2076 616c 7565 2064  r a weak value d
-0000a3c0: 6963 7469 6f6e 6172 7920 746f 2063 6f6e  ictionary to con
-0000a3d0: 7472 6f6c 2064 6565 7020 636c 6f6e 696e  trol deep clonin
-0000a3e0: 670a 2020 2020 2020 2020 6f66 2073 7562  g.        of sub
-0000a3f0: 6d6f 6475 6c65 732e 2049 6620 5472 7565  modules. If True
-0000a400: 2c20 7375 626d 6f64 756c 6573 2077 696c  , submodules wil
-0000a410: 6c20 6265 2063 6c6f 6e65 6420 7265 6375  l be cloned recu
-0000a420: 7273 6976 656c 792e 2049 6620 610a 2020  rsively. If a.  
-0000a430: 2020 2020 2020 7765 616b 2076 616c 7565        weak value
-0000a440: 2064 6963 7469 6f6e 6172 7920 6973 2070   dictionary is p
-0000a450: 6173 7365 642c 2069 7420 7769 6c6c 2062  assed, it will b
-0000a460: 6520 7573 6564 2074 6f20 6361 6368 6520  e used to cache 
-0000a470: 636c 6f6e 6564 0a20 2020 2020 2020 2073  cloned.        s
-0000a480: 7562 6d6f 6475 6c65 732e 2054 6869 7320  ubmodules. This 
-0000a490: 666c 6167 2069 7320 7573 6564 2062 7920  flag is used by 
-0000a4a0: 696e 6974 2f61 7070 6c79 2f62 696e 6420  init/apply/bind 
-0000a4b0: 746f 2061 766f 6964 2073 636f 7065 0a20  to avoid scope. 
-0000a4c0: 2020 2020 2020 206c 6561 6b61 6765 2e0a         leakage..
-0000a4d0: 2020 2020 2020 2a2a 7570 6461 7465 733a        **updates:
-0000a4e0: 2041 7474 7269 6275 7465 2075 7064 6174   Attribute updat
-0000a4f0: 6573 2e0a 2020 2020 5265 7475 726e 733a  es..    Returns:
-0000a500: 0a20 2020 2020 2041 2063 6c6f 6e65 206f  .      A clone o
-0000a510: 6620 7468 6520 7468 6973 204d 6f64 756c  f the this Modul
-0000a520: 6520 7769 7468 2074 6865 2075 7064 6174  e with the updat
-0000a530: 6564 2061 7474 7269 6275 7465 7320 616e  ed attributes an
-0000a540: 6420 7061 7265 6e74 2e0a 2020 2020 2222  d parent..    ""
-0000a550: 220a 2020 2020 6174 7472 7320 3d20 7b66  ".    attrs = {f
-0000a560: 2e6e 616d 653a 2067 6574 6174 7472 2873  .name: getattr(s
-0000a570: 656c 662c 2066 2e6e 616d 6529 2066 6f72  elf, f.name) for
-0000a580: 2066 2069 6e20 6461 7461 636c 6173 7365   f in dataclasse
-0000a590: 732e 6669 656c 6473 2873 656c 6629 2069  s.fields(self) i
-0000a5a0: 6620 662e 696e 6974 7d0a 0a20 2020 2061  f f.init}..    a
-0000a5b0: 7474 7273 2e75 7064 6174 6528 7061 7265  ttrs.update(pare
-0000a5c0: 6e74 3d70 6172 656e 742c 202a 2a75 7064  nt=parent, **upd
-0000a5d0: 6174 6573 290a 0a20 2020 2023 2048 6572  ates)..    # Her
-0000a5e0: 6520 7765 2069 6d70 6c65 6d65 6e74 2064  e we implement d
-0000a5f0: 6565 7020 636c 6f6e 696e 6720 6f66 2073  eep cloning of s
-0000a600: 7562 6d6f 6475 6c65 732c 2074 6869 7320  ubmodules, this 
-0000a610: 6973 206e 6563 6573 7361 7279 2074 6f20  is necessary to 
-0000a620: 6176 6f69 6420 7363 6f70 6520 6c65 616b  avoid scope leak
-0000a630: 6167 650a 2020 2020 2320 6672 6f6d 2065  age.    # from e
-0000a640: 7874 6572 6e61 6c20 7375 626d 6f64 756c  xternal submodul
-0000a650: 6573 2069 6e74 6f20 696e 6974 2f61 7070  es into init/app
-0000a660: 6c79 2f62 696e 6420 7768 696c 6520 7072  ly/bind while pr
-0000a670: 6573 6572 7669 6e67 2073 6861 7269 6e67  eserving sharing
-0000a680: 2d62 792d 7265 6665 7265 6e63 650a 2020  -by-reference.  
-0000a690: 2020 2320 7265 6c61 7469 6f6e 7368 6970    # relationship
-0000a6a0: 7320 6265 7477 6565 6e20 7375 626d 6f64  s between submod
-0000a6b0: 756c 6573 2e0a 2020 2020 6966 205f 6465  ules..    if _de
-0000a6c0: 6570 5f63 6c6f 6e65 2021 3d20 4661 6c73  ep_clone != Fals
-0000a6d0: 653a 0a20 2020 2020 2023 2057 6520 7573  e:.      # We us
-0000a6e0: 6520 6120 7765 616b 2076 616c 7565 2064  e a weak value d
-0000a6f0: 6963 7469 6f6e 6172 7920 746f 2063 6163  ictionary to cac
-0000a700: 6865 2063 6c6f 6e65 6420 7375 626d 6f64  he cloned submod
-0000a710: 756c 6573 2e20 5768 656e 2061 2073 6861  ules. When a sha
-0000a720: 7265 640a 2020 2020 2020 2320 7375 626d  red.      # subm
-0000a730: 6f64 756c 6520 6973 2063 6c6f 6e65 642c  odule is cloned,
-0000a740: 2069 7473 206f 6e6c 7920 636c 6f6e 6564   its only cloned
-0000a750: 206f 6e63 6520 656c 7365 2069 7473 2066   once else its f
-0000a760: 6574 6368 6564 2066 726f 6d20 7468 6520  etched from the 
-0000a770: 6361 6368 652e 0a20 2020 2020 2063 6163  cache..      cac
-0000a780: 6865 203d 2077 6561 6b72 6566 2e57 6561  he = weakref.Wea
-0000a790: 6b56 616c 7565 4469 6374 696f 6e61 7279  kValueDictionary
-0000a7a0: 2829 2069 6620 6973 696e 7374 616e 6365  () if isinstance
-0000a7b0: 285f 6465 6570 5f63 6c6f 6e65 2c20 626f  (_deep_clone, bo
-0000a7c0: 6f6c 2920 656c 7365 205f 6465 6570 5f63  ol) else _deep_c
-0000a7d0: 6c6f 6e65 0a20 2020 2020 2064 6566 2063  lone.      def c
-0000a7e0: 6c6f 6e65 5f66 6e28 6d3a 204d 6f64 756c  lone_fn(m: Modul
-0000a7f0: 6529 202d 3e20 4d6f 6475 6c65 3a0a 2020  e) -> Module:.  
-0000a800: 2020 2020 2020 6966 2068 6173 6174 7472        if hasattr
-0000a810: 286d 2c20 275f 6964 2729 3a0a 2020 2020  (m, '_id'):.    
-0000a820: 2020 2020 2020 6b65 7920 3d20 6d2e 5f69        key = m._i
-0000a830: 640a 2020 2020 2020 2020 2020 6966 206b  d.          if k
-0000a840: 6579 2069 6e20 6361 6368 653a 0a20 2020  ey in cache:.   
-0000a850: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
-0000a860: 6361 6368 655b 6b65 795d 0a20 2020 2020  cache[key].     
-0000a870: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-0000a880: 2020 2020 2020 2063 6c6f 6e65 203d 206d         clone = m
-0000a890: 2e63 6c6f 6e65 285f 6465 6570 5f63 6c6f  .clone(_deep_clo
-0000a8a0: 6e65 3d63 6163 6865 290a 2020 2020 2020  ne=cache).      
-0000a8b0: 2020 2020 2020 6361 6368 655b 6b65 795d        cache[key]
-0000a8c0: 203d 2063 6c6f 6e65 0a20 2020 2020 2020   = clone.       
-0000a8d0: 2020 2020 2072 6574 7572 6e20 636c 6f6e       return clon
-0000a8e0: 650a 2020 2020 2020 2020 656c 7365 3a0a  e.        else:.
-0000a8f0: 2020 2020 2020 2020 2020 2320 4966 2074            # If t
-0000a900: 6865 206d 6f64 756c 6520 646f 6573 6e27  he module doesn'
-0000a910: 7420 6861 7665 2061 6e20 5f69 6420 6174  t have an _id at
-0000a920: 7472 6962 7574 6520 6974 2063 6f75 6c64  tribute it could
-0000a930: 2062 6520 6120 6d6f 636b 206f 626a 6563   be a mock objec
-0000a940: 740a 2020 2020 2020 2020 2020 2320 736f  t.          # so
-0000a950: 2077 6520 7265 7475 726e 2069 7420 6173   we return it as
-0000a960: 2069 732e 0a20 2020 2020 2020 2020 2072   is..          r
-0000a970: 6574 7572 6e20 6d0a 0a20 2020 2020 2023  eturn m..      #
-0000a980: 205f 6d61 705f 7375 626d 6f64 756c 6573   _map_submodules
-0000a990: 2077 696c 6c20 6d61 7020 6f76 6572 2061   will map over a
-0000a9a0: 6c6c 2073 7562 6d6f 6475 6c65 7320 696e  ll submodules in
-0000a9b0: 7369 6465 2061 7474 7273 0a20 2020 2020  side attrs.     
-0000a9c0: 2023 2076 616c 7565 2068 6572 6520 6361   # value here ca
-0000a9d0: 6e20 6265 2061 6e79 2070 7974 7265 652c  n be any pytree,
-0000a9e0: 206e 6f6e 2d6d 6f64 756c 6520 7661 6c75   non-module valu
-0000a9f0: 6573 2061 7265 2069 676e 6f72 6564 0a20  es are ignored. 
-0000aa00: 2020 2020 2066 6f72 2066 6965 6c64 5f6e       for field_n
-0000aa10: 616d 652c 2076 616c 7565 2069 6e20 6174  ame, value in at
-0000aa20: 7472 732e 6974 656d 7328 293a 0a20 2020  trs.items():.   
-0000aa30: 2020 2020 2061 7474 7273 5b66 6965 6c64       attrs[field
-0000aa40: 5f6e 616d 655d 203d 205f 6d61 705f 7375  _name] = _map_su
-0000aa50: 626d 6f64 756c 6573 2863 6c6f 6e65 5f66  bmodules(clone_f
-0000aa60: 6e2c 2076 616c 7565 290a 0a20 2020 206d  n, value)..    m
-0000aa70: 6f64 756c 6520 3d20 7365 6c66 2e5f 5f63  odule = self.__c
-0000aa80: 6c61 7373 5f5f 282a 2a61 7474 7273 290a  lass__(**attrs).
-0000aa90: 0a20 2020 2072 6574 7572 6e20 6d6f 6475  .    return modu
-0000aaa0: 6c65 0a0a 2020 6465 6620 7661 7269 6162  le..  def variab
-0000aab0: 6c65 2873 656c 662c 2063 6f6c 3a20 7374  le(self, col: st
-0000aac0: 722c 206e 616d 653a 2073 7472 2c0a 2020  r, name: str,.  
-0000aad0: 2020 2020 2020 2020 2020 2020 2069 6e69               ini
-0000aae0: 745f 666e 3a20 4f70 7469 6f6e 616c 5b43  t_fn: Optional[C
-0000aaf0: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
-0000ab00: 5d5d 203d 204e 6f6e 652c 0a20 2020 2020  ]] = None,.     
-0000ab10: 2020 2020 2020 2020 2020 2a69 6e69 745f            *init_
-0000ab20: 6172 6773 2c0a 2020 2020 2020 2020 2020  args,.          
-0000ab30: 2020 2020 2075 6e62 6f78 3a20 626f 6f6c       unbox: bool
-0000ab40: 203d 2054 7275 6529 202d 3e20 5661 7269   = True) -> Vari
-0000ab50: 6162 6c65 3a0a 2020 2020 2222 2244 6563  able:.    """Dec
-0000ab60: 6c61 7265 7320 616e 6420 7265 7475 726e  lares and return
-0000ab70: 7320 6120 7661 7269 6162 6c65 2069 6e20  s a variable in 
-0000ab80: 7468 6973 204d 6f64 756c 652e 0a0a 2020  this Module...  
-0000ab90: 2020 5365 6520 3a6d 6f64 3a60 666c 6178    See :mod:`flax
-0000aba0: 2e63 6f72 652e 7661 7269 6162 6c65 7360  .core.variables`
-0000abb0: 2066 6f72 206d 6f72 6520 696e 666f 726d   for more inform
-0000abc0: 6174 696f 6e2e 2053 6565 2061 6c73 6f20  ation. See also 
-0000abd0: 3a6d 6574 683a 6070 6172 616d 600a 2020  :meth:`param`.  
-0000abe0: 2020 666f 7220 6120 7368 6f72 7468 616e    for a shorthan
-0000abf0: 6420 7761 7920 746f 2064 6566 696e 6520  d way to define 
-0000ac00: 7265 6164 2d6f 6e6c 7920 7661 7269 6162  read-only variab
-0000ac10: 6c65 7320 696e 2074 6865 2022 7061 7261  les in the "para
-0000ac20: 6d73 220a 2020 2020 636f 6c6c 6563 7469  ms".    collecti
-0000ac30: 6f6e 2e0a 0a20 2020 2043 6f6e 7472 6172  on...    Contrar
-0000ac40: 7920 746f 203a 6d65 7468 3a60 7061 7261  y to :meth:`para
-0000ac50: 6d60 2c20 616c 6c20 6172 6775 6d65 6e74  m`, all argument
-0000ac60: 7320 7061 7373 696e 6720 7573 696e 6720  s passing using 
-0000ac70: 6069 6e69 745f 666e 6020 7368 6f75 6c64  `init_fn` should
-0000ac80: 2062 650a 2020 2020 7061 7373 6564 206f   be.    passed o
-0000ac90: 6e20 6578 706c 6963 6974 6c79 3a3a 0a0a  n explicitly::..
-0000aca0: 2020 2020 2020 6b65 7920 3d20 7365 6c66        key = self
-0000acb0: 2e6d 616b 655f 726e 6728 2773 7461 7473  .make_rng('stats
-0000acc0: 2729 0a20 2020 2020 206d 6561 6e20 3d20  ').      mean = 
-0000acd0: 7365 6c66 2e76 6172 6961 626c 6528 2773  self.variable('s
-0000ace0: 7461 7473 272c 2027 6d65 616e 272c 206c  tats', 'mean', l
-0000acf0: 6563 756e 5f6e 6f72 6d61 6c28 292c 206b  ecun_normal(), k
-0000ad00: 6579 2c20 2832 2c20 3229 290a 0a20 2020  ey, (2, 2))..   
-0000ad10: 2049 6e20 7468 6520 6578 616d 706c 6520   In the example 
-0000ad20: 6162 6f76 652c 2074 6865 2066 756e 6374  above, the funct
-0000ad30: 696f 6e20 606c 6563 756e 5f6e 6f72 6d61  ion `lecun_norma
-0000ad40: 6c60 2065 7870 6563 7473 2074 776f 2061  l` expects two a
-0000ad50: 7267 756d 656e 7473 3a0a 2020 2020 606b  rguments:.    `k
-0000ad60: 6579 6020 616e 6420 6073 6861 7065 602c  ey` and `shape`,
-0000ad70: 2061 6e64 2062 6f74 6820 6861 7665 2074   and both have t
-0000ad80: 6f20 6265 2070 6173 7365 6420 6f6e 2e20  o be passed on. 
-0000ad90: 5468 6520 5052 4e47 2066 6f72 2060 7374  The PRNG for `st
-0000ada0: 6174 7360 2068 6173 0a20 2020 2074 6f20  ats` has.    to 
-0000adb0: 6265 2070 726f 7669 6465 6420 6578 706c  be provided expl
-0000adc0: 6963 6974 6c79 2077 6865 6e20 6361 6c6c  icitly when call
-0000add0: 696e 6720 3a6d 6574 683a 6069 6e69 7460  ing :meth:`init`
-0000ade0: 2061 6e64 203a 6d65 7468 3a60 6170 706c   and :meth:`appl
-0000adf0: 7960 2e0a 0a20 2020 2041 7267 733a 0a20  y`...    Args:. 
-0000ae00: 2020 2020 2063 6f6c 3a20 5468 6520 7661       col: The va
-0000ae10: 7269 6162 6c65 2063 6f6c 6c65 6374 696f  riable collectio
-0000ae20: 6e20 6e61 6d65 2e0a 2020 2020 2020 6e61  n name..      na
-0000ae30: 6d65 3a20 5468 6520 7661 7269 6162 6c65  me: The variable
-0000ae40: 206e 616d 652e 0a20 2020 2020 2069 6e69   name..      ini
-0000ae50: 745f 666e 3a20 5468 6520 6675 6e63 7469  t_fn: The functi
-0000ae60: 6f6e 2074 6861 7420 7769 6c6c 2062 6520  on that will be 
-0000ae70: 6361 6c6c 6564 2074 6f20 636f 6d70 7574  called to comput
-0000ae80: 6520 7468 6520 696e 6974 6961 6c20 7661  e the initial va
-0000ae90: 6c75 650a 2020 2020 2020 2020 6f66 2074  lue.        of t
-0000aea0: 6869 7320 7661 7269 6162 6c65 2e20 5468  his variable. Th
-0000aeb0: 6973 2066 756e 6374 696f 6e20 7769 6c6c  is function will
-0000aec0: 206f 6e6c 7920 6265 2063 616c 6c65 6420   only be called 
-0000aed0: 7468 6520 6669 7273 7420 7469 6d65 0a20  the first time. 
-0000aee0: 2020 2020 2020 2074 6869 7320 7661 7269         this vari
-0000aef0: 6162 6c65 2069 7320 7573 6564 2069 6e20  able is used in 
-0000af00: 7468 6973 206d 6f64 756c 652e 2049 6620  this module. If 
-0000af10: 4e6f 6e65 2c20 7468 6520 7661 7269 6162  None, the variab
-0000af20: 6c65 206d 7573 740a 2020 2020 2020 2020  le must.        
-0000af30: 616c 7265 6164 7920 6265 2069 6e69 7469  already be initi
-0000af40: 616c 697a 6564 206f 7468 6572 7769 7365  alized otherwise
-0000af50: 2061 6e20 6572 726f 7220 6973 2072 6169   an error is rai
-0000af60: 7365 642e 0a20 2020 2020 202a 696e 6974  sed..      *init
-0000af70: 5f61 7267 733a 2054 6865 2061 7267 756d  _args: The argum
-0000af80: 656e 7473 2074 6f20 7061 7373 2074 6f20  ents to pass to 
-0000af90: 696e 6974 5f66 6e2e 0a20 2020 2020 2075  init_fn..      u
-0000afa0: 6e62 6f78 3a20 4966 2054 7275 652c 2060  nbox: If True, `
-0000afb0: 6041 7869 734d 6574 6164 6174 6160 6020  `AxisMetadata`` 
-0000afc0: 696e 7374 616e 6365 7320 6172 6520 7265  instances are re
-0000afd0: 706c 6163 6564 2062 7920 7468 6569 7220  placed by their 
-0000afe0: 756e 626f 7865 640a 2020 2020 2020 2020  unboxed.        
-0000aff0: 7661 6c75 652c 2073 6565 2060 6066 6c61  value, see ``fla
-0000b000: 782e 6e6e 2e6d 6574 612e 756e 626f 7860  x.nn.meta.unbox`
-0000b010: 6020 2864 6566 6175 6c74 3a20 5472 7565  ` (default: True
-0000b020: 292e 0a0a 2020 2020 5265 7475 726e 733a  )...    Returns:
-0000b030: 0a20 2020 2020 2041 203a 636c 6173 733a  .      A :class:
-0000b040: 6066 6c61 782e 636f 7265 2e76 6172 6961  `flax.core.varia
-0000b050: 626c 6573 2e56 6172 6961 626c 6560 2074  bles.Variable` t
-0000b060: 6861 7420 6361 6e20 6265 2072 6561 6420  hat can be read 
-0000b070: 6f72 2073 6574 2076 6961 0a20 2020 2020  or set via.     
-0000b080: 2022 2e76 616c 7565 2220 6174 7472 6962   ".value" attrib
-0000b090: 7574 652e 2054 6872 6f77 7320 616e 2065  ute. Throws an e
-0000b0a0: 7272 6f72 2069 6620 7468 6520 7661 7269  rror if the vari
-0000b0b0: 6162 6c65 2065 7869 7374 7320 616c 7265  able exists alre
-0000b0c0: 6164 792e 0a20 2020 2022 2222 0a20 2020  ady..    """.   
-0000b0d0: 2069 6620 6e6f 7420 7365 6c66 2e5f 696e   if not self._in
-0000b0e0: 6974 6961 6c69 7a61 7469 6f6e 5f61 6c6c  itialization_all
-0000b0f0: 6f77 6564 3a0a 2020 2020 2020 7261 6973  owed:.      rais
-0000b100: 6520 5661 6c75 6545 7272 6f72 280a 2020  e ValueError(.  
-0000b110: 2020 2020 2020 2020 2756 6172 6961 626c          'Variabl
-0000b120: 6573 206d 7573 7420 6265 2069 6e69 7469  es must be initi
-0000b130: 616c 697a 6564 2069 6e20 6073 6574 7570  alized in `setup
-0000b140: 2829 6020 6f72 2069 6e20 6120 6d65 7468  ()` or in a meth
-0000b150: 6f64 2027 0a20 2020 2020 2020 2020 2027  od '.          '
-0000b160: 7772 6170 7065 6420 696e 2060 4063 6f6d  wrapped in `@com
-0000b170: 7061 6374 6027 290a 2020 2020 6966 2073  pact`').    if s
-0000b180: 656c 662e 5f6e 616d 655f 7461 6b65 6e28  elf._name_taken(
-0000b190: 6e61 6d65 2c20 636f 6c6c 6563 7469 6f6e  name, collection
-0000b1a0: 3d63 6f6c 293a 0a20 2020 2020 2072 6169  =col):.      rai
-0000b1b0: 7365 2065 7272 6f72 732e 4e61 6d65 496e  se errors.NameIn
-0000b1c0: 5573 6545 7272 6f72 2827 7661 7269 6162  UseError('variab
-0000b1d0: 6c65 272c 206e 616d 652c 2073 656c 662e  le', name, self.
-0000b1e0: 5f5f 636c 6173 735f 5f2e 5f5f 6e61 6d65  __class__.__name
-0000b1f0: 5f5f 290a 2020 2020 6173 7365 7274 2073  __).    assert s
-0000b200: 656c 662e 7363 6f70 6520 6973 206e 6f74  elf.scope is not
-0000b210: 204e 6f6e 650a 2020 2020 7620 3d20 7365   None.    v = se
-0000b220: 6c66 2e73 636f 7065 2e76 6172 6961 626c  lf.scope.variabl
-0000b230: 6528 636f 6c2c 206e 616d 652c 2069 6e69  e(col, name, ini
-0000b240: 745f 666e 2c20 2a69 6e69 745f 6172 6773  t_fn, *init_args
-0000b250: 2c20 756e 626f 783d 756e 626f 7829 0a20  , unbox=unbox). 
-0000b260: 2020 2073 656c 662e 5f73 7461 7465 2e63     self._state.c
-0000b270: 6869 6c64 7265 6e5b 6e61 6d65 5d20 3d20  hildren[name] = 
-0000b280: 636f 6c0a 2020 2020 7265 7475 726e 2076  col.    return v
-0000b290: 0a0a 2020 6465 6620 7061 7261 6d28 7365  ..  def param(se
-0000b2a0: 6c66 2c20 6e61 6d65 3a20 7374 722c 2069  lf, name: str, i
-0000b2b0: 6e69 745f 666e 3a20 4361 6c6c 6162 6c65  nit_fn: Callable
-0000b2c0: 5b2e 2e2e 2c20 545d 2c20 2a69 6e69 745f  [..., T], *init_
-0000b2d0: 6172 6773 2c0a 2020 2020 2020 2020 2020  args,.          
-0000b2e0: 2020 756e 626f 783a 2062 6f6f 6c20 3d20    unbox: bool = 
-0000b2f0: 5472 7565 2920 2d3e 2054 3a0a 2020 2020  True) -> T:.    
-0000b300: 2222 2244 6563 6c61 7265 7320 616e 6420  """Declares and 
-0000b310: 7265 7475 726e 7320 6120 7061 7261 6d65  returns a parame
-0000b320: 7465 7220 696e 2074 6869 7320 4d6f 6475  ter in this Modu
-0000b330: 6c65 2e0a 0a20 2020 2050 6172 616d 6574  le...    Paramet
-0000b340: 6572 7320 6172 6520 7265 6164 2d6f 6e6c  ers are read-onl
-0000b350: 7920 7661 7269 6162 6c65 7320 696e 2074  y variables in t
-0000b360: 6865 2063 6f6c 6c65 6374 696f 6e20 6e61  he collection na
-0000b370: 6d65 6420 2270 6172 616d 7322 2e20 5365  med "params". Se
-0000b380: 650a 2020 2020 3a6d 6f64 3a60 666c 6178  e.    :mod:`flax
-0000b390: 2e63 6f72 652e 7661 7269 6162 6c65 7360  .core.variables`
-0000b3a0: 2066 6f72 206d 6f72 6520 6465 7461 696c   for more detail
-0000b3b0: 7320 6f6e 2076 6172 6961 626c 6573 2e0a  s on variables..
-0000b3c0: 0a20 2020 2054 6865 2066 6972 7374 2061  .    The first a
-0000b3d0: 7267 756d 656e 7420 6f66 2060 696e 6974  rgument of `init
-0000b3e0: 5f66 6e60 2069 7320 6173 7375 6d65 6420  _fn` is assumed 
-0000b3f0: 746f 2062 6520 6120 5052 4e47 206b 6579  to be a PRNG key
-0000b400: 2c20 7768 6963 6820 6973 0a20 2020 2070  , which is.    p
-0000b410: 726f 7669 6465 6420 6175 746f 6d61 7469  rovided automati
-0000b420: 6361 6c6c 7920 616e 6420 646f 6573 206e  cally and does n
-0000b430: 6f74 2068 6176 6520 746f 2062 6520 7061  ot have to be pa
-0000b440: 7373 6564 2075 7369 6e67 2060 696e 6974  ssed using `init
-0000b450: 5f61 7267 7360 3a3a 0a0a 2020 2020 2020  _args`::..      
-0000b460: 6d65 616e 203d 2073 656c 662e 7061 7261  mean = self.para
-0000b470: 6d28 276d 6561 6e27 2c20 6c65 6375 6e5f  m('mean', lecun_
-0000b480: 6e6f 726d 616c 2829 2c20 2832 2c20 3229  normal(), (2, 2)
-0000b490: 290a 0a20 2020 2049 6e20 7468 6520 6578  )..    In the ex
-0000b4a0: 616d 706c 6520 6162 6f76 652c 2074 6865  ample above, the
-0000b4b0: 2066 756e 6374 696f 6e20 606c 6563 756e   function `lecun
-0000b4c0: 5f6e 6f72 6d61 6c60 2065 7870 6563 7473  _normal` expects
-0000b4d0: 2074 776f 2061 7267 756d 656e 7473 3a0a   two arguments:.
-0000b4e0: 2020 2020 606b 6579 6020 616e 6420 6073      `key` and `s
-0000b4f0: 6861 7065 602c 2062 7574 206f 6e6c 7920  hape`, but only 
-0000b500: 6073 6861 7065 6020 6861 7320 746f 2062  `shape` has to b
-0000b510: 6520 7072 6f76 6964 6564 2065 7870 6c69  e provided expli
-0000b520: 6369 746c 793b 2060 6b65 7960 0a20 2020  citly; `key`.   
-0000b530: 2069 7320 7365 7420 6175 746f 6d61 7469   is set automati
-0000b540: 6361 6c6c 7920 7573 696e 6720 7468 6520  cally using the 
-0000b550: 5052 4e47 2066 6f72 2060 7061 7261 6d73  PRNG for `params
-0000b560: 6020 7468 6174 2069 7320 7061 7373 6564  ` that is passed
-0000b570: 2077 6865 6e0a 2020 2020 696e 6974 6961   when.    initia
-0000b580: 6c69 7a69 6e67 2074 6865 206d 6f64 756c  lizing the modul
-0000b590: 6520 7573 696e 6720 3a6d 6574 683a 6069  e using :meth:`i
-0000b5a0: 6e69 7460 2e0a 0a20 2020 2041 7267 733a  nit`...    Args:
-0000b5b0: 0a20 2020 2020 206e 616d 653a 2054 6865  .      name: The
-0000b5c0: 2070 6172 616d 6574 6572 206e 616d 652e   parameter name.
-0000b5d0: 0a20 2020 2020 2069 6e69 745f 666e 3a20  .      init_fn: 
-0000b5e0: 5468 6520 6675 6e63 7469 6f6e 2074 6861  The function tha
-0000b5f0: 7420 7769 6c6c 2062 6520 6361 6c6c 6564  t will be called
-0000b600: 2074 6f20 636f 6d70 7574 6520 7468 6520   to compute the 
-0000b610: 696e 6974 6961 6c20 7661 6c75 650a 2020  initial value.  
-0000b620: 2020 2020 2020 6f66 2074 6869 7320 7661        of this va
-0000b630: 7269 6162 6c65 2e20 5468 6973 2066 756e  riable. This fun
-0000b640: 6374 696f 6e20 7769 6c6c 206f 6e6c 7920  ction will only 
-0000b650: 6265 2063 616c 6c65 6420 7468 6520 6669  be called the fi
-0000b660: 7273 7420 7469 6d65 0a20 2020 2020 2020  rst time.       
-0000b670: 2074 6869 7320 7061 7261 6d65 7465 7220   this parameter 
-0000b680: 6973 2075 7365 6420 696e 2074 6869 7320  is used in this 
-0000b690: 6d6f 6475 6c65 2e0a 2020 2020 2020 2a69  module..      *i
-0000b6a0: 6e69 745f 6172 6773 3a20 5468 6520 6172  nit_args: The ar
-0000b6b0: 6775 6d65 6e74 7320 746f 2070 6173 7320  guments to pass 
-0000b6c0: 746f 2069 6e69 745f 666e 2e0a 2020 2020  to init_fn..    
-0000b6d0: 2020 756e 626f 783a 2049 6620 5472 7565    unbox: If True
-0000b6e0: 2c20 6060 4178 6973 4d65 7461 6461 7461  , ``AxisMetadata
-0000b6f0: 6060 2069 6e73 7461 6e63 6573 2061 7265  `` instances are
-0000b700: 2072 6570 6c61 6365 6420 6279 2074 6865   replaced by the
-0000b710: 6972 2075 6e62 6f78 6564 0a20 2020 2020  ir unboxed.     
-0000b720: 2020 2076 616c 7565 2c20 7365 6520 6060     value, see ``
-0000b730: 666c 6178 2e6e 6e2e 6d65 7461 2e75 6e62  flax.nn.meta.unb
-0000b740: 6f78 6060 2028 6465 6661 756c 743a 2054  ox`` (default: T
-0000b750: 7275 6529 2e0a 0a20 2020 2052 6574 7572  rue)...    Retur
-0000b760: 6e73 3a0a 2020 2020 2020 5468 6520 7661  ns:.      The va
-0000b770: 6c75 6520 6f66 2074 6865 2069 6e69 7469  lue of the initi
-0000b780: 616c 697a 6564 2070 6172 616d 6574 6572  alized parameter
-0000b790: 2e20 5468 726f 7773 2061 6e20 6572 726f  . Throws an erro
-0000b7a0: 7220 6966 2074 6865 2070 6172 616d 6574  r if the paramet
-0000b7b0: 6572 0a20 2020 2020 2065 7869 7374 7320  er.      exists 
-0000b7c0: 616c 7265 6164 792e 0a20 2020 2022 2222  already..    """
-0000b7d0: 0a20 2020 2069 6620 6e6f 7420 7365 6c66  .    if not self
-0000b7e0: 2e5f 696e 6974 6961 6c69 7a61 7469 6f6e  ._initialization
-0000b7f0: 5f61 6c6c 6f77 6564 3a0a 2020 2020 2020  _allowed:.      
-0000b800: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0000b810: 280a 2020 2020 2020 2020 2020 2750 6172  (.          'Par
-0000b820: 616d 6574 6572 7320 6d75 7374 2062 6520  ameters must be 
-0000b830: 696e 6974 6961 6c69 7a65 6420 696e 2060  initialized in `
-0000b840: 7365 7475 7028 2960 206f 7220 696e 2061  setup()` or in a
-0000b850: 206d 6574 686f 6420 270a 2020 2020 2020   method '.      
-0000b860: 2020 2020 2777 7261 7070 6564 2069 6e20      'wrapped in 
-0000b870: 6040 636f 6d70 6163 7460 2729 0a20 2020  `@compact`').   
-0000b880: 2069 6620 7365 6c66 2e5f 6e61 6d65 5f74   if self._name_t
-0000b890: 616b 656e 286e 616d 652c 2063 6f6c 6c65  aken(name, colle
-0000b8a0: 6374 696f 6e3d 2770 6172 616d 7327 293a  ction='params'):
-0000b8b0: 0a20 2020 2020 2072 6169 7365 2065 7272  .      raise err
-0000b8c0: 6f72 732e 4e61 6d65 496e 5573 6545 7272  ors.NameInUseErr
-0000b8d0: 6f72 2827 7061 7261 6d27 2c20 6e61 6d65  or('param', name
-0000b8e0: 2c20 7365 6c66 2e5f 5f63 6c61 7373 5f5f  , self.__class__
-0000b8f0: 2e5f 5f6e 616d 655f 5f29 0a20 2020 2061  .__name__).    a
-0000b900: 7373 6572 7420 7365 6c66 2e73 636f 7065  ssert self.scope
-0000b910: 2069 7320 6e6f 7420 4e6f 6e65 0a20 2020   is not None.   
-0000b920: 2076 203d 2073 656c 662e 7363 6f70 652e   v = self.scope.
-0000b930: 7061 7261 6d28 6e61 6d65 2c20 696e 6974  param(name, init
-0000b940: 5f66 6e2c 202a 696e 6974 5f61 7267 732c  _fn, *init_args,
-0000b950: 2075 6e62 6f78 3d75 6e62 6f78 290a 2020   unbox=unbox).  
-0000b960: 2020 7365 6c66 2e5f 7374 6174 652e 6368    self._state.ch
-0000b970: 696c 6472 656e 5b6e 616d 655d 203d 2027  ildren[name] = '
-0000b980: 7061 7261 6d73 270a 2020 2020 7265 7475  params'.    retu
-0000b990: 726e 2076 0a0a 2020 6465 6620 6861 735f  rn v..  def has_
-0000b9a0: 7661 7269 6162 6c65 2873 656c 662c 2063  variable(self, c
-0000b9b0: 6f6c 3a20 7374 722c 206e 616d 653a 2073  ol: str, name: s
-0000b9c0: 7472 2920 2d3e 2062 6f6f 6c3a 0a20 2020  tr) -> bool:.   
-0000b9d0: 2022 2222 4368 6563 6b73 2069 6620 6120   """Checks if a 
-0000b9e0: 7661 7269 6162 6c65 206f 6620 6769 7665  variable of give
-0000b9f0: 6e20 636f 6c6c 6563 7469 6f6e 2061 6e64  n collection and
-0000ba00: 206e 616d 6520 6578 6973 7473 2069 6e20   name exists in 
-0000ba10: 7468 6973 204d 6f64 756c 652e 0a0a 2020  this Module...  
-0000ba20: 2020 5365 6520 3a6d 6f64 3a60 666c 6178    See :mod:`flax
-0000ba30: 2e63 6f72 652e 7661 7269 6162 6c65 7360  .core.variables`
-0000ba40: 2066 6f72 206d 6f72 6520 6578 706c 616e   for more explan
-0000ba50: 6174 696f 6e20 6f6e 2076 6172 6961 626c  ation on variabl
-0000ba60: 6573 2061 6e64 0a20 2020 2063 6f6c 6c65  es and.    colle
-0000ba70: 6374 696f 6e73 2e0a 0a20 2020 2041 7267  ctions...    Arg
-0000ba80: 733a 0a20 2020 2020 2063 6f6c 3a20 5468  s:.      col: Th
-0000ba90: 6520 7661 7269 6162 6c65 2063 6f6c 6c65  e variable colle
-0000baa0: 6374 696f 6e20 6e61 6d65 2e0a 2020 2020  ction name..    
-0000bab0: 2020 6e61 6d65 3a20 5468 6520 6e61 6d65    name: The name
-0000bac0: 206f 6620 7468 6520 7661 7269 6162 6c65   of the variable
-0000bad0: 2e0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
-0000bae0: 2020 2020 2054 7275 6520 6966 2074 6865       True if the
-0000baf0: 2076 6172 6961 626c 6520 6578 6973 7473   variable exists
-0000bb00: 2e0a 2020 2020 2222 220a 2020 2020 6966  ..    """.    if
-0000bb10: 2073 656c 662e 7363 6f70 6520 6973 204e   self.scope is N
-0000bb20: 6f6e 653a 0a20 2020 2020 2072 6169 7365  one:.      raise
-0000bb30: 2056 616c 7565 4572 726f 7228 2243 616e   ValueError("Can
-0000bb40: 2774 2061 6363 6573 7320 7661 7269 6162  't access variab
-0000bb50: 6c65 7320 6f6e 2075 6e62 6f75 6e64 206d  les on unbound m
-0000bb60: 6f64 756c 6573 2229 0a20 2020 2072 6574  odules").    ret
-0000bb70: 7572 6e20 7365 6c66 2e73 636f 7065 2e68  urn self.scope.h
-0000bb80: 6173 5f76 6172 6961 626c 6528 636f 6c2c  as_variable(col,
-0000bb90: 206e 616d 6529 0a0a 2020 6465 6620 6973   name)..  def is
-0000bba0: 5f6d 7574 6162 6c65 5f63 6f6c 6c65 6374  _mutable_collect
-0000bbb0: 696f 6e28 7365 6c66 2c20 636f 6c3a 2073  ion(self, col: s
-0000bbc0: 7472 2920 2d3e 2062 6f6f 6c3a 0a20 2020  tr) -> bool:.   
-0000bbd0: 2022 2222 5265 7475 726e 7320 7472 7565   """Returns true
-0000bbe0: 2069 6620 7468 6520 636f 6c6c 6563 7469   if the collecti
-0000bbf0: 6f6e 2060 636f 6c60 2069 7320 6d75 7461  on `col` is muta
-0000bc00: 626c 652e 2222 220a 2020 2020 6966 2073  ble.""".    if s
-0000bc10: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
-0000bc20: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
-0000bc30: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
-0000bc40: 2063 6865 636b 206d 7574 6162 696c 6974   check mutabilit
-0000bc50: 7920 6f6e 2075 6e62 6f75 6e64 206d 6f64  y on unbound mod
-0000bc60: 756c 6573 2229 0a20 2020 2072 6574 7572  ules").    retur
-0000bc70: 6e20 7365 6c66 2e73 636f 7065 2e69 735f  n self.scope.is_
-0000bc80: 6d75 7461 626c 655f 636f 6c6c 6563 7469  mutable_collecti
-0000bc90: 6f6e 2863 6f6c 290a 0a20 2064 6566 2068  on(col)..  def h
-0000bca0: 6173 5f72 6e67 2873 656c 662c 206e 616d  as_rng(self, nam
-0000bcb0: 653a 2073 7472 2920 2d3e 2062 6f6f 6c3a  e: str) -> bool:
-0000bcc0: 0a20 2020 2022 2222 5265 7475 726e 7320  .    """Returns 
-0000bcd0: 7472 7565 2069 6620 6120 5052 4e47 5365  true if a PRNGSe
-0000bce0: 7175 656e 6365 2077 6974 6820 6e61 6d65  quence with name
-0000bcf0: 2060 6e61 6d65 6020 6578 6973 7473 2e22   `name` exists."
-0000bd00: 2222 0a20 2020 2069 6620 7365 6c66 2e73  "".    if self.s
-0000bd10: 636f 7065 2069 7320 4e6f 6e65 3a0a 2020  cope is None:.  
-0000bd20: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-0000bd30: 7272 6f72 2822 4361 6e27 7420 7175 6572  rror("Can't quer
-0000bd40: 7920 666f 7220 524e 4773 206f 6e20 756e  y for RNGs on un
-0000bd50: 626f 756e 6420 6d6f 6475 6c65 7322 290a  bound modules").
-0000bd60: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-0000bd70: 7363 6f70 652e 6861 735f 726e 6728 6e61  scope.has_rng(na
-0000bd80: 6d65 290a 0a20 2064 6566 206d 616b 655f  me)..  def make_
-0000bd90: 726e 6728 7365 6c66 2c20 6e61 6d65 3a20  rng(self, name: 
-0000bda0: 7374 7229 202d 3e20 4b65 7941 7272 6179  str) -> KeyArray
-0000bdb0: 3a0a 2020 2020 2222 2252 6574 7572 6e73  :.    """Returns
-0000bdc0: 2061 206e 6577 2052 4e47 206b 6579 2066   a new RNG key f
-0000bdd0: 726f 6d20 6120 6769 7665 6e20 524e 4720  rom a given RNG 
-0000bde0: 7365 7175 656e 6365 2066 6f72 2074 6869  sequence for thi
-0000bdf0: 7320 4d6f 6475 6c65 2e0a 0a20 2020 2054  s Module...    T
-0000be00: 6865 206e 6577 2052 4e47 206b 6579 2069  he new RNG key i
-0000be10: 7320 7370 6c69 7420 6672 6f6d 2074 6865  s split from the
-0000be20: 2070 7265 7669 6f75 7320 6f6e 652e 2054   previous one. T
-0000be30: 6875 732c 2065 7665 7279 2063 616c 6c20  hus, every call 
-0000be40: 746f 0a20 2020 2060 6d61 6b65 5f72 6e67  to.    `make_rng
-0000be50: 6020 7265 7475 726e 7320 6120 6e65 7720  ` returns a new 
-0000be60: 524e 4720 6b65 792c 2077 6869 6c65 2073  RNG key, while s
-0000be70: 7469 6c6c 2067 7561 7261 6e74 6565 696e  till guaranteein
-0000be80: 6720 6675 6c6c 0a20 2020 2072 6570 726f  g full.    repro
-0000be90: 6475 6369 6269 6c69 7479 2e0a 0a20 2020  ducibility...   
-0000bea0: 2054 4f44 4f3a 204c 696e 6b20 746f 2046   TODO: Link to F
-0000beb0: 6c61 7820 524e 4720 6465 7369 676e 206e  lax RNG design n
-0000bec0: 6f74 652e 0a0a 2020 2020 4172 6773 3a0a  ote...    Args:.
-0000bed0: 2020 2020 2020 6e61 6d65 3a20 5468 6520        name: The 
-0000bee0: 524e 4720 7365 7175 656e 6365 206e 616d  RNG sequence nam
-0000bef0: 652e 0a20 2020 2052 6574 7572 6e73 3a0a  e..    Returns:.
-0000bf00: 2020 2020 2020 5468 6520 6e65 776c 7920        The newly 
-0000bf10: 6765 6e65 7261 7465 6420 524e 4720 6b65  generated RNG ke
-0000bf20: 792e 0a20 2020 2022 2222 0a20 2020 2069  y..    """.    i
-0000bf30: 6620 7365 6c66 2e73 636f 7065 2069 7320  f self.scope is 
-0000bf40: 4e6f 6e65 3a0a 2020 2020 2020 7261 6973  None:.      rais
-0000bf50: 6520 5661 6c75 6545 7272 6f72 2822 4361  e ValueError("Ca
-0000bf60: 6e27 7420 7573 6520 524e 4773 206f 6e20  n't use RNGs on 
-0000bf70: 756e 626f 756e 6420 6d6f 6475 6c65 7322  unbound modules"
-0000bf80: 290a 2020 2020 7265 7475 726e 2073 656c  ).    return sel
-0000bf90: 662e 7363 6f70 652e 6d61 6b65 5f72 6e67  f.scope.make_rng
-0000bfa0: 286e 616d 6529 0a0a 2020 6465 6620 6973  (name)..  def is
-0000bfb0: 5f69 6e69 7469 616c 697a 696e 6728 7365  _initializing(se
-0000bfc0: 6c66 2920 2d3e 2062 6f6f 6c3a 0a20 2020  lf) -> bool:.   
-0000bfd0: 2022 2222 5265 7475 726e 7320 5472 7565   """Returns True
-0000bfe0: 2069 6620 7275 6e6e 696e 6720 756e 6465   if running unde
-0000bff0: 7220 7365 6c66 2e69 6e69 7428 2e2e 2e29  r self.init(...)
-0000c000: 206f 7220 6e6e 2e69 6e69 7428 2e2e 2e29   or nn.init(...)
-0000c010: 2829 2e0a 0a20 2020 2054 6869 7320 6973  ()...    This is
-0000c020: 2061 2068 656c 7065 7220 6d65 7468 6f64   a helper method
-0000c030: 2074 6f20 6861 6e64 6c65 2074 6865 2063   to handle the c
-0000c040: 6f6d 6d6f 6e20 6361 7365 206f 6620 7369  ommon case of si
-0000c050: 6d70 6c65 2069 6e69 7469 616c 697a 6174  mple initializat
-0000c060: 696f 6e0a 2020 2020 7768 6572 6520 7765  ion.    where we
-0000c070: 2077 6973 6820 746f 2068 6176 6520 7365   wish to have se
-0000c080: 7475 7020 6c6f 6769 6320 6f63 6375 7220  tup logic occur 
-0000c090: 7768 656e 206f 6e6c 7920 6361 6c6c 6564  when only called
-0000c0a0: 2075 6e64 6572 0a20 2020 2060 606d 6f64   under.    ``mod
-0000c0b0: 756c 652e 696e 6974 6060 206f 7220 6060  ule.init`` or ``
-0000c0c0: 6e6e 2e69 6e69 7460 602e 2020 466f 7220  nn.init``.  For 
-0000c0d0: 6d6f 7265 2063 6f6d 706c 6963 6174 6564  more complicated
-0000c0e0: 206d 756c 7469 2d70 6861 7365 0a20 2020   multi-phase.   
-0000c0f0: 2069 6e69 7469 616c 697a 6174 696f 6e20   initialization 
-0000c100: 7363 656e 6172 696f 7320 6974 2069 7320  scenarios it is 
-0000c110: 6265 7474 6572 2074 6f20 7465 7374 2066  better to test f
-0000c120: 6f72 2074 6865 206d 7574 6162 696c 6974  or the mutabilit
-0000c130: 7920 6f66 0a20 2020 2070 6172 7469 6375  y of.    particu
-0000c140: 6c61 7220 7661 7269 6162 6c65 2063 6f6c  lar variable col
-0000c150: 6c65 6374 696f 6e73 206f 7220 666f 7220  lections or for 
-0000c160: 7468 6520 7072 6573 656e 6365 206f 6620  the presence of 
-0000c170: 7061 7274 6963 756c 6172 0a20 2020 2076  particular.    v
-0000c180: 6172 6961 626c 6573 2074 6861 7420 706f  ariables that po
-0000c190: 7465 6e74 6961 6c6c 7920 6e65 6564 2074  tentially need t
-0000c1a0: 6f20 6265 2069 6e69 7469 616c 697a 6564  o be initialized
-0000c1b0: 2e0a 2020 2020 2222 220a 2020 2020 6966  ..    """.    if
-0000c1c0: 2073 656c 662e 7363 6f70 6520 6973 204e   self.scope is N
-0000c1d0: 6f6e 653a 0a20 2020 2020 2072 6169 7365  one:.      raise
-0000c1e0: 2056 616c 7565 4572 726f 7228 2243 616e   ValueError("Can
-0000c1f0: 2774 2063 6865 636b 2069 6620 7275 6e6e  't check if runn
-0000c200: 696e 6720 756e 6465 7220 696e 6974 2829  ing under init()
-0000c210: 206f 6e20 756e 626f 756e 6420 6d6f 6475   on unbound modu
-0000c220: 6c65 7322 290a 2020 2020 7265 7475 726e  les").    return
-0000c230: 2073 656c 662e 7363 6f70 652e 6765 745f   self.scope.get_
-0000c240: 666c 6167 2827 696e 6974 6961 6c69 7a69  flag('initializi
-0000c250: 6e67 272c 2046 616c 7365 290a 0a20 2064  ng', False)..  d
-0000c260: 6566 205f 6d6f 6475 6c65 5f63 6865 636b  ef _module_check
-0000c270: 7328 7365 6c66 293a 0a20 2020 2022 2222  s(self):.    """
-0000c280: 5275 6e20 7374 616e 6461 7264 2072 756e  Run standard run
-0000c290: 7469 6d65 2063 6865 636b 732e 2222 220a  time checks.""".
-0000c2a0: 0a20 2020 2069 6620 6e6f 7420 6973 696e  .    if not isin
-0000c2b0: 7374 616e 6365 2873 656c 662c 204d 6f64  stance(self, Mod
-0000c2c0: 756c 6529 3a0a 2020 2020 2020 7261 6973  ule):.      rais
-0000c2d0: 6520 6572 726f 7273 2e49 6e76 616c 6964  e errors.Invalid
-0000c2e0: 496e 7374 616e 6365 4d6f 6475 6c65 4572  InstanceModuleEr
-0000c2f0: 726f 7228 290a 0a20 2020 206f 7665 7272  ror()..    overr
-0000c300: 6964 6465 6e5f 706f 7374 5f69 6e69 7420  idden_post_init 
-0000c310: 3d20 7365 6c66 2e5f 5f70 6f73 745f 696e  = self.__post_in
-0000c320: 6974 5f5f 2021 3d20 4d6f 6475 6c65 2e5f  it__ != Module._
-0000c330: 5f70 6f73 745f 696e 6974 5f5f 0a20 2020  _post_init__.   
-0000c340: 2069 6620 6f76 6572 7269 6464 656e 5f70   if overridden_p
-0000c350: 6f73 745f 696e 6974 2061 6e64 206e 6f74  ost_init and not
-0000c360: 2068 6173 6174 7472 2873 656c 662c 2022   hasattr(self, "
-0000c370: 5f69 6422 293a 0a20 2020 2020 2072 6169  _id"):.      rai
-0000c380: 7365 2065 7272 6f72 732e 496e 636f 7272  se errors.Incorr
-0000c390: 6563 7450 6f73 7449 6e69 744f 7665 7272  ectPostInitOverr
-0000c3a0: 6964 6545 7272 6f72 2829 0a0a 2020 4074  ideError()..  @t
-0000c3b0: 7261 6365 6261 636b 5f75 7469 6c2e 6170  raceback_util.ap
-0000c3c0: 695f 626f 756e 6461 7279 0a20 2064 6566  i_boundary.  def
-0000c3d0: 2062 696e 6428 7365 6c66 3a20 4d2c 0a20   bind(self: M,. 
-0000c3e0: 2020 2020 2020 2020 2020 7661 7269 6162            variab
-0000c3f0: 6c65 733a 2056 6172 6961 626c 6544 6963  les: VariableDic
-0000c400: 742c 0a20 2020 2020 2020 2020 2020 2a61  t,.           *a
-0000c410: 7267 732c 0a20 2020 2020 2020 2020 2020  rgs,.           
-0000c420: 726e 6773 3a20 4f70 7469 6f6e 616c 5b52  rngs: Optional[R
-0000c430: 4e47 5365 7175 656e 6365 735d 203d 204e  NGSequences] = N
-0000c440: 6f6e 652c 0a20 2020 2020 2020 2020 2020  one,.           
-0000c450: 6d75 7461 626c 653a 2043 6f6c 6c65 6374  mutable: Collect
-0000c460: 696f 6e46 696c 7465 7220 3d20 4661 6c73  ionFilter = Fals
-0000c470: 6529 202d 3e20 4d3a 0a20 2020 2022 2222  e) -> M:.    """
-0000c480: 4372 6561 7465 7320 616e 2069 6e74 6572  Creates an inter
-0000c490: 6163 7469 7665 204d 6f64 756c 6520 696e  active Module in
-0000c4a0: 7374 616e 6365 2062 7920 6269 6e64 696e  stance by bindin
-0000c4b0: 6720 7661 7269 6162 6c65 7320 616e 6420  g variables and 
-0000c4c0: 524e 4773 2e0a 0a20 2020 2060 6062 696e  RNGs...    ``bin
-0000c4d0: 6460 6020 7072 6f76 6964 6573 2061 6e20  d`` provides an 
-0000c4e0: 2269 6e74 6572 6163 7469 7665 2220 696e  "interactive" in
-0000c4f0: 7374 616e 6365 206f 6620 6120 4d6f 6475  stance of a Modu
-0000c500: 6c65 2064 6972 6563 746c 7920 7769 7468  le directly with
-0000c510: 6f75 740a 2020 2020 7472 616e 7366 6f72  out.    transfor
-0000c520: 6d69 6e67 2061 2066 756e 6374 696f 6e20  ming a function 
-0000c530: 7769 7468 2060 6061 7070 6c79 6060 2e20  with ``apply``. 
-0000c540: 5468 6973 2069 7320 7061 7274 6963 756c  This is particul
-0000c550: 6172 6c79 2075 7365 6675 6c20 666f 720a  arly useful for.
-0000c560: 2020 2020 6465 6275 6767 696e 6720 616e      debugging an
-0000c570: 6420 696e 7465 7261 6374 6976 6520 7573  d interactive us
-0000c580: 6520 6361 7365 7320 6c69 6b65 206e 6f74  e cases like not
-0000c590: 6562 6f6f 6b73 2077 6865 7265 2061 2066  ebooks where a f
-0000c5a0: 756e 6374 696f 6e20 776f 756c 640a 2020  unction would.  
-0000c5b0: 2020 6c69 6d69 7420 7468 6520 6162 696c    limit the abil
-0000c5c0: 6974 7920 746f 2073 706c 6974 2075 7020  ity to split up 
-0000c5d0: 636f 6465 2069 6e74 6f20 6469 6666 6572  code into differ
-0000c5e0: 656e 7420 6365 6c6c 732e 0a0a 2020 2020  ent cells...    
-0000c5f0: 4f6e 6365 2074 6865 2076 6172 6961 626c  Once the variabl
-0000c600: 6573 2028 616e 6420 6f70 7469 6f6e 616c  es (and optional
-0000c610: 6c79 2052 4e47 7329 2061 7265 2062 6f75  ly RNGs) are bou
-0000c620: 6e64 2074 6f20 6120 6060 4d6f 6475 6c65  nd to a ``Module
-0000c630: 6060 2069 740a 2020 2020 6265 636f 6d65  `` it.    become
-0000c640: 7320 6120 7374 6174 6566 756c 206f 626a  s a stateful obj
-0000c650: 6563 742e 204e 6f74 6520 7468 6174 2069  ect. Note that i
-0000c660: 6469 6f6d 6174 6963 204a 4158 2069 7320  diomatic JAX is 
-0000c670: 6675 6e63 7469 6f6e 616c 2061 6e64 0a20  functional and. 
-0000c680: 2020 2074 6865 7265 666f 7265 2061 6e20     therefore an 
-0000c690: 696e 7465 7261 6374 6976 6520 696e 7374  interactive inst
-0000c6a0: 616e 6365 2064 6f65 7320 6e6f 7420 6d69  ance does not mi
-0000c6b0: 7820 7765 6c6c 2077 6974 6820 7661 6e69  x well with vani
-0000c6c0: 6c6c 6120 4a41 5820 4150 4973 2e0a 2020  lla JAX APIs..  
-0000c6d0: 2020 6060 6269 6e64 2829 6060 2073 686f    ``bind()`` sho
-0000c6e0: 756c 6420 6f6e 6c79 2062 6520 7573 6564  uld only be used
-0000c6f0: 2066 6f72 2069 6e74 6572 6163 7469 7665   for interactive
-0000c700: 2065 7870 6572 696d 656e 7461 7469 6f6e   experimentation
-0000c710: 2c20 616e 6420 696e 2061 6c6c 0a20 2020  , and in all.   
-0000c720: 206f 7468 6572 2063 6173 6573 2077 6520   other cases we 
-0000c730: 7374 726f 6e67 6c79 2065 6e63 6f75 7261  strongly encoura
-0000c740: 6765 2075 7365 7273 2074 6f20 7573 6520  ge users to use 
-0000c750: 6060 6170 706c 7928 2960 6020 696e 7374  ``apply()`` inst
-0000c760: 6561 642e 0a0a 2020 2020 4578 616d 706c  ead...    Exampl
-0000c770: 653a 3a0a 0a20 2020 2020 2069 6d70 6f72  e::..      impor
-0000c780: 7420 6a61 780a 2020 2020 2020 696d 706f  t jax.      impo
-0000c790: 7274 206a 6178 2e6e 756d 7079 2061 7320  rt jax.numpy as 
-0000c7a0: 6a6e 700a 2020 2020 2020 696d 706f 7274  jnp.      import
-0000c7b0: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
-0000c7c0: 6e0a 0a20 2020 2020 2063 6c61 7373 2041  n..      class A
-0000c7d0: 7574 6f45 6e63 6f64 6572 286e 6e2e 4d6f  utoEncoder(nn.Mo
-0000c7e0: 6475 6c65 293a 0a20 2020 2020 2020 2064  dule):.        d
-0000c7f0: 6566 2073 6574 7570 2873 656c 6629 3a0a  ef setup(self):.
-0000c800: 2020 2020 2020 2020 2020 7365 6c66 2e65            self.e
-0000c810: 6e63 6f64 6572 203d 206e 6e2e 4465 6e73  ncoder = nn.Dens
-0000c820: 6528 3329 0a20 2020 2020 2020 2020 2073  e(3).          s
-0000c830: 656c 662e 6465 636f 6465 7220 3d20 6e6e  elf.decoder = nn
-0000c840: 2e44 656e 7365 2835 290a 0a20 2020 2020  .Dense(5)..     
-0000c850: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
-0000c860: 7365 6c66 2c20 7829 3a0a 2020 2020 2020  self, x):.      
-0000c870: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-0000c880: 6465 636f 6465 7228 7365 6c66 2e65 6e63  decoder(self.enc
-0000c890: 6f64 6572 2878 2929 0a0a 2020 2020 2020  oder(x))..      
-0000c8a0: 7820 3d20 6a6e 702e 6f6e 6573 2828 3136  x = jnp.ones((16
-0000c8b0: 2c20 3929 290a 2020 2020 2020 6165 203d  , 9)).      ae =
-0000c8c0: 2041 7574 6f45 6e63 6f64 6572 2829 0a20   AutoEncoder(). 
-0000c8d0: 2020 2020 2076 6172 6961 626c 6573 203d       variables =
-0000c8e0: 2061 652e 696e 6974 286a 6178 2e72 616e   ae.init(jax.ran
-0000c8f0: 646f 6d2e 5052 4e47 4b65 7928 3029 2c20  dom.PRNGKey(0), 
-0000c900: 7829 0a20 2020 2020 206d 6f64 656c 203d  x).      model =
-0000c910: 2061 652e 6269 6e64 2876 6172 6961 626c   ae.bind(variabl
-0000c920: 6573 290a 2020 2020 2020 7a20 3d20 6d6f  es).      z = mo
-0000c930: 6465 6c2e 656e 636f 6465 7228 7829 0a20  del.encoder(x). 
-0000c940: 2020 2020 2078 5f72 6563 6f6e 7374 7275       x_reconstru
-0000c950: 6374 6564 203d 206d 6f64 656c 2e64 6563  cted = model.dec
-0000c960: 6f64 6572 287a 290a 0a20 2020 2041 7267  oder(z)..    Arg
-0000c970: 733a 0a20 2020 2020 2076 6172 6961 626c  s:.      variabl
-0000c980: 6573 3a20 4120 6469 6374 696f 6e61 7279  es: A dictionary
-0000c990: 2063 6f6e 7461 696e 696e 6720 7661 7269   containing vari
-0000c9a0: 6162 6c65 7320 6b65 7965 6420 6279 2076  ables keyed by v
-0000c9b0: 6172 6961 626c 650a 2020 2020 2020 2020  ariable.        
-0000c9c0: 636f 6c6c 6563 7469 6f6e 732e 2053 6565  collections. See
-0000c9d0: 203a 6d6f 643a 6066 6c61 782e 636f 7265   :mod:`flax.core
-0000c9e0: 2e76 6172 6961 626c 6573 6020 666f 7220  .variables` for 
-0000c9f0: 6d6f 7265 2064 6574 6169 6c73 0a20 2020  more details.   
-0000ca00: 2020 2020 2061 626f 7574 2076 6172 6961       about varia
-0000ca10: 626c 6573 2e0a 2020 2020 2020 2a61 7267  bles..      *arg
-0000ca20: 733a 204e 616d 6564 2061 7267 756d 656e  s: Named argumen
-0000ca30: 7473 2028 6e6f 7420 7573 6564 292e 0a20  ts (not used).. 
-0000ca40: 2020 2020 2072 6e67 733a 2061 2064 6963       rngs: a dic
-0000ca50: 7420 6f66 2050 524e 474b 6579 7320 746f  t of PRNGKeys to
-0000ca60: 2069 6e69 7469 616c 697a 6520 7468 6520   initialize the 
-0000ca70: 5052 4e47 2073 6571 7565 6e63 6573 2e0a  PRNG sequences..
-0000ca80: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
-0000ca90: 616e 2062 6520 626f 6f6c 2c20 7374 722c  an be bool, str,
-0000caa0: 206f 7220 6c69 7374 2e20 5370 6563 6966   or list. Specif
-0000cab0: 6965 7320 7768 6963 6820 636f 6c6c 6563  ies which collec
-0000cac0: 7469 6f6e 7320 7368 6f75 6c64 2062 650a  tions should be.
-0000cad0: 2020 2020 2020 2020 7472 6561 7465 6420          treated 
-0000cae0: 6173 206d 7574 6162 6c65 3a0a 2020 2020  as mutable:.    
-0000caf0: 2020 2020 2020 6060 626f 6f6c 6060 3a20        ``bool``: 
-0000cb00: 616c 6c2f 6e6f 2063 6f6c 6c65 6374 696f  all/no collectio
-0000cb10: 6e73 2061 7265 206d 7574 6162 6c65 2e0a  ns are mutable..
-0000cb20: 2020 2020 2020 2020 2020 6060 7374 7260            ``str`
-0000cb30: 603a 2054 6865 206e 616d 6520 6f66 2061  `: The name of a
-0000cb40: 2073 696e 676c 6520 6d75 7461 626c 6520   single mutable 
-0000cb50: 636f 6c6c 6563 7469 6f6e 2e0a 2020 2020  collection..    
-0000cb60: 2020 2020 2020 6060 6c69 7374 6060 3a20        ``list``: 
-0000cb70: 4120 6c69 7374 206f 6620 6e61 6d65 7320  A list of names 
-0000cb80: 6f66 206d 7574 6162 6c65 2063 6f6c 6c65  of mutable colle
-0000cb90: 6374 696f 6e73 2e0a 0a20 2020 2052 6574  ctions...    Ret
-0000cba0: 7572 6e73 3a0a 2020 2020 2020 4120 636f  urns:.      A co
-0000cbb0: 7079 206f 6620 7468 6973 2069 6e73 7461  py of this insta
-0000cbc0: 6e63 6520 7769 7468 2062 6f75 6e64 2076  nce with bound v
-0000cbd0: 6172 6961 626c 6573 2061 6e64 2052 4e47  ariables and RNG
-0000cbe0: 732e 0a20 2020 2022 2222 0a20 2020 204d  s..    """.    M
-0000cbf0: 6f64 756c 652e 5f6d 6f64 756c 655f 6368  odule._module_ch
-0000cc00: 6563 6b73 2873 656c 6629 0a0a 2020 2020  ecks(self)..    
-0000cc10: 6465 6c20 6172 6773 0a20 2020 2073 636f  del args.    sco
-0000cc20: 7065 203d 2063 6f72 652e 6269 6e64 2876  pe = core.bind(v
-0000cc30: 6172 6961 626c 6573 2c20 726e 6773 3d72  ariables, rngs=r
-0000cc40: 6e67 732c 206d 7574 6162 6c65 3d6d 7574  ngs, mutable=mut
-0000cc50: 6162 6c65 290a 2020 2020 7265 7475 726e  able).    return
-0000cc60: 2073 656c 662e 636c 6f6e 6528 7061 7265   self.clone(pare
-0000cc70: 6e74 3d73 636f 7065 2c20 5f64 6565 705f  nt=scope, _deep_
-0000cc80: 636c 6f6e 653d 5472 7565 290a 0a20 2064  clone=True)..  d
-0000cc90: 6566 2075 6e62 696e 6428 7365 6c66 3a20  ef unbind(self: 
-0000cca0: 4d29 202d 3e20 5475 706c 655b 4d2c 2056  M) -> Tuple[M, V
-0000ccb0: 6172 6961 626c 6544 6963 745d 3a0a 2020  ariableDict]:.  
-0000ccc0: 2020 2222 2252 6574 7572 6e73 2061 6e20    """Returns an 
-0000ccd0: 756e 626f 756e 6420 636f 7079 206f 6620  unbound copy of 
-0000cce0: 6120 4d6f 6475 6c65 2061 6e64 2069 7473  a Module and its
-0000ccf0: 2076 6172 6961 626c 6573 2e0a 0a20 2020   variables...   
-0000cd00: 2060 6075 6e62 696e 6460 6020 6865 6c70   ``unbind`` help
-0000cd10: 7320 6372 6561 7465 2061 2073 7461 7465  s create a state
-0000cd20: 6c65 7373 2076 6572 7369 6f6e 206f 6620  less version of 
-0000cd30: 6120 626f 756e 6420 4d6f 6475 6c65 2e0a  a bound Module..
-0000cd40: 0a20 2020 2041 6e20 6578 616d 706c 6520  .    An example 
-0000cd50: 6f66 2061 2063 6f6d 6d6f 6e20 7573 6520  of a common use 
-0000cd60: 6361 7365 3a20 746f 2065 7874 7261 6374  case: to extract
-0000cd70: 2061 2073 7562 2d4d 6f64 756c 6520 6465   a sub-Module de
-0000cd80: 6669 6e65 6420 696e 7369 6465 0a20 2020  fined inside.   
-0000cd90: 2060 6073 6574 7570 2829 6060 2061 6e64   ``setup()`` and
-0000cda0: 2069 7473 2063 6f72 7265 7370 6f6e 6469   its correspondi
-0000cdb0: 6e67 2076 6172 6961 626c 6573 3a20 3129  ng variables: 1)
-0000cdc0: 2074 656d 706f 7261 7269 6c79 2060 6062   temporarily ``b
-0000cdd0: 696e 6460 6020 7468 6520 7061 7265 6e74  ind`` the parent
-0000cde0: 0a20 2020 204d 6f64 756c 653b 2061 6e64  .    Module; and
-0000cdf0: 2074 6865 6e20 3229 2060 6075 6e62 696e   then 2) ``unbin
-0000ce00: 6460 6020 7468 6520 6465 7369 7265 6420  d`` the desired 
-0000ce10: 7375 622d 4d6f 6475 6c65 2e20 2852 6563  sub-Module. (Rec
-0000ce20: 616c 6c20 7468 6174 2060 6073 6574 7570  all that ``setup
-0000ce30: 2829 6060 0a20 2020 2069 7320 6f6e 6c79  ()``.    is only
-0000ce40: 2063 616c 6c65 6420 7768 656e 2074 6865   called when the
-0000ce50: 204d 6f64 756c 6520 6973 2062 6f75 6e64   Module is bound
-0000ce60: 2e29 3a3a 0a0a 2020 2020 2020 636c 6173  .)::..      clas
-0000ce70: 7320 4175 746f 456e 636f 6465 7228 6e6e  s AutoEncoder(nn
-0000ce80: 2e4d 6f64 756c 6529 3a0a 2020 2020 2020  .Module):.      
-0000ce90: 2020 6465 6620 7365 7475 7028 7365 6c66    def setup(self
-0000cea0: 293a 0a20 2020 2020 2020 2020 2073 656c  ):.          sel
-0000ceb0: 662e 656e 636f 6465 7220 3d20 456e 636f  f.encoder = Enco
-0000cec0: 6465 7228 290a 2020 2020 2020 2020 2020  der().          
-0000ced0: 7365 6c66 2e64 6563 6f64 6572 203d 2044  self.decoder = D
-0000cee0: 6563 6f64 6572 2829 0a0a 2020 2020 2020  ecoder()..      
-0000cef0: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
-0000cf00: 656c 662c 2078 293a 0a20 2020 2020 2020  elf, x):.       
-0000cf10: 2020 2072 6574 7572 6e20 7365 6c66 2e64     return self.d
-0000cf20: 6563 6f64 6572 2873 656c 662e 656e 636f  ecoder(self.enco
-0000cf30: 6465 7228 7829 290a 0a20 2020 2020 206d  der(x))..      m
-0000cf40: 6f64 756c 6520 3d20 4175 746f 456e 636f  odule = AutoEnco
-0000cf50: 6465 7228 290a 2020 2020 2020 7661 7269  der().      vari
-0000cf60: 6162 6c65 7320 3d20 6d6f 6475 6c65 2e69  ables = module.i
-0000cf70: 6e69 7428 6a61 782e 7261 6e64 6f6d 2e50  nit(jax.random.P
-0000cf80: 524e 474b 6579 2830 292c 206a 6e70 2e6f  RNGKey(0), jnp.o
-0000cf90: 6e65 7328 2831 2c20 3738 3429 2929 0a20  nes((1, 784))). 
-0000cfa0: 2020 2020 202e 2e2e 0a20 2020 2020 2023       ....      #
-0000cfb0: 2045 7874 7261 6374 2074 6865 2045 6e63   Extract the Enc
-0000cfc0: 6f64 6572 2073 7562 2d4d 6f64 756c 6520  oder sub-Module 
-0000cfd0: 616e 6420 6974 7320 7661 7269 6162 6c65  and its variable
-0000cfe0: 730a 2020 2020 2020 656e 636f 6465 722c  s.      encoder,
-0000cff0: 2065 6e63 6f64 6572 5f76 6172 7320 3d20   encoder_vars = 
-0000d000: 6d6f 6475 6c65 2e62 696e 6428 7661 7269  module.bind(vari
-0000d010: 6162 6c65 7329 2e65 6e63 6f64 6572 2e75  ables).encoder.u
-0000d020: 6e62 696e 6428 290a 0a20 2020 2052 6574  nbind()..    Ret
-0000d030: 7572 6e73 3a0a 2020 2020 2020 4120 7475  urns:.      A tu
-0000d040: 706c 6520 7769 7468 2061 6e20 756e 626f  ple with an unbo
-0000d050: 756e 6420 636f 7079 206f 6620 7468 6973  und copy of this
-0000d060: 204d 6f64 756c 6520 616e 6420 6974 7320   Module and its 
-0000d070: 7661 7269 6162 6c65 732e 0a20 2020 2022  variables..    "
-0000d080: 2222 0a20 2020 204d 6f64 756c 652e 5f6d  "".    Module._m
-0000d090: 6f64 756c 655f 6368 6563 6b73 2873 656c  odule_checks(sel
-0000d0a0: 6629 0a0a 2020 2020 6966 2073 656c 662e  f)..    if self.
-0000d0b0: 7363 6f70 6520 6973 204e 6f6e 653a 0a20  scope is None:. 
-0000d0c0: 2020 2020 2072 6169 7365 2065 7272 6f72       raise error
-0000d0d0: 732e 4361 6c6c 556e 6269 6e64 4f6e 556e  s.CallUnbindOnUn
-0000d0e0: 626f 756e 644d 6f64 756c 6545 7272 6f72  boundModuleError
-0000d0f0: 2829 0a0a 2020 2020 7661 7269 6162 6c65  ()..    variable
-0000d100: 7320 3d20 7365 6c66 2e76 6172 6961 626c  s = self.variabl
-0000d110: 6573 0a20 2020 206d 6f64 756c 6520 3d20  es.    module = 
-0000d120: 7365 6c66 2e63 6c6f 6e65 2829 0a20 2020  self.clone().   
-0000d130: 2072 6574 7572 6e20 6d6f 6475 6c65 2c20   return module, 
-0000d140: 7661 7269 6162 6c65 730a 0a20 2040 7472  variables..  @tr
-0000d150: 6163 6562 6163 6b5f 7574 696c 2e61 7069  aceback_util.api
-0000d160: 5f62 6f75 6e64 6172 790a 2020 6465 6620  _boundary.  def 
-0000d170: 6170 706c 7928 7365 6c66 2c0a 2020 2020  apply(self,.    
-0000d180: 2020 2020 2020 2020 7661 7269 6162 6c65          variable
-0000d190: 733a 2056 6172 6961 626c 6544 6963 742c  s: VariableDict,
-0000d1a0: 0a20 2020 2020 2020 2020 2020 202a 6172  .            *ar
-0000d1b0: 6773 2c0a 2020 2020 2020 2020 2020 2020  gs,.            
-0000d1c0: 726e 6773 3a20 4f70 7469 6f6e 616c 5b52  rngs: Optional[R
-0000d1d0: 4e47 5365 7175 656e 6365 735d 203d 204e  NGSequences] = N
-0000d1e0: 6f6e 652c 0a20 2020 2020 2020 2020 2020  one,.           
-0000d1f0: 206d 6574 686f 643a 2055 6e69 6f6e 5b43   method: Union[C
-0000d200: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
-0000d210: 5d2c 2073 7472 2c20 4e6f 6e65 5d20 3d20  ], str, None] = 
-0000d220: 4e6f 6e65 2c0a 2020 2020 2020 2020 2020  None,.          
-0000d230: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
-0000d240: 6374 696f 6e46 696c 7465 7220 3d20 4661  ctionFilter = Fa
-0000d250: 6c73 652c 0a20 2020 2020 2020 2020 2020  lse,.           
-0000d260: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
-0000d270: 6469 6174 6573 3a20 556e 696f 6e5b 626f  diates: Union[bo
-0000d280: 6f6c 2c20 4361 6c6c 6162 6c65 5b5b 274d  ol, Callable[['M
-0000d290: 6f64 756c 6527 2c20 7374 725d 2c20 626f  odule', str], bo
-0000d2a0: 6f6c 5d5d 203d 2046 616c 7365 2c0a 2020  ol]] = False,.  
-0000d2b0: 2020 2020 2020 2020 2020 2a2a 6b77 6172            **kwar
-0000d2c0: 6773 2920 2d3e 2055 6e69 6f6e 5b41 6e79  gs) -> Union[Any
-0000d2d0: 2c20 5475 706c 655b 416e 792c 2055 6e69  , Tuple[Any, Uni
-0000d2e0: 6f6e 5b46 726f 7a65 6e56 6172 6961 626c  on[FrozenVariabl
-0000d2f0: 6544 6963 742c 2044 6963 745b 7374 722c  eDict, Dict[str,
-0000d300: 2041 6e79 5d5d 5d5d 3a0a 2020 2020 2222   Any]]]]:.    ""
-0000d310: 2241 7070 6c69 6573 2061 206d 6f64 756c  "Applies a modul
-0000d320: 6520 6d65 7468 6f64 2074 6f20 7661 7269  e method to vari
-0000d330: 6162 6c65 7320 616e 6420 7265 7475 726e  ables and return
-0000d340: 7320 6f75 7470 7574 2061 6e64 206d 6f64  s output and mod
-0000d350: 6966 6965 6420 7661 7269 6162 6c65 732e  ified variables.
-0000d360: 0a0a 2020 2020 4e6f 7465 2074 6861 7420  ..    Note that 
-0000d370: 606d 6574 686f 6460 2073 686f 756c 6420  `method` should 
-0000d380: 6265 2073 6574 2069 6620 6f6e 6520 776f  be set if one wo
-0000d390: 756c 6420 6c69 6b65 2074 6f20 6361 6c6c  uld like to call
-0000d3a0: 2060 6170 706c 7960 206f 6e20 610a 2020   `apply` on a.  
-0000d3b0: 2020 6469 6666 6572 656e 7420 636c 6173    different clas
-0000d3c0: 7320 6d65 7468 6f64 2074 6861 6e20 6060  s method than ``
-0000d3d0: 5f5f 6361 6c6c 5f5f 6060 2e20 466f 7220  __call__``. For 
-0000d3e0: 696e 7374 616e 6365 2c20 7375 7070 6f73  instance, suppos
-0000d3f0: 6520 610a 2020 2020 5472 616e 7366 6f72  e a.    Transfor
-0000d400: 6d65 7220 6d6f 6475 6c65 7320 6861 7320  mer modules has 
-0000d410: 6120 6d65 7468 6f64 2063 616c 6c65 6420  a method called 
-0000d420: 6065 6e63 6f64 6560 2c20 7468 656e 2074  `encode`, then t
-0000d430: 6865 2066 6f6c 6c6f 7769 6e67 2063 616c  he following cal
-0000d440: 6c73 0a20 2020 2060 6170 706c 7960 206f  ls.    `apply` o
-0000d450: 6e20 7468 6174 206d 6574 686f 643a 3a0a  n that method::.
-0000d460: 0a20 2020 2020 206d 6f64 656c 203d 2054  .      model = T
-0000d470: 7261 6e73 666f 726d 6572 2829 0a20 2020  ransformer().   
-0000d480: 2020 2065 6e63 6f64 6564 203d 206d 6f64     encoded = mod
-0000d490: 656c 2e61 7070 6c79 287b 2770 6172 616d  el.apply({'param
-0000d4a0: 7327 3a20 7061 7261 6d73 7d2c 2078 2c20  s': params}, x, 
-0000d4b0: 6d65 7468 6f64 3d54 7261 6e73 666f 726d  method=Transform
-0000d4c0: 6572 2e65 6e63 6f64 6529 0a0a 2020 2020  er.encode)..    
-0000d4d0: 4966 2061 2066 756e 6374 696f 6e20 696e  If a function in
-0000d4e0: 7374 616e 6365 2069 7320 7072 6f76 6964  stance is provid
-0000d4f0: 6564 2c20 7468 6520 756e 626f 756e 6420  ed, the unbound 
-0000d500: 6675 6e63 7469 6f6e 2069 7320 7573 6564  function is used
-0000d510: 2e20 466f 720a 2020 2020 696e 7374 616e  . For.    instan
-0000d520: 6365 2c20 7468 6520 6578 616d 706c 6520  ce, the example 
-0000d530: 6265 6c6f 7720 6973 2065 7175 6976 616c  below is equival
-0000d540: 656e 7420 746f 2074 6865 206f 6e65 2061  ent to the one a
-0000d550: 626f 7665 3a3a 0a0a 2020 2020 2020 656e  bove::..      en
-0000d560: 636f 6465 6420 3d20 6d6f 6465 6c2e 6170  coded = model.ap
-0000d570: 706c 7928 7b27 7061 7261 6d73 273a 2070  ply({'params': p
-0000d580: 6172 616d 737d 2c20 782c 206d 6574 686f  arams}, x, metho
-0000d590: 643d 6d6f 6465 6c2e 656e 636f 6465 290a  d=model.encode).
-0000d5a0: 0a20 2020 2059 6f75 2063 616e 2061 6c73  .    You can als
-0000d5b0: 6f20 7061 7373 2061 2073 7472 696e 6720  o pass a string 
-0000d5c0: 746f 2061 2063 616c 6c61 626c 6520 6174  to a callable at
-0000d5d0: 7472 6962 7574 6520 6f66 2074 6865 206d  tribute of the m
-0000d5e0: 6f64 756c 652e 2046 6f72 0a20 2020 2065  odule. For.    e
-0000d5f0: 7861 6d70 6c65 2c20 7468 6520 7072 6576  xample, the prev
-0000d600: 696f 7573 2063 616e 2062 6520 7772 6974  ious can be writ
-0000d610: 7465 6e20 6173 3a3a 0a0a 2020 2020 2020  ten as::..      
-0000d620: 656e 636f 6465 6420 3d20 6d6f 6465 6c2e  encoded = model.
-0000d630: 6170 706c 7928 7b27 7061 7261 6d73 273a  apply({'params':
-0000d640: 2070 6172 616d 737d 2c20 782c 206d 6574   params}, x, met
-0000d650: 686f 643d 2765 6e63 6f64 6527 290a 0a20  hod='encode').. 
-0000d660: 2020 204e 6f74 6520 6060 6d65 7468 6f64     Note ``method
-0000d670: 6060 2063 616e 2061 6c73 6f20 6265 2061  `` can also be a
-0000d680: 2066 756e 6374 696f 6e20 7468 6174 2069   function that i
-0000d690: 7320 6e6f 7420 6465 6669 6e65 6420 696e  s not defined in
-0000d6a0: 0a20 2020 2060 6054 7261 6e73 666f 726d  .    ``Transform
-0000d6b0: 6572 6060 2e20 496e 2074 6861 7420 6361  er``. In that ca
-0000d6c0: 7365 2c20 7468 6520 6675 6e63 7469 6f6e  se, the function
-0000d6d0: 2073 686f 756c 6420 6861 7665 2061 7420   should have at 
-0000d6e0: 6c65 6173 7420 6f6e 650a 2020 2020 6172  least one.    ar
-0000d6f0: 6775 6d65 6e74 2072 6570 7265 7365 6e74  gument represent
-0000d700: 696e 6720 616e 2069 6e73 7461 6e63 6520  ing an instance 
-0000d710: 6f66 2074 6865 204d 6f64 756c 6520 636c  of the Module cl
-0000d720: 6173 733a 3a0a 0a20 2020 2020 2064 6566  ass::..      def
-0000d730: 206f 7468 6572 5f66 6e28 696e 7374 616e   other_fn(instan
-0000d740: 6365 2c20 2e2e 2e29 3a0a 2020 2020 2020  ce, ...):.      
-0000d750: 2020 696e 7374 616e 6365 2e73 6f6d 655f    instance.some_
-0000d760: 6d6f 6475 6c65 5f61 7474 7228 2e2e 2e29  module_attr(...)
-0000d770: 0a20 2020 2020 2020 202e 2e2e 0a0a 2020  .        .....  
-0000d780: 2020 2020 6d6f 6465 6c2e 6170 706c 7928      model.apply(
-0000d790: 7b27 7061 7261 6d73 273a 2070 6172 616d  {'params': param
-0000d7a0: 737d 2c20 782c 206d 6574 686f 643d 6f74  s}, x, method=ot
-0000d7b0: 6865 725f 666e 290a 0a20 2020 2041 7267  her_fn)..    Arg
-0000d7c0: 733a 0a20 2020 2020 2076 6172 6961 626c  s:.      variabl
-0000d7d0: 6573 3a20 4120 6469 6374 696f 6e61 7279  es: A dictionary
-0000d7e0: 2063 6f6e 7461 696e 696e 6720 7661 7269   containing vari
-0000d7f0: 6162 6c65 7320 6b65 7965 6420 6279 2076  ables keyed by v
-0000d800: 6172 6961 626c 650a 2020 2020 2020 2020  ariable.        
-0000d810: 636f 6c6c 6563 7469 6f6e 732e 2053 6565  collections. See
-0000d820: 203a 6d6f 643a 6066 6c61 782e 636f 7265   :mod:`flax.core
-0000d830: 2e76 6172 6961 626c 6573 6020 666f 7220  .variables` for 
-0000d840: 6d6f 7265 2064 6574 6169 6c73 0a20 2020  more details.   
-0000d850: 2020 2020 2061 626f 7574 2076 6172 6961       about varia
-0000d860: 626c 6573 2e0a 2020 2020 2020 2a61 7267  bles..      *arg
-0000d870: 733a 204e 616d 6564 2061 7267 756d 656e  s: Named argumen
-0000d880: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
-0000d890: 2073 7065 6369 6669 6564 2061 7070 6c79   specified apply
-0000d8a0: 206d 6574 686f 642e 0a20 2020 2020 2072   method..      r
-0000d8b0: 6e67 733a 2061 2064 6963 7420 6f66 2050  ngs: a dict of P
-0000d8c0: 524e 474b 6579 7320 746f 2069 6e69 7469  RNGKeys to initi
-0000d8d0: 616c 697a 6520 7468 6520 5052 4e47 2073  alize the PRNG s
-0000d8e0: 6571 7565 6e63 6573 2e0a 2020 2020 2020  equences..      
-0000d8f0: 2020 5468 6520 2270 6172 616d 7322 2050    The "params" P
-0000d900: 524e 4720 7365 7175 656e 6365 2069 7320  RNG sequence is 
-0000d910: 7573 6564 2074 6f20 696e 6974 6961 6c69  used to initiali
-0000d920: 7a65 2070 6172 616d 6574 6572 732e 0a20  ze parameters.. 
-0000d930: 2020 2020 206d 6574 686f 643a 2041 2066       method: A f
-0000d940: 756e 6374 696f 6e20 746f 2063 616c 6c20  unction to call 
-0000d950: 6170 706c 7920 6f6e 2e20 5468 6973 2069  apply on. This i
-0000d960: 7320 6765 6e65 7261 6c6c 7920 6120 6675  s generally a fu
-0000d970: 6e63 7469 6f6e 2069 6e20 7468 650a 2020  nction in the.  
-0000d980: 2020 2020 2020 6d6f 6475 6c65 2e20 4966        module. If
-0000d990: 2070 726f 7669 6465 642c 2061 7070 6c69   provided, appli
-0000d9a0: 6573 2074 6869 7320 6d65 7468 6f64 2e20  es this method. 
-0000d9b0: 4966 206e 6f74 2070 726f 7669 6465 642c  If not provided,
-0000d9c0: 2061 7070 6c69 6573 2074 6865 0a20 2020   applies the.   
-0000d9d0: 2020 2020 2060 605f 5f63 616c 6c5f 5f60       ``__call__`
-0000d9e0: 6020 6d65 7468 6f64 206f 6620 7468 6520  ` method of the 
-0000d9f0: 6d6f 6475 6c65 2e20 4120 7374 7269 6e67  module. A string
-0000da00: 2063 616e 2061 6c73 6f20 6265 2070 726f   can also be pro
-0000da10: 7669 6465 6420 746f 0a20 2020 2020 2020  vided to.       
-0000da20: 2073 7065 6369 6679 2061 206d 6574 686f   specify a metho
-0000da30: 6420 6279 206e 616d 652e 0a20 2020 2020  d by name..     
-0000da40: 206d 7574 6162 6c65 3a20 4361 6e20 6265   mutable: Can be
-0000da50: 2062 6f6f 6c2c 2073 7472 2c20 6f72 206c   bool, str, or l
-0000da60: 6973 742e 2053 7065 6369 6669 6573 2077  ist. Specifies w
-0000da70: 6869 6368 2063 6f6c 6c65 6374 696f 6e73  hich collections
-0000da80: 2073 686f 756c 6420 6265 0a20 2020 2020   should be.     
-0000da90: 2020 2020 2020 2020 2020 7472 6561 7465            treate
-0000daa0: 6420 6173 206d 7574 6162 6c65 3a20 6060  d as mutable: ``
-0000dab0: 626f 6f6c 6060 3a20 616c 6c2f 6e6f 2063  bool``: all/no c
-0000dac0: 6f6c 6c65 6374 696f 6e73 2061 7265 206d  ollections are m
-0000dad0: 7574 6162 6c65 2e0a 2020 2020 2020 2020  utable..        
-0000dae0: 2020 2020 2020 2060 6073 7472 6060 3a20         ``str``: 
-0000daf0: 5468 6520 6e61 6d65 206f 6620 6120 7369  The name of a si
-0000db00: 6e67 6c65 206d 7574 6162 6c65 2063 6f6c  ngle mutable col
-0000db10: 6c65 6374 696f 6e2e 2060 606c 6973 7460  lection. ``list`
-0000db20: 603a 2041 0a20 2020 2020 2020 2020 2020  `: A.           
-0000db30: 2020 2020 6c69 7374 206f 6620 6e61 6d65      list of name
-0000db40: 7320 6f66 206d 7574 6162 6c65 2063 6f6c  s of mutable col
-0000db50: 6c65 6374 696f 6e73 2e0a 2020 2020 2020  lections..      
-0000db60: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-0000db70: 6961 7465 733a 2049 6620 6054 7275 6560  iates: If `True`
-0000db80: 2c20 6361 7074 7572 6573 2069 6e74 6572  , captures inter
-0000db90: 6d65 6469 6174 6520 7265 7475 726e 2076  mediate return v
-0000dba0: 616c 7565 730a 2020 2020 2020 2020 6f66  alues.        of
-0000dbb0: 2061 6c6c 204d 6f64 756c 6573 2069 6e73   all Modules ins
-0000dbc0: 6964 6520 7468 6520 2269 6e74 6572 6d65  ide the "interme
-0000dbd0: 6469 6174 6573 2220 636f 6c6c 6563 7469  diates" collecti
-0000dbe0: 6f6e 2e20 4279 2064 6566 6175 6c74 206f  on. By default o
-0000dbf0: 6e6c 790a 2020 2020 2020 2020 7468 6520  nly.        the 
-0000dc00: 7265 7475 726e 2076 616c 7565 7320 6f66  return values of
-0000dc10: 2061 6c6c 2060 605f 5f63 616c 6c5f 5f60   all ``__call__`
-0000dc20: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
-0000dc30: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
-0000dc40: 2063 616e 0a20 2020 2020 2020 2062 6520   can.        be 
-0000dc50: 7061 7373 6564 2074 6f20 6368 616e 6765  passed to change
-0000dc60: 2074 6865 2066 696c 7465 7220 6265 6861   the filter beha
-0000dc70: 7669 6f72 2e20 5468 6520 6669 6c74 6572  vior. The filter
-0000dc80: 2066 756e 6374 696f 6e20 7461 6b65 730a   function takes.
-0000dc90: 2020 2020 2020 2020 7468 6520 4d6f 6475          the Modu
-0000dca0: 6c65 2069 6e73 7461 6e63 6520 616e 6420  le instance and 
-0000dcb0: 6d65 7468 6f64 206e 616d 6520 616e 6420  method name and 
-0000dcc0: 7265 7475 726e 7320 6120 626f 6f6c 2069  returns a bool i
-0000dcd0: 6e64 6963 6174 696e 670a 2020 2020 2020  ndicating.      
-0000dce0: 2020 7768 6574 6865 7220 7468 6520 6f75    whether the ou
-0000dcf0: 7470 7574 206f 6620 7468 6174 206d 6574  tput of that met
-0000dd00: 686f 6420 696e 766f 6361 7469 6f6e 2073  hod invocation s
-0000dd10: 686f 756c 6420 6265 2073 746f 7265 642e  hould be stored.
-0000dd20: 0a20 2020 2020 202a 2a6b 7761 7267 733a  .      **kwargs:
-0000dd30: 204b 6579 776f 7264 2061 7267 756d 656e   Keyword argumen
-0000dd40: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
-0000dd50: 2073 7065 6369 6669 6564 2061 7070 6c79   specified apply
-0000dd60: 206d 6574 686f 642e 0a20 2020 2052 6574   method..    Ret
-0000dd70: 7572 6e73 3a0a 2020 2020 2020 4966 2060  urns:.      If `
-0000dd80: 606d 7574 6162 6c65 6060 2069 7320 4661  `mutable`` is Fa
-0000dd90: 6c73 652c 2072 6574 7572 6e73 206f 7574  lse, returns out
-0000dda0: 7075 742e 2049 6620 616e 7920 636f 6c6c  put. If any coll
-0000ddb0: 6563 7469 6f6e 7320 6172 650a 2020 2020  ections are.    
-0000ddc0: 2020 6d75 7461 626c 652c 2072 6574 7572    mutable, retur
-0000ddd0: 6e73 2060 6028 6f75 7470 7574 2c20 7661  ns ``(output, va
-0000dde0: 7273 2960 602c 2077 6865 7265 2060 6076  rs)``, where ``v
-0000ddf0: 6172 7360 6020 6172 6520 6973 2061 2064  ars`` are is a d
-0000de00: 6963 740a 2020 2020 2020 6f66 2074 6865  ict.      of the
-0000de10: 206d 6f64 6966 6965 6420 636f 6c6c 6563   modified collec
-0000de20: 7469 6f6e 732e 0a20 2020 2022 2222 0a20  tions..    """. 
-0000de30: 2020 204d 6f64 756c 652e 5f6d 6f64 756c     Module._modul
-0000de40: 655f 6368 6563 6b73 2873 656c 6629 0a0a  e_checks(self)..
-0000de50: 2020 2020 6966 2069 7369 6e73 7461 6e63      if isinstanc
-0000de60: 6528 6d65 7468 6f64 2c20 7374 7229 3a0a  e(method, str):.
-0000de70: 2020 2020 2020 6174 7472 6962 7574 655f        attribute_
-0000de80: 6e61 6d65 203d 206d 6574 686f 640a 2020  name = method.  
-0000de90: 2020 2020 6d65 7468 6f64 203d 2067 6574      method = get
-0000dea0: 6174 7472 2873 656c 662c 2061 7474 7269  attr(self, attri
-0000deb0: 6275 7465 5f6e 616d 6529 0a20 2020 2020  bute_name).     
-0000dec0: 2069 6620 6e6f 7420 6361 6c6c 6162 6c65   if not callable
-0000ded0: 286d 6574 686f 6429 3a0a 2020 2020 2020  (method):.      
-0000dee0: 2020 636c 6173 735f 6e61 6d65 203d 2074    class_name = t
-0000def0: 7970 6528 7365 6c66 292e 5f5f 6e61 6d65  ype(self).__name
-0000df00: 5f5f 0a20 2020 2020 2020 2072 6169 7365  __.        raise
-0000df10: 2054 7970 6545 7272 6f72 2866 275c 277b   TypeError(f'\'{
-0000df20: 636c 6173 735f 6e61 6d65 7d2e 7b61 7474  class_name}.{att
-0000df30: 7269 6275 7465 5f6e 616d 657d 5c27 206d  ribute_name}\' m
-0000df40: 7573 7420 6265 2061 2063 616c 6c61 626c  ust be a callabl
-0000df50: 652c 2067 6f74 207b 7479 7065 286d 6574  e, got {type(met
-0000df60: 686f 6429 7d2e 2729 0a20 2020 2065 6c69  hod)}.').    eli
-0000df70: 6620 6d65 7468 6f64 2069 7320 4e6f 6e65  f method is None
-0000df80: 3a0a 2020 2020 2020 6d65 7468 6f64 203d  :.      method =
-0000df90: 2073 656c 662e 5f5f 6361 6c6c 5f5f 0a20   self.__call__. 
-0000dfa0: 2020 206d 6574 686f 6420 3d20 5f67 6574     method = _get
-0000dfb0: 5f75 6e62 6f75 6e64 5f66 6e28 6d65 7468  _unbound_fn(meth
-0000dfc0: 6f64 290a 2020 2020 7265 7475 726e 2061  od).    return a
-0000dfd0: 7070 6c79 280a 2020 2020 2020 2020 6d65  pply(.        me
-0000dfe0: 7468 6f64 2c20 7365 6c66 2c0a 2020 2020  thod, self,.    
-0000dff0: 2020 2020 6d75 7461 626c 653d 6d75 7461      mutable=muta
-0000e000: 626c 652c 0a20 2020 2020 2020 2063 6170  ble,.        cap
-0000e010: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
-0000e020: 6573 3d63 6170 7475 7265 5f69 6e74 6572  es=capture_inter
-0000e030: 6d65 6469 6174 6573 2c0a 2020 2020 2928  mediates,.    )(
-0000e040: 7661 7269 6162 6c65 732c 202a 6172 6773  variables, *args
-0000e050: 2c20 2a2a 6b77 6172 6773 2c20 726e 6773  , **kwargs, rngs
-0000e060: 3d72 6e67 7329 0a0a 2020 4074 7261 6365  =rngs)..  @trace
-0000e070: 6261 636b 5f75 7469 6c2e 6170 695f 626f  back_util.api_bo
-0000e080: 756e 6461 7279 0a20 2064 6566 2069 6e69  undary.  def ini
-0000e090: 745f 7769 7468 5f6f 7574 7075 7428 7365  t_with_output(se
-0000e0a0: 6c66 2c0a 2020 2020 2020 2020 2020 2020  lf,.            
-0000e0b0: 2020 2020 2020 2020 2020 2072 6e67 733a             rngs:
-0000e0c0: 2055 6e69 6f6e 5b4b 6579 4172 7261 792c   Union[KeyArray,
-0000e0d0: 2052 4e47 5365 7175 656e 6365 735d 2c0a   RNGSequences],.
-0000e0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e0f0: 2020 2020 2020 202a 6172 6773 2c0a 2020         *args,.  
-0000e100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e110: 2020 2020 206d 6574 686f 643a 2055 6e69       method: Uni
-0000e120: 6f6e 5b43 616c 6c61 626c 655b 2e2e 2e2c  on[Callable[...,
-0000e130: 2041 6e79 5d2c 2073 7472 2c20 4e6f 6e65   Any], str, None
-0000e140: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 2020  ] = None,.      
-0000e150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e160: 206d 7574 6162 6c65 3a20 436f 6c6c 6563   mutable: Collec
-0000e170: 7469 6f6e 4669 6c74 6572 203d 2044 656e  tionFilter = Den
-0000e180: 794c 6973 7428 2769 6e74 6572 6d65 6469  yList('intermedi
-0000e190: 6174 6573 2729 2c0a 2020 2020 2020 2020  ates'),.        
-0000e1a0: 2020 2020 2020 2020 2020 2020 2020 2063                 c
-0000e1b0: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
-0000e1c0: 6174 6573 3a20 556e 696f 6e5b 626f 6f6c  ates: Union[bool
-0000e1d0: 2c20 4361 6c6c 6162 6c65 5b5b 274d 6f64  , Callable[['Mod
-0000e1e0: 756c 6527 2c20 7374 725d 2c20 626f 6f6c  ule', str], bool
-0000e1f0: 5d5d 203d 2046 616c 7365 2c0a 2020 2020  ]] = False,.    
-0000e200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e210: 2020 202a 2a6b 7761 7267 7329 202d 3e20     **kwargs) -> 
-0000e220: 5475 706c 655b 416e 792c 2055 6e69 6f6e  Tuple[Any, Union
-0000e230: 5b46 726f 7a65 6e56 6172 6961 626c 6544  [FrozenVariableD
-0000e240: 6963 742c 2044 6963 745b 7374 722c 2041  ict, Dict[str, A
-0000e250: 6e79 5d5d 5d3a 0a20 2020 2022 2222 496e  ny]]]:.    """In
-0000e260: 6974 6961 6c69 7a65 7320 6120 6d6f 6475  itializes a modu
-0000e270: 6c65 206d 6574 686f 6420 7769 7468 2076  le method with v
-0000e280: 6172 6961 626c 6573 2061 6e64 2072 6574  ariables and ret
-0000e290: 7572 6e73 206f 7574 7075 7420 616e 6420  urns output and 
-0000e2a0: 6d6f 6469 6669 6564 2076 6172 6961 626c  modified variabl
-0000e2b0: 6573 2e0a 0a20 2020 2041 7267 733a 0a20  es...    Args:. 
-0000e2c0: 2020 2020 2072 6e67 733a 2054 6865 2072       rngs: The r
-0000e2d0: 6e67 7320 666f 7220 7468 6520 7661 7269  ngs for the vari
-0000e2e0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
-0000e2f0: 2e0a 2020 2020 2020 2a61 7267 733a 204e  ..      *args: N
-0000e300: 616d 6564 2061 7267 756d 656e 7473 2070  amed arguments p
-0000e310: 6173 7365 6420 746f 2074 6865 2069 6e69  assed to the ini
-0000e320: 7420 6675 6e63 7469 6f6e 2e0a 2020 2020  t function..    
-0000e330: 2020 6d65 7468 6f64 3a20 416e 206f 7074    method: An opt
-0000e340: 696f 6e61 6c20 6d65 7468 6f64 2e20 4966  ional method. If
-0000e350: 2070 726f 7669 6465 642c 2061 7070 6c69   provided, appli
-0000e360: 6573 2074 6869 7320 6d65 7468 6f64 2e20  es this method. 
-0000e370: 4966 206e 6f74 0a20 2020 2020 2020 2070  If not.        p
-0000e380: 726f 7669 6465 642c 2061 7070 6c69 6573  rovided, applies
-0000e390: 2074 6865 2060 605f 5f63 616c 6c5f 5f60   the ``__call__`
-0000e3a0: 6020 6d65 7468 6f64 2e20 4120 7374 7269  ` method. A stri
-0000e3b0: 6e67 2063 616e 2061 6c73 6f20 6265 270a  ng can also be'.
-0000e3c0: 2020 2020 2020 2020 7072 6f76 6964 6564          provided
-0000e3d0: 2074 6f20 7370 6563 6966 7920 6120 6d65   to specify a me
-0000e3e0: 7468 6f64 2062 7920 6e61 6d65 2e0a 2020  thod by name..  
-0000e3f0: 2020 2020 6d75 7461 626c 653a 2043 616e      mutable: Can
-0000e400: 2062 6520 626f 6f6c 2c20 7374 722c 206f   be bool, str, o
-0000e410: 7220 6c69 7374 2e20 5370 6563 6966 6965  r list. Specifie
-0000e420: 7320 7768 6963 6820 636f 6c6c 6563 7469  s which collecti
-0000e430: 6f6e 7320 7368 6f75 6c64 2062 650a 2020  ons should be.  
-0000e440: 2020 2020 2020 7472 6561 7465 6420 6173        treated as
-0000e450: 206d 7574 6162 6c65 3a20 6060 626f 6f6c   mutable: ``bool
-0000e460: 6060 3a20 616c 6c2f 6e6f 2063 6f6c 6c65  ``: all/no colle
-0000e470: 6374 696f 6e73 2061 7265 206d 7574 6162  ctions are mutab
-0000e480: 6c65 2e0a 2020 2020 2020 2020 6060 7374  le..        ``st
-0000e490: 7260 603a 2054 6865 206e 616d 6520 6f66  r``: The name of
-0000e4a0: 2061 2073 696e 676c 6520 6d75 7461 626c   a single mutabl
-0000e4b0: 6520 636f 6c6c 6563 7469 6f6e 2e20 6060  e collection. ``
-0000e4c0: 6c69 7374 6060 3a20 410a 2020 2020 2020  list``: A.      
-0000e4d0: 2020 6c69 7374 206f 6620 6e61 6d65 7320    list of names 
-0000e4e0: 6f66 206d 7574 6162 6c65 2063 6f6c 6c65  of mutable colle
-0000e4f0: 6374 696f 6e73 2e20 4279 2064 6566 6175  ctions. By defau
-0000e500: 6c74 2061 6c6c 2063 6f6c 6c65 6374 696f  lt all collectio
-0000e510: 6e73 0a20 2020 2020 2020 2065 7863 6570  ns.        excep
-0000e520: 7420 2269 6e74 6572 6d65 6469 6174 6573  t "intermediates
-0000e530: 2220 6172 6520 6d75 7461 626c 652e 0a20  " are mutable.. 
-0000e540: 2020 2020 2063 6170 7475 7265 5f69 6e74       capture_int
-0000e550: 6572 6d65 6469 6174 6573 3a20 4966 2060  ermediates: If `
-0000e560: 5472 7565 602c 2063 6170 7475 7265 7320  True`, captures 
-0000e570: 696e 7465 726d 6564 6961 7465 2072 6574  intermediate ret
-0000e580: 7572 6e20 7661 6c75 6573 0a20 2020 2020  urn values.     
-0000e590: 2020 206f 6620 616c 6c20 4d6f 6475 6c65     of all Module
-0000e5a0: 7320 696e 7369 6465 2074 6865 2022 696e  s inside the "in
-0000e5b0: 7465 726d 6564 6961 7465 7322 2063 6f6c  termediates" col
-0000e5c0: 6c65 6374 696f 6e2e 2042 7920 6465 6661  lection. By defa
-0000e5d0: 756c 7420 6f6e 6c79 0a20 2020 2020 2020  ult only.       
-0000e5e0: 2074 6865 2072 6574 7572 6e20 7661 6c75   the return valu
-0000e5f0: 6573 206f 6620 616c 6c20 6060 5f5f 6361  es of all ``__ca
-0000e600: 6c6c 5f5f 6060 206d 6574 686f 6473 2061  ll__`` methods a
-0000e610: 7265 2073 746f 7265 642e 2041 2066 756e  re stored. A fun
-0000e620: 6374 696f 6e20 6361 6e0a 2020 2020 2020  ction can.      
-0000e630: 2020 6265 2070 6173 7365 6420 746f 2063    be passed to c
-0000e640: 6861 6e67 6520 7468 6520 6669 6c74 6572  hange the filter
-0000e650: 2062 6568 6176 696f 722e 2054 6865 2066   behavior. The f
-0000e660: 696c 7465 7220 6675 6e63 7469 6f6e 2074  ilter function t
-0000e670: 616b 6573 0a20 2020 2020 2020 2074 6865  akes.        the
-0000e680: 204d 6f64 756c 6520 696e 7374 616e 6365   Module instance
-0000e690: 2061 6e64 206d 6574 686f 6420 6e61 6d65   and method name
-0000e6a0: 2061 6e64 2072 6574 7572 6e73 2061 2062   and returns a b
-0000e6b0: 6f6f 6c20 696e 6469 6361 7469 6e67 0a20  ool indicating. 
-0000e6c0: 2020 2020 2020 2077 6865 7468 6572 2074         whether t
-0000e6d0: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
-0000e6e0: 7420 6d65 7468 6f64 2069 6e76 6f63 6174  t method invocat
-0000e6f0: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
-0000e700: 6f72 6564 2e0a 2020 2020 2020 2a2a 6b77  ored..      **kw
-0000e710: 6172 6773 3a20 4b65 7977 6f72 6420 6172  args: Keyword ar
-0000e720: 6775 6d65 6e74 7320 7061 7373 6564 2074  guments passed t
-0000e730: 6f20 7468 6520 696e 6974 2066 756e 6374  o the init funct
-0000e740: 696f 6e2e 0a20 2020 2052 6574 7572 6e73  ion..    Returns
-0000e750: 3a0a 2020 2020 2020 6028 6f75 7470 7574  :.      `(output
-0000e760: 2c20 7661 7273 2960 602c 2077 6865 7265  , vars)``, where
-0000e770: 2060 6076 6172 7360 6020 6172 6520 6973   ``vars`` are is
-0000e780: 2061 2064 6963 7420 6f66 2074 6865 206d   a dict of the m
-0000e790: 6f64 6966 6965 640a 2020 2020 2020 636f  odified.      co
-0000e7a0: 6c6c 6563 7469 6f6e 732e 0a20 2020 2022  llections..    "
-0000e7b0: 2222 0a20 2020 204d 6f64 756c 652e 5f6d  "".    Module._m
-0000e7c0: 6f64 756c 655f 6368 6563 6b73 2873 656c  odule_checks(sel
-0000e7d0: 6629 0a0a 2020 2020 6966 206e 6f74 2069  f)..    if not i
-0000e7e0: 7369 6e73 7461 6e63 6528 726e 6773 2c20  sinstance(rngs, 
-0000e7f0: 6469 6374 293a 0a20 2020 2020 2069 6620  dict):.      if 
-0000e800: 6e6f 7420 636f 7265 2e73 636f 7065 2e5f  not core.scope._
-0000e810: 6973 5f76 616c 6964 5f72 6e67 2872 6e67  is_valid_rng(rng
-0000e820: 7329 3a0a 2020 2020 2020 2020 7261 6973  s):.        rais
-0000e830: 6520 6572 726f 7273 2e49 6e76 616c 6964  e errors.Invalid
-0000e840: 526e 6745 7272 6f72 280a 2020 2020 2020  RngError(.      
-0000e850: 2020 2020 2020 2752 4e47 7320 7368 6f75        'RNGs shou
-0000e860: 6c64 2062 6520 6f66 2073 6861 7065 2028  ld be of shape (
-0000e870: 322c 2920 6f72 204b 6579 4172 7261 7920  2,) or KeyArray 
-0000e880: 696e 204d 6f64 756c 6520 270a 2020 2020  in Module '.    
-0000e890: 2020 2020 2020 2020 6627 7b73 656c 662e          f'{self.
-0000e8a0: 5f5f 636c 6173 735f 5f2e 5f5f 6e61 6d65  __class__.__name
-0000e8b0: 5f5f 7d2c 2062 7574 2072 6e67 7320 6172  __}, but rngs ar
-0000e8c0: 653a 207b 726e 6773 7d27 290a 2020 2020  e: {rngs}').    
-0000e8d0: 2020 726e 6773 203d 207b 2770 6172 616d    rngs = {'param
-0000e8e0: 7327 3a20 726e 6773 7d0a 0a20 2020 2069  s': rngs}..    i
-0000e8f0: 6620 6973 696e 7374 616e 6365 286d 6574  f isinstance(met
-0000e900: 686f 642c 2073 7472 293a 0a20 2020 2020  hod, str):.     
-0000e910: 2061 7474 7269 6275 7465 5f6e 616d 6520   attribute_name 
-0000e920: 3d20 6d65 7468 6f64 0a20 2020 2020 206d  = method.      m
-0000e930: 6574 686f 6420 3d20 6765 7461 7474 7228  ethod = getattr(
-0000e940: 7365 6c66 2c20 6174 7472 6962 7574 655f  self, attribute_
-0000e950: 6e61 6d65 290a 2020 2020 2020 6966 206e  name).      if n
-0000e960: 6f74 2063 616c 6c61 626c 6528 6d65 7468  ot callable(meth
-0000e970: 6f64 293a 0a20 2020 2020 2020 2063 6c61  od):.        cla
-0000e980: 7373 5f6e 616d 6520 3d20 7479 7065 2873  ss_name = type(s
-0000e990: 656c 6629 2e5f 5f6e 616d 655f 5f0a 2020  elf).__name__.  
-0000e9a0: 2020 2020 2020 7261 6973 6520 5479 7065        raise Type
-0000e9b0: 4572 726f 7228 6627 5c27 7b63 6c61 7373  Error(f'\'{class
-0000e9c0: 5f6e 616d 657d 2e7b 6174 7472 6962 7574  _name}.{attribut
-0000e9d0: 655f 6e61 6d65 7d5c 2720 6d75 7374 2062  e_name}\' must b
-0000e9e0: 6520 6120 6361 6c6c 6162 6c65 2c20 676f  e a callable, go
-0000e9f0: 7420 7b74 7970 6528 6d65 7468 6f64 297d  t {type(method)}
-0000ea00: 2e27 290a 2020 2020 656c 6966 206d 6574  .').    elif met
-0000ea10: 686f 6420 6973 204e 6f6e 653a 0a20 2020  hod is None:.   
-0000ea20: 2020 206d 6574 686f 6420 3d20 7365 6c66     method = self
-0000ea30: 2e5f 5f63 616c 6c5f 5f0a 2020 2020 6d65  .__call__.    me
-0000ea40: 7468 6f64 203d 205f 6765 745f 756e 626f  thod = _get_unbo
-0000ea50: 756e 645f 666e 286d 6574 686f 6429 0a20  und_fn(method). 
-0000ea60: 2020 2072 6574 7572 6e20 696e 6974 5f77     return init_w
-0000ea70: 6974 685f 6f75 7470 7574 280a 2020 2020  ith_output(.    
-0000ea80: 2020 2020 6d65 7468 6f64 2c0a 2020 2020      method,.    
-0000ea90: 2020 2020 7365 6c66 2c0a 2020 2020 2020      self,.      
-0000eaa0: 2020 6d75 7461 626c 653d 6d75 7461 626c    mutable=mutabl
-0000eab0: 652c 0a20 2020 2020 2020 2063 6170 7475  e,.        captu
-0000eac0: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
-0000ead0: 3d63 6170 7475 7265 5f69 6e74 6572 6d65  =capture_interme
-0000eae0: 6469 6174 6573 0a20 2020 2029 2872 6e67  diates.    )(rng
-0000eaf0: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
-0000eb00: 6773 290a 0a20 2040 7472 6163 6562 6163  gs)..  @tracebac
-0000eb10: 6b5f 7574 696c 2e61 7069 5f62 6f75 6e64  k_util.api_bound
-0000eb20: 6172 790a 2020 6465 6620 696e 6974 2873  ary.  def init(s
-0000eb30: 656c 662c 0a20 2020 2020 2020 2020 2020  elf,.           
-0000eb40: 726e 6773 3a20 556e 696f 6e5b 4b65 7941  rngs: Union[KeyA
-0000eb50: 7272 6179 2c20 524e 4753 6571 7565 6e63  rray, RNGSequenc
-0000eb60: 6573 5d2c 0a20 2020 2020 2020 2020 2020  es],.           
-0000eb70: 2a61 7267 732c 0a20 2020 2020 2020 2020  *args,.         
-0000eb80: 2020 6d65 7468 6f64 3a20 556e 696f 6e5b    method: Union[
-0000eb90: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
-0000eba0: 795d 2c20 7374 722c 204e 6f6e 655d 203d  y], str, None] =
-0000ebb0: 204e 6f6e 652c 0a20 2020 2020 2020 2020   None,.         
-0000ebc0: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
-0000ebd0: 6374 696f 6e46 696c 7465 7220 3d20 4465  ctionFilter = De
-0000ebe0: 6e79 4c69 7374 2827 696e 7465 726d 6564  nyList('intermed
-0000ebf0: 6961 7465 7327 292c 0a20 2020 2020 2020  iates'),.       
-0000ec00: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
-0000ec10: 726d 6564 6961 7465 733a 2055 6e69 6f6e  rmediates: Union
-0000ec20: 5b62 6f6f 6c2c 2043 616c 6c61 626c 655b  [bool, Callable[
-0000ec30: 5b27 4d6f 6475 6c65 272c 2073 7472 5d2c  ['Module', str],
-0000ec40: 2062 6f6f 6c5d 5d20 3d20 4661 6c73 652c   bool]] = False,
-0000ec50: 0a20 2020 2020 2020 2020 2020 2a2a 6b77  .           **kw
-0000ec60: 6172 6773 2920 2d3e 2055 6e69 6f6e 5b46  args) -> Union[F
-0000ec70: 726f 7a65 6e56 6172 6961 626c 6544 6963  rozenVariableDic
-0000ec80: 742c 2044 6963 745b 7374 722c 2041 6e79  t, Dict[str, Any
-0000ec90: 5d5d 3a0a 2020 2020 2222 2249 6e69 7469  ]]:.    """Initi
-0000eca0: 616c 697a 6573 2061 206d 6f64 756c 6520  alizes a module 
-0000ecb0: 6d65 7468 6f64 2077 6974 6820 7661 7269  method with vari
-0000ecc0: 6162 6c65 7320 616e 6420 7265 7475 726e  ables and return
-0000ecd0: 7320 6d6f 6469 6669 6564 2076 6172 6961  s modified varia
-0000ece0: 626c 6573 2e0a 0a20 2020 2060 6069 6e69  bles...    ``ini
-0000ecf0: 7460 6020 7461 6b65 7320 6173 2066 6972  t`` takes as fir
-0000ed00: 7374 2061 7267 756d 656e 7420 6569 7468  st argument eith
-0000ed10: 6572 2061 2073 696e 676c 6520 6060 5052  er a single ``PR
-0000ed20: 4e47 4b65 7960 602c 206f 7220 6120 6469  NGKey``, or a di
-0000ed30: 6374 696f 6e61 7279 206d 6170 7069 6e67  ctionary mapping
-0000ed40: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
-0000ed50: 7469 6f6e 7320 6e61 6d65 7320 746f 2074  tions names to t
-0000ed60: 6865 6972 2060 6050 524e 474b 6579 7360  heir ``PRNGKeys`
-0000ed70: 602c 2061 6e64 2077 696c 6c20 6361 6c6c  `, and will call
-0000ed80: 2060 606d 6574 686f 6460 6020 2877 6869   ``method`` (whi
-0000ed90: 6368 2069 7320 7468 6520 6d6f 6475 6c65  ch is the module
-0000eda0: 2773 2060 605f 5f63 616c 6c5f 5f60 6020  's ``__call__`` 
-0000edb0: 6675 6e63 7469 6f6e 2062 7920 6465 6661  function by defa
-0000edc0: 756c 7429 2070 6173 7369 6e67 2060 602a  ult) passing ``*
-0000edd0: 6172 6773 6060 2061 6e64 2060 602a 2a6b  args`` and ``**k
-0000ede0: 7761 7267 7360 602c 2061 6e64 2072 6574  wargs``, and ret
-0000edf0: 7572 6e73 0a20 2020 2061 2064 6963 7469  urns.    a dicti
-0000ee00: 6f6e 6172 7920 6f66 2069 6e69 7469 616c  onary of initial
-0000ee10: 697a 6564 2076 6172 6961 626c 6573 2e0a  ized variables..
-0000ee20: 0a20 2020 2045 7861 6d70 6c65 3a3a 0a0a  .    Example::..
-0000ee30: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
-0000ee40: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
-0000ee50: 6e0a 2020 2020 2020 3e3e 3e20 696d 706f  n.      >>> impo
-0000ee60: 7274 206a 6178 2e6e 756d 7079 2061 7320  rt jax.numpy as 
-0000ee70: 6a6e 700a 2020 2020 2020 3e3e 3e20 696d  jnp.      >>> im
-0000ee80: 706f 7274 206a 6178 0a20 2020 2020 202e  port jax.      .
-0000ee90: 2e2e 0a20 2020 2020 203e 3e3e 2063 6c61  ...      >>> cla
-0000eea0: 7373 2046 6f6f 286e 6e2e 4d6f 6475 6c65  ss Foo(nn.Module
-0000eeb0: 293a 0a20 2020 2020 202e 2e2e 2020 2040  ):.      ...   @
-0000eec0: 6e6e 2e63 6f6d 7061 6374 0a20 2020 2020  nn.compact.     
-0000eed0: 202e 2e2e 2020 2064 6566 205f 5f63 616c   ...   def __cal
-0000eee0: 6c5f 5f28 7365 6c66 2c20 782c 2074 7261  l__(self, x, tra
-0000eef0: 696e 293a 0a20 2020 2020 202e 2e2e 2020  in):.      ...  
-0000ef00: 2020 2078 203d 206e 6e2e 4465 6e73 6528     x = nn.Dense(
-0000ef10: 3136 2928 7829 0a20 2020 2020 202e 2e2e  16)(x).      ...
-0000ef20: 2020 2020 2078 203d 206e 6e2e 4261 7463       x = nn.Batc
-0000ef30: 684e 6f72 6d28 7573 655f 7275 6e6e 696e  hNorm(use_runnin
-0000ef40: 675f 6176 6572 6167 653d 6e6f 7420 7472  g_average=not tr
-0000ef50: 6169 6e29 2878 290a 2020 2020 2020 2e2e  ain)(x).      ..
-0000ef60: 2e20 2020 2020 7820 3d20 6e6e 2e72 656c  .     x = nn.rel
-0000ef70: 7528 7829 0a20 2020 2020 202e 2e2e 2020  u(x).      ...  
-0000ef80: 2020 2072 6574 7572 6e20 6e6e 2e44 656e     return nn.Den
-0000ef90: 7365 2831 2928 7829 0a20 2020 2020 202e  se(1)(x).      .
-0000efa0: 2e2e 0a20 2020 2020 203e 3e3e 206d 6f64  ...      >>> mod
-0000efb0: 756c 6520 3d20 466f 6f28 290a 2020 2020  ule = Foo().    
-0000efc0: 2020 3e3e 3e20 6b65 7920 3d20 6a61 782e    >>> key = jax.
-0000efd0: 7261 6e64 6f6d 2e50 524e 474b 6579 2830  random.PRNGKey(0
-0000efe0: 290a 2020 2020 2020 3e3e 3e20 7661 7269  ).      >>> vari
-0000eff0: 6162 6c65 7320 3d20 6d6f 6475 6c65 2e69  ables = module.i
-0000f000: 6e69 7428 6b65 792c 206a 6e70 2e65 6d70  nit(key, jnp.emp
-0000f010: 7479 2828 312c 2037 2929 2c20 7472 6169  ty((1, 7)), trai
-0000f020: 6e3d 4661 6c73 6529 0a0a 2020 2020 4966  n=False)..    If
-0000f030: 2079 6f75 2070 6173 7320 6120 7369 6e67   you pass a sing
-0000f040: 6c65 2060 6050 524e 474b 6579 6060 2c20  le ``PRNGKey``, 
-0000f050: 466c 6178 2077 696c 6c20 7573 6520 6974  Flax will use it
-0000f060: 2074 6f20 6665 6564 2074 6865 2060 6027   to feed the ``'
-0000f070: 7061 7261 6d73 2760 6020 524e 4720 7374  params'`` RNG st
-0000f080: 7265 616d 2e0a 2020 2020 4966 2079 6f75  ream..    If you
-0000f090: 2077 616e 7420 746f 2075 7365 2061 2064   want to use a d
-0000f0a0: 6966 6665 7265 6e74 2052 4e47 2073 7472  ifferent RNG str
-0000f0b0: 6561 6d20 6f72 206e 6565 6420 746f 2075  eam or need to u
-0000f0c0: 7365 206d 756c 7469 706c 6520 7374 7265  se multiple stre
-0000f0d0: 616d 732c 2079 6f75 206d 7573 7420 7061  ams, you must pa
-0000f0e0: 7373 2061 0a20 2020 2064 6963 7469 6f6e  ss a.    diction
-0000f0f0: 6172 7920 6d61 7070 696e 6720 6561 6368  ary mapping each
-0000f100: 2052 4e47 2073 7472 6561 6d20 6e61 6d65   RNG stream name
-0000f110: 2074 6f20 6974 7320 636f 7272 6573 706f   to its correspo
-0000f120: 6e64 696e 6720 6060 5052 4e47 4b65 7960  nding ``PRNGKey`
-0000f130: 6020 746f 2060 6069 6e69 7460 602e 0a0a  ` to ``init``...
-0000f140: 2020 2020 4578 616d 706c 653a 3a0a 0a20      Example::.. 
-0000f150: 2020 2020 203e 3e3e 2063 6c61 7373 2046       >>> class F
-0000f160: 6f6f 286e 6e2e 4d6f 6475 6c65 293a 0a20  oo(nn.Module):. 
-0000f170: 2020 2020 202e 2e2e 2020 2040 6e6e 2e63       ...   @nn.c
-0000f180: 6f6d 7061 6374 0a20 2020 2020 202e 2e2e  ompact.      ...
-0000f190: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
-0000f1a0: 7365 6c66 2c20 782c 2074 7261 696e 293a  self, x, train):
-0000f1b0: 0a20 2020 2020 202e 2e2e 2020 2020 2078  .      ...     x
-0000f1c0: 203d 206e 6e2e 4465 6e73 6528 3136 2928   = nn.Dense(16)(
-0000f1d0: 7829 0a20 2020 2020 202e 2e2e 2020 2020  x).      ...    
-0000f1e0: 2078 203d 206e 6e2e 4261 7463 684e 6f72   x = nn.BatchNor
-0000f1f0: 6d28 7573 655f 7275 6e6e 696e 675f 6176  m(use_running_av
-0000f200: 6572 6167 653d 6e6f 7420 7472 6169 6e29  erage=not train)
-0000f210: 2878 290a 2020 2020 2020 2e2e 2e20 2020  (x).      ...   
-0000f220: 2020 7820 3d20 6e6e 2e72 656c 7528 7829    x = nn.relu(x)
-0000f230: 0a20 2020 2020 202e 2e2e 0a20 2020 2020  .      ....     
-0000f240: 202e 2e2e 2020 2020 2023 2041 6464 2067   ...     # Add g
-0000f250: 6175 7373 6961 6e20 6e6f 6973 650a 2020  aussian noise.  
-0000f260: 2020 2020 2e2e 2e20 2020 2020 6e6f 6973      ...     nois
-0000f270: 655f 6b65 7920 3d20 7365 6c66 2e6d 616b  e_key = self.mak
-0000f280: 655f 726e 6728 276e 6f69 7365 2729 0a20  e_rng('noise'). 
-0000f290: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
-0000f2a0: 2078 202b 206a 6178 2e72 616e 646f 6d2e   x + jax.random.
-0000f2b0: 6e6f 726d 616c 286e 6f69 7365 5f6b 6579  normal(noise_key
-0000f2c0: 2c20 782e 7368 6170 6529 0a20 2020 2020  , x.shape).     
-0000f2d0: 202e 2e2e 0a20 2020 2020 202e 2e2e 2020   ....      ...  
-0000f2e0: 2020 2072 6574 7572 6e20 6e6e 2e44 656e     return nn.Den
-0000f2f0: 7365 2831 2928 7829 0a20 2020 2020 202e  se(1)(x).      .
-0000f300: 2e2e 0a20 2020 2020 203e 3e3e 206d 6f64  ...      >>> mod
-0000f310: 756c 6520 3d20 466f 6f28 290a 2020 2020  ule = Foo().    
-0000f320: 2020 3e3e 3e20 726e 6773 203d 207b 2770    >>> rngs = {'p
-0000f330: 6172 616d 7327 3a20 6a61 782e 7261 6e64  arams': jax.rand
-0000f340: 6f6d 2e50 524e 474b 6579 2830 292c 2027  om.PRNGKey(0), '
-0000f350: 6e6f 6973 6527 3a20 6a61 782e 7261 6e64  noise': jax.rand
-0000f360: 6f6d 2e50 524e 474b 6579 2831 297d 0a20  om.PRNGKey(1)}. 
-0000f370: 2020 2020 203e 3e3e 2076 6172 6961 626c       >>> variabl
-0000f380: 6573 203d 206d 6f64 756c 652e 696e 6974  es = module.init
-0000f390: 2872 6e67 732c 206a 6e70 2e65 6d70 7479  (rngs, jnp.empty
-0000f3a0: 2828 312c 2037 2929 2c20 7472 6169 6e3d  ((1, 7)), train=
-0000f3b0: 4661 6c73 6529 0a0a 2020 2020 4a69 7474  False)..    Jitt
-0000f3c0: 696e 6720 6069 6e69 7460 2069 6e69 7469  ing `init` initi
-0000f3d0: 616c 697a 6573 2061 206d 6f64 656c 206c  alizes a model l
-0000f3e0: 617a 696c 7920 7573 696e 6720 6f6e 6c79  azily using only
-0000f3f0: 2074 6865 2073 6861 7065 7320 6f66 2074   the shapes of t
-0000f400: 6865 0a20 2020 2070 726f 7669 6465 6420  he.    provided 
-0000f410: 6172 6775 6d65 6e74 732c 2061 6e64 2061  arguments, and a
-0000f420: 766f 6964 7320 636f 6d70 7574 696e 6720  voids computing 
-0000f430: 7468 6520 666f 7277 6172 6420 7061 7373  the forward pass
-0000f440: 2077 6974 6820 6163 7475 616c 0a20 2020   with actual.   
-0000f450: 2076 616c 7565 732e 2045 7861 6d70 6c65   values. Example
-0000f460: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 6d6f  ::..      >>> mo
-0000f470: 6475 6c65 203d 206e 6e2e 4465 6e73 6528  dule = nn.Dense(
-0000f480: 3129 0a20 2020 2020 203e 3e3e 2069 6e69  1).      >>> ini
-0000f490: 745f 6a69 7420 3d20 6a61 782e 6a69 7428  t_jit = jax.jit(
-0000f4a0: 6d6f 6475 6c65 2e69 6e69 7429 0a20 2020  module.init).   
-0000f4b0: 2020 203e 3e3e 2076 6172 6961 626c 6573     >>> variables
-0000f4c0: 203d 2069 6e69 745f 6a69 7428 6a61 782e   = init_jit(jax.
-0000f4d0: 7261 6e64 6f6d 2e50 524e 474b 6579 2830  random.PRNGKey(0
-0000f4e0: 292c 206a 6e70 2e65 6d70 7479 2828 312c  ), jnp.empty((1,
-0000f4f0: 2037 2929 290a 0a20 2020 2060 6069 6e69   7)))..    ``ini
-0000f500: 7460 6020 6973 2061 206c 6967 6874 2077  t`` is a light w
-0000f510: 7261 7070 6572 206f 7665 7220 6060 6170  rapper over ``ap
-0000f520: 706c 7960 602c 2073 6f20 6f74 6865 7220  ply``, so other 
-0000f530: 6060 6170 706c 7960 6020 6172 6775 6d65  ``apply`` argume
-0000f540: 6e74 7320 6c69 6b65 0a20 2020 2060 606d  nts like.    ``m
-0000f550: 6574 686f 6460 602c 2060 606d 7574 6162  ethod``, ``mutab
-0000f560: 6c65 6060 2c20 616e 6420 6060 6361 7074  le``, and ``capt
-0000f570: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
-0000f580: 7360 6020 6172 6520 616c 736f 2061 7661  s`` are also ava
-0000f590: 696c 6162 6c65 2e0a 0a20 2020 2041 7267  ilable...    Arg
-0000f5a0: 733a 0a20 2020 2020 2072 6e67 733a 2054  s:.      rngs: T
-0000f5b0: 6865 2072 6e67 7320 666f 7220 7468 6520  he rngs for the 
-0000f5c0: 7661 7269 6162 6c65 2063 6f6c 6c65 6374  variable collect
-0000f5d0: 696f 6e73 2e0a 2020 2020 2020 2a61 7267  ions..      *arg
-0000f5e0: 733a 204e 616d 6564 2061 7267 756d 656e  s: Named argumen
-0000f5f0: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
-0000f600: 2069 6e69 7420 6675 6e63 7469 6f6e 2e0a   init function..
-0000f610: 2020 2020 2020 6d65 7468 6f64 3a20 416e        method: An
-0000f620: 206f 7074 696f 6e61 6c20 6d65 7468 6f64   optional method
-0000f630: 2e20 4966 2070 726f 7669 6465 642c 2061  . If provided, a
-0000f640: 7070 6c69 6573 2074 6869 7320 6d65 7468  pplies this meth
-0000f650: 6f64 2e20 4966 206e 6f74 0a20 2020 2020  od. If not.     
-0000f660: 2020 2070 726f 7669 6465 642c 2061 7070     provided, app
-0000f670: 6c69 6573 2074 6865 2060 605f 5f63 616c  lies the ``__cal
-0000f680: 6c5f 5f60 6020 6d65 7468 6f64 2e20 4120  l__`` method. A 
-0000f690: 7374 7269 6e67 2063 616e 2061 6c73 6f20  string can also 
-0000f6a0: 6265 0a20 2020 2020 2020 2070 726f 7669  be.        provi
-0000f6b0: 6465 6420 746f 2073 7065 6369 6679 2061  ded to specify a
-0000f6c0: 206d 6574 686f 6420 6279 206e 616d 652e   method by name.
-0000f6d0: 0a20 2020 2020 206d 7574 6162 6c65 3a20  .      mutable: 
-0000f6e0: 4361 6e20 6265 2062 6f6f 6c2c 2073 7472  Can be bool, str
-0000f6f0: 2c20 6f72 206c 6973 742e 2053 7065 6369  , or list. Speci
-0000f700: 6669 6573 2077 6869 6368 2063 6f6c 6c65  fies which colle
-0000f710: 6374 696f 6e73 2073 686f 756c 6420 6265  ctions should be
-0000f720: 0a20 2020 2020 2020 2074 7265 6174 6564  .        treated
-0000f730: 2061 7320 6d75 7461 626c 653a 2060 6062   as mutable: ``b
-0000f740: 6f6f 6c60 603a 2061 6c6c 2f6e 6f20 636f  ool``: all/no co
-0000f750: 6c6c 6563 7469 6f6e 7320 6172 6520 6d75  llections are mu
-0000f760: 7461 626c 652e 0a20 2020 2020 2020 2060  table..        `
-0000f770: 6073 7472 6060 3a20 5468 6520 6e61 6d65  `str``: The name
-0000f780: 206f 6620 6120 7369 6e67 6c65 206d 7574   of a single mut
-0000f790: 6162 6c65 2063 6f6c 6c65 6374 696f 6e2e  able collection.
-0000f7a0: 2060 606c 6973 7460 603a 2041 0a20 2020   ``list``: A.   
-0000f7b0: 2020 2020 206c 6973 7420 6f66 206e 616d       list of nam
-0000f7c0: 6573 206f 6620 6d75 7461 626c 6520 636f  es of mutable co
-0000f7d0: 6c6c 6563 7469 6f6e 732e 2042 7920 6465  llections. By de
-0000f7e0: 6661 756c 7420 616c 6c20 636f 6c6c 6563  fault all collec
-0000f7f0: 7469 6f6e 730a 2020 2020 2020 2020 6578  tions.        ex
-0000f800: 6365 7074 2022 696e 7465 726d 6564 6961  cept "intermedia
-0000f810: 7465 7322 2061 7265 206d 7574 6162 6c65  tes" are mutable
-0000f820: 2e0a 2020 2020 2020 6361 7074 7572 655f  ..      capture_
-0000f830: 696e 7465 726d 6564 6961 7465 733a 2049  intermediates: I
-0000f840: 6620 6054 7275 6560 2c20 6361 7074 7572  f `True`, captur
-0000f850: 6573 2069 6e74 6572 6d65 6469 6174 6520  es intermediate 
-0000f860: 7265 7475 726e 2076 616c 7565 730a 2020  return values.  
-0000f870: 2020 2020 2020 6f66 2061 6c6c 204d 6f64        of all Mod
-0000f880: 756c 6573 2069 6e73 6964 6520 7468 6520  ules inside the 
-0000f890: 2269 6e74 6572 6d65 6469 6174 6573 2220  "intermediates" 
-0000f8a0: 636f 6c6c 6563 7469 6f6e 2e20 4279 2064  collection. By d
-0000f8b0: 6566 6175 6c74 206f 6e6c 790a 2020 2020  efault only.    
-0000f8c0: 2020 2020 7468 6520 7265 7475 726e 2076      the return v
-0000f8d0: 616c 7565 7320 6f66 2061 6c6c 2060 605f  alues of all ``_
-0000f8e0: 5f63 616c 6c5f 5f60 6020 6d65 7468 6f64  _call__`` method
-0000f8f0: 7320 6172 6520 7374 6f72 6564 2e20 4120  s are stored. A 
-0000f900: 6675 6e63 7469 6f6e 2063 616e 0a20 2020  function can.   
-0000f910: 2020 2020 2062 6520 7061 7373 6564 2074       be passed t
-0000f920: 6f20 6368 616e 6765 2074 6865 2066 696c  o change the fil
-0000f930: 7465 7220 6265 6861 7669 6f72 2e20 5468  ter behavior. Th
-0000f940: 6520 6669 6c74 6572 2066 756e 6374 696f  e filter functio
-0000f950: 6e20 7461 6b65 730a 2020 2020 2020 2020  n takes.        
-0000f960: 7468 6520 4d6f 6475 6c65 2069 6e73 7461  the Module insta
-0000f970: 6e63 6520 616e 6420 6d65 7468 6f64 206e  nce and method n
-0000f980: 616d 6520 616e 6420 7265 7475 726e 7320  ame and returns 
-0000f990: 6120 626f 6f6c 2069 6e64 6963 6174 696e  a bool indicatin
-0000f9a0: 670a 2020 2020 2020 2020 7768 6574 6865  g.        whethe
-0000f9b0: 7220 7468 6520 6f75 7470 7574 206f 6620  r the output of 
-0000f9c0: 7468 6174 206d 6574 686f 6420 696e 766f  that method invo
-0000f9d0: 6361 7469 6f6e 2073 686f 756c 6420 6265  cation should be
-0000f9e0: 2073 746f 7265 642e 0a20 2020 2020 202a   stored..      *
-0000f9f0: 2a6b 7761 7267 733a 204b 6579 776f 7264  *kwargs: Keyword
-0000fa00: 2061 7267 756d 656e 7473 2070 6173 7365   arguments passe
-0000fa10: 6420 746f 2074 6865 2069 6e69 7420 6675  d to the init fu
-0000fa20: 6e63 7469 6f6e 2e0a 2020 2020 5265 7475  nction..    Retu
-0000fa30: 726e 733a 0a20 2020 2020 2054 6865 2069  rns:.      The i
-0000fa40: 6e69 7469 616c 697a 6564 2076 6172 6961  nitialized varia
-0000fa50: 626c 6520 6469 6374 2e0a 2020 2020 2222  ble dict..    ""
-0000fa60: 220a 2020 2020 4d6f 6475 6c65 2e5f 6d6f  ".    Module._mo
-0000fa70: 6475 6c65 5f63 6865 636b 7328 7365 6c66  dule_checks(self
-0000fa80: 290a 0a20 2020 205f 2c20 765f 6f75 7420  )..    _, v_out 
-0000fa90: 3d20 7365 6c66 2e69 6e69 745f 7769 7468  = self.init_with
-0000faa0: 5f6f 7574 7075 7428 0a20 2020 2020 2020  _output(.       
-0000fab0: 2072 6e67 732c 0a20 2020 2020 2020 202a   rngs,.        *
-0000fac0: 6172 6773 2c0a 2020 2020 2020 2020 6d65  args,.        me
-0000fad0: 7468 6f64 3d6d 6574 686f 642c 0a20 2020  thod=method,.   
-0000fae0: 2020 2020 206d 7574 6162 6c65 3d6d 7574       mutable=mut
-0000faf0: 6162 6c65 2c0a 2020 2020 2020 2020 6361  able,.        ca
-0000fb00: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-0000fb10: 7465 733d 6361 7074 7572 655f 696e 7465  tes=capture_inte
-0000fb20: 726d 6564 6961 7465 732c 0a20 2020 2020  rmediates,.     
-0000fb30: 2020 202a 2a6b 7761 7267 7329 0a20 2020     **kwargs).   
-0000fb40: 2072 6574 7572 6e20 765f 6f75 740a 0a20   return v_out.. 
-0000fb50: 2040 7472 6163 6562 6163 6b5f 7574 696c   @traceback_util
-0000fb60: 2e61 7069 5f62 6f75 6e64 6172 790a 2020  .api_boundary.  
-0000fb70: 6465 6620 6c61 7a79 5f69 6e69 7428 7365  def lazy_init(se
-0000fb80: 6c66 2c0a 2020 2020 2020 2020 2020 2072  lf,.           r
-0000fb90: 6e67 733a 2055 6e69 6f6e 5b4b 6579 4172  ngs: Union[KeyAr
-0000fba0: 7261 792c 2052 4e47 5365 7175 656e 6365  ray, RNGSequence
-0000fbb0: 735d 2c0a 2020 2020 2020 2020 2020 202a  s],.           *
-0000fbc0: 6172 6773 2c0a 2020 2020 2020 2020 2020  args,.          
-0000fbd0: 206d 6574 686f 643a 204f 7074 696f 6e61   method: Optiona
-0000fbe0: 6c5b 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  l[Callable[..., 
-0000fbf0: 416e 795d 5d20 3d20 4e6f 6e65 2c0a 2020  Any]] = None,.  
-0000fc00: 2020 2020 2020 2020 206d 7574 6162 6c65           mutable
-0000fc10: 3a20 436f 6c6c 6563 7469 6f6e 4669 6c74  : CollectionFilt
-0000fc20: 6572 203d 2044 656e 794c 6973 7428 2769  er = DenyList('i
-0000fc30: 6e74 6572 6d65 6469 6174 6573 2729 2c0a  ntermediates'),.
-0000fc40: 2020 2020 2020 2020 2020 202a 2a6b 7761             **kwa
-0000fc50: 7267 7329 202d 3e20 4672 6f7a 656e 5661  rgs) -> FrozenVa
-0000fc60: 7269 6162 6c65 4469 6374 3a0a 2020 2020  riableDict:.    
-0000fc70: 2222 2249 6e69 7469 616c 697a 6573 2061  """Initializes a
-0000fc80: 206d 6f64 756c 6520 7769 7468 6f75 7420   module without 
-0000fc90: 636f 6d70 7574 696e 6720 6f6e 2061 6e20  computing on an 
-0000fca0: 6163 7475 616c 2069 6e70 7574 2e0a 0a20  actual input... 
-0000fcb0: 2020 206c 617a 795f 696e 6974 2077 696c     lazy_init wil
-0000fcc0: 6c20 696e 6974 6961 6c69 7a65 2074 6865  l initialize the
-0000fcd0: 2076 6172 6961 626c 6573 2077 6974 686f   variables witho
-0000fce0: 7574 2064 6f69 6e67 2075 6e6e 6563 6573  ut doing unneces
-0000fcf0: 7361 7279 2063 6f6d 7075 7465 2e0a 2020  sary compute..  
-0000fd00: 2020 5468 6520 696e 7075 7420 6461 7461    The input data
-0000fd10: 2073 686f 756c 6420 6265 2070 6173 7365   should be passe
-0000fd20: 6420 6173 2061 2060 606a 6178 2e53 6861  d as a ``jax.Sha
-0000fd30: 7065 4474 7970 6553 7472 7563 7460 6020  peDtypeStruct`` 
-0000fd40: 7768 6963 6820 7370 6563 6966 6965 730a  which specifies.
-0000fd50: 2020 2020 7468 6520 7368 6170 6520 616e      the shape an
-0000fd60: 6420 6474 7970 6520 6f66 2074 6865 2069  d dtype of the i
-0000fd70: 6e70 7574 2062 7574 206e 6f20 636f 6e63  nput but no conc
-0000fd80: 7265 7465 2064 6174 612e 0a0a 2020 2020  rete data...    
-0000fd90: 4578 616d 706c 653a 3a0a 0a20 2020 2020  Example::..     
-0000fda0: 206d 6f64 656c 203d 206e 6e2e 4465 6e73   model = nn.Dens
-0000fdb0: 6528 6665 6174 7572 6573 3d32 3536 290a  e(features=256).
-0000fdc0: 2020 2020 2020 7661 7269 6162 6c65 7320        variables 
-0000fdd0: 3d20 6d6f 6465 6c2e 6c61 7a79 5f69 6e69  = model.lazy_ini
-0000fde0: 7428 726e 672c 206a 6178 2e53 6861 7065  t(rng, jax.Shape
-0000fdf0: 4474 7970 6553 7472 7563 7428 2831 2c20  DtypeStruct((1, 
-0000fe00: 3132 3829 2c20 6a6e 702e 666c 6f61 7433  128), jnp.float3
-0000fe10: 3229 290a 0a20 2020 2054 6865 2061 7267  2))..    The arg
-0000fe20: 7320 616e 6420 6b77 6172 6773 2061 7267  s and kwargs arg
-0000fe30: 7320 7061 7373 6564 2074 6f20 6060 6c61  s passed to ``la
-0000fe40: 7a79 5f69 6e69 7460 6020 6361 6e20 6265  zy_init`` can be
-0000fe50: 2061 206d 6978 206f 660a 2020 2020 636f   a mix of.    co
-0000fe60: 6e63 7265 7465 2028 6a61 7820 6172 7261  ncrete (jax arra
-0000fe70: 7973 2c20 7363 616c 6172 732c 2062 6f6f  ys, scalars, boo
-0000fe80: 6c73 2920 616e 6420 6162 7374 7261 6374  ls) and abstract
-0000fe90: 2028 5368 6170 6544 7479 7065 5374 7275   (ShapeDtypeStru
-0000fea0: 6374 2920 7661 6c75 6573 2e0a 2020 2020  ct) values..    
-0000feb0: 436f 6e63 7265 7465 2076 616c 7565 7320  Concrete values 
-0000fec0: 6172 6520 6f6e 6c79 206e 6563 6573 7361  are only necessa
-0000fed0: 7279 2066 6f72 2061 7267 756d 656e 7473  ry for arguments
-0000fee0: 2074 6861 7420 6166 6665 6374 0a20 2020   that affect.   
-0000fef0: 2074 6865 2069 6e69 7469 616c 697a 6174   the initializat
-0000ff00: 696f 6e20 6f66 2076 6172 6961 626c 6573  ion of variables
-0000ff10: 2e20 466f 7220 6578 616d 706c 652c 2074  . For example, t
-0000ff20: 6865 206d 6f64 656c 206d 6967 6874 2065  he model might e
-0000ff30: 7870 6563 740a 2020 2020 6120 6b65 7977  xpect.    a keyw
-0000ff40: 6f72 6420 6172 6720 7468 6174 2065 6e61  ord arg that ena
-0000ff50: 626c 6573 2f64 6973 6162 6c65 7320 6120  bles/disables a 
-0000ff60: 7375 6270 6172 7420 6f66 2074 6865 206d  subpart of the m
-0000ff70: 6f64 656c 2e0a 2020 2020 496e 2074 6869  odel..    In thi
-0000ff80: 7320 6361 7365 2c20 616e 2065 7870 6c69  s case, an expli
-0000ff90: 6369 7420 7661 6c75 6520 2854 7275 652f  cit value (True/
-0000ffa0: 466c 6173 6529 2073 686f 756c 6420 6265  Flase) should be
-0000ffb0: 2070 6173 7365 6420 6f74 6865 7277 6973   passed otherwis
-0000ffc0: 650a 2020 2020 6060 6c61 7a79 5f69 6e69  e.    ``lazy_ini
-0000ffd0: 7460 6020 6361 6e6e 6f74 2069 6e66 6572  t`` cannot infer
-0000ffe0: 2077 6869 6368 2076 6172 6961 626c 6573   which variables
-0000fff0: 2073 686f 756c 6420 6265 2069 6e69 7469   should be initi
-00010000: 616c 697a 6564 2e0a 0a20 2020 2041 7267  alized...    Arg
-00010010: 733a 0a20 2020 2020 2072 6e67 733a 2054  s:.      rngs: T
-00010020: 6865 2072 6e67 7320 666f 7220 7468 6520  he rngs for the 
-00010030: 7661 7269 6162 6c65 2063 6f6c 6c65 6374  variable collect
-00010040: 696f 6e73 2e0a 2020 2020 2020 2a61 7267  ions..      *arg
-00010050: 733a 2061 7267 756d 656e 7473 2070 6173  s: arguments pas
-00010060: 7365 6420 746f 2074 6865 2069 6e69 7420  sed to the init 
-00010070: 6675 6e63 7469 6f6e 2e0a 2020 2020 2020  function..      
-00010080: 6d65 7468 6f64 3a20 416e 206f 7074 696f  method: An optio
-00010090: 6e61 6c20 6d65 7468 6f64 2e20 4966 2070  nal method. If p
-000100a0: 726f 7669 6465 642c 2061 7070 6c69 6573  rovided, applies
-000100b0: 2074 6869 7320 6d65 7468 6f64 2e20 4966   this method. If
-000100c0: 206e 6f74 0a20 2020 2020 2020 2070 726f   not.        pro
-000100d0: 7669 6465 642c 2061 7070 6c69 6573 2074  vided, applies t
-000100e0: 6865 2060 605f 5f63 616c 6c5f 5f60 6020  he ``__call__`` 
-000100f0: 6d65 7468 6f64 2e0a 2020 2020 2020 6d75  method..      mu
-00010100: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
-00010110: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
-00010120: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
-00010130: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
-00010140: 6f75 6c64 2062 650a 2020 2020 2020 2020  ould be.        
-00010150: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
-00010160: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
-00010170: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
-00010180: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
-00010190: 2020 2020 2020 6060 7374 7260 603a 2054        ``str``: T
-000101a0: 6865 206e 616d 6520 6f66 2061 2073 696e  he name of a sin
-000101b0: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
-000101c0: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
-000101d0: 3a20 410a 2020 2020 2020 2020 6c69 7374  : A.        list
-000101e0: 206f 6620 6e61 6d65 7320 6f66 206d 7574   of names of mut
-000101f0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
-00010200: 2e20 4279 2064 6566 6175 6c74 2061 6c6c  . By default all
-00010210: 2063 6f6c 6c65 6374 696f 6e73 0a20 2020   collections.   
-00010220: 2020 2020 2065 7863 6570 7420 2269 6e74       except "int
-00010230: 6572 6d65 6469 6174 6573 2220 6172 6520  ermediates" are 
-00010240: 6d75 7461 626c 652e 0a20 2020 2020 202a  mutable..      *
-00010250: 2a6b 7761 7267 733a 204b 6579 776f 7264  *kwargs: Keyword
-00010260: 2061 7267 756d 656e 7473 2070 6173 7365   arguments passe
-00010270: 6420 746f 2074 6865 2069 6e69 7420 6675  d to the init fu
-00010280: 6e63 7469 6f6e 2e0a 2020 2020 5265 7475  nction..    Retu
-00010290: 726e 733a 0a20 2020 2020 2054 6865 2069  rns:.      The i
-000102a0: 6e69 7469 616c 697a 6564 2076 6172 6961  nitialized varia
-000102b0: 626c 6520 6469 6374 2e0a 2020 2020 2222  ble dict..    ""
-000102c0: 220a 2020 2020 4d6f 6475 6c65 2e5f 6d6f  ".    Module._mo
-000102d0: 6475 6c65 5f63 6865 636b 7328 7365 6c66  dule_checks(self
-000102e0: 290a 2020 2020 6465 6620 6c61 7a79 5f77  ).    def lazy_w
-000102f0: 7261 7070 6572 2872 6e67 732c 202a 6172  rapper(rngs, *ar
-00010300: 6773 2c20 2a2a 6b77 6172 6773 293a 0a20  gs, **kwargs):. 
-00010310: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-00010320: 2e69 6e69 7428 726e 6773 2c20 2a61 7267  .init(rngs, *arg
-00010330: 732c 206d 6574 686f 643d 6d65 7468 6f64  s, method=method
-00010340: 2c20 6d75 7461 626c 653d 6d75 7461 626c  , mutable=mutabl
-00010350: 652c 202a 2a6b 7761 7267 7329 0a20 2020  e, **kwargs).   
-00010360: 2072 6574 7572 6e20 7061 7274 6961 6c5f   return partial_
-00010370: 6576 616c 2e6c 617a 795f 696e 6974 286c  eval.lazy_init(l
-00010380: 617a 795f 7772 6170 7065 7229 2872 6e67  azy_wrapper)(rng
-00010390: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
-000103a0: 6773 290a 0a20 2040 7072 6f70 6572 7479  gs)..  @property
-000103b0: 0a20 2064 6566 2076 6172 6961 626c 6573  .  def variables
-000103c0: 2873 656c 6629 202d 3e20 5661 7269 6162  (self) -> Variab
-000103d0: 6c65 4469 6374 3a0a 2020 2020 2222 2252  leDict:.    """R
-000103e0: 6574 7572 6e73 2074 6865 2076 6172 6961  eturns the varia
-000103f0: 626c 6573 2069 6e20 7468 6973 206d 6f64  bles in this mod
-00010400: 756c 652e 2222 220a 2020 2020 6966 2073  ule.""".    if s
-00010410: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
-00010420: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
-00010430: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
-00010440: 2061 6363 6573 7320 7661 7269 6162 6c65   access variable
-00010450: 7320 6f6e 2075 6e62 6f75 6e64 206d 6f64  s on unbound mod
-00010460: 756c 6573 2229 0a20 2020 2072 6574 7572  ules").    retur
-00010470: 6e20 7365 6c66 2e73 636f 7065 2e76 6172  n self.scope.var
-00010480: 6961 626c 6573 2829 0a0a 2020 6465 6620  iables()..  def 
-00010490: 6765 745f 7661 7269 6162 6c65 2873 656c  get_variable(sel
-000104a0: 662c 2063 6f6c 3a20 7374 722c 206e 616d  f, col: str, nam
-000104b0: 653a 2073 7472 2c20 6465 6661 756c 743a  e: str, default:
-000104c0: 204f 7074 696f 6e61 6c5b 545d 203d 204e   Optional[T] = N
-000104d0: 6f6e 6529 202d 3e20 543a 0a20 2020 2022  one) -> T:.    "
-000104e0: 2222 5265 7472 6965 7665 7320 7468 6520  ""Retrieves the 
-000104f0: 7661 6c75 6520 6f66 2061 2056 6172 6961  value of a Varia
-00010500: 626c 652e 0a0a 2020 2020 4172 6773 3a0a  ble...    Args:.
-00010510: 2020 2020 2020 636f 6c3a 2074 6865 2076        col: the v
-00010520: 6172 6961 626c 6520 636f 6c6c 6563 7469  ariable collecti
-00010530: 6f6e 2e0a 2020 2020 2020 6e61 6d65 3a20  on..      name: 
-00010540: 7468 6520 6e61 6d65 206f 6620 7468 6520  the name of the 
-00010550: 7661 7269 6162 6c65 2e0a 2020 2020 2020  variable..      
-00010560: 6465 6661 756c 743a 2074 6865 2064 6566  default: the def
-00010570: 6175 6c74 2076 616c 7565 2074 6f20 7265  ault value to re
-00010580: 7475 726e 2069 6620 7468 6520 7661 7269  turn if the vari
-00010590: 6162 6c65 2064 6f65 7320 6e6f 7420 6578  able does not ex
-000105a0: 6973 7420 696e 0a20 2020 2020 2020 2074  ist in.        t
-000105b0: 6869 7320 7363 6f70 652e 0a0a 2020 2020  his scope...    
-000105c0: 5265 7475 726e 733a 0a20 2020 2020 2054  Returns:.      T
-000105d0: 6865 2076 616c 7565 206f 6620 7468 6520  he value of the 
-000105e0: 696e 7075 7420 7661 7269 6162 6c65 2c20  input variable, 
-000105f0: 6f66 2074 6865 2064 6566 6175 6c74 2076  of the default v
-00010600: 616c 7565 2069 6620 7468 6520 7661 7269  alue if the vari
-00010610: 6162 6c65 0a20 2020 2020 2064 6f65 736e  able.      doesn
-00010620: 2774 2065 7869 7374 2069 6e20 7468 6973  't exist in this
-00010630: 2073 636f 7065 2e0a 2020 2020 2222 220a   scope..    """.
-00010640: 2020 2020 6966 2073 656c 662e 7363 6f70      if self.scop
-00010650: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     
-00010660: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-00010670: 7228 2243 616e 2774 2061 6363 6573 7320  r("Can't access 
-00010680: 7661 7269 6162 6c65 7320 6f6e 2075 6e62  variables on unb
-00010690: 6f75 6e64 206d 6f64 756c 6573 2229 0a20  ound modules"). 
-000106a0: 2020 2072 6574 7572 6e20 7365 6c66 2e73     return self.s
-000106b0: 636f 7065 2e67 6574 5f76 6172 6961 626c  cope.get_variabl
-000106c0: 6528 636f 6c2c 206e 616d 652c 2064 6566  e(col, name, def
-000106d0: 6175 6c74 290a 0a20 2064 6566 2070 7574  ault)..  def put
-000106e0: 5f76 6172 6961 626c 6528 7365 6c66 2c20  _variable(self, 
-000106f0: 636f 6c3a 2073 7472 2c20 6e61 6d65 3a20  col: str, name: 
-00010700: 7374 722c 2076 616c 7565 3a20 416e 7929  str, value: Any)
-00010710: 3a0a 2020 2020 2222 2255 7064 6174 6573  :.    """Updates
-00010720: 2074 6865 2076 616c 7565 206f 6620 7468   the value of th
-00010730: 6520 6769 7665 6e20 7661 7269 6162 6c65  e given variable
-00010740: 2069 6620 6974 2069 7320 6d75 7461 626c   if it is mutabl
-00010750: 652c 206f 7220 616e 2065 7272 6f72 206f  e, or an error o
-00010760: 7468 6572 7769 7365 2e0a 0a20 2020 2041  therwise...    A
-00010770: 7267 733a 0a20 2020 2020 2063 6f6c 3a20  rgs:.      col: 
-00010780: 7468 6520 7661 7269 6162 6c65 2063 6f6c  the variable col
-00010790: 6c65 6374 696f 6e2e 0a20 2020 2020 206e  lection..      n
-000107a0: 616d 653a 2074 6865 206e 616d 6520 6f66  ame: the name of
-000107b0: 2074 6865 2076 6172 6961 626c 652e 0a20   the variable.. 
-000107c0: 2020 2020 2076 616c 7565 3a20 7468 6520       value: the 
-000107d0: 6e65 7720 7661 6c75 6520 6f66 2074 6865  new value of the
-000107e0: 2076 6172 6961 626c 652e 0a20 2020 2022   variable..    "
-000107f0: 2222 0a20 2020 2069 6620 7365 6c66 2e73  "".    if self.s
-00010800: 636f 7065 2069 7320 4e6f 6e65 3a0a 2020  cope is None:.  
-00010810: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00010820: 7272 6f72 2822 4361 6e27 7420 6163 6365  rror("Can't acce
-00010830: 7373 2076 6172 6961 626c 6573 206f 6e20  ss variables on 
-00010840: 756e 626f 756e 6420 6d6f 6475 6c65 7322  unbound modules"
-00010850: 290a 2020 2020 7365 6c66 2e73 636f 7065  ).    self.scope
-00010860: 2e70 7574 5f76 6172 6961 626c 6528 636f  .put_variable(co
-00010870: 6c2c 206e 616d 652c 2076 616c 7565 290a  l, name, value).
-00010880: 0a20 2040 6f76 6572 6c6f 6164 0a20 2064  .  @overload.  d
-00010890: 6566 2073 6f77 2873 656c 662c 2063 6f6c  ef sow(self, col
-000108a0: 3a20 7374 722c 206e 616d 653a 2073 7472  : str, name: str
-000108b0: 2c20 7661 6c75 653a 2041 6e79 2920 2d3e  , value: Any) ->
-000108c0: 2062 6f6f 6c3a 0a20 2020 202e 2e2e 0a0a   bool:.    .....
-000108d0: 2020 406f 7665 726c 6f61 640a 2020 6465    @overload.  de
-000108e0: 6620 736f 7728 7365 6c66 2c20 636f 6c3a  f sow(self, col:
-000108f0: 2073 7472 2c20 6e61 6d65 3a20 7374 722c   str, name: str,
-00010900: 2076 616c 7565 3a20 542c 0a20 2020 2020   value: T,.     
-00010910: 2020 2020 2072 6564 7563 655f 666e 3a20       reduce_fn: 
-00010920: 4361 6c6c 6162 6c65 5b5b 4b2c 2054 5d2c  Callable[[K, T],
-00010930: 204b 5d20 3d20 7475 706c 655f 7265 6475   K] = tuple_redu
-00010940: 6365 2c0a 2020 2020 2020 2020 2020 696e  ce,.          in
-00010950: 6974 5f66 6e3a 2043 616c 6c61 626c 655b  it_fn: Callable[
-00010960: 5b5d 2c20 4b5d 203d 2074 7570 6c65 5f69  [], K] = tuple_i
-00010970: 6e69 7429 202d 3e20 626f 6f6c 3a20 2320  nit) -> bool: # 
-00010980: 7479 7065 3a20 6967 6e6f 7265 0a20 2020  type: ignore.   
-00010990: 202e 2e2e 0a0a 2020 6465 6620 736f 7728   .....  def sow(
-000109a0: 7365 6c66 2c20 636f 6c3a 2073 7472 2c20  self, col: str, 
-000109b0: 6e61 6d65 3a20 7374 722c 2076 616c 7565  name: str, value
-000109c0: 3a20 542c 0a20 2020 2020 2020 2020 2072  : T,.          r
-000109d0: 6564 7563 655f 666e 3a20 4361 6c6c 6162  educe_fn: Callab
-000109e0: 6c65 5b5b 4b2c 2054 5d2c 204b 5d20 3d20  le[[K, T], K] = 
-000109f0: 7475 706c 655f 7265 6475 6365 2c0a 2020  tuple_reduce,.  
-00010a00: 2020 2020 2020 2020 696e 6974 5f66 6e3a          init_fn:
-00010a10: 2043 616c 6c61 626c 655b 5b5d 2c20 4b5d   Callable[[], K]
-00010a20: 203d 2074 7570 6c65 5f69 6e69 7429 202d   = tuple_init) -
-00010a30: 3e20 626f 6f6c 3a20 2320 7479 7065 3a20  > bool: # type: 
-00010a40: 6967 6e6f 7265 0a20 2020 2022 2222 5374  ignore.    """St
-00010a50: 6f72 6573 2061 2076 616c 7565 2069 6e20  ores a value in 
-00010a60: 6120 636f 6c6c 6563 7469 6f6e 2e0a 0a20  a collection... 
-00010a70: 2020 2043 6f6c 6c65 6374 696f 6e73 2063     Collections c
-00010a80: 616e 2062 6520 7573 6564 2074 6f20 636f  an be used to co
-00010a90: 6c6c 6563 7420 696e 7465 726d 6564 6961  llect intermedia
-00010aa0: 7465 2076 616c 7565 7320 7769 7468 6f75  te values withou
-00010ab0: 740a 2020 2020 7468 6520 6f76 6572 6865  t.    the overhe
-00010ac0: 6164 206f 6620 6578 706c 6963 6974 6c79  ad of explicitly
-00010ad0: 2070 6173 7369 6e67 2061 2063 6f6e 7461   passing a conta
-00010ae0: 696e 6572 2074 6872 6f75 6768 2065 6163  iner through eac
-00010af0: 6820 4d6f 6475 6c65 2063 616c 6c2e 0a0a  h Module call...
-00010b00: 2020 2020 4966 2074 6865 2074 6172 6765      If the targe
-00010b10: 7420 636f 6c6c 6563 7469 6f6e 2069 7320  t collection is 
-00010b20: 6e6f 7420 6d75 7461 626c 6520 6073 6f77  not mutable `sow
-00010b30: 6020 6265 6861 7665 7320 6c69 6b65 2061  ` behaves like a
-00010b40: 206e 6f2d 6f70 0a20 2020 2061 6e64 2072   no-op.    and r
-00010b50: 6574 7572 6e73 2060 4661 6c73 6560 2e0a  eturns `False`..
-00010b60: 0a20 2020 2045 7861 6d70 6c65 3a3a 0a0a  .    Example::..
-00010b70: 2020 2020 2020 696d 706f 7274 206a 6178        import jax
-00010b80: 0a20 2020 2020 2069 6d70 6f72 7420 6a61  .      import ja
-00010b90: 782e 6e75 6d70 7920 6173 206a 6e70 0a20  x.numpy as jnp. 
-00010ba0: 2020 2020 2069 6d70 6f72 7420 666c 6178       import flax
-00010bb0: 2e6c 696e 656e 2061 7320 6e6e 0a0a 2020  .linen as nn..  
-00010bc0: 2020 2020 636c 6173 7320 466f 6f28 6e6e      class Foo(nn
-00010bd0: 2e4d 6f64 756c 6529 3a0a 2020 2020 2020  .Module):.      
-00010be0: 2020 406e 6e2e 636f 6d70 6163 740a 2020    @nn.compact.  
-00010bf0: 2020 2020 2020 6465 6620 5f5f 6361 6c6c        def __call
-00010c00: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
-00010c10: 2020 2020 2020 2068 203d 206e 6e2e 4465         h = nn.De
-00010c20: 6e73 6528 3429 2878 290a 2020 2020 2020  nse(4)(x).      
-00010c30: 2020 2020 7365 6c66 2e73 6f77 2827 696e      self.sow('in
-00010c40: 7465 726d 6564 6961 7465 7327 2c20 2768  termediates', 'h
-00010c50: 272c 2068 290a 2020 2020 2020 2020 2020  ', h).          
-00010c60: 7265 7475 726e 206e 6e2e 4465 6e73 6528  return nn.Dense(
-00010c70: 3229 2868 290a 0a20 2020 2020 2078 203d  2)(h)..      x =
-00010c80: 206a 6e70 2e6f 6e65 7328 2831 362c 2039   jnp.ones((16, 9
-00010c90: 2929 0a20 2020 2020 206d 6f64 656c 203d  )).      model =
-00010ca0: 2046 6f6f 2829 0a20 2020 2020 2076 6172   Foo().      var
-00010cb0: 6961 626c 6573 203d 206d 6f64 656c 2e69  iables = model.i
-00010cc0: 6e69 7428 6a61 782e 7261 6e64 6f6d 2e50  nit(jax.random.P
-00010cd0: 524e 474b 6579 2830 292c 2078 290a 2020  RNGKey(0), x).  
-00010ce0: 2020 2020 792c 2073 7461 7465 203d 206d      y, state = m
-00010cf0: 6f64 656c 2e61 7070 6c79 2876 6172 6961  odel.apply(varia
-00010d00: 626c 6573 2c20 782c 206d 7574 6162 6c65  bles, x, mutable
-00010d10: 3d5b 2769 6e74 6572 6d65 6469 6174 6573  =['intermediates
-00010d20: 275d 290a 2020 2020 2020 7072 696e 7428  ']).      print(
-00010d30: 7374 6174 655b 2769 6e74 6572 6d65 6469  state['intermedi
-00010d40: 6174 6573 275d 2920 2023 207b 2768 273a  ates'])  # {'h':
-00010d50: 2028 2e2e 2e2c 297d 0a0a 2020 2020 4279   (...,)}..    By
-00010d60: 2064 6566 6175 6c74 2074 6865 2076 616c   default the val
-00010d70: 7565 7320 6172 6520 7374 6f72 6564 2069  ues are stored i
-00010d80: 6e20 6120 7475 706c 6520 616e 6420 6561  n a tuple and ea
-00010d90: 6368 2073 746f 7265 6420 7661 6c75 650a  ch stored value.
-00010da0: 2020 2020 6973 2061 7070 656e 6465 6420      is appended 
-00010db0: 6174 2074 6865 2065 6e64 2e20 5468 6973  at the end. This
-00010dc0: 2077 6179 2061 6c6c 2069 6e74 6572 6d65   way all interme
-00010dd0: 6469 6174 6573 2063 616e 2062 6520 7472  diates can be tr
-00010de0: 6163 6b65 6420 7768 656e 0a20 2020 2074  acked when.    t
-00010df0: 6865 2073 616d 6520 6d6f 6475 6c65 2069  he same module i
-00010e00: 7320 6361 6c6c 6564 206d 756c 7469 706c  s called multipl
-00010e10: 6520 7469 6d65 732e 2041 6c74 6572 6e61  e times. Alterna
-00010e20: 7469 7665 6c79 2c20 6120 6375 7374 6f6d  tively, a custom
-00010e30: 0a20 2020 2069 6e69 742f 7265 6475 6365  .    init/reduce
-00010e40: 2066 756e 6374 696f 6e20 6361 6e20 6265   function can be
-00010e50: 2070 6173 7365 643a 3a0a 0a20 2020 2020   passed::..     
-00010e60: 2063 6c61 7373 2046 6f6f 3228 6e6e 2e4d   class Foo2(nn.M
-00010e70: 6f64 756c 6529 3a0a 2020 2020 2020 2020  odule):.        
-00010e80: 406e 6e2e 636f 6d70 6163 740a 2020 2020  @nn.compact.    
-00010e90: 2020 2020 6465 6620 5f5f 6361 6c6c 5f5f      def __call__
-00010ea0: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
-00010eb0: 2020 2020 2069 6e69 745f 666e 203d 206c       init_fn = l
-00010ec0: 616d 6264 613a 2030 0a20 2020 2020 2020  ambda: 0.       
-00010ed0: 2020 2072 6564 7563 655f 666e 203d 206c     reduce_fn = l
-00010ee0: 616d 6264 6120 612c 2062 3a20 6120 2b20  ambda a, b: a + 
-00010ef0: 620a 2020 2020 2020 2020 2020 7365 6c66  b.          self
-00010f00: 2e73 6f77 2827 696e 7465 726d 6564 6961  .sow('intermedia
-00010f10: 7465 7327 2c20 2768 272c 2078 2c0a 2020  tes', 'h', x,.  
-00010f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010f30: 2069 6e69 745f 666e 3d69 6e69 745f 666e   init_fn=init_fn
-00010f40: 2c20 7265 6475 6365 5f66 6e3d 7265 6475  , reduce_fn=redu
-00010f50: 6365 5f66 6e29 0a20 2020 2020 2020 2020  ce_fn).         
-00010f60: 2073 656c 662e 736f 7728 2769 6e74 6572   self.sow('inter
-00010f70: 6d65 6469 6174 6573 272c 2027 6827 2c20  mediates', 'h', 
-00010f80: 7820 2a20 322c 0a20 2020 2020 2020 2020  x * 2,.         
-00010f90: 2020 2020 2020 2020 2020 696e 6974 5f66            init_f
-00010fa0: 6e3d 696e 6974 5f66 6e2c 2072 6564 7563  n=init_fn, reduc
-00010fb0: 655f 666e 3d72 6564 7563 655f 666e 290a  e_fn=reduce_fn).
-00010fc0: 2020 2020 2020 2020 2020 7265 7475 726e            return
-00010fd0: 2078 0a0a 2020 2020 2020 6d6f 6465 6c20   x..      model 
-00010fe0: 3d20 466f 6f32 2829 0a20 2020 2020 2076  = Foo2().      v
-00010ff0: 6172 6961 626c 6573 203d 206d 6f64 656c  ariables = model
-00011000: 2e69 6e69 7428 6a61 782e 7261 6e64 6f6d  .init(jax.random
-00011010: 2e50 524e 474b 6579 2830 292c 2078 290a  .PRNGKey(0), x).
-00011020: 2020 2020 2020 792c 2073 7461 7465 203d        y, state =
-00011030: 206d 6f64 656c 2e61 7070 6c79 2876 6172   model.apply(var
-00011040: 6961 626c 6573 2c20 6a6e 702e 6f6e 6573  iables, jnp.ones
-00011050: 2828 312c 2031 2929 2c20 6d75 7461 626c  ((1, 1)), mutabl
-00011060: 653d 5b27 696e 7465 726d 6564 6961 7465  e=['intermediate
-00011070: 7327 5d29 0a20 2020 2020 2070 7269 6e74  s']).      print
-00011080: 2873 7461 7465 5b27 696e 7465 726d 6564  (state['intermed
-00011090: 6961 7465 7327 5d29 2020 2320 3d3d 3e20  iates'])  # ==> 
-000110a0: 7b27 6827 3a20 5b5b 332e 5d5d 7d0a 0a20  {'h': [[3.]]}.. 
-000110b0: 2020 2041 7267 733a 0a20 2020 2020 2063     Args:.      c
-000110c0: 6f6c 3a20 5468 6520 6e61 6d65 206f 6620  ol: The name of 
-000110d0: 7468 6520 7661 7269 6162 6c65 2063 6f6c  the variable col
-000110e0: 6c65 6374 696f 6e2e 0a20 2020 2020 206e  lection..      n
-000110f0: 616d 653a 2054 6865 206e 616d 6520 6f66  ame: The name of
-00011100: 2074 6865 2076 6172 6961 626c 652e 0a20   the variable.. 
-00011110: 2020 2020 2076 616c 7565 3a20 5468 6520       value: The 
-00011120: 7661 6c75 6520 6f66 2074 6865 2076 6172  value of the var
-00011130: 6961 626c 652e 0a20 2020 2020 2072 6564  iable..      red
-00011140: 7563 655f 666e 3a20 5468 6520 6675 6e63  uce_fn: The func
-00011150: 7469 6f6e 2075 7365 6420 746f 2063 6f6d  tion used to com
-00011160: 6269 6e65 2074 6865 2065 7869 7374 696e  bine the existin
-00011170: 6720 7661 6c75 6520 7769 7468 0a20 2020  g value with.   
-00011180: 2020 2020 2074 6865 206e 6577 2076 616c       the new val
-00011190: 7565 2e20 5468 6520 6465 6661 756c 7420  ue. The default 
-000111a0: 6973 2074 6f20 6170 7065 6e64 2074 6865  is to append the
-000111b0: 2076 616c 7565 2074 6f20 6120 7475 706c   value to a tupl
-000111c0: 652e 0a20 2020 2020 2069 6e69 745f 666e  e..      init_fn
-000111d0: 3a20 466f 7220 7468 6520 6669 7273 7420  : For the first 
-000111e0: 7661 6c75 6520 7374 6f72 6564 2c20 6072  value stored, `r
-000111f0: 6564 7563 655f 666e 6020 7769 6c6c 2062  educe_fn` will b
-00011200: 6520 7061 7373 6564 0a20 2020 2020 2020  e passed.       
-00011210: 2074 6865 2072 6573 756c 7420 6f66 2060   the result of `
-00011220: 696e 6974 5f66 6e60 2074 6f67 6574 6865  init_fn` togethe
-00011230: 7220 7769 7468 2074 6865 2076 616c 7565  r with the value
-00011240: 2074 6f20 6265 2073 746f 7265 642e 0a20   to be stored.. 
-00011250: 2020 2020 2020 2054 6865 2064 6566 6175         The defau
-00011260: 6c74 2069 7320 616e 2065 6d70 7479 2074  lt is an empty t
-00011270: 7570 6c65 2e0a 0a20 2020 2052 6574 7572  uple...    Retur
-00011280: 6e73 3a0a 2020 2020 2020 6054 7275 6560  ns:.      `True`
-00011290: 2069 6620 7468 6520 7661 6c75 6520 6861   if the value ha
-000112a0: 7320 6265 656e 2073 746f 7265 6420 7375  s been stored su
-000112b0: 6363 6573 7366 756c 6c79 2c20 6046 616c  ccessfully, `Fal
-000112c0: 7365 6020 6f74 6865 7277 6973 652e 0a20  se` otherwise.. 
-000112d0: 2020 2022 2222 0a20 2020 2069 6620 7365     """.    if se
-000112e0: 6c66 2e73 636f 7065 2069 7320 4e6f 6e65  lf.scope is None
-000112f0: 3a0a 2020 2020 2020 7261 6973 6520 5661  :.      raise Va
-00011300: 6c75 6545 7272 6f72 2822 4361 6e27 7420  lueError("Can't 
-00011310: 7374 6f72 6520 7661 7269 6162 6c65 7320  store variables 
-00011320: 6f6e 2075 6e62 6f75 6e64 206d 6f64 756c  on unbound modul
-00011330: 6573 2229 0a20 2020 2069 6620 6e6f 7420  es").    if not 
-00011340: 7365 6c66 2e73 636f 7065 2e69 735f 6d75  self.scope.is_mu
-00011350: 7461 626c 655f 636f 6c6c 6563 7469 6f6e  table_collection
-00011360: 2863 6f6c 293a 0a20 2020 2020 2072 6574  (col):.      ret
-00011370: 7572 6e20 4661 6c73 650a 2020 2020 6966  urn False.    if
-00011380: 2073 656c 662e 7363 6f70 652e 6861 735f   self.scope.has_
-00011390: 7661 7269 6162 6c65 2863 6f6c 2c20 6e61  variable(col, na
-000113a0: 6d65 293a 0a20 2020 2020 2078 7320 3d20  me):.      xs = 
-000113b0: 7365 6c66 2e73 636f 7065 2e67 6574 5f76  self.scope.get_v
-000113c0: 6172 6961 626c 6528 636f 6c2c 206e 616d  ariable(col, nam
-000113d0: 6529 0a20 2020 2065 6c73 653a 0a20 2020  e).    else:.   
-000113e0: 2020 2073 656c 662e 7363 6f70 652e 7265     self.scope.re
-000113f0: 7365 7276 6528 6e61 6d65 2c20 636f 6c29  serve(name, col)
-00011400: 0a20 2020 2020 2073 656c 662e 5f73 7461  .      self._sta
-00011410: 7465 2e63 6869 6c64 7265 6e5b 6e61 6d65  te.children[name
-00011420: 5d20 3d20 636f 6c0a 2020 2020 2020 7873  ] = col.      xs
-00011430: 203d 2069 6e69 745f 666e 2829 0a20 2020   = init_fn().   
-00011440: 2078 7320 3d20 7265 6475 6365 5f66 6e28   xs = reduce_fn(
-00011450: 7873 2c20 7661 6c75 6529 0a20 2020 2073  xs, value).    s
-00011460: 656c 662e 7363 6f70 652e 7075 745f 7661  elf.scope.put_va
-00011470: 7269 6162 6c65 2863 6f6c 2c20 6e61 6d65  riable(col, name
-00011480: 2c20 7873 290a 2020 2020 7265 7475 726e  , xs).    return
-00011490: 2054 7275 650a 0a20 2064 6566 2070 6572   True..  def per
-000114a0: 7475 7262 2873 656c 662c 206e 616d 653a  turb(self, name:
-000114b0: 2073 7472 2c20 7661 6c75 653a 2054 2c20   str, value: T, 
-000114c0: 636f 6c6c 6563 7469 6f6e 3a20 7374 7220  collection: str 
-000114d0: 3d20 2770 6572 7475 7262 6174 696f 6e73  = 'perturbations
-000114e0: 2729 202d 3e20 543a 0a20 2020 2022 2222  ') -> T:.    """
-000114f0: 4164 6420 616e 207a 6572 6f2d 7661 6c75  Add an zero-valu
-00011500: 6520 7661 7269 6162 6c65 2028 2770 6572  e variable ('per
-00011510: 7475 7262 6174 696f 6e27 2920 746f 2074  turbation') to t
-00011520: 6865 2069 6e74 6572 6d65 6469 6174 6520  he intermediate 
-00011530: 7661 6c75 652e 0a0a 2020 2020 5468 6520  value...    The 
-00011540: 6772 6164 6965 6e74 206f 6620 6076 616c  gradient of `val
-00011550: 7565 6020 776f 756c 6420 6265 2074 6865  ue` would be the
-00011560: 2073 616d 6520 6173 2074 6865 2067 7261   same as the gra
-00011570: 6469 656e 7420 6f66 2074 6869 730a 2020  dient of this.  
-00011580: 2020 7065 7274 7572 6261 7469 6f6e 2076    perturbation v
-00011590: 6172 6961 626c 652e 2054 6865 7265 666f  ariable. Therefo
-000115a0: 7265 2c20 6966 2079 6f75 2064 6566 696e  re, if you defin
-000115b0: 6520 796f 7572 206c 6f73 7320 6675 6e63  e your loss func
-000115c0: 7469 6f6e 2077 6974 680a 2020 2020 626f  tion with.    bo
-000115d0: 7468 2070 6172 616d 7320 616e 6420 7065  th params and pe
-000115e0: 7274 7572 6261 7469 6f6e 7320 6173 2073  rturbations as s
-000115f0: 7461 6e64 616c 6f6e 6520 6172 6775 6d65  tandalone argume
-00011600: 6e74 732c 2079 6f75 2063 616e 2067 6574  nts, you can get
-00011610: 2074 6865 0a20 2020 2069 6e74 6572 6d65   the.    interme
-00011620: 6469 6174 6520 6772 6164 6965 6e74 7320  diate gradients 
-00011630: 6f66 2060 7661 6c75 6560 2062 7920 7275  of `value` by ru
-00011640: 6e6e 696e 6720 606a 6178 2e67 7261 6460  nning `jax.grad`
-00011650: 206f 6e20 7468 6520 7065 7274 7572 6261   on the perturba
-00011660: 7469 6f6e 0a20 2020 2061 7267 756d 656e  tion.    argumen
-00011670: 742e 0a0a 2020 2020 4e6f 7465 3a20 7468  t...    Note: th
-00011680: 6973 2069 7320 616e 2065 7870 6572 696d  is is an experim
-00011690: 656e 7461 6c20 4150 4920 616e 6420 6d61  ental API and ma
-000116a0: 7920 6265 2074 7765 616b 6564 206c 6174  y be tweaked lat
-000116b0: 6572 2066 6f72 2062 6574 7465 720a 2020  er for better.  
-000116c0: 2020 7065 7266 6f72 6d61 6e63 6520 616e    performance an
-000116d0: 6420 7573 6162 696c 6974 792e 0a20 2020  d usability..   
-000116e0: 2041 7420 6974 7320 6375 7272 656e 7420   At its current 
-000116f0: 7374 6167 652c 2069 7420 6372 6561 7465  stage, it create
-00011700: 7320 6578 7472 6120 6475 6d6d 7920 7661  s extra dummy va
-00011710: 7269 6162 6c65 7320 7468 6174 206f 6363  riables that occ
-00011720: 7570 6965 7320 6578 7472 610a 2020 2020  upies extra.    
-00011730: 6d65 6d6f 7279 2073 7061 6365 2e20 5573  memory space. Us
-00011740: 6520 6974 206f 6e6c 7920 746f 2064 6562  e it only to deb
-00011750: 7567 2067 7261 6469 656e 7473 2069 6e20  ug gradients in 
-00011760: 7472 6169 6e69 6e67 2e0a 0a20 2020 2045  training...    E
-00011770: 7861 6d70 6c65 3a3a 0a0a 2020 2020 2020  xample::..      
-00011780: 696d 706f 7274 206a 6178 0a20 2020 2020  import jax.     
-00011790: 2069 6d70 6f72 7420 6a61 782e 6e75 6d70   import jax.nump
-000117a0: 7920 6173 206a 6e70 0a20 2020 2020 2069  y as jnp.      i
-000117b0: 6d70 6f72 7420 666c 6178 2e6c 696e 656e  mport flax.linen
-000117c0: 2061 7320 6e6e 0a0a 2020 2020 2020 636c   as nn..      cl
-000117d0: 6173 7320 466f 6f28 6e6e 2e4d 6f64 756c  ass Foo(nn.Modul
-000117e0: 6529 3a0a 2020 2020 2020 2020 2020 406e  e):.          @n
-000117f0: 6e2e 636f 6d70 6163 740a 2020 2020 2020  n.compact.      
-00011800: 2020 2020 6465 6620 5f5f 6361 6c6c 5f5f      def __call__
-00011810: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
-00011820: 2020 2020 2020 2020 2078 203d 206e 6e2e           x = nn.
-00011830: 4465 6e73 6528 3329 2878 290a 2020 2020  Dense(3)(x).    
-00011840: 2020 2020 2020 2020 2020 7820 3d20 7365            x = se
-00011850: 6c66 2e70 6572 7475 7262 2827 6465 6e73  lf.perturb('dens
-00011860: 6533 272c 2078 290a 2020 2020 2020 2020  e3', x).        
-00011870: 2020 2020 2020 7265 7475 726e 206e 6e2e        return nn.
-00011880: 4465 6e73 6528 3229 2878 290a 0a20 2020  Dense(2)(x)..   
-00011890: 2020 2064 6566 206c 6f73 7328 7061 7261     def loss(para
-000118a0: 6d73 2c20 7065 7274 7572 6261 7469 6f6e  ms, perturbation
-000118b0: 732c 2069 6e70 7574 732c 2074 6172 6765  s, inputs, targe
-000118c0: 7473 293a 0a20 2020 2020 2020 2076 6172  ts):.        var
-000118d0: 6961 626c 6573 203d 207b 2770 6172 616d  iables = {'param
-000118e0: 7327 3a20 7061 7261 6d73 2c20 2770 6572  s': params, 'per
-000118f0: 7475 7262 6174 696f 6e73 273a 2070 6572  turbations': per
-00011900: 7475 7262 6174 696f 6e73 7d0a 2020 2020  turbations}.    
-00011910: 2020 2020 7072 6564 7320 3d20 6d6f 6465      preds = mode
-00011920: 6c2e 6170 706c 7928 7661 7269 6162 6c65  l.apply(variable
-00011930: 732c 2069 6e70 7574 7329 0a20 2020 2020  s, inputs).     
-00011940: 2020 2072 6574 7572 6e20 6a6e 702e 7371     return jnp.sq
-00011950: 7561 7265 2870 7265 6473 202d 2074 6172  uare(preds - tar
-00011960: 6765 7473 292e 6d65 616e 2829 0a0a 2020  gets).mean()..  
-00011970: 2020 2020 7820 3d20 6a6e 702e 6f6e 6573      x = jnp.ones
-00011980: 2828 322c 2039 2929 0a20 2020 2020 2079  ((2, 9)).      y
-00011990: 203d 206a 6e70 2e6f 6e65 7328 2832 2c20   = jnp.ones((2, 
-000119a0: 3229 290a 2020 2020 2020 6d6f 6465 6c20  2)).      model 
-000119b0: 3d20 466f 6f28 290a 2020 2020 2020 7661  = Foo().      va
-000119c0: 7269 6162 6c65 7320 3d20 6d6f 6465 6c2e  riables = model.
-000119d0: 696e 6974 286a 6178 2e72 616e 646f 6d2e  init(jax.random.
-000119e0: 5052 4e47 4b65 7928 3029 2c20 7829 0a20  PRNGKey(0), x). 
-000119f0: 2020 2020 2069 6e74 6d5f 6772 6164 7320       intm_grads 
-00011a00: 3d20 6a61 782e 6772 6164 286c 6f73 732c  = jax.grad(loss,
-00011a10: 2061 7267 6e75 6d73 3d31 2928 7661 7269   argnums=1)(vari
-00011a20: 6162 6c65 735b 2770 6172 616d 7327 5d2c  ables['params'],
-00011a30: 2076 6172 6961 626c 6573 5b27 7065 7274   variables['pert
-00011a40: 7572 6261 7469 6f6e 7327 5d2c 2078 2c20  urbations'], x, 
-00011a50: 7929 0a20 2020 2020 2070 7269 6e74 2869  y).      print(i
-00011a60: 6e74 6d5f 6772 6164 735b 2764 656e 7365  ntm_grads['dense
-00011a70: 3327 5d29 2023 203d 3d3e 205b 5b2d 312e  3']) # ==> [[-1.
-00011a80: 3435 3639 3234 2020 202d 302e 3434 3333  456924   -0.4433
-00011a90: 3235 3337 2020 302e 3032 3432 3238 3437  2537  0.02422847
-00011aa0: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
-00011ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011ac0: 2020 2020 2320 2020 2020 205b 2d31 2e34      #      [-1.4
-00011ad0: 3536 3932 3420 2020 2d30 2e34 3433 3332  56924   -0.44332
-00011ae0: 3533 3720 2030 2e30 3234 3232 3834 375d  537  0.02422847]
-00011af0: 5d0a 0a20 2020 2049 6620 7065 7274 7572  ]..    If pertur
-00011b00: 6261 7469 6f6e 7320 6172 6520 6e6f 7420  bations are not 
-00011b10: 7061 7373 6564 2074 6f20 6061 7070 6c79  passed to `apply
-00011b20: 602c 2060 7065 7274 7572 6260 2062 6568  `, `perturb` beh
-00011b30: 6176 6573 206c 696b 6520 6120 6e6f 2d6f  aves like a no-o
-00011b40: 700a 2020 2020 736f 2079 6f75 2063 616e  p.    so you can
-00011b50: 2065 6173 696c 7920 6469 7361 626c 6520   easily disable 
-00011b60: 7468 6520 6265 6861 7669 6f72 2077 6865  the behavior whe
-00011b70: 6e20 6e6f 7420 6e65 6564 6564 3a3a 0a0a  n not needed::..
-00011b80: 2020 2020 2020 6d6f 6465 6c2e 6170 706c        model.appl
-00011b90: 7928 7b27 7061 7261 6d73 273a 2070 6172  y({'params': par
-00011ba0: 616d 732c 2027 7065 7274 7572 6261 7469  ams, 'perturbati
-00011bb0: 6f6e 7327 3a20 7065 7274 7572 6261 7469  ons': perturbati
-00011bc0: 6f6e 737d 2c20 7829 2023 2077 6f72 6b73  ons}, x) # works
-00011bd0: 2061 7320 6578 7065 6374 6564 0a20 2020   as expected.   
-00011be0: 2020 206d 6f64 656c 2e61 7070 6c79 287b     model.apply({
-00011bf0: 2770 6172 616d 7327 3a20 7061 7261 6d73  'params': params
-00011c00: 7d2c 2078 2920 2320 6265 6861 7665 7320  }, x) # behaves 
-00011c10: 6c69 6b65 2061 206e 6f2d 6f70 0a0a 2020  like a no-op..  
-00011c20: 2020 2222 220a 2020 2020 6465 6620 5f72    """.    def _r
-00011c30: 6f6f 745f 6861 735f 636f 6c6c 6563 7469  oot_has_collecti
-00011c40: 6f6e 2829 3a0a 2020 2020 2020 2222 2252  on():.      """R
-00011c50: 6574 7572 6e73 2054 7275 6520 6966 2074  eturns True if t
-00011c60: 6865 2072 6f6f 7420 7363 6f70 6520 6861  he root scope ha
-00011c70: 7320 7468 6520 636f 6c6c 6563 7469 6f6e  s the collection
-00011c80: 2e22 2222 0a20 2020 2020 2061 7373 6572  .""".      asser
-00011c90: 7420 7365 6c66 2e73 636f 7065 2069 7320  t self.scope is 
-00011ca0: 6e6f 7420 4e6f 6e65 0a20 2020 2020 2072  not None.      r
-00011cb0: 6574 7572 6e20 636f 6c6c 6563 7469 6f6e  eturn collection
-00011cc0: 2069 6e20 7365 6c66 2e73 636f 7065 2e72   in self.scope.r
-00011cd0: 6f6f 742e 5f76 6172 6961 626c 6573 0a20  oot._variables. 
-00011ce0: 2020 2023 2077 6520 7769 6c6c 206f 6e6c     # we will onl
-00011cf0: 7920 6164 6420 7468 6520 7065 7274 7572  y add the pertur
-00011d00: 6261 7469 6f6e 2076 6172 6961 626c 6520  bation variable 
-00011d10: 6966 2074 6865 2063 6f6c 6c65 6374 696f  if the collectio
-00011d20: 6e20 6973 206d 7574 6162 6c65 0a20 2020  n is mutable.   
-00011d30: 2023 2028 652e 672e 2064 7572 696e 6720   # (e.g. during 
-00011d40: 6069 6e69 7460 2920 6f72 2069 6620 7468  `init`) or if th
-00011d50: 6520 636f 6c6c 6563 7469 6f6e 2077 6173  e collection was
-00011d60: 2070 6173 7365 6420 746f 2060 6170 706c   passed to `appl
-00011d70: 7960 2028 636f 6e74 6169 6e65 6420 696e  y` (contained in
-00011d80: 0a20 2020 2023 2074 6865 2072 6f6f 7420  .    # the root 
-00011d90: 7363 6f70 6529 2e0a 2020 2020 6966 2073  scope)..    if s
-00011da0: 656c 662e 6973 5f6d 7574 6162 6c65 5f63  elf.is_mutable_c
-00011db0: 6f6c 6c65 6374 696f 6e28 636f 6c6c 6563  ollection(collec
-00011dc0: 7469 6f6e 2920 6f72 205f 726f 6f74 5f68  tion) or _root_h
-00011dd0: 6173 5f63 6f6c 6c65 6374 696f 6e28 293a  as_collection():
-00011de0: 0a20 2020 2020 2076 616c 7565 202b 3d20  .      value += 
-00011df0: 7365 6c66 2e76 6172 6961 626c 6528 636f  self.variable(co
-00011e00: 6c6c 6563 7469 6f6e 2c20 6e61 6d65 2c20  llection, name, 
-00011e10: 6c61 6d62 6461 3a20 6a6e 702e 7a65 726f  lambda: jnp.zero
-00011e20: 735f 6c69 6b65 2876 616c 7565 2929 2e76  s_like(value)).v
-00011e30: 616c 7565 2023 2074 7970 653a 2069 676e  alue # type: ign
-00011e40: 6f72 650a 2020 2020 7265 7475 726e 2076  ore.    return v
-00011e50: 616c 7565 0a0a 2020 6465 6620 7461 6275  alue..  def tabu
-00011e60: 6c61 7465 280a 2020 2020 7365 6c66 2c0a  late(.    self,.
-00011e70: 2020 2020 726e 6773 3a20 556e 696f 6e5b      rngs: Union[
-00011e80: 4b65 7941 7272 6179 2c20 524e 4753 6571  KeyArray, RNGSeq
-00011e90: 7565 6e63 6573 5d2c 0a20 2020 202a 6172  uences],.    *ar
-00011ea0: 6773 2c0a 2020 2020 6465 7074 683a 204f  gs,.    depth: O
-00011eb0: 7074 696f 6e61 6c5b 696e 745d 203d 204e  ptional[int] = N
-00011ec0: 6f6e 652c 0a20 2020 2073 686f 775f 7265  one,.    show_re
-00011ed0: 7065 6174 6564 3a20 626f 6f6c 203d 2046  peated: bool = F
-00011ee0: 616c 7365 2c0a 2020 2020 6d75 7461 626c  alse,.    mutabl
-00011ef0: 653a 2043 6f6c 6c65 6374 696f 6e46 696c  e: CollectionFil
-00011f00: 7465 7220 3d20 5472 7565 2c0a 2020 2020  ter = True,.    
-00011f10: 636f 6e73 6f6c 655f 6b77 6172 6773 3a20  console_kwargs: 
-00011f20: 4f70 7469 6f6e 616c 5b4d 6170 7069 6e67  Optional[Mapping
-00011f30: 5b73 7472 2c20 416e 795d 5d20 3d20 4e6f  [str, Any]] = No
-00011f40: 6e65 2c0a 2020 2020 2a2a 6b77 6172 6773  ne,.    **kwargs
-00011f50: 2920 2d3e 2073 7472 3a0a 2020 2020 2222  ) -> str:.    ""
-00011f60: 2243 7265 6174 6573 2061 2073 756d 6d61  "Creates a summa
-00011f70: 7279 206f 6620 7468 6520 4d6f 6475 6c65  ry of the Module
-00011f80: 2072 6570 7265 7365 6e74 6564 2061 7320   represented as 
-00011f90: 6120 7461 626c 652e 0a0a 2020 2020 5468  a table...    Th
-00011fa0: 6973 206d 6574 686f 6420 6861 7320 7468  is method has th
-00011fb0: 6520 7361 6d65 2073 6967 6e61 7475 7265  e same signature
-00011fc0: 2061 6e64 2069 6e74 6572 6e61 6c6c 7920   and internally 
-00011fd0: 6361 6c6c 7320 604d 6f64 756c 652e 696e  calls `Module.in
-00011fe0: 6974 602c 0a20 2020 2062 7574 2069 6e73  it`,.    but ins
-00011ff0: 7465 6164 206f 6620 7265 7475 726e 696e  tead of returnin
-00012000: 6720 7468 6520 7661 7269 6162 6c65 732c  g the variables,
-00012010: 2069 7420 7265 7475 726e 7320 7468 6520   it returns the 
-00012020: 7374 7269 6e67 2073 756d 6d61 7269 7a69  string summarizi
-00012030: 6e67 0a20 2020 2074 6865 204d 6f64 756c  ng.    the Modul
-00012040: 6520 696e 2061 2074 6162 6c65 2e20 6074  e in a table. `t
-00012050: 6162 756c 6174 6560 2075 7365 7320 606a  abulate` uses `j
-00012060: 6178 2e65 7661 6c5f 7368 6170 6560 2074  ax.eval_shape` t
-00012070: 6f20 7275 6e20 7468 6520 666f 7277 6172  o run the forwar
-00012080: 640a 2020 2020 636f 6d70 7574 6174 696f  d.    computatio
-00012090: 6e20 7769 7468 6f75 7420 636f 6e73 756d  n without consum
-000120a0: 696e 6720 616e 7920 464c 4f50 7320 6f72  ing any FLOPs or
-000120b0: 2061 6c6c 6f63 6174 696e 6720 6d65 6d6f   allocating memo
-000120c0: 7279 2e0a 0a20 2020 2041 6464 6974 696f  ry...    Additio
-000120d0: 6e61 6c20 6172 6775 6d65 6e74 7320 6361  nal arguments ca
-000120e0: 6e20 6265 2070 6173 7365 6420 696e 746f  n be passed into
-000120f0: 2074 6865 2060 636f 6e73 6f6c 655f 6b77   the `console_kw
-00012100: 6172 6773 6020 6172 6775 6d65 6e74 2c20  args` argument, 
-00012110: 666f 7220 6578 616d 706c 652c 0a20 2020  for example,.   
-00012120: 2060 7b27 7769 6474 6827 3a20 3132 307d   `{'width': 120}
-00012130: 602e 2046 6f72 2061 2066 756c 6c20 6c69  `. For a full li
-00012140: 7374 206f 6620 6063 6f6e 736f 6c65 5f6b  st of `console_k
-00012150: 7761 7267 7360 2061 7267 756d 656e 7473  wargs` arguments
-00012160: 2c20 7365 653a 0a20 2020 2068 7474 7073  , see:.    https
-00012170: 3a2f 2f72 6963 682e 7265 6164 7468 6564  ://rich.readthed
-00012180: 6f63 732e 696f 2f65 6e2f 7374 6162 6c65  ocs.io/en/stable
-00012190: 2f72 6566 6572 656e 6365 2f63 6f6e 736f  /reference/conso
-000121a0: 6c65 2e68 746d 6c23 7269 6368 2e63 6f6e  le.html#rich.con
-000121b0: 736f 6c65 2e43 6f6e 736f 6c65 0a0a 2020  sole.Console..  
-000121c0: 2020 4578 616d 706c 653a 3a0a 0a20 2020    Example::..   
-000121d0: 2020 2069 6d70 6f72 7420 6a61 780a 2020     import jax.  
-000121e0: 2020 2020 696d 706f 7274 206a 6178 2e6e      import jax.n
-000121f0: 756d 7079 2061 7320 6a6e 700a 2020 2020  umpy as jnp.    
-00012200: 2020 696d 706f 7274 2066 6c61 782e 6c69    import flax.li
-00012210: 6e65 6e20 6173 206e 6e0a 0a20 2020 2020  nen as nn..     
-00012220: 2063 6c61 7373 2046 6f6f 286e 6e2e 4d6f   class Foo(nn.Mo
-00012230: 6475 6c65 293a 0a20 2020 2020 2020 2020  dule):.         
-00012240: 2040 6e6e 2e63 6f6d 7061 6374 0a20 2020   @nn.compact.   
-00012250: 2020 2020 2020 2064 6566 205f 5f63 616c         def __cal
-00012260: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
-00012270: 2020 2020 2020 2020 2020 2020 6820 3d20              h = 
-00012280: 6e6e 2e44 656e 7365 2834 2928 7829 0a20  nn.Dense(4)(x). 
-00012290: 2020 2020 2020 2020 2020 2020 2072 6574               ret
-000122a0: 7572 6e20 6e6e 2e44 656e 7365 2832 2928  urn nn.Dense(2)(
-000122b0: 6829 0a0a 2020 2020 2020 7820 3d20 6a6e  h)..      x = jn
-000122c0: 702e 6f6e 6573 2828 3136 2c20 3929 290a  p.ones((16, 9)).
-000122d0: 0a20 2020 2020 2070 7269 6e74 2846 6f6f  .      print(Foo
-000122e0: 2829 2e74 6162 756c 6174 6528 6a61 782e  ().tabulate(jax.
-000122f0: 7261 6e64 6f6d 2e50 524e 474b 6579 2830  random.PRNGKey(0
-00012300: 292c 2078 2929 0a0a 0a20 2020 2054 6869  ), x))...    Thi
-00012310: 7320 6769 7665 7320 7468 6520 666f 6c6c  s gives the foll
-00012320: 6f77 696e 6720 6f75 7470 7574 3a3a 0a0a  owing output::..
-00012330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012350: 2020 2020 2020 466f 6f20 5375 6d6d 6172        Foo Summar
-00012360: 790a 2020 2020 2020 e294 8fe2 9481 e294  y.      ........
-00012370: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00012380: e294 81e2 9481 e294 b3e2 9481 e294 81e2  ................
-00012390: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-000123a0: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
-000123b0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-000123c0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-000123d0: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
-000123e0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-000123f0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00012400: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
-00012410: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00012420: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00012430: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00012440: e294 81e2 9481 e294 930a 2020 2020 2020  ..........      
-00012450: e294 8320 7061 7468 2020 2020 e294 8320  ... path    ... 
-00012460: 6d6f 6475 6c65 20e2 9483 2069 6e70 7574  module ... input
-00012470: 7320 2020 2020 2020 20e2 9483 206f 7574  s        ... out
-00012480: 7075 7473 2020 2020 2020 20e2 9483 2070  puts       ... p
-00012490: 6172 616d 7320 2020 2020 2020 2020 2020  arams           
-000124a0: 2020 2020 e294 830a 2020 2020 2020 e294      ....      ..
-000124b0: a1e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-000124c0: e294 81e2 9481 e294 81e2 9481 e295 87e2  ................
-000124d0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-000124e0: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
-000124f0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00012500: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00012510: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
-00012520: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00012530: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00012540: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
-00012550: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00012560: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00012570: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00012580: e294 81e2 9481 e294 81e2 9481 e294 a90a  ................
-00012590: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-000125a0: 2020 e294 8220 466f 6f20 2020 20e2 9482    ... Foo    ...
-000125b0: 2066 6c6f 6174 3332 5b31 362c 395d 20e2   float32[16,9] .
-000125c0: 9482 2066 6c6f 6174 3332 5b31 362c 325d  .. float32[16,2]
-000125d0: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
-000125e0: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
-000125f0: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
-00012600: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012610: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
-00012620: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012630: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012640: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012650: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012660: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012670: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012680: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012690: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000126a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000126b0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000126c0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000126d0: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
-000126e0: 4465 6e73 655f 3020 e294 8220 4465 6e73  Dense_0 ... Dens
-000126f0: 6520 20e2 9482 2066 6c6f 6174 3332 5b31  e  ... float32[1
-00012700: 362c 395d 20e2 9482 2066 6c6f 6174 3332  6,9] ... float32
-00012710: 5b31 362c 345d 20e2 9482 2062 6961 733a  [16,4] ... bias:
-00012720: 2066 6c6f 6174 3332 5b34 5d20 2020 2020   float32[4]     
-00012730: e294 820a 2020 2020 2020 e294 8220 2020  ....      ...   
-00012740: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00012750: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
-00012760: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-00012770: 2020 2020 20e2 9482 206b 6572 6e65 6c3a       ... kernel:
-00012780: 2066 6c6f 6174 3332 5b39 2c34 5d20 e294   float32[9,4] ..
-00012790: 820a 2020 2020 2020 e294 8220 2020 2020  ..      ...     
-000127a0: 2020 2020 e294 8220 2020 2020 2020 20e2      ...        .
-000127b0: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
-000127c0: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
-000127d0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-000127e0: 2020 2020 2020 2020 2020 2020 e294 820a              ....
-000127f0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00012800: 2020 e294 8220 2020 2020 2020 20e2 9482    ...        ...
-00012810: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
-00012820: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
-00012830: 20e2 9482 2034 3020 2831 3630 2042 2920   ... 40 (160 B) 
-00012840: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
-00012850: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
-00012860: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012870: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
-00012880: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012890: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000128a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000128b0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-000128c0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000128d0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000128e0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-000128f0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012900: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012910: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012920: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012930: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
-00012940: 4465 6e73 655f 3120 e294 8220 4465 6e73  Dense_1 ... Dens
-00012950: 6520 20e2 9482 2066 6c6f 6174 3332 5b31  e  ... float32[1
-00012960: 362c 345d 20e2 9482 2066 6c6f 6174 3332  6,4] ... float32
-00012970: 5b31 362c 325d 20e2 9482 2062 6961 733a  [16,2] ... bias:
-00012980: 2066 6c6f 6174 3332 5b32 5d20 2020 2020   float32[2]     
-00012990: e294 820a 2020 2020 2020 e294 8220 2020  ....      ...   
-000129a0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-000129b0: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
-000129c0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-000129d0: 2020 2020 20e2 9482 206b 6572 6e65 6c3a       ... kernel:
-000129e0: 2066 6c6f 6174 3332 5b34 2c32 5d20 e294   float32[4,2] ..
-000129f0: 820a 2020 2020 2020 e294 8220 2020 2020  ..      ...     
-00012a00: 2020 2020 e294 8220 2020 2020 2020 20e2      ...        .
-00012a10: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+000002d0: 6672 6f6d 2074 7970 6573 2069 6d70 6f72  from types impor
+000002e0: 7420 4d61 7070 696e 6750 726f 7879 5479  t MappingProxyTy
+000002f0: 7065 0a69 6d70 6f72 7420 7479 7069 6e67  pe.import typing
+00000300: 0a66 726f 6d20 7479 7069 6e67 2069 6d70  .from typing imp
+00000310: 6f72 7420 280a 2020 2020 416e 792c 0a20  ort (.    Any,. 
+00000320: 2020 2043 616c 6c61 626c 652c 0a20 2020     Callable,.   
+00000330: 2044 6963 742c 0a20 2020 2049 7465 7261   Dict,.    Itera
+00000340: 626c 652c 0a20 2020 204c 6973 742c 0a20  ble,.    List,. 
+00000350: 2020 204d 6170 7069 6e67 2c0a 2020 2020     Mapping,.    
+00000360: 4e61 6d65 6454 7570 6c65 2c0a 2020 2020  NamedTuple,.    
+00000370: 4f70 7469 6f6e 616c 2c0a 2020 2020 5365  Optional,.    Se
+00000380: 7175 656e 6365 2c0a 2020 2020 5365 742c  quence,.    Set,
+00000390: 0a20 2020 2054 7570 6c65 2c0a 2020 2020  .    Tuple,.    
+000003a0: 5479 7065 2c0a 2020 2020 5479 7065 5661  Type,.    TypeVa
+000003b0: 722c 0a20 2020 2055 6e69 6f6e 2c0a 2020  r,.    Union,.  
+000003c0: 2020 6f76 6572 6c6f 6164 2c0a 290a 696d    overload,.).im
+000003d0: 706f 7274 2077 6561 6b72 6566 0a0a 696d  port weakref..im
+000003e0: 706f 7274 2066 6c61 780a 6672 6f6d 2066  port flax.from f
+000003f0: 6c61 7820 696d 706f 7274 2028 0a20 2020  lax import (.   
+00000400: 2063 6f6e 6669 672c 0a20 2020 2063 6f72   config,.    cor
+00000410: 652c 0a20 2020 2065 7272 6f72 732c 0a20  e,.    errors,. 
+00000420: 2020 2073 6572 6961 6c69 7a61 7469 6f6e     serialization
+00000430: 2c0a 2020 2020 7472 6163 6562 6163 6b5f  ,.    traceback_
+00000440: 7574 696c 2c0a 2020 2020 7472 6176 6572  util,.    traver
+00000450: 7365 5f75 7469 6c2c 0a29 0a66 726f 6d20  se_util,.).from 
+00000460: 666c 6178 2e63 6f72 6520 696d 706f 7274  flax.core import
+00000470: 2070 6172 7469 616c 5f65 7661 6c0a 6672   partial_eval.fr
+00000480: 6f6d 2066 6c61 782e 636f 7265 2069 6d70  om flax.core imp
+00000490: 6f72 7420 5363 6f70 650a 6672 6f6d 2066  ort Scope.from f
+000004a0: 6c61 782e 636f 7265 2e66 726f 7a65 6e5f  lax.core.frozen_
+000004b0: 6469 6374 2069 6d70 6f72 7420 4672 6f7a  dict import Froz
+000004c0: 656e 4469 6374 0a66 726f 6d20 666c 6178  enDict.from flax
+000004d0: 2e63 6f72 652e 7363 6f70 6520 696d 706f  .core.scope impo
+000004e0: 7274 2028 2020 2320 7079 6c69 6e74 3a20  rt (  # pylint: 
+000004f0: 6469 7361 626c 653d 672d 6d75 6c74 6970  disable=g-multip
+00000500: 6c65 2d69 6d70 6f72 740a 2020 2020 436f  le-import.    Co
+00000510: 6c6c 6563 7469 6f6e 4669 6c74 6572 2c0a  llectionFilter,.
+00000520: 2020 2020 4465 6e79 4c69 7374 2c0a 2020      DenyList,.  
+00000530: 2020 4672 6f7a 656e 5661 7269 6162 6c65    FrozenVariable
+00000540: 4469 6374 2c0a 2020 2020 5661 7269 6162  Dict,.    Variab
+00000550: 6c65 2c0a 2020 2020 5661 7269 6162 6c65  le,.    Variable
+00000560: 4469 6374 2c0a 2020 2020 756e 696f 6e5f  Dict,.    union_
+00000570: 6669 6c74 6572 732c 0a29 0a66 726f 6d20  filters,.).from 
+00000580: 666c 6178 2e69 6473 2069 6d70 6f72 7420  flax.ids import 
+00000590: 466c 6178 4964 0a66 726f 6d20 666c 6178  FlaxId.from flax
+000005a0: 2e69 6473 2069 6d70 6f72 7420 7575 6964  .ids import uuid
+000005b0: 0a69 6d70 6f72 7420 666c 6178 2e6c 696e  .import flax.lin
+000005c0: 656e 2061 7320 6e6e 0a66 726f 6d20 666c  en as nn.from fl
+000005d0: 6178 2e6c 696e 656e 2069 6d70 6f72 7420  ax.linen import 
+000005e0: 6b77 5f6f 6e6c 795f 6461 7461 636c 6173  kw_only_dataclas
+000005f0: 7365 730a 696d 706f 7274 206a 6178 0a69  ses.import jax.i
+00000600: 6d70 6f72 7420 6a61 782e 6e75 6d70 7920  mport jax.numpy 
+00000610: 6173 206a 6e70 0a69 6d70 6f72 7420 6e75  as jnp.import nu
+00000620: 6d70 7920 6173 206e 700a 6672 6f6d 2074  mpy as np.from t
+00000630: 7970 696e 675f 6578 7465 6e73 696f 6e73  yping_extensions
+00000640: 2069 6d70 6f72 7420 5072 6f74 6f63 6f6c   import Protocol
+00000650: 2c20 6461 7461 636c 6173 735f 7472 616e  , dataclass_tran
+00000660: 7366 6f72 6d20 2023 2070 7974 7970 653a  sform  # pytype:
+00000670: 2064 6973 6162 6c65 3d6e 6f74 2d73 7570   disable=not-sup
+00000680: 706f 7274 6564 2d79 6574 0a0a 0a74 7261  ported-yet...tra
+00000690: 6365 6261 636b 5f75 7469 6c2e 7265 6769  ceback_util.regi
+000006a0: 7374 6572 5f65 7863 6c75 7369 6f6e 285f  ster_exclusion(_
+000006b0: 5f66 696c 655f 5f29 0a0a 4b65 7941 7272  _file__)..KeyArr
+000006c0: 6179 203d 2055 6e69 6f6e 5b6a 6178 2e41  ay = Union[jax.A
+000006d0: 7272 6179 2c20 6a61 782e 7261 6e64 6f6d  rray, jax.random
+000006e0: 2e4b 6579 4172 7261 795d 2020 2320 7079  .KeyArray]  # py
+000006f0: 6c69 6e74 3a20 6469 7361 626c 653d 696e  lint: disable=in
+00000700: 7661 6c69 642d 6e61 6d65 0a52 4e47 5365  valid-name.RNGSe
+00000710: 7175 656e 6365 7320 3d20 4469 6374 5b73  quences = Dict[s
+00000720: 7472 2c20 4b65 7941 7272 6179 5d0a 4172  tr, KeyArray].Ar
+00000730: 7261 7920 3d20 416e 7920 2023 2070 796c  ray = Any  # pyl
+00000740: 696e 743a 2064 6973 6162 6c65 3d69 6e76  int: disable=inv
+00000750: 616c 6964 2d6e 616d 650a 0a0a 5420 3d20  alid-name...T = 
+00000760: 5479 7065 5661 7228 2754 2729 0a4b 203d  TypeVar('T').K =
+00000770: 2054 7970 6556 6172 2827 4b27 290a 4d20   TypeVar('K').M 
+00000780: 3d20 5479 7065 5661 7228 274d 272c 2062  = TypeVar('M', b
+00000790: 6f75 6e64 3d27 4d6f 6475 6c65 2729 0a5f  ound='Module')._
+000007a0: 4361 6c6c 6162 6c65 5420 3d20 5479 7065  CallableT = Type
+000007b0: 5661 7228 275f 4361 6c6c 6162 6c65 5427  Var('_CallableT'
+000007c0: 2c20 626f 756e 643d 4361 6c6c 6162 6c65  , bound=Callable
+000007d0: 290a 0a0a 2320 5573 6564 2066 6f72 2061  )...# Used for a
+000007e0: 6273 7472 6163 746c 7920 7465 7374 696e  bstractly testin
+000007f0: 6720 6d6f 6475 6c65 2062 6568 6176 696f  g module behavio
+00000800: 722e 0a54 6573 7453 636f 7065 203d 2074  r..TestScope = t
+00000810: 7970 6528 0a20 2020 2027 5465 7374 5363  ype(.    'TestSc
+00000820: 6f70 6527 2c0a 2020 2020 2853 636f 7065  ope',.    (Scope
+00000830: 2c29 2c0a 2020 2020 7b27 6d61 6b65 5f72  ,),.    {'make_r
+00000840: 6e67 273a 206c 616d 6264 6120 7365 6c66  ng': lambda self
+00000850: 2c20 6e61 6d65 3a20 6a61 782e 7261 6e64  , name: jax.rand
+00000860: 6f6d 2e50 524e 474b 6579 2830 297d 2c0a  om.PRNGKey(0)},.
+00000870: 290a 0a0a 2320 7079 6c69 6e74 3a20 6469  )...# pylint: di
+00000880: 7361 626c 653d 7072 6f74 6563 7465 642d  sable=protected-
+00000890: 6163 6365 7373 2c61 7474 7269 6275 7465  access,attribute
+000008a0: 2d64 6566 696e 6564 2d6f 7574 7369 6465  -defined-outside
+000008b0: 2d69 6e69 740a 0a0a 6465 6620 5f69 6e64  -init...def _ind
+000008c0: 656e 7428 783a 2073 7472 2c20 6e75 6d5f  ent(x: str, num_
+000008d0: 7370 6163 6573 3a20 696e 7429 3a0a 2020  spaces: int):.  
+000008e0: 696e 6465 6e74 5f73 7472 203d 2027 2027  indent_str = ' '
+000008f0: 202a 206e 756d 5f73 7061 6365 730a 2020   * num_spaces.  
+00000900: 6c69 6e65 7320 3d20 782e 7370 6c69 7428  lines = x.split(
+00000910: 275c 6e27 290a 2020 2320 736b 6970 206c  '\n').  # skip l
+00000920: 6173 7420 6c69 6e65 2062 6563 6175 7365  ast line because
+00000930: 2069 7420 6973 2061 6c77 6179 7320 656d   it is always em
+00000940: 7074 7920 616e 6420 7368 6f75 6c64 206e  pty and should n
+00000950: 6f74 2062 6520 696e 6465 6e74 6564 2e0a  ot be indented..
+00000960: 2020 6173 7365 7274 206e 6f74 206c 696e    assert not lin
+00000970: 6573 5b2d 315d 0a20 2072 6574 7572 6e20  es[-1].  return 
+00000980: 275c 6e27 2e6a 6f69 6e28 696e 6465 6e74  '\n'.join(indent
+00000990: 5f73 7472 202b 206c 696e 6520 666f 7220  _str + line for 
+000009a0: 6c69 6e65 2069 6e20 6c69 6e65 735b 3a2d  line in lines[:-
+000009b0: 315d 2920 2b20 275c 6e27 0a0a 0a64 6566  1]) + '\n'...def
+000009c0: 205f 6174 7472 5f72 6570 7228 7661 6c75   _attr_repr(valu
+000009d0: 653a 2041 6e79 293a 0a20 2069 6620 6361  e: Any):.  if ca
+000009e0: 6c6c 6162 6c65 2876 616c 7565 2920 616e  llable(value) an
+000009f0: 6420 280a 2020 2020 2020 2869 7369 6e73  d (.      (isins
+00000a00: 7461 6e63 6528 7661 6c75 652c 206e 6e2e  tance(value, nn.
+00000a10: 4d6f 6475 6c65 2920 616e 6420 7661 6c75  Module) and valu
+00000a20: 652e 5f5f 6469 6374 5f5f 2e67 6574 2827  e.__dict__.get('
+00000a30: 5f5f 6e61 6d65 5f5f 272c 204e 6f6e 6529  __name__', None)
+00000a40: 290a 2020 2020 2020 6f72 2028 6e6f 7420  ).      or (not 
+00000a50: 6973 696e 7374 616e 6365 2876 616c 7565  isinstance(value
+00000a60: 2c20 6e6e 2e4d 6f64 756c 6529 2061 6e64  , nn.Module) and
+00000a70: 2067 6574 6174 7472 2876 616c 7565 2c20   getattr(value, 
+00000a80: 275f 5f6e 616d 655f 5f27 2c20 4e6f 6e65  '__name__', None
+00000a90: 2929 0a20 2029 3a0a 2020 2020 7661 6c75  )).  ):.    valu
+00000aa0: 655f 7265 7020 3d20 7661 6c75 652e 5f5f  e_rep = value.__
+00000ab0: 6e61 6d65 5f5f 0a20 2065 6c73 653a 0a20  name__.  else:. 
+00000ac0: 2020 2076 616c 7565 5f72 6570 203d 2072     value_rep = r
+00000ad0: 6570 7228 7661 6c75 6529 0a20 2072 6574  epr(value).  ret
+00000ae0: 7572 6e20 7661 6c75 655f 7265 700a 0a0a  urn value_rep...
+00000af0: 6465 6620 5f6d 6f64 756c 655f 7265 7072  def _module_repr
+00000b00: 286d 6f64 756c 653a 2027 4d6f 6475 6c65  (module: 'Module
+00000b10: 272c 206e 756d 5f73 7061 6365 733a 2069  ', num_spaces: i
+00000b20: 6e74 203d 2034 293a 0a20 2022 2222 5265  nt = 4):.  """Re
+00000b30: 7475 726e 7320 6120 7072 6574 7479 2070  turns a pretty p
+00000b40: 7269 6e74 6564 2072 6570 7265 7365 6e74  rinted represent
+00000b50: 6174 696f 6e20 6f66 2074 6865 206d 6f64  ation of the mod
+00000b60: 756c 652e 2222 220a 2020 636c 7320 3d20  ule.""".  cls = 
+00000b70: 7479 7065 286d 6f64 756c 6529 0a20 2063  type(module).  c
+00000b80: 6c73 5f6e 616d 6520 3d20 636c 732e 5f5f  ls_name = cls.__
+00000b90: 6e61 6d65 5f5f 0a20 2072 6570 203d 2027  name__.  rep = '
+00000ba0: 270a 0a20 2061 7474 7269 6275 7465 7320  '..  attributes 
+00000bb0: 3d20 7b0a 2020 2020 2020 662e 6e61 6d65  = {.      f.name
+00000bc0: 3a20 662e 7479 7065 0a20 2020 2020 2066  : f.type.      f
+00000bd0: 6f72 2066 2069 6e20 6461 7461 636c 6173  or f in dataclas
+00000be0: 7365 732e 6669 656c 6473 2863 6c73 290a  ses.fields(cls).
+00000bf0: 2020 2020 2020 6966 2066 2e6e 616d 6520        if f.name 
+00000c00: 6e6f 7420 696e 2028 2770 6172 656e 7427  not in ('parent'
+00000c10: 2c20 276e 616d 6527 2920 616e 6420 662e  , 'name') and f.
+00000c20: 7265 7072 0a20 207d 0a20 2063 6869 6c64  repr.  }.  child
+00000c30: 5f6d 6f64 756c 6573 203d 207b 0a20 2020  _modules = {.   
+00000c40: 2020 206b 3a20 760a 2020 2020 2020 666f     k: v.      fo
+00000c50: 7220 6b2c 2076 2069 6e20 6d6f 6475 6c65  r k, v in module
+00000c60: 2e5f 7374 6174 652e 6368 696c 6472 656e  ._state.children
+00000c70: 2e69 7465 6d73 2829 2020 2320 7079 7479  .items()  # pyty
+00000c80: 7065 3a20 6469 7361 626c 653d 6174 7472  pe: disable=attr
+00000c90: 6962 7574 652d 6572 726f 720a 2020 2020  ibute-error.    
+00000ca0: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
+00000cb0: 762c 204d 6f64 756c 6529 0a20 207d 0a20  v, Module).  }. 
+00000cc0: 2069 6620 6174 7472 6962 7574 6573 3a0a   if attributes:.
+00000cd0: 2020 2020 7265 7020 2b3d 2027 2320 6174      rep += '# at
+00000ce0: 7472 6962 7574 6573 5c6e 270a 2020 2020  tributes\n'.    
+00000cf0: 666f 7220 6174 7472 2069 6e20 6174 7472  for attr in attr
+00000d00: 6962 7574 6573 2e6b 6579 7328 293a 0a20  ibutes.keys():. 
+00000d10: 2020 2020 2023 2054 4f44 4f28 6a68 6565       # TODO(jhee
+00000d20: 6b29 3a20 6361 6e20 7765 2067 6574 2061  k): can we get a
+00000d30: 206e 6963 6520 7374 7269 6e67 2072 6570   nice string rep
+00000d40: 7265 7365 6e74 6174 696f 6e20 6f66 2061  resentation of a
+00000d50: 7474 7269 6275 7465 2074 7970 6573 3f0a  ttribute types?.
+00000d60: 2020 2020 2020 7661 6c75 6520 3d20 6d6f        value = mo
+00000d70: 6475 6c65 2e5f 5f64 6963 745f 5f2e 6765  dule.__dict__.ge
+00000d80: 7428 6174 7472 2c20 4e6f 6e65 290a 2020  t(attr, None).  
+00000d90: 2020 2020 7661 6c75 655f 7265 7020 3d20      value_rep = 
+00000da0: 5f61 7474 725f 7265 7072 2876 616c 7565  _attr_repr(value
+00000db0: 290a 2020 2020 2020 7265 7020 2b3d 2066  ).      rep += f
+00000dc0: 277b 6174 7472 7d20 3d20 7b76 616c 7565  '{attr} = {value
+00000dd0: 5f72 6570 7d5c 6e27 0a20 2069 6620 6368  _rep}\n'.  if ch
+00000de0: 696c 645f 6d6f 6475 6c65 733a 0a20 2020  ild_modules:.   
+00000df0: 2072 6570 202b 3d20 2723 2063 6869 6c64   rep += '# child
+00000e00: 7265 6e5c 6e27 0a20 2020 2066 6f72 206e  ren\n'.    for n
+00000e10: 616d 652c 2063 6869 6c64 2069 6e20 6368  ame, child in ch
+00000e20: 696c 645f 6d6f 6475 6c65 732e 6974 656d  ild_modules.item
+00000e30: 7328 293a 0a20 2020 2020 2063 6869 6c64  s():.      child
+00000e40: 5f72 6570 203d 205f 6d6f 6475 6c65 5f72  _rep = _module_r
+00000e50: 6570 7228 6368 696c 642c 206e 756d 5f73  epr(child, num_s
+00000e60: 7061 6365 7329 0a20 2020 2020 2072 6570  paces).      rep
+00000e70: 202b 3d20 6627 7b6e 616d 657d 203d 207b   += f'{name} = {
+00000e80: 6368 696c 645f 7265 707d 5c6e 270a 2020  child_rep}\n'.  
+00000e90: 6966 2072 6570 3a0a 2020 2020 7265 7475  if rep:.    retu
+00000ea0: 726e 2066 277b 636c 735f 6e61 6d65 7d28  rn f'{cls_name}(
+00000eb0: 5c6e 7b5f 696e 6465 6e74 2872 6570 2c20  \n{_indent(rep, 
+00000ec0: 6e75 6d5f 7370 6163 6573 297d 2927 0a20  num_spaces)})'. 
+00000ed0: 2065 6c73 653a 0a20 2020 2072 6574 7572   else:.    retur
+00000ee0: 6e20 6627 7b63 6c73 5f6e 616d 657d 2829  n f'{cls_name}()
+00000ef0: 270a 0a0a 2320 5461 6275 6c61 7469 6f6e  '...# Tabulation
+00000f00: 2075 7469 6c69 7469 6573 2e0a 2320 2d2d   utilities..# --
+00000f10: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000f20: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000f30: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000f40: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000f50: 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 0a5f 6669  -----------.._fi
+00000f60: 6e64 5f6e 6f6e 5f6c 6966 7465 645f 6d6f  nd_non_lifted_mo
+00000f70: 6475 6c65 203d 2072 652e 636f 6d70 696c  dule = re.compil
+00000f80: 6528 7227 2e2a 5c28 282e 2a29 5c29 2729  e(r'.*\((.*)\)')
+00000f90: 0a0a 0a64 6566 205f 6669 785f 7061 7468  ...def _fix_path
+00000fa0: 5f70 6172 7428 7061 7274 3a20 7374 7229  _part(part: str)
+00000fb0: 3a0a 2020 2222 2246 6978 6573 2061 2070  :.  """Fixes a p
+00000fc0: 6174 6820 7061 7274 2062 7920 7265 6d6f  ath part by remo
+00000fd0: 7669 6e67 2074 7261 6e73 666f 726d 6174  ving transformat
+00000fe0: 696f 6e20 6e61 6d65 2061 6e64 2070 6172  ion name and par
+00000ff0: 656e 7468 6573 6973 2073 6f6d 6574 696d  enthesis sometim
+00001000: 6573 0a20 2069 6e73 6572 7465 6420 6279  es.  inserted by
+00001010: 206c 6966 7465 6420 7472 616e 7366 6f72   lifted transfor
+00001020: 6d61 7469 6f6e 7322 2222 0a20 206d 6174  mations""".  mat
+00001030: 6368 203d 205f 6669 6e64 5f6e 6f6e 5f6c  ch = _find_non_l
+00001040: 6966 7465 645f 6d6f 6475 6c65 2e6d 6174  ifted_module.mat
+00001050: 6368 2870 6172 7429 0a20 2069 6620 6d61  ch(part).  if ma
+00001060: 7463 683a 0a20 2020 2072 6574 7572 6e20  tch:.    return 
+00001070: 6d61 7463 682e 6772 6f75 7028 3129 0a20  match.group(1). 
+00001080: 2072 6574 7572 6e20 7061 7274 0a0a 0a40   return part...@
+00001090: 6461 7461 636c 6173 7365 732e 6461 7461  dataclasses.data
+000010a0: 636c 6173 730a 636c 6173 7320 5f43 616c  class.class _Cal
+000010b0: 6c49 6e66 6f3a 0a20 2069 6e64 6578 3a20  lInfo:.  index: 
+000010c0: 696e 740a 2020 7061 7468 3a20 5475 706c  int.  path: Tupl
+000010d0: 655b 7374 722c 202e 2e2e 5d0a 2020 6d6f  e[str, ...].  mo
+000010e0: 6475 6c65 5f74 7970 653a 2054 7970 655b  dule_type: Type[
+000010f0: 274d 6f64 756c 6527 5d0a 2020 6d65 7468  'Module'].  meth
+00001100: 6f64 3a20 7374 720a 2020 6172 6773 3a20  od: str.  args: 
+00001110: 5475 706c 655b 416e 792c 202e 2e2e 5d0a  Tuple[Any, ...].
+00001120: 2020 6b77 6172 6773 3a20 4469 6374 5b73    kwargs: Dict[s
+00001130: 7472 2c20 416e 795d 0a20 206f 7574 7075  tr, Any].  outpu
+00001140: 7473 3a20 416e 790a 0a0a 4064 6174 6163  ts: Any...@datac
+00001150: 6c61 7373 6573 2e64 6174 6163 6c61 7373  lasses.dataclass
+00001160: 0a63 6c61 7373 205f 4361 6c6c 496e 666f  .class _CallInfo
+00001170: 436f 6e74 6578 7428 7468 7265 6164 696e  Context(threadin
+00001180: 672e 6c6f 6361 6c29 3a0a 2020 696e 6465  g.local):.  inde
+00001190: 783a 2069 6e74 0a20 2063 616c 6c73 3a20  x: int.  calls: 
+000011a0: 4c69 7374 5b5f 4361 6c6c 496e 666f 5d0a  List[_CallInfo].
+000011b0: 0a20 2064 6566 2067 6574 5f63 616c 6c5f  .  def get_call_
+000011c0: 696e 6465 7828 7365 6c66 2c20 6d6f 6475  index(self, modu
+000011d0: 6c65 3a20 274d 6f64 756c 6527 2920 2d3e  le: 'Module') ->
+000011e0: 2069 6e74 3a0a 2020 2020 696e 6465 7820   int:.    index 
+000011f0: 3d20 7365 6c66 2e69 6e64 6578 0a20 2020  = self.index.   
+00001200: 2073 656c 662e 696e 6465 7820 2b3d 2031   self.index += 1
+00001210: 0a20 2020 2072 6574 7572 6e20 696e 6465  .    return inde
+00001220: 780a 0a0a 4063 6f6e 7465 7874 6c69 622e  x...@contextlib.
+00001230: 636f 6e74 6578 746d 616e 6167 6572 0a64  contextmanager.d
+00001240: 6566 205f 7461 6275 6c61 7465 5f63 6f6e  ef _tabulate_con
+00001250: 7465 7874 2829 3a0a 2020 5f63 6f6e 7465  text():.  _conte
+00001260: 7874 2e63 616c 6c5f 696e 666f 5f73 7461  xt.call_info_sta
+00001270: 636b 2e61 7070 656e 6428 5f43 616c 6c49  ck.append(_CallI
+00001280: 6e66 6f43 6f6e 7465 7874 2830 2c20 5b5d  nfoContext(0, []
+00001290: 2929 0a20 2074 7279 3a0a 2020 2020 7969  )).  try:.    yi
+000012a0: 656c 640a 2020 6669 6e61 6c6c 793a 0a20  eld.  finally:. 
+000012b0: 2020 205f 636f 6e74 6578 742e 6361 6c6c     _context.call
+000012c0: 5f69 6e66 6f5f 7374 6163 6b2e 706f 7028  _info_stack.pop(
+000012d0: 290a 0a0a 2320 5472 6163 6b20 7061 7265  )...# Track pare
+000012e0: 6e74 2072 656c 6174 696f 6e73 6869 7020  nt relationship 
+000012f0: 6163 726f 7373 204d 6f64 756c 6573 2e0a  across Modules..
+00001300: 2320 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  # --------------
+00001310: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001320: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001330: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001340: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a  ---------------.
+00001350: 636c 6173 7320 5f44 796e 616d 6963 436f  class _DynamicCo
+00001360: 6e74 6578 7428 7468 7265 6164 696e 672e  ntext(threading.
+00001370: 6c6f 6361 6c29 3a0a 2020 2222 2244 796e  local):.  """Dyn
+00001380: 616d 6963 2063 6f6e 7465 7874 2e22 2222  amic context."""
+00001390: 0a0a 2020 2320 544f 444f 286d 6172 6376  ..  # TODO(marcv
+000013a0: 616e 7a65 6529 3a20 7377 6974 6368 2074  anzee): switch t
+000013b0: 6f20 7573 696e 6720 636f 6e74 6578 7476  o using contextv
+000013c0: 6172 7320 6f6e 6365 206d 696e 696d 756d  ars once minimum
+000013d0: 2070 7974 686f 6e20 7665 7273 696f 6e20   python version 
+000013e0: 6973 0a20 2023 2033 2e37 0a0a 2020 6465  is.  # 3.7..  de
+000013f0: 6620 5f5f 696e 6974 5f5f 2873 656c 6629  f __init__(self)
+00001400: 3a0a 2020 2020 7365 6c66 2e6d 6f64 756c  :.    self.modul
+00001410: 655f 7374 6163 6b20 3d20 5b0a 2020 2020  e_stack = [.    
+00001420: 2020 2020 4e6f 6e65 2c0a 2020 2020 5d0a      None,.    ].
+00001430: 2020 2020 7365 6c66 2e63 6170 7475 7265      self.capture
+00001440: 5f73 7461 636b 203d 205b 5d0a 2020 2020  _stack = [].    
+00001450: 7365 6c66 2e63 616c 6c5f 696e 666f 5f73  self.call_info_s
+00001460: 7461 636b 203d 205b 5d0a 0a0a 2320 5468  tack = []...# Th
+00001470: 6520 676c 6f62 616c 2063 6f6e 7465 7874  e global context
+00001480: 0a5f 636f 6e74 6578 7420 3d20 5f44 796e  ._context = _Dyn
+00001490: 616d 6963 436f 6e74 6578 7428 290a 0a0a  amicContext()...
+000014a0: 636c 6173 7320 5f53 656e 7469 6e65 6c3a  class _Sentinel:
+000014b0: 0a0a 2020 6465 6620 5f5f 636f 7079 5f5f  ..  def __copy__
+000014c0: 2873 656c 6629 3a0a 2020 2020 7265 7475  (self):.    retu
+000014d0: 726e 2073 656c 6620 2023 2044 6f20 6e6f  rn self  # Do no
+000014e0: 7420 636f 7079 2073 696e 676c 6574 6f6e  t copy singleton
+000014f0: 2073 656e 7469 6e65 6c2e 0a0a 2020 6465   sentinel...  de
+00001500: 6620 5f5f 6465 6570 636f 7079 5f5f 2873  f __deepcopy__(s
+00001510: 656c 662c 206d 656d 6f29 3a0a 2020 2020  elf, memo):.    
+00001520: 6465 6c20 6d65 6d6f 0a20 2020 2072 6574  del memo.    ret
+00001530: 7572 6e20 7365 6c66 2020 2320 446f 206e  urn self  # Do n
+00001540: 6f74 2063 6f70 7920 7369 6e67 6c65 746f  ot copy singleto
+00001550: 6e20 7365 6e74 696e 656c 2e0a 0a0a 5f75  n sentinel...._u
+00001560: 6e73 7065 6369 6669 6564 5f70 6172 656e  nspecified_paren
+00001570: 7420 3d20 5f53 656e 7469 6e65 6c28 290a  t = _Sentinel().
+00001580: 0a0a 2320 456e 6162 6c65 2061 7574 6f6d  ..# Enable autom
+00001590: 6174 6963 206e 616d 6564 5f63 616c 6c20  atic named_call 
+000015a0: 7772 6170 7069 6e67 2066 6f72 206c 6162  wrapping for lab
+000015b0: 656c 6c69 6e67 2070 726f 6669 6c65 2074  elling profile t
+000015c0: 7261 6365 732e 0a23 202d 2d2d 2d2d 2d2d  races..# -------
+000015d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001600: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001610: 2d2d 2d2d 2d2d 0a5f 7573 655f 6e61 6d65  ------._use_name
+00001620: 645f 6361 6c6c 203d 2063 6f6e 6669 672e  d_call = config.
+00001630: 666c 6178 5f70 726f 6669 6c65 0a0a 0a64  flax_profile...d
+00001640: 6566 205f 6465 7269 7665 5f70 726f 6669  ef _derive_profi
+00001650: 6c69 6e67 5f6e 616d 6528 6d6f 6475 6c65  ling_name(module
+00001660: 2c20 666e 293a 0a20 2064 6566 205f 6765  , fn):.  def _ge
+00001670: 745f 666e 5f6e 616d 6528 666e 293a 0a20  t_fn_name(fn):. 
+00001680: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance
+00001690: 2866 6e2c 2066 756e 6374 6f6f 6c73 2e70  (fn, functools.p
+000016a0: 6172 7469 616c 293a 0a20 2020 2020 2072  artial):.      r
+000016b0: 6574 7572 6e20 5f67 6574 5f66 6e5f 6e61  eturn _get_fn_na
+000016c0: 6d65 2866 6e2e 6675 6e63 290a 2020 2020  me(fn.func).    
+000016d0: 7265 7475 726e 2066 6e2e 5f5f 6e61 6d65  return fn.__name
+000016e0: 5f5f 0a0a 2020 666e 5f6e 616d 6520 3d20  __..  fn_name = 
+000016f0: 5f67 6574 5f66 6e5f 6e61 6d65 2866 6e29  _get_fn_name(fn)
+00001700: 0a20 206d 6574 686f 645f 7375 6666 6978  .  method_suffix
+00001710: 203d 2066 272e 7b66 6e5f 6e61 6d65 7d27   = f'.{fn_name}'
+00001720: 2069 6620 666e 5f6e 616d 6520 213d 2027   if fn_name != '
+00001730: 5f5f 6361 6c6c 5f5f 2720 656c 7365 2027  __call__' else '
+00001740: 270a 2020 6d6f 6475 6c65 5f6e 616d 6520  '.  module_name 
+00001750: 3d20 6d6f 6475 6c65 2e6e 616d 6520 6f72  = module.name or
+00001760: 206d 6f64 756c 652e 5f5f 636c 6173 735f   module.__class_
+00001770: 5f2e 5f5f 6e61 6d65 5f5f 0a20 2072 6574  _.__name__.  ret
+00001780: 7572 6e20 6627 7b6d 6f64 756c 655f 6e61  urn f'{module_na
+00001790: 6d65 7d7b 6d65 7468 6f64 5f73 7566 6669  me}{method_suffi
+000017a0: 787d 270a 0a0a 6465 6620 656e 6162 6c65  x}'...def enable
+000017b0: 5f6e 616d 6564 5f63 616c 6c28 293a 0a20  _named_call():. 
+000017c0: 2022 2222 456e 6162 6c65 7320 6e61 6d65   """Enables name
+000017d0: 6420 6361 6c6c 2077 7261 7070 696e 6720  d call wrapping 
+000017e0: 666f 7220 6c61 6265 6c6c 696e 6720 7072  for labelling pr
+000017f0: 6f66 696c 6520 7472 6163 6573 2e0a 0a20  ofile traces... 
+00001800: 2057 6865 6e20 6e61 6d65 6420 6361 6c6c   When named call
+00001810: 2077 7261 7070 696e 6720 6973 2065 6e61   wrapping is ena
+00001820: 626c 6564 2061 6c6c 204a 4158 206f 7073  bled all JAX ops
+00001830: 2065 7865 6375 7465 6420 696e 2061 204d   executed in a M
+00001840: 6f64 756c 650a 2020 7769 6c6c 2062 6520  odule.  will be 
+00001850: 7275 6e20 756e 6465 7220 6060 6a61 782e  run under ``jax.
+00001860: 6e61 6d65 645f 7363 6f70 6560 602e 2054  named_scope``. T
+00001870: 6865 2060 604d 6f64 756c 6560 6020 636c  he ``Module`` cl
+00001880: 6173 7320 6e61 6d65 2077 696c 6c0a 2020  ass name will.  
+00001890: 7368 6f77 2075 7020 6172 6f75 6e64 2074  show up around t
+000018a0: 6865 206f 7065 7261 7469 6f6e 7320 6265  he operations be
+000018b0: 6c6f 6e67 696e 6720 746f 2074 6861 7420  longing to that 
+000018c0: 4d6f 6475 6c65 2069 6e20 7468 650a 2020  Module in the.  
+000018d0: 5465 6e73 6f72 626f 6172 6420 7072 6f66  Tensorboard prof
+000018e0: 696c 696e 6720 5549 2c20 7369 6d70 6c69  iling UI, simpli
+000018f0: 6679 696e 6720 7468 6520 7072 6f66 696c  fying the profil
+00001900: 696e 6720 7072 6f63 6573 732e 0a0a 2020  ing process...  
+00001910: 4e6f 7465 2074 6861 7420 6060 6a61 782e  Note that ``jax.
+00001920: 6e61 6d65 645f 7363 6f70 6560 6020 6f6e  named_scope`` on
+00001930: 6c79 2077 6f72 6b73 2066 6f72 0a20 2063  ly works for.  c
+00001940: 6f6d 7069 6c65 6420 6675 6e63 7469 6f6e  ompiled function
+00001950: 7320 2865 2e67 2e3a 2075 7369 6e67 206a  s (e.g.: using j
+00001960: 6178 2e6a 6974 206f 7220 6a61 782e 706d  ax.jit or jax.pm
+00001970: 6170 292e 0a20 2022 2222 0a20 2067 6c6f  ap)..  """.  glo
+00001980: 6261 6c20 5f75 7365 5f6e 616d 6564 5f63  bal _use_named_c
+00001990: 616c 6c0a 2020 5f75 7365 5f6e 616d 6564  all.  _use_named
+000019a0: 5f63 616c 6c20 3d20 5472 7565 0a0a 0a64  _call = True...d
+000019b0: 6566 2064 6973 6162 6c65 5f6e 616d 6564  ef disable_named
+000019c0: 5f63 616c 6c28 293a 0a20 2022 2222 4469  _call():.  """Di
+000019d0: 7361 626c 6573 206e 616d 6564 2063 616c  sables named cal
+000019e0: 6c20 7772 6170 7069 6e67 2e0a 0a20 2053  l wrapping...  S
+000019f0: 6565 2060 6065 6e61 626c 655f 6e61 6d65  ee ``enable_name
+00001a00: 645f 6361 6c6c 6060 0a20 2022 2222 0a20  d_call``.  """. 
+00001a10: 2067 6c6f 6261 6c20 5f75 7365 5f6e 616d   global _use_nam
+00001a20: 6564 5f63 616c 6c0a 2020 5f75 7365 5f6e  ed_call.  _use_n
+00001a30: 616d 6564 5f63 616c 6c20 3d20 4661 6c73  amed_call = Fals
+00001a40: 650a 0a0a 4063 6f6e 7465 7874 6c69 622e  e...@contextlib.
+00001a50: 636f 6e74 6578 746d 616e 6167 6572 0a64  contextmanager.d
+00001a60: 6566 206f 7665 7272 6964 655f 6e61 6d65  ef override_name
+00001a70: 645f 6361 6c6c 2865 6e61 626c 653a 2062  d_call(enable: b
+00001a80: 6f6f 6c20 3d20 5472 7565 293a 0a20 2023  ool = True):.  #
+00001a90: 2070 796c 696e 743a 2064 6973 6162 6c65   pylint: disable
+00001aa0: 3d67 2d64 6f63 2d72 6574 7572 6e2d 6f72  =g-doc-return-or
+00001ab0: 2d79 6965 6c64 0a20 2022 2222 5265 7475  -yield.  """Retu
+00001ac0: 726e 7320 6120 636f 6e74 6578 7420 6d61  rns a context ma
+00001ad0: 6e61 6765 7220 7468 6174 2065 6e61 626c  nager that enabl
+00001ae0: 6573 2f64 6973 6162 6c65 7320 6e61 6d65  es/disables name
+00001af0: 6420 6361 6c6c 2077 7261 7070 696e 672e  d call wrapping.
+00001b00: 0a0a 2020 4172 6773 3a0a 2020 2020 656e  ..  Args:.    en
+00001b10: 6162 6c65 3a20 4966 2074 7275 652c 2065  able: If true, e
+00001b20: 6e61 626c 6573 206e 616d 6564 2063 616c  nables named cal
+00001b30: 6c20 7772 6170 7069 6e67 2066 6f72 206c  l wrapping for l
+00001b40: 6162 656c 6c69 6e67 2070 726f 6669 6c65  abelling profile
+00001b50: 2074 7261 6365 732e 0a20 2020 2020 2028   traces..      (
+00001b60: 7365 6520 6060 656e 6162 6c65 645f 6e61  see ``enabled_na
+00001b70: 6d65 645f 6361 6c6c 6060 292e 0a20 2022  med_call``)..  "
+00001b80: 2222 0a20 2023 2070 796c 696e 743a 2065  "".  # pylint: e
+00001b90: 6e61 626c 653d 672d 646f 632d 7265 7475  nable=g-doc-retu
+00001ba0: 726e 2d6f 722d 7969 656c 640a 2020 676c  rn-or-yield.  gl
+00001bb0: 6f62 616c 205f 7573 655f 6e61 6d65 645f  obal _use_named_
+00001bc0: 6361 6c6c 0a20 2075 7365 5f6e 616d 6564  call.  use_named
+00001bd0: 5f63 616c 6c5f 7072 6576 203d 205f 7573  _call_prev = _us
+00001be0: 655f 6e61 6d65 645f 6361 6c6c 0a20 205f  e_named_call.  _
+00001bf0: 7573 655f 6e61 6d65 645f 6361 6c6c 203d  use_named_call =
+00001c00: 2065 6e61 626c 650a 2020 7472 793a 0a20   enable.  try:. 
+00001c10: 2020 2079 6965 6c64 0a20 2066 696e 616c     yield.  final
+00001c20: 6c79 3a0a 2020 2020 5f75 7365 5f6e 616d  ly:.    _use_nam
+00001c30: 6564 5f63 616c 6c20 3d20 7573 655f 6e61  ed_call = use_na
+00001c40: 6d65 645f 6361 6c6c 5f70 7265 760a 0a0a  med_call_prev...
+00001c50: 2320 5574 696c 6974 6965 7320 666f 7220  # Utilities for 
+00001c60: 7079 7472 6565 7320 6f66 204d 6f64 756c  pytrees of Modul
+00001c70: 6573 2064 6566 696e 6564 2069 6e73 6964  es defined insid
+00001c80: 6520 7365 7475 7028 290a 2320 2d2d 2d2d  e setup().# ----
+00001c90: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001ca0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001cb0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001cc0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001cd0: 2d2d 2d2d 2d2d 2d2d 2d0a 0a0a 6465 6620  ---------...def 
+00001ce0: 5f73 6f72 7465 645f 6974 656d 7328 7829  _sorted_items(x)
+00001cf0: 3a0a 2020 2222 2252 6574 7572 6e73 2069  :.  """Returns i
+00001d00: 7465 6d73 206f 6620 6120 6469 6374 206f  tems of a dict o
+00001d10: 7264 6572 6564 2062 7920 6b65 7973 2e22  rdered by keys."
+00001d20: 2222 0a20 2072 6574 7572 6e20 736f 7274  "".  return sort
+00001d30: 6564 2878 2e69 7465 6d73 2829 2c20 6b65  ed(x.items(), ke
+00001d40: 793d 6c61 6d62 6461 2078 3a20 785b 305d  y=lambda x: x[0]
+00001d50: 290a 0a0a 6465 6620 5f67 6574 5f73 7566  )...def _get_suf
+00001d60: 6669 785f 7661 6c75 655f 7061 6972 7328  fix_value_pairs(
+00001d70: 0a20 2020 2074 7265 655f 6f72 5f6c 6561  .    tree_or_lea
+00001d80: 663a 2041 6e79 2c0a 2920 2d3e 204c 6973  f: Any,.) -> Lis
+00001d90: 745b 5475 706c 655b 7374 722c 2054 7970  t[Tuple[str, Typ
+00001da0: 655b 274d 6f64 756c 6527 5d5d 5d3a 0a20  e['Module']]]:. 
+00001db0: 2022 2222 4865 6c70 6572 2066 6f72 206e   """Helper for n
+00001dc0: 616d 696e 6720 7079 7472 6565 7320 6f66  aming pytrees of
+00001dd0: 2073 7562 6d6f 6475 6c65 732e 2222 220a   submodules.""".
+00001de0: 2020 6469 6374 5f6f 725f 6c65 6166 203d    dict_or_leaf =
+00001df0: 2073 6572 6961 6c69 7a61 7469 6f6e 2e74   serialization.t
+00001e00: 6f5f 7374 6174 655f 6469 6374 2874 7265  o_state_dict(tre
+00001e10: 655f 6f72 5f6c 6561 6629 0a20 2069 6620  e_or_leaf).  if 
+00001e20: 6e6f 7420 6973 696e 7374 616e 6365 2864  not isinstance(d
+00001e30: 6963 745f 6f72 5f6c 6561 662c 2064 6963  ict_or_leaf, dic
+00001e40: 7429 206f 7220 6e6f 7420 6469 6374 5f6f  t) or not dict_o
+00001e50: 725f 6c65 6166 3a0a 2020 2020 7265 7475  r_leaf:.    retu
+00001e60: 726e 205b 2827 272c 2074 7265 655f 6f72  rn [('', tree_or
+00001e70: 5f6c 6561 6629 5d0a 2020 656c 7365 3a0a  _leaf)].  else:.
+00001e80: 2020 2020 666c 6174 5f64 6963 7420 3d20      flat_dict = 
+00001e90: 7472 6176 6572 7365 5f75 7469 6c2e 666c  traverse_util.fl
+00001ea0: 6174 7465 6e5f 6469 6374 2864 6963 745f  atten_dict(dict_
+00001eb0: 6f72 5f6c 6561 6629 0a20 2020 2072 6574  or_leaf).    ret
+00001ec0: 7572 6e20 5b28 275f 2720 2b20 275f 272e  urn [('_' + '_'.
+00001ed0: 6a6f 696e 286b 292c 2076 2920 666f 7220  join(k), v) for 
+00001ee0: 6b2c 2076 2069 6e20 5f73 6f72 7465 645f  k, v in _sorted_
+00001ef0: 6974 656d 7328 666c 6174 5f64 6963 7429  items(flat_dict)
+00001f00: 5d0a 0a0a 6465 6620 5f6d 6170 5f6f 7665  ]...def _map_ove
+00001f10: 725f 6d6f 6475 6c65 735f 696e 5f74 7265  r_modules_in_tre
+00001f20: 6528 666e 2c20 7472 6565 5f6f 725f 6c65  e(fn, tree_or_le
+00001f30: 6166 293a 0a20 2022 2222 4865 6c70 6572  af):.  """Helper
+00001f40: 2066 6f72 206d 6170 7069 6e67 2066 756e   for mapping fun
+00001f50: 6374 696f 6e20 6f76 6572 2073 7562 6d6f  ction over submo
+00001f60: 6475 6c65 732e 2222 220a 2020 6469 6374  dules.""".  dict
+00001f70: 5f6f 725f 6c65 6166 203d 2073 6572 6961  _or_leaf = seria
+00001f80: 6c69 7a61 7469 6f6e 2e74 6f5f 7374 6174  lization.to_stat
+00001f90: 655f 6469 6374 2874 7265 655f 6f72 5f6c  e_dict(tree_or_l
+00001fa0: 6561 6629 0a20 2069 6620 6e6f 7420 6973  eaf).  if not is
+00001fb0: 696e 7374 616e 6365 2864 6963 745f 6f72  instance(dict_or
+00001fc0: 5f6c 6561 662c 2064 6963 7429 206f 7220  _leaf, dict) or 
+00001fd0: 6e6f 7420 6469 6374 5f6f 725f 6c65 6166  not dict_or_leaf
+00001fe0: 3a0a 2020 2020 7265 7475 726e 2066 6e28  :.    return fn(
+00001ff0: 2727 2c20 7472 6565 5f6f 725f 6c65 6166  '', tree_or_leaf
+00002000: 290a 2020 656c 7365 3a0a 2020 2020 666c  ).  else:.    fl
+00002010: 6174 5f64 6963 7420 3d20 7472 6176 6572  at_dict = traver
+00002020: 7365 5f75 7469 6c2e 666c 6174 7465 6e5f  se_util.flatten_
+00002030: 6469 6374 2864 6963 745f 6f72 5f6c 6561  dict(dict_or_lea
+00002040: 662c 206b 6565 705f 656d 7074 795f 6e6f  f, keep_empty_no
+00002050: 6465 733d 5472 7565 290a 2020 2020 6d61  des=True).    ma
+00002060: 7070 6564 5f66 6c61 745f 6469 6374 203d  pped_flat_dict =
+00002070: 207b 0a20 2020 2020 2020 206b 3a20 666e   {.        k: fn
+00002080: 2827 5f27 202b 2027 5f27 2e6a 6f69 6e28  ('_' + '_'.join(
+00002090: 6b29 2c20 7629 2066 6f72 206b 2c20 7620  k), v) for k, v 
+000020a0: 696e 205f 736f 7274 6564 5f69 7465 6d73  in _sorted_items
+000020b0: 2866 6c61 745f 6469 6374 290a 2020 2020  (flat_dict).    
+000020c0: 7d0a 2020 2020 7265 7475 726e 2073 6572  }.    return ser
+000020d0: 6961 6c69 7a61 7469 6f6e 2e66 726f 6d5f  ialization.from_
+000020e0: 7374 6174 655f 6469 6374 280a 2020 2020  state_dict(.    
+000020f0: 2020 2020 7472 6565 5f6f 725f 6c65 6166      tree_or_leaf
+00002100: 2c20 7472 6176 6572 7365 5f75 7469 6c2e  , traverse_util.
+00002110: 756e 666c 6174 7465 6e5f 6469 6374 286d  unflatten_dict(m
+00002120: 6170 7065 645f 666c 6174 5f64 6963 7429  apped_flat_dict)
+00002130: 0a20 2020 2029 0a0a 0a64 6566 205f 6672  .    )...def _fr
+00002140: 6565 7a65 5f61 7474 7228 7661 6c3a 2041  eeze_attr(val: A
+00002150: 6e79 2920 2d3e 2041 6e79 3a0a 2020 2222  ny) -> Any:.  ""
+00002160: 2252 6563 7572 7369 7665 6c79 2077 7261  "Recursively wra
+00002170: 7020 7468 6520 6769 7665 6e20 6174 7472  p the given attr
+00002180: 6962 7574 6520 6076 6172 6020 696e 2060  ibute `var` in `
+00002190: 6046 726f 7a65 6e44 6963 7460 602e 2222  `FrozenDict``.""
+000021a0: 220a 2020 6966 2069 7369 6e73 7461 6e63  ".  if isinstanc
+000021b0: 6528 7661 6c2c 2028 6469 6374 2c20 4672  e(val, (dict, Fr
+000021c0: 6f7a 656e 4469 6374 2929 3a0a 2020 2020  ozenDict)):.    
+000021d0: 7265 7475 726e 2046 726f 7a65 6e44 6963  return FrozenDic
+000021e0: 7428 7b6b 3a20 5f66 7265 657a 655f 6174  t({k: _freeze_at
+000021f0: 7472 2876 2920 666f 7220 6b2c 2076 2069  tr(v) for k, v i
+00002200: 6e20 7661 6c2e 6974 656d 7328 297d 290a  n val.items()}).
+00002210: 2020 656c 6966 2069 7369 6e73 7461 6e63    elif isinstanc
+00002220: 6528 7661 6c2c 2074 7570 6c65 293a 0a20  e(val, tuple):. 
+00002230: 2020 2023 2053 7065 6369 616c 2063 6173     # Special cas
+00002240: 6520 6e61 6d65 6474 7570 6c65 7320 616e  e namedtuples an
+00002250: 6420 7370 6563 6961 6c20 4a41 5820 7475  d special JAX tu
+00002260: 706c 6520 7374 7275 6374 7572 6573 206f  ple structures o
+00002270: 7468 6572 7769 7365 2074 6865 790a 2020  therwise they.  
+00002280: 2020 2320 776f 756c 6420 6265 2064 6f77    # would be dow
+00002290: 6e67 7261 6465 6420 746f 206e 6f72 6d61  ngraded to norma
+000022a0: 6c20 7475 706c 6573 2e0a 2020 2020 6966  l tuples..    if
+000022b0: 2068 6173 6174 7472 2876 616c 2c20 275f   hasattr(val, '_
+000022c0: 6669 656c 6473 2729 206f 7220 7479 7065  fields') or type
+000022d0: 2876 616c 292e 5f5f 6e61 6d65 5f5f 203d  (val).__name__ =
+000022e0: 3d20 2750 6172 7469 7469 6f6e 5370 6563  = 'PartitionSpec
+000022f0: 273a 0a20 2020 2020 2072 6574 7572 6e20  ':.      return 
+00002300: 7479 7065 2876 616c 2928 2a5b 5f66 7265  type(val)(*[_fre
+00002310: 657a 655f 6174 7472 2876 2920 666f 7220  eze_attr(v) for 
+00002320: 7620 696e 2076 616c 5d29 0a20 2020 2065  v in val]).    e
+00002330: 6c73 653a 0a20 2020 2020 2072 6574 7572  lse:.      retur
+00002340: 6e20 7475 706c 6528 5f66 7265 657a 655f  n tuple(_freeze_
+00002350: 6174 7472 2876 2920 666f 7220 7620 696e  attr(v) for v in
+00002360: 2076 616c 290a 2020 656c 6966 2069 7369   val).  elif isi
+00002370: 6e73 7461 6e63 6528 7661 6c2c 206c 6973  nstance(val, lis
+00002380: 7429 3a0a 2020 2020 7265 7475 726e 2074  t):.    return t
+00002390: 7570 6c65 285f 6672 6565 7a65 5f61 7474  uple(_freeze_att
+000023a0: 7228 7629 2066 6f72 2076 2069 6e20 7661  r(v) for v in va
+000023b0: 6c29 0a20 2065 6c73 653a 0a20 2020 2072  l).  else:.    r
+000023c0: 6574 7572 6e20 7661 6c0a 0a0a 2320 4d65  eturn val...# Me
+000023d0: 7468 6f64 2077 7261 7070 696e 6720 6f66  thod wrapping of
+000023e0: 2022 636f 6d70 6163 7420 6d65 7468 6f64   "compact method
+000023f0: 7322 2061 6e64 2073 6574 7570 2829 0a23  s" and setup().#
+00002400: 202d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d   ---------------
+00002410: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00002420: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00002430: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00002440: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 0a64  --------------.d
+00002450: 6566 2063 6f6d 7061 6374 2866 756e 3a20  ef compact(fun: 
+00002460: 5f43 616c 6c61 626c 6554 2920 2d3e 205f  _CallableT) -> _
+00002470: 4361 6c6c 6162 6c65 543a 0a20 2022 2222  CallableT:.  """
+00002480: 4d61 726b 7320 7468 6520 6769 7665 6e20  Marks the given 
+00002490: 6d6f 6475 6c65 206d 6574 686f 6420 616c  module method al
+000024a0: 6c6f 7769 6e67 2069 6e6c 696e 6564 2073  lowing inlined s
+000024b0: 7562 6d6f 6475 6c65 732e 0a0a 2020 4d65  ubmodules...  Me
+000024c0: 7468 6f64 7320 7772 6170 7065 6420 696e  thods wrapped in
+000024d0: 2040 636f 6d70 6163 7420 6361 6e20 6465   @compact can de
+000024e0: 6669 6e65 2073 7562 6d6f 6475 6c65 7320  fine submodules 
+000024f0: 6469 7265 6374 6c79 2077 6974 6869 6e20  directly within 
+00002500: 7468 6520 6d65 7468 6f64 2e0a 0a20 2046  the method...  F
+00002510: 6f72 2069 6e73 7461 6e63 653a 3a0a 0a20  or instance::.. 
+00002520: 2020 2040 636f 6d70 6163 740a 2020 2020     @compact.    
+00002530: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
+00002540: 2c20 6665 6174 7572 6573 293a 0a20 2020  , features):.   
+00002550: 2020 2078 203d 206e 6e2e 4465 6e73 6528     x = nn.Dense(
+00002560: 6665 6174 7572 6573 2928 7829 0a20 2020  features)(x).   
+00002570: 2020 202e 2e2e 0a0a 2020 4174 206d 6f73     .....  At mos
+00002580: 7420 6f6e 6520 6d65 7468 6f64 2069 6e20  t one method in 
+00002590: 6561 6368 204d 6f64 756c 6520 6d61 7920  each Module may 
+000025a0: 6265 2077 7261 7070 6564 2077 6974 6820  be wrapped with 
+000025b0: 4063 6f6d 7061 6374 2e0a 0a20 2041 7267  @compact...  Arg
+000025c0: 733a 0a20 2020 2066 756e 3a20 5468 6520  s:.    fun: The 
+000025d0: 4d6f 6475 6c65 206d 6574 686f 6420 746f  Module method to
+000025e0: 206d 6172 6b20 6173 2063 6f6d 7061 6374   mark as compact
+000025f0: 2e0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
+00002600: 2054 6865 2067 6976 656e 2066 756e 6374   The given funct
+00002610: 696f 6e20 6066 756e 6020 6d61 726b 6564  ion `fun` marked
+00002620: 2061 7320 636f 6d70 6163 742e 0a20 2022   as compact..  "
+00002630: 2222 0a20 2066 756e 2e63 6f6d 7061 6374  "".  fun.compact
+00002640: 203d 2054 7275 6520 2023 2074 7970 653a   = True  # type:
+00002650: 2069 676e 6f72 655b 6174 7472 2d64 6566   ignore[attr-def
+00002660: 696e 6564 5d0a 2020 7265 7475 726e 2066  ined].  return f
+00002670: 756e 0a0a 0a64 6566 206e 6f77 7261 7028  un...def nowrap(
+00002680: 6675 6e3a 205f 4361 6c6c 6162 6c65 5429  fun: _CallableT)
+00002690: 202d 3e20 5f43 616c 6c61 626c 6554 3a0a   -> _CallableT:.
+000026a0: 2020 2222 224d 6172 6b73 2074 6865 2067    """Marks the g
+000026b0: 6976 656e 206d 6f64 756c 6520 6d65 7468  iven module meth
+000026c0: 6f64 2061 7320 6120 6865 6c70 6572 206d  od as a helper m
+000026d0: 6574 686f 6420 7468 6174 206e 6565 646e  ethod that needn
+000026e0: 2774 2062 6520 7772 6170 7065 642e 0a0a  't be wrapped...
+000026f0: 2020 4d65 7468 6f64 7320 7772 6170 7065    Methods wrappe
+00002700: 6420 696e 2040 6e6f 7772 6170 2061 7265  d in @nowrap are
+00002710: 2070 7269 7661 7465 2068 656c 7065 7220   private helper 
+00002720: 6d65 7468 6f64 7320 7468 6174 206e 6565  methods that nee
+00002730: 646e 2774 2062 6520 7772 6170 7065 640a  dn't be wrapped.
+00002740: 2020 7769 7468 2074 6865 2073 7461 7465    with the state
+00002750: 2068 616e 646c 6572 206f 7220 6120 7365   handler or a se
+00002760: 7061 7261 7465 206e 616d 6564 5f63 616c  parate named_cal
+00002770: 6c20 7472 616e 7366 6f72 6d2e 0a0a 2020  l transform...  
+00002780: 5468 6973 2069 7320 6e65 6564 6564 2069  This is needed i
+00002790: 6e20 7365 7665 7261 6c20 636f 6e63 7265  n several concre
+000027a0: 7465 2069 6e73 7461 6e63 6573 3a0a 2020  te instances:.  
+000027b0: 202d 2069 6620 796f 7527 7265 2073 7562   - if you're sub
+000027c0: 636c 6173 7369 6e67 2061 206d 6574 686f  classing a metho
+000027d0: 6420 6c69 6b65 204d 6f64 756c 652e 7061  d like Module.pa
+000027e0: 7261 6d20 616e 6420 646f 6e27 7420 7761  ram and don't wa
+000027f0: 6e74 2074 6869 730a 2020 2020 206f 7665  nt this.     ove
+00002800: 7272 6964 656e 2063 6f72 6520 6675 6e63  rriden core func
+00002810: 7469 6f6e 2064 6563 6f72 6174 6564 2077  tion decorated w
+00002820: 6974 6820 7468 6520 7374 6174 6520 6d61  ith the state ma
+00002830: 6e61 6765 6d65 6e74 2077 7261 7070 6572  nagement wrapper
+00002840: 2e0a 2020 202d 2049 6620 796f 7520 7761  ..   - If you wa
+00002850: 6e74 2061 206d 6574 686f 6420 746f 2062  nt a method to b
+00002860: 6520 6361 6c6c 6162 6c65 2066 726f 6d20  e callable from 
+00002870: 616e 2075 6e62 6f75 6e64 204d 6f64 756c  an unbound Modul
+00002880: 6520 2865 2e67 2e3a 2061 0a20 2020 2020  e (e.g.: a.     
+00002890: 6675 6e63 7469 6f6e 206f 6620 636f 6e73  function of cons
+000028a0: 7472 7563 7469 6f6e 206f 6620 6172 6775  truction of argu
+000028b0: 6d65 6e74 7320 7468 6174 2064 6f65 736e  ments that doesn
+000028c0: 2774 2064 6570 656e 6420 6f6e 2070 6172  't depend on par
+000028d0: 616d 732f 524e 4773 290a 0a20 2046 6f72  ams/RNGs)..  For
+000028e0: 2069 6e73 7461 6e63 653a 3a0a 0a20 2020   instance::..   
+000028f0: 2040 6e6f 7772 6170 0a20 2020 2064 6566   @nowrap.    def
+00002900: 205f 6d61 6b65 5f64 656e 7365 2873 656c   _make_dense(sel
+00002910: 662c 206e 756d 5f66 6561 7475 7265 7329  f, num_features)
+00002920: 3a0a 2020 2020 2020 7265 7475 726e 206e  :.      return n
+00002930: 6e2e 4465 6e73 6528 6e75 6d5f 6665 6174  n.Dense(num_feat
+00002940: 7572 6573 290a 0a20 2020 2040 636f 6d70  ures)..    @comp
+00002950: 6163 740a 2020 2020 6465 6620 5f5f 6361  act.    def __ca
+00002960: 6c6c 5f5f 2873 656c 662c 2078 293a 0a20  ll__(self, x):. 
+00002970: 2020 2020 2023 206e 6f77 2073 6166 6520       # now safe 
+00002980: 746f 2075 7365 2063 6f6e 7374 7275 6374  to use construct
+00002990: 6f72 2068 656c 7065 7220 6576 656e 2069  or helper even i
+000029a0: 6620 7573 696e 6720 6e61 6d65 645f 6361  f using named_ca
+000029b0: 6c6c 0a20 2020 2020 2064 656e 7365 203d  ll.      dense =
+000029c0: 2073 656c 662e 5f6d 616b 655f 6465 6e73   self._make_dens
+000029d0: 6528 7365 6c66 2e6e 756d 5f66 6561 7475  e(self.num_featu
+000029e0: 7265 7329 0a20 2020 2020 2072 6574 7572  res).      retur
+000029f0: 6e20 6465 6e73 6528 7829 0a0a 2020 4172  n dense(x)..  Ar
+00002a00: 6773 3a0a 2020 2020 6675 6e3a 2054 6865  gs:.    fun: The
+00002a10: 204d 6f64 756c 6520 6d65 7468 6f64 2074   Module method t
+00002a20: 6f20 6d61 726b 2061 7320 6e6f 7772 6170  o mark as nowrap
+00002a30: 2e0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
+00002a40: 2054 6865 2067 6976 656e 2066 756e 6374   The given funct
+00002a50: 696f 6e20 6066 756e 6020 6d61 726b 6564  ion `fun` marked
+00002a60: 2061 7320 6e6f 7772 6170 2e0a 2020 2222   as nowrap..  ""
+00002a70: 220a 2020 6675 6e2e 6e6f 7772 6170 203d  ".  fun.nowrap =
+00002a80: 2054 7275 6520 2023 2074 7970 653a 2069   True  # type: i
+00002a90: 676e 6f72 655b 6174 7472 2d64 6566 696e  gnore[attr-defin
+00002aa0: 6564 5d0a 2020 7265 7475 726e 2066 756e  ed].  return fun
+00002ab0: 0a0a 0a64 6566 205f 6765 745f 6c6f 6361  ...def _get_loca
+00002ac0: 6c5f 6d65 7468 6f64 5f6e 616d 6573 280a  l_method_names(.
+00002ad0: 2020 2020 636c 733a 2041 6e79 2c20 6578      cls: Any, ex
+00002ae0: 636c 7564 653a 2049 7465 7261 626c 655b  clude: Iterable[
+00002af0: 7374 725d 203d 2028 290a 2920 2d3e 2054  str] = ().) -> T
+00002b00: 7570 6c65 5b73 7472 2c20 2e2e 2e5d 3a0a  uple[str, ...]:.
+00002b10: 2020 2222 2247 6574 7320 6d65 7468 6f64    """Gets method
+00002b20: 206e 616d 6573 206f 6620 6120 636c 6173   names of a clas
+00002b30: 732c 2065 7863 6c75 6469 6e67 2063 6c61  s, excluding cla
+00002b40: 7373 2061 6e64 2073 7461 7469 6320 6d65  ss and static me
+00002b50: 7468 6f64 732e 0a0a 2020 4172 6773 3a0a  thods...  Args:.
+00002b60: 2020 2020 636c 733a 2054 6865 2063 6c61      cls: The cla
+00002b70: 7373 2074 6f20 6765 7420 6d65 7468 6f64  ss to get method
+00002b80: 206e 616d 6573 2066 6f72 2e0a 2020 2020   names for..    
+00002b90: 6578 636c 7564 653a 204e 616d 6573 2074  exclude: Names t
+00002ba0: 6f20 6578 636c 7564 6520 6672 6f6d 206f  o exclude from o
+00002bb0: 7574 7075 742e 0a20 2052 6574 7572 6e73  utput..  Returns
+00002bc0: 3a0a 2020 2020 4120 6c69 7374 206f 6620  :.    A list of 
+00002bd0: 6d65 7468 6f64 206e 616d 6573 2e0a 2020  method names..  
+00002be0: 2222 220a 2020 7472 7565 5f6d 6574 686f  """.  true_metho
+00002bf0: 6473 203d 2073 6574 2829 0a20 2066 6f72  ds = set().  for
+00002c00: 206d 2069 6e20 636c 732e 5f5f 6469 6374   m in cls.__dict
+00002c10: 5f5f 3a0a 2020 2020 6966 2063 616c 6c61  __:.    if calla
+00002c20: 626c 6528 636c 732e 5f5f 6469 6374 5f5f  ble(cls.__dict__
+00002c30: 5b6d 5d29 2061 6e64 206e 6f74 2069 6e73  [m]) and not ins
+00002c40: 7065 6374 2e69 7363 6c61 7373 2863 6c73  pect.isclass(cls
+00002c50: 2e5f 5f64 6963 745f 5f5b 6d5d 293a 2020  .__dict__[m]):  
+00002c60: 2320 7079 7479 7065 3a20 6469 7361 626c  # pytype: disabl
+00002c70: 653d 6e6f 742d 7375 7070 6f72 7465 642d  e=not-supported-
+00002c80: 7965 740a 2020 2020 2020 6d74 7970 6520  yet.      mtype 
+00002c90: 3d20 7479 7065 2863 6c73 2e5f 5f64 6963  = type(cls.__dic
+00002ca0: 745f 5f5b 6d5d 290a 2020 2020 2020 6966  t__[m]).      if
+00002cb0: 206d 7479 7065 2021 3d20 7374 6174 6963   mtype != static
+00002cc0: 6d65 7468 6f64 2061 6e64 206d 7479 7065  method and mtype
+00002cd0: 2021 3d20 636c 6173 736d 6574 686f 643a   != classmethod:
+00002ce0: 0a20 2020 2020 2020 2074 7275 655f 6d65  .        true_me
+00002cf0: 7468 6f64 732e 6164 6428 6d29 0a20 2072  thods.add(m).  r
+00002d00: 6574 7572 6e20 7475 706c 6528 7472 7565  eturn tuple(true
+00002d10: 5f6d 6574 686f 6473 2e64 6966 6665 7265  _methods.differe
+00002d20: 6e63 6528 7365 7428 6578 636c 7564 6529  nce(set(exclude)
+00002d30: 2929 0a0a 0a64 6566 205f 6765 745f 6c6f  ))...def _get_lo
+00002d40: 6361 6c5f 6465 7363 7269 7074 6f72 5f6e  cal_descriptor_n
+00002d50: 616d 6573 280a 2020 2020 636c 733a 2041  ames(.    cls: A
+00002d60: 6e79 2c20 6578 636c 7564 653a 2049 7465  ny, exclude: Ite
+00002d70: 7261 626c 655b 7374 725d 203d 2028 290a  rable[str] = ().
+00002d80: 2920 2d3e 2054 7570 6c65 5b73 7472 2c20  ) -> Tuple[str, 
+00002d90: 2e2e 2e5d 3a0a 2020 2222 2247 6574 7320  ...]:.  """Gets 
+00002da0: 6465 7363 7269 7074 6f72 206e 616d 6573  descriptor names
+00002db0: 206f 6620 6120 636c 6173 732e 0a0a 2020   of a class...  
+00002dc0: 4172 6773 3a0a 2020 2020 636c 733a 2054  Args:.    cls: T
+00002dd0: 6865 2063 6c61 7373 2074 6f20 6765 7420  he class to get 
+00002de0: 7072 6f70 6572 7479 206e 616d 6573 2066  property names f
+00002df0: 6f72 2e0a 2020 2020 6578 636c 7564 653a  or..    exclude:
+00002e00: 204e 616d 6573 2074 6f20 6578 636c 7564   Names to exclud
+00002e10: 6520 6672 6f6d 206f 7574 7075 742e 0a20  e from output.. 
+00002e20: 2052 6574 7572 6e73 3a0a 2020 2020 4120   Returns:.    A 
+00002e30: 6c69 7374 206f 6620 7072 6f70 6572 7479  list of property
+00002e40: 206e 616d 6573 2e0a 2020 2222 220a 2020   names..  """.  
+00002e50: 7472 7565 5f70 726f 7065 7274 6965 7320  true_properties 
+00002e60: 3d20 7365 7428 290a 2020 666f 7220 6d2c  = set().  for m,
+00002e70: 2061 7474 7220 696e 2063 6c73 2e5f 5f64   attr in cls.__d
+00002e80: 6963 745f 5f2e 6974 656d 7328 293a 0a20  ict__.items():. 
+00002e90: 2020 2069 6620 6e6f 7420 6361 6c6c 6162     if not callab
+00002ea0: 6c65 2861 7474 7229 2061 6e64 2028 0a20  le(attr) and (. 
+00002eb0: 2020 2020 2020 2068 6173 6174 7472 2861         hasattr(a
+00002ec0: 7474 722c 2027 5f5f 6765 745f 5f27 290a  ttr, '__get__').
+00002ed0: 2020 2020 2020 2020 6f72 2068 6173 6174          or hasat
+00002ee0: 7472 2861 7474 722c 2027 5f5f 7365 745f  tr(attr, '__set_
+00002ef0: 5f27 290a 2020 2020 2020 2020 6f72 2068  _').        or h
+00002f00: 6173 6174 7472 2861 7474 722c 2027 5f5f  asattr(attr, '__
+00002f10: 6465 6c65 7465 5f5f 2729 0a20 2020 2029  delete__').    )
+00002f20: 3a0a 2020 2020 2020 6d74 7970 6520 3d20  :.      mtype = 
+00002f30: 7479 7065 2861 7474 7229 0a20 2020 2020  type(attr).     
+00002f40: 2069 6620 6d74 7970 6520 213d 2073 7461   if mtype != sta
+00002f50: 7469 636d 6574 686f 6420 616e 6420 6d74  ticmethod and mt
+00002f60: 7970 6520 213d 2063 6c61 7373 6d65 7468  ype != classmeth
+00002f70: 6f64 3a0a 2020 2020 2020 2020 7472 7565  od:.        true
+00002f80: 5f70 726f 7065 7274 6965 732e 6164 6428  _properties.add(
+00002f90: 6d29 0a20 2072 6574 7572 6e20 7475 706c  m).  return tupl
+00002fa0: 6528 7472 7565 5f70 726f 7065 7274 6965  e(true_propertie
+00002fb0: 732e 6469 6666 6572 656e 6365 2873 6574  s.difference(set
+00002fc0: 2865 7863 6c75 6465 2929 290a 0a0a 6465  (exclude)))...de
+00002fd0: 6620 7772 6170 5f6d 6574 686f 645f 6f6e  f wrap_method_on
+00002fe0: 6365 2866 756e 3a20 4361 6c6c 6162 6c65  ce(fun: Callable
+00002ff0: 5b2e 2e2e 2c20 416e 795d 2920 2d3e 2043  [..., Any]) -> C
+00003000: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
+00003010: 5d3a 0a20 2022 2222 4d61 6e61 6765 7320  ]:.  """Manages 
+00003020: 4d6f 6475 6c65 2073 7461 7465 2066 6f72  Module state for
+00003030: 2061 2067 6976 656e 2075 7365 722d 6465   a given user-de
+00003040: 6669 6e65 6420 6d65 7468 6f64 2e0a 0a20  fined method... 
+00003050: 2041 7267 733a 0a20 2020 2066 756e 3a20   Args:.    fun: 
+00003060: 5573 6572 2d64 6566 696e 6564 204d 6f64  User-defined Mod
+00003070: 756c 6520 6d65 7468 6f64 2074 6f20 6d61  ule method to ma
+00003080: 6e61 6765 2073 7461 7465 2066 6f72 2e0a  nage state for..
+00003090: 2020 5265 7475 726e 733a 0a20 2020 2057    Returns:.    W
+000030a0: 7261 7070 6564 206d 6574 686f 642e 0a20  rapped method.. 
+000030b0: 2022 2222 0a20 2023 2044 6f6e 2774 2072   """.  # Don't r
+000030c0: 6577 7261 7020 6d65 7468 6f64 7320 7468  ewrap methods th
+000030d0: 6174 2068 6176 6520 616c 7265 6164 7920  at have already 
+000030e0: 6861 6420 7468 6520 7374 6174 6520 6d61  had the state ma
+000030f0: 6e61 6765 6d65 6e74 2077 7261 7070 6572  nagement wrapper
+00003100: 0a20 2023 2061 7070 6c69 6564 2069 6e20  .  # applied in 
+00003110: 7468 6520 6465 636f 7261 746f 7220 7374  the decorator st
+00003120: 6163 6b2e 2020 5468 6973 2077 7261 7070  ack.  This wrapp
+00003130: 6572 2073 686f 756c 6420 616c 7761 7973  er should always
+00003140: 2062 6520 6170 706c 6965 640a 2020 2320   be applied.  # 
+00003150: 6265 666f 7265 2074 7261 6e73 666f 726d  before transform
+00003160: 6174 696f 6e20 7772 6170 7065 7273 2e0a  ation wrappers..
+00003170: 2020 6966 2068 6173 6174 7472 2866 756e    if hasattr(fun
+00003180: 2c20 276d 6574 686f 645f 6861 6e64 6c65  , 'method_handle
+00003190: 725f 7772 6170 7065 6427 293a 0a20 2020  r_wrapped'):.   
+000031a0: 2072 6574 7572 6e20 6675 6e0a 0a20 2040   return fun..  @
+000031b0: 6675 6e63 746f 6f6c 732e 7772 6170 7328  functools.wraps(
+000031c0: 6675 6e29 0a20 2064 6566 2077 7261 7070  fun).  def wrapp
+000031d0: 6564 5f6d 6f64 756c 655f 6d65 7468 6f64  ed_module_method
+000031e0: 282a 6172 6773 2c20 2a2a 6b77 6172 6773  (*args, **kwargs
+000031f0: 293a 0a20 2020 2023 2057 6520 6d69 6768  ):.    # We migh
+00003200: 7420 6861 7665 2069 6e63 6f72 7265 6374  t have incorrect
+00003210: 6c79 2077 7261 7070 7065 6420 6120 6361  ly wrappped a ca
+00003220: 6c6c 6162 6c65 0a20 2020 2023 2074 6861  llable.    # tha
+00003230: 7420 6973 206e 6f74 2061 206d 6574 686f  t is not a metho
+00003240: 642e 2043 6865 636b 2077 6865 7468 6572  d. Check whether
+00003250: 2074 6865 2066 6972 7374 2061 7267 2069   the first arg i
+00003260: 7320 7365 6c66 2c0a 2020 2020 2320 6f74  s self,.    # ot
+00003270: 6865 7277 6973 6520 6361 6c6c 2074 6865  herwise call the
+00003280: 2077 7261 7070 6564 2066 756e 6374 696f   wrapped functio
+00003290: 6e20 6173 2069 732e 0a20 2020 2069 6620  n as is..    if 
+000032a0: 6172 6773 2061 6e64 2069 7369 6e73 7461  args and isinsta
+000032b0: 6e63 6528 6172 6773 5b30 5d2c 204d 6f64  nce(args[0], Mod
+000032c0: 756c 6529 3a0a 2020 2020 2020 7365 6c66  ule):.      self
+000032d0: 2c20 6172 6773 203d 2061 7267 735b 305d  , args = args[0]
+000032e0: 2c20 6172 6773 5b31 3a5d 0a20 2020 2020  , args[1:].     
+000032f0: 2072 6574 7572 6e20 7365 6c66 2e5f 6361   return self._ca
+00003300: 6c6c 5f77 7261 7070 6564 5f6d 6574 686f  ll_wrapped_metho
+00003310: 6428 6675 6e2c 2061 7267 732c 206b 7761  d(fun, args, kwa
+00003320: 7267 7329 0a20 2020 2065 6c73 653a 0a20  rgs).    else:. 
+00003330: 2020 2020 2072 6574 7572 6e20 6675 6e28       return fun(
+00003340: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+00003350: 0a0a 2020 7772 6170 7065 645f 6d6f 6475  ..  wrapped_modu
+00003360: 6c65 5f6d 6574 686f 642e 6d65 7468 6f64  le_method.method
+00003370: 5f68 616e 646c 6572 5f77 7261 7070 6564  _handler_wrapped
+00003380: 203d 2054 7275 6520 2023 2074 7970 653a   = True  # type:
+00003390: 2069 676e 6f72 655b 6174 7472 2d64 6566   ignore[attr-def
+000033a0: 696e 6564 5d0a 2020 7265 7475 726e 2077  ined].  return w
+000033b0: 7261 7070 6564 5f6d 6f64 756c 655f 6d65  rapped_module_me
+000033c0: 7468 6f64 0a0a 0a64 6566 2077 7261 705f  thod...def wrap_
+000033d0: 6465 7363 7269 7074 6f72 5f6f 6e63 6528  descriptor_once(
+000033e0: 6465 7363 7269 7074 6f72 2920 2d3e 2027  descriptor) -> '
+000033f0: 4465 7363 7269 7074 6f72 5772 6170 7065  DescriptorWrappe
+00003400: 7227 3a0a 2020 2222 2257 7261 7073 2061  r':.  """Wraps a
+00003410: 2064 6573 6372 6970 746f 7220 746f 2067   descriptor to g
+00003420: 6976 6520 6265 7474 6572 2065 7272 6f72  ive better error
+00003430: 206d 6573 7361 6765 732e 0a0a 2020 4172   messages...  Ar
+00003440: 6773 3a0a 2020 2020 7072 6f70 3a20 5573  gs:.    prop: Us
+00003450: 6572 2d64 6566 696e 6564 204d 6f64 756c  er-defined Modul
+00003460: 6520 6174 7472 6962 7574 6520 6465 7363  e attribute desc
+00003470: 7269 7074 6f72 2e0a 2020 5265 7475 726e  riptor..  Return
+00003480: 733a 0a20 2020 2057 7261 7070 6564 2064  s:.    Wrapped d
+00003490: 6573 6372 6970 746f 722e 0a20 2022 2222  escriptor..  """
+000034a0: 0a20 2023 2044 6f6e 2774 2072 6577 7261  .  # Don't rewra
+000034b0: 7020 6465 7363 7269 7074 6f72 732e 0a20  p descriptors.. 
+000034c0: 2069 6620 6973 696e 7374 616e 6365 2864   if isinstance(d
+000034d0: 6573 6372 6970 746f 722c 2044 6573 6372  escriptor, Descr
+000034e0: 6970 746f 7257 7261 7070 6572 293a 0a20  iptorWrapper):. 
+000034f0: 2020 2072 6574 7572 6e20 6465 7363 7269     return descri
+00003500: 7074 6f72 0a0a 2020 7265 7475 726e 2063  ptor..  return c
+00003510: 7265 6174 655f 6465 7363 7269 7074 6f72  reate_descriptor
+00003520: 5f77 7261 7070 6572 2864 6573 6372 6970  _wrapper(descrip
+00003530: 746f 7229 0a0a 0a64 6566 205f 7772 6170  tor)...def _wrap
+00003540: 5f68 6173 6828 6861 7368 5f66 6e3a 2043  _hash(hash_fn: C
+00003550: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
+00003560: 5d29 202d 3e20 4361 6c6c 6162 6c65 5b2e  ]) -> Callable[.
+00003570: 2e2e 2c20 416e 795d 3a0a 2020 2222 2257  .., Any]:.  """W
+00003580: 7261 7073 2061 2068 6173 6820 6675 6e63  raps a hash func
+00003590: 7469 6f6e 2077 6974 6820 736f 6d65 2063  tion with some c
+000035a0: 6865 636b 2066 6f72 2046 6c61 7820 4d6f  heck for Flax Mo
+000035b0: 6475 6c65 732e 2222 220a 0a20 2040 6675  dules."""..  @fu
+000035c0: 6e63 746f 6f6c 732e 7772 6170 7328 6861  nctools.wraps(ha
+000035d0: 7368 5f66 6e29 0a20 2064 6566 2077 7261  sh_fn).  def wra
+000035e0: 7070 6564 2873 656c 6629 3a0a 2020 2020  pped(self):.    
+000035f0: 6966 2073 656c 662e 7363 6f70 6520 6973  if self.scope is
+00003600: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
+00003610: 2072 6169 7365 2054 7970 6545 7272 6f72   raise TypeError
+00003620: 2822 4361 6e27 7420 6361 6c6c 205f 5f68  ("Can't call __h
+00003630: 6173 685f 5f20 6f6e 206d 6f64 756c 6573  ash__ on modules
+00003640: 2074 6861 7420 686f 6c64 2076 6172 6961   that hold varia
+00003650: 626c 6573 2e22 290a 2020 2020 7472 793a  bles.").    try:
+00003660: 0a20 2020 2020 2068 6173 685f 7661 6c75  .      hash_valu
+00003670: 6520 3d20 6861 7368 5f66 6e28 7365 6c66  e = hash_fn(self
+00003680: 290a 2020 2020 6578 6365 7074 2054 7970  ).    except Typ
+00003690: 6545 7272 6f72 2061 7320 6578 633a 0a20  eError as exc:. 
+000036a0: 2020 2020 2072 6169 7365 2054 7970 6545       raise TypeE
+000036b0: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
+000036c0: 2746 6169 6c65 6420 746f 2068 6173 6820  'Failed to hash 
+000036d0: 466c 6178 204d 6f64 756c 652e 2020 270a  Flax Module.  '.
+000036e0: 2020 2020 2020 2020 2020 2754 6865 206d            'The m
+000036f0: 6f64 756c 6520 7072 6f62 6162 6c79 2063  odule probably c
+00003700: 6f6e 7461 696e 7320 756e 6861 7368 6162  ontains unhashab
+00003710: 6c65 2061 7474 7269 6275 7465 732e 2020  le attributes.  
+00003720: 270a 2020 2020 2020 2020 2020 6627 4d6f  '.          f'Mo
+00003730: 6475 6c65 3d7b 7365 6c66 7d27 0a20 2020  dule={self}'.   
+00003740: 2020 2029 2066 726f 6d20 6578 630a 2020     ) from exc.  
+00003750: 2020 7265 7475 726e 2068 6173 685f 7661    return hash_va
+00003760: 6c75 650a 0a20 2072 6574 7572 6e20 7772  lue..  return wr
+00003770: 6170 7065 640a 0a0a 6465 6620 5f67 6574  apped...def _get
+00003780: 5f75 6e62 6f75 6e64 5f66 6e28 6d65 7468  _unbound_fn(meth
+00003790: 6f64 5f6f 725f 666e 3a20 4361 6c6c 6162  od_or_fn: Callab
+000037a0: 6c65 5b2e 2e2e 2c20 416e 795d 2920 2d3e  le[..., Any]) ->
+000037b0: 2043 616c 6c61 626c 655b 2e2e 2e2c 2041   Callable[..., A
+000037c0: 6e79 5d3a 0a20 2022 2222 5265 7475 726e  ny]:.  """Return
+000037d0: 7320 616e 2075 6e62 6f75 6e64 2066 756e  s an unbound fun
+000037e0: 6374 696f 6e20 6672 6f6d 2061 206d 6574  ction from a met
+000037f0: 686f 6420 7468 6174 2069 7320 706f 7373  hod that is poss
+00003800: 6962 6c79 2062 6f75 6e64 2e0a 0a20 2054  ibly bound...  T
+00003810: 6869 7320 6d65 616e 7320 7468 6174 2069  his means that i
+00003820: 6620 7468 6520 7061 7373 6564 2066 756e  f the passed fun
+00003830: 6374 696f 6e20 6265 6c6f 6e67 7320 6f66  ction belongs of
+00003840: 2061 6e20 696e 7374 616e 6365 206f 6620   an instance of 
+00003850: 6120 636c 6173 732c 2074 6865 6e0a 2020  a class, then.  
+00003860: 7468 6520 7265 7475 726e 6564 2066 756e  the returned fun
+00003870: 6374 696f 6e20 646f 6573 206e 6f20 6c6f  ction does no lo
+00003880: 6e67 6572 2064 6570 656e 6420 6f6e 2074  nger depend on t
+00003890: 6865 2069 6e73 7461 6e63 652c 2077 6869  he instance, whi
+000038a0: 6368 2069 7320 7061 7373 6564 0a20 2061  ch is passed.  a
+000038b0: 7320 7468 6520 6669 7273 7420 6172 6775  s the first argu
+000038c0: 6d65 6e74 2074 6f20 7468 6520 6675 6e63  ment to the func
+000038d0: 7469 6f6e 2e0a 0a20 2041 7267 733a 0a20  tion...  Args:. 
+000038e0: 2020 206d 6574 686f 645f 6f72 5f66 6e3a     method_or_fn:
+000038f0: 2041 2063 6c61 7373 206d 6574 686f 6420   A class method 
+00003900: 6f72 2066 756e 6374 696f 6e2e 0a20 2052  or function..  R
+00003910: 6574 7572 6e73 3a0a 2020 2020 416e 2075  eturns:.    An u
+00003920: 6e62 6f75 6e64 2076 6572 7369 6f6e 206f  nbound version o
+00003930: 6620 696e 7075 7420 6675 6e63 7469 6f6e  f input function
+00003940: 2e0a 2020 2222 220a 2020 6966 2069 6e73  ..  """.  if ins
+00003950: 7065 6374 2e69 736d 6574 686f 6428 6d65  pect.ismethod(me
+00003960: 7468 6f64 5f6f 725f 666e 2920 616e 6420  thod_or_fn) and 
+00003970: 6973 696e 7374 616e 6365 280a 2020 2020  isinstance(.    
+00003980: 2020 6d65 7468 6f64 5f6f 725f 666e 2e5f    method_or_fn._
+00003990: 5f73 656c 665f 5f2c 204d 6f64 756c 650a  _self__, Module.
+000039a0: 2020 293a 2020 2320 7079 7479 7065 3a20    ):  # pytype: 
+000039b0: 6469 7361 626c 653d 6174 7472 6962 7574  disable=attribut
+000039c0: 652d 6572 726f 720a 2020 2020 6d65 7468  e-error.    meth
+000039d0: 6f64 5f6f 725f 666e 203d 206d 6574 686f  od_or_fn = metho
+000039e0: 645f 6f72 5f66 6e2e 5f5f 6675 6e63 5f5f  d_or_fn.__func__
+000039f0: 2020 2320 7079 7479 7065 3a20 6469 7361    # pytype: disa
+00003a00: 626c 653d 6174 7472 6962 7574 652d 6572  ble=attribute-er
+00003a10: 726f 720a 0a20 2023 2054 6865 206d 6574  ror..  # The met
+00003a20: 686f 6420 7368 6f75 6c64 2062 6520 6361  hod should be ca
+00003a30: 6c6c 6162 6c65 2c20 616e 6420 6974 2073  llable, and it s
+00003a40: 686f 756c 6420 6861 7665 2061 7420 6c65  hould have at le
+00003a50: 6173 7420 6f6e 6520 6172 6775 6d65 6e74  ast one argument
+00003a60: 0a20 2023 2072 6570 7265 7365 6e74 696e  .  # representin
+00003a70: 6720 7468 6520 636c 6173 7320 7468 6174  g the class that
+00003a80: 2069 7320 7061 7373 6564 2069 6e2e 0a20   is passed in.. 
+00003a90: 2069 6620 280a 2020 2020 2020 6e6f 7420   if (.      not 
+00003aa0: 6361 6c6c 6162 6c65 286d 6574 686f 645f  callable(method_
+00003ab0: 6f72 5f66 6e29 0a20 2020 2020 206f 7220  or_fn).      or 
+00003ac0: 6c65 6e28 696e 7370 6563 742e 7369 676e  len(inspect.sign
+00003ad0: 6174 7572 6528 6d65 7468 6f64 5f6f 725f  ature(method_or_
+00003ae0: 666e 292e 7061 7261 6d65 7465 7273 2920  fn).parameters) 
+00003af0: 3c20 310a 2020 293a 0a20 2020 2072 6169  < 1.  ):.    rai
+00003b00: 7365 2065 7272 6f72 732e 4170 706c 794d  se errors.ApplyM
+00003b10: 6f64 756c 6549 6e76 616c 6964 4d65 7468  oduleInvalidMeth
+00003b20: 6f64 4572 726f 7228 6d65 7468 6f64 5f6f  odError(method_o
+00003b30: 725f 666e 290a 0a20 2072 6574 7572 6e20  r_fn)..  return 
+00003b40: 6d65 7468 6f64 5f6f 725f 666e 0a0a 0a64  method_or_fn...d
+00003b50: 6566 205f 6d61 705f 7375 626d 6f64 756c  ef _map_submodul
+00003b60: 6573 2866 6e3a 2043 616c 6c61 626c 655b  es(fn: Callable[
+00003b70: 5b27 4d6f 6475 6c65 275d 2c20 416e 795d  ['Module'], Any]
+00003b80: 2c20 7472 6565 293a 0a20 2022 2222 4d61  , tree):.  """Ma
+00003b90: 7020 6120 6675 6e63 7469 6f6e 206f 7665  p a function ove
+00003ba0: 7220 616c 6c20 7375 626d 6f64 756c 6573  r all submodules
+00003bb0: 2069 6e20 6120 7472 6565 2e22 2222 0a20   in a tree.""". 
+00003bc0: 2067 203d 206c 616d 6264 6120 5f2c 2078   g = lambda _, x
+00003bd0: 3a20 666e 2878 2920 6966 2069 7369 6e73  : fn(x) if isins
+00003be0: 7461 6e63 6528 782c 204d 6f64 756c 6529  tance(x, Module)
+00003bf0: 2065 6c73 6520 780a 2020 7265 7475 726e   else x.  return
+00003c00: 205f 6672 6565 7a65 5f61 7474 7228 5f6d   _freeze_attr(_m
+00003c10: 6170 5f6f 7665 725f 6d6f 6475 6c65 735f  ap_over_modules_
+00003c20: 696e 5f74 7265 6528 672c 2074 7265 6529  in_tree(g, tree)
+00003c30: 290a 0a0a 636c 6173 7320 5365 7475 7053  )...class SetupS
+00003c40: 7461 7465 2865 6e75 6d2e 496e 7445 6e75  tate(enum.IntEnu
+00003c50: 6d29 3a0a 2020 2320 7365 7475 7028 2920  m):.  # setup() 
+00003c60: 6861 7320 6e6f 7420 6265 656e 2063 616c  has not been cal
+00003c70: 6c65 642e 0a20 204e 4557 203d 2030 0a20  led..  NEW = 0. 
+00003c80: 2023 2073 6574 7570 2829 2068 6173 2062   # setup() has b
+00003c90: 6565 6e20 6361 6c6c 6564 206f 7574 7369  een called outsi
+00003ca0: 6465 2061 2074 7261 6e73 666f 726d 2062  de a transform b
+00003cb0: 6f75 6e64 6172 792e 0a20 2054 5241 4e53  oundary..  TRANS
+00003cc0: 464f 524d 4544 203d 2031 0a20 2023 2073  FORMED = 1.  # s
+00003cd0: 6574 7570 2829 2068 6173 2062 6565 6e20  etup() has been 
+00003ce0: 6361 6c6c 6564 2e0a 2020 444f 4e45 203d  called..  DONE =
+00003cf0: 2032 0a0a 0a40 6461 7461 636c 6173 7365   2...@dataclasse
+00003d00: 732e 6461 7461 636c 6173 730a 636c 6173  s.dataclass.clas
+00003d10: 7320 5f4d 6f64 756c 6549 6e74 6572 6e61  s _ModuleInterna
+00003d20: 6c53 7461 7465 3a0a 2020 2222 2245 7068  lState:.  """Eph
+00003d30: 656d 6572 616c 204d 6f64 756c 6520 4576  emeral Module Ev
+00003d40: 616c 7561 7469 6f6e 2053 7461 7465 2e0a  aluation State..
+00003d50: 0a20 2046 6f72 2063 6c61 7269 7479 2c20  .  For clarity, 
+00003d60: 7765 2063 6f6c 6c65 6374 2061 6c6c 206f  we collect all o
+00003d70: 6620 7468 6520 7465 6d70 6f72 6172 7920  f the temporary 
+00003d80: 666c 6167 7320 616e 6420 6570 6865 6d65  flags and epheme
+00003d90: 7261 6c20 7374 6174 6520 7573 6564 2062  ral state used b
+00003da0: 790a 2020 4d6f 6475 6c65 7320 666f 7220  y.  Modules for 
+00003db0: 6175 746f 6e61 6d69 6e67 2061 6e64 2065  autonaming and e
+00003dc0: 7272 6f72 206d 6573 7361 6765 7320 6865  rror messages he
+00003dd0: 7265 2c20 616c 6f6e 6773 6964 6520 7468  re, alongside th
+00003de0: 6520 7275 6c65 7320 7573 6564 0a20 2074  e rules used.  t
+00003df0: 6f20 7061 7373 2074 6869 7320 6570 6865  o pass this ephe
+00003e00: 6d65 7261 6c20 7374 6174 6520 6163 726f  meral state acro
+00003e10: 7373 2074 7261 6e73 666f 726d 2062 6f75  ss transform bou
+00003e20: 6e64 6172 6965 732e 0a20 2022 2222 0a0a  ndaries..  """..
+00003e30: 2020 696e 5f63 6f6d 7061 6374 5f6d 6574    in_compact_met
+00003e40: 686f 643a 2062 6f6f 6c20 3d20 4661 6c73  hod: bool = Fals
+00003e50: 650a 2020 696e 5f73 6574 7570 3a20 626f  e.  in_setup: bo
+00003e60: 6f6c 203d 2046 616c 7365 0a20 2073 6574  ol = False.  set
+00003e70: 7570 5f63 616c 6c65 643a 2053 6574 7570  up_called: Setup
+00003e80: 5374 6174 6520 3d20 5365 7475 7053 7461  State = SetupSta
+00003e90: 7465 2e4e 4557 0a20 2069 735f 696e 6974  te.NEW.  is_init
+00003ea0: 6961 6c69 7a65 643a 2062 6f6f 6c20 3d20  ialized: bool = 
+00003eb0: 4661 6c73 650a 2020 6175 746f 6e61 6d65  False.  autoname
+00003ec0: 5f63 7572 736f 723a 2044 6963 745b 7374  _cursor: Dict[st
+00003ed0: 722c 2069 6e74 5d20 3d20 6461 7461 636c  r, int] = datacl
+00003ee0: 6173 7365 732e 6669 656c 6428 6465 6661  asses.field(defa
+00003ef0: 756c 745f 6661 6374 6f72 793d 6469 6374  ult_factory=dict
+00003f00: 290a 2020 6368 696c 6472 656e 3a20 4469  ).  children: Di
+00003f10: 6374 5b73 7472 2c20 556e 696f 6e5b 7374  ct[str, Union[st
+00003f20: 722c 2027 4d6f 6475 6c65 275d 5d20 3d20  r, 'Module']] = 
+00003f30: 6461 7461 636c 6173 7365 732e 6669 656c  dataclasses.fiel
+00003f40: 6428 0a20 2020 2020 2064 6566 6175 6c74  d(.      default
+00003f50: 5f66 6163 746f 7279 3d64 6963 740a 2020  _factory=dict.  
+00003f60: 290a 0a20 2064 6566 2072 6573 6574 2873  )..  def reset(s
+00003f70: 656c 6629 202d 3e20 4e6f 6e65 3a0a 2020  elf) -> None:.  
+00003f80: 2020 2222 2252 6573 6574 7320 7472 616e    """Resets tran
+00003f90: 7369 656e 7420 7374 6174 652e 0a0a 2020  sient state...  
+00003fa0: 2020 5468 6973 2066 756e 6374 696f 6e20    This function 
+00003fb0: 6973 2063 616c 6c65 6420 6166 7465 7220  is called after 
+00003fc0: 6561 6368 206d 6f64 756c 6520 6d65 7468  each module meth
+00003fd0: 6f64 2c20 736f 206f 6e6c 7920 6174 7472  od, so only attr
+00003fe0: 6962 7574 6573 2074 6861 740a 2020 2020  ibutes that.    
+00003ff0: 6172 6520 6d65 7468 6f64 2d64 6570 656e  are method-depen
+00004000: 6465 6e74 2061 7265 2072 6573 6574 2e0a  dent are reset..
+00004010: 2020 2020 2222 220a 2020 2020 7365 6c66      """.    self
+00004020: 2e69 6e5f 636f 6d70 6163 745f 6d65 7468  .in_compact_meth
+00004030: 6f64 203d 2046 616c 7365 0a20 2020 2073  od = False.    s
+00004040: 656c 662e 696e 5f73 6574 7570 203d 2046  elf.in_setup = F
+00004050: 616c 7365 0a20 2020 2073 656c 662e 6175  alse.    self.au
+00004060: 746f 6e61 6d65 5f63 7572 736f 7220 3d20  toname_cursor = 
+00004070: 6469 6374 2829 0a0a 2020 6465 6620 6578  dict()..  def ex
+00004080: 706f 7274 2873 656c 6629 202d 3e20 275f  port(self) -> '_
+00004090: 4d6f 6475 6c65 496e 7465 726e 616c 5374  ModuleInternalSt
+000040a0: 6174 6527 3a0a 2020 2020 2222 2245 7870  ate':.    """Exp
+000040b0: 6f72 7473 2074 7261 6e73 666f 726d 2d70  orts transform-p
+000040c0: 7265 7365 7276 6564 2073 7461 7465 2061  reserved state a
+000040d0: 6372 6f73 7320 7472 616e 7366 6f72 6d20  cross transform 
+000040e0: 626f 756e 6461 7279 2e22 2222 0a20 2020  boundary.""".   
+000040f0: 2073 6574 7570 5f73 7461 7465 203d 2028   setup_state = (
+00004100: 0a20 2020 2020 2020 2053 6574 7570 5374  .        SetupSt
+00004110: 6174 652e 5452 414e 5346 4f52 4d45 4420  ate.TRANSFORMED 
+00004120: 6966 2073 656c 662e 7365 7475 705f 6361  if self.setup_ca
+00004130: 6c6c 6564 2065 6c73 6520 5365 7475 7053  lled else SetupS
+00004140: 7461 7465 2e4e 4557 0a20 2020 2029 0a20  tate.NEW.    ). 
+00004150: 2020 2063 6c6f 6e65 6420 3d20 5f4d 6f64     cloned = _Mod
+00004160: 756c 6549 6e74 6572 6e61 6c53 7461 7465  uleInternalState
+00004170: 280a 2020 2020 2020 2020 696e 5f63 6f6d  (.        in_com
+00004180: 7061 6374 5f6d 6574 686f 643d 7365 6c66  pact_method=self
+00004190: 2e69 6e5f 636f 6d70 6163 745f 6d65 7468  .in_compact_meth
+000041a0: 6f64 2c0a 2020 2020 2020 2020 696e 5f73  od,.        in_s
+000041b0: 6574 7570 3d73 656c 662e 696e 5f73 6574  etup=self.in_set
+000041c0: 7570 2c0a 2020 2020 2020 2020 7365 7475  up,.        setu
+000041d0: 705f 6361 6c6c 6564 3d73 6574 7570 5f73  p_called=setup_s
+000041e0: 7461 7465 2c0a 2020 2020 2020 2020 6973  tate,.        is
+000041f0: 5f69 6e69 7469 616c 697a 6564 3d73 656c  _initialized=sel
+00004200: 662e 6973 5f69 6e69 7469 616c 697a 6564  f.is_initialized
+00004210: 2c0a 2020 2020 2020 2020 6175 746f 6e61  ,.        autona
+00004220: 6d65 5f63 7572 736f 723d 6469 6374 2873  me_cursor=dict(s
+00004230: 656c 662e 6175 746f 6e61 6d65 5f63 7572  elf.autoname_cur
+00004240: 736f 7229 2c0a 2020 2020 290a 2020 2020  sor),.    ).    
+00004250: 7265 7475 726e 2063 6c6f 6e65 640a 0a20  return cloned.. 
+00004260: 2064 6566 2072 6569 6d70 6f72 7428 7365   def reimport(se
+00004270: 6c66 2c20 6f74 6865 723a 2027 5f4d 6f64  lf, other: '_Mod
+00004280: 756c 6549 6e74 6572 6e61 6c53 7461 7465  uleInternalState
+00004290: 2729 202d 3e20 4e6f 6e65 3a0a 2020 2020  ') -> None:.    
+000042a0: 2222 2252 652d 696d 706f 7274 7320 7472  """Re-imports tr
+000042b0: 616e 7366 6f72 6d2d 7072 6573 6572 7665  ansform-preserve
+000042c0: 6420 7374 6174 6520 6672 6f6d 2061 6372  d state from acr
+000042d0: 6f73 7320 7472 616e 7366 6f72 6d20 626f  oss transform bo
+000042e0: 756e 6461 7279 2e22 2222 0a20 2020 2073  undary.""".    s
+000042f0: 656c 662e 696e 5f63 6f6d 7061 6374 5f6d  elf.in_compact_m
+00004300: 6574 686f 6420 3d20 6f74 6865 722e 696e  ethod = other.in
+00004310: 5f63 6f6d 7061 6374 5f6d 6574 686f 640a  _compact_method.
+00004320: 2020 2020 7365 6c66 2e69 6e5f 7365 7475      self.in_setu
+00004330: 7020 3d20 6f74 6865 722e 696e 5f73 6574  p = other.in_set
+00004340: 7570 0a20 2020 2073 656c 662e 6973 5f69  up.    self.is_i
+00004350: 6e69 7469 616c 697a 6564 203d 206f 7468  nitialized = oth
+00004360: 6572 2e69 735f 696e 6974 6961 6c69 7a65  er.is_initialize
+00004370: 640a 2020 2020 7365 6c66 2e61 7574 6f6e  d.    self.auton
+00004380: 616d 655f 6375 7273 6f72 203d 2064 6963  ame_cursor = dic
+00004390: 7428 6f74 6865 722e 6175 746f 6e61 6d65  t(other.autoname
+000043a0: 5f63 7572 736f 7229 0a0a 0a5f 756e 696e  _cursor)..._unin
+000043b0: 6974 6961 6c69 7a65 645f 6d6f 6475 6c65  itialized_module
+000043c0: 5f69 6e74 6572 6e61 6c5f 7374 6174 6520  _internal_state 
+000043d0: 3d20 5f4d 6f64 756c 6549 6e74 6572 6e61  = _ModuleInterna
+000043e0: 6c53 7461 7465 2829 0a0a 0a5f 554e 4445  lState()..._UNDE
+000043f0: 4649 4e45 445f 434f 5059 5f50 4943 4b4c  FINED_COPY_PICKL
+00004400: 455f 4d45 5448 4f44 5320 3d20 280a 2020  E_METHODS = (.  
+00004410: 2020 275f 5f67 6574 7374 6174 655f 5f27    '__getstate__'
+00004420: 2c0a 2020 2020 275f 5f73 6574 7374 6174  ,.    '__setstat
+00004430: 655f 5f27 2c0a 2020 2020 275f 5f67 6574  e__',.    '__get
+00004440: 6e65 7761 7267 735f 6578 5f5f 272c 0a20  newargs_ex__',. 
+00004450: 2020 2027 5f5f 7265 6475 6365 5f5f 272c     '__reduce__',
+00004460: 0a20 2020 2027 5f5f 7265 6475 6365 5f65  .    '__reduce_e
+00004470: 785f 5f27 2c0a 2020 2020 275f 5f63 6f70  x__',.    '__cop
+00004480: 795f 5f27 2c0a 2020 2020 275f 5f64 6565  y__',.    '__dee
+00004490: 7063 6f70 795f 5f27 2c0a 290a 0a0a 5f63  pcopy__',.)..._c
+000044a0: 6163 6865 733a 2027 7765 616b 7265 662e  aches: 'weakref.
+000044b0: 5765 616b 4b65 7944 6963 7469 6f6e 6172  WeakKeyDictionar
+000044c0: 795b 5363 6f70 652c 2077 6561 6b72 6566  y[Scope, weakref
+000044d0: 2e57 6561 6b56 616c 7565 4469 6374 696f  .WeakValueDictio
+000044e0: 6e61 7279 5b46 6c61 7849 642c 204d 6f64  nary[FlaxId, Mod
+000044f0: 756c 655d 5d27 203d 2028 0a20 2020 2077  ule]]' = (.    w
+00004500: 6561 6b72 6566 2e57 6561 6b4b 6579 4469  eakref.WeakKeyDi
+00004510: 6374 696f 6e61 7279 2829 0a29 0a0a 0a74  ctionary().)...t
+00004520: 7570 6c65 5f72 6564 7563 6520 3d20 6c61  uple_reduce = la
+00004530: 6d62 6461 2078 732c 2078 3a20 7873 202b  mbda xs, x: xs +
+00004540: 2028 782c 290a 7475 706c 655f 696e 6974   (x,).tuple_init
+00004550: 203d 206c 616d 6264 613a 2028 290a 0a0a   = lambda: ()...
+00004560: 6361 7074 7572 655f 6361 6c6c 5f69 6e74  capture_call_int
+00004570: 6572 6d65 6469 6174 6573 203d 206c 616d  ermediates = lam
+00004580: 6264 6120 5f2c 206d 6574 686f 645f 6e61  bda _, method_na
+00004590: 6d65 3a20 6d65 7468 6f64 5f6e 616d 6520  me: method_name 
+000045a0: 3d3d 2027 5f5f 6361 6c6c 5f5f 270a 0a0a  == '__call__'...
+000045b0: 636c 6173 7320 5061 7265 6e74 4465 7363  class ParentDesc
+000045c0: 7269 7074 6f72 3a0a 2020 2222 2257 7261  riptor:.  """Wra
+000045d0: 7073 2070 6172 656e 7420 6d6f 6475 6c65  ps parent module
+000045e0: 2072 6566 6572 656e 6365 7320 696e 2077   references in w
+000045f0: 6561 6b20 7265 6673 2e0a 0a20 2054 6869  eak refs...  Thi
+00004600: 7320 7072 6576 656e 7473 2072 6566 6572  s prevents refer
+00004610: 656e 6365 2063 7963 6c65 7320 6672 6f6d  ence cycles from
+00004620: 2066 6f72 6d69 6e67 2076 6961 2070 6172   forming via par
+00004630: 656e 7420 6c69 6e6b 7320 7768 6963 6820  ent links which 
+00004640: 6361 6e20 6c65 6164 0a20 2074 6f20 6163  can lead.  to ac
+00004650: 6369 6465 6e74 616c 204f 4f4d 7320 696e  cidental OOMs in
+00004660: 2065 6167 6572 206d 6f64 6520 6475 6520   eager mode due 
+00004670: 746f 2073 6c6f 7720 6761 7262 6167 6520  to slow garbage 
+00004680: 636f 6c6c 6563 7469 6f6e 2061 7320 7765  collection as we
+00004690: 6c6c 2061 730a 2020 7370 7572 696f 7573  ll as.  spurious
+000046a0: 2074 7261 6365 7220 6c65 616b 7320 6475   tracer leaks du
+000046b0: 7269 6e67 206a 6974 2063 6f6d 7069 6c61  ring jit compila
+000046c0: 7469 6f6e 2e0a 0a20 204e 6f74 653a 2022  tion...  Note: "
+000046d0: 6465 7363 7269 7074 6f72 7322 2061 7265  descriptors" are
+000046e0: 2074 6865 2075 6e64 6572 6c79 696e 6720   the underlying 
+000046f0: 7079 7468 6f6e 206d 6563 6861 6e69 736d  python mechanism
+00004700: 2066 6f72 2069 6d70 6c65 6d65 6e74 696e   for implementin
+00004710: 670a 2020 6479 6e61 6d69 6320 4070 726f  g.  dynamic @pro
+00004720: 7065 7274 7920 6465 636f 7261 746f 7273  perty decorators
+00004730: 2e20 2057 6520 6e65 6564 2074 6f20 7573  .  We need to us
+00004740: 6520 6120 7261 7720 6465 7363 7269 7074  e a raw descript
+00004750: 6f72 2069 6e73 7465 6164 206f 6620 7468  or instead of th
+00004760: 650a 2020 6d6f 7265 2063 6f6d 6d6f 6e20  e.  more common 
+00004770: 6465 636f 7261 746f 7220 696e 206f 7264  decorator in ord
+00004780: 6572 2074 6f20 666f 7263 6520 7468 6174  er to force that
+00004790: 2074 6865 2061 7070 726f 7072 6961 7465   the appropriate
+000047a0: 2067 6574 7465 722f 7365 7474 6572 0a20   getter/setter. 
+000047b0: 206c 6f67 6963 2061 7070 6c69 6573 2069   logic applies i
+000047c0: 6e20 7375 6263 6c61 7373 6573 2065 7665  n subclasses eve
+000047d0: 6e20 6166 7465 7220 7661 7269 6f75 7320  n after various 
+000047e0: 6461 7461 636c 6173 7320 7472 616e 7366  dataclass transf
+000047f0: 6f72 6d73 2e0a 2020 2222 220a 0a20 2064  orms..  """..  d
+00004800: 6566 205f 5f67 6574 5f5f 2873 656c 662c  ef __get__(self,
+00004810: 206f 626a 2c20 6f62 6a74 7970 653d 4e6f   obj, objtype=No
+00004820: 6e65 293a 0a20 2020 2023 2063 6865 636b  ne):.    # check
+00004830: 2069 6620 6f62 6a20 6973 204e 6f6e 652c   if obj is None,
+00004840: 2068 6170 7065 6e73 2064 7572 696e 6720   happens during 
+00004850: 2561 7574 6f72 656c 6f61 640a 2020 2020  %autoreload.    
+00004860: 6966 206f 626a 2069 7320 4e6f 6e65 3a0a  if obj is None:.
+00004870: 2020 2020 2020 7265 7475 726e 204e 6f6e        return Non
+00004880: 650a 2020 2020 7061 7265 6e74 203d 206f  e.    parent = o
+00004890: 626a 6563 742e 5f5f 6765 7461 7474 7269  bject.__getattri
+000048a0: 6275 7465 5f5f 286f 626a 2c20 275f 7061  bute__(obj, '_pa
+000048b0: 7265 6e74 5f72 6566 2729 0a20 2020 2072  rent_ref').    r
+000048c0: 6574 7572 6e20 7061 7265 6e74 2829 2069  eturn parent() i
+000048d0: 6620 6973 696e 7374 616e 6365 2870 6172  f isinstance(par
+000048e0: 656e 742c 2077 6561 6b72 6566 2e52 6566  ent, weakref.Ref
+000048f0: 6572 656e 6365 5479 7065 2920 656c 7365  erenceType) else
+00004900: 2070 6172 656e 740a 0a20 2064 6566 205f   parent..  def _
+00004910: 5f73 6574 5f5f 2873 656c 662c 206f 626a  _set__(self, obj
+00004920: 2c20 7661 6c75 6529 3a0a 2020 2020 6d61  , value):.    ma
+00004930: 7962 655f 7765 616b 203d 2077 6561 6b72  ybe_weak = weakr
+00004940: 6566 2e72 6566 2876 616c 7565 2920 6966  ef.ref(value) if
+00004950: 2069 7369 6e73 7461 6e63 6528 7661 6c75   isinstance(valu
+00004960: 652c 204d 6f64 756c 6529 2065 6c73 6520  e, Module) else 
+00004970: 7661 6c75 650a 2020 2020 6f62 6a65 6374  value.    object
+00004980: 2e5f 5f73 6574 6174 7472 5f5f 286f 626a  .__setattr__(obj
+00004990: 2c20 275f 7061 7265 6e74 5f72 6566 272c  , '_parent_ref',
+000049a0: 206d 6179 6265 5f77 6561 6b29 0a0a 0a63   maybe_weak)...c
+000049b0: 6c61 7373 2044 6573 6372 6970 746f 7228  lass Descriptor(
+000049c0: 5072 6f74 6f63 6f6c 293a 0a20 205f 5f69  Protocol):.  __i
+000049d0: 7361 6273 7472 6163 746d 6574 686f 645f  sabstractmethod_
+000049e0: 5f3a 2062 6f6f 6c0a 0a20 2064 6566 205f  _: bool..  def _
+000049f0: 5f67 6574 5f5f 2873 656c 662c 206f 626a  _get__(self, obj
+00004a00: 2c20 6f62 6a74 7970 653d 4e6f 6e65 2920  , objtype=None) 
+00004a10: 2d3e 2041 6e79 3a0a 2020 2020 2e2e 2e0a  -> Any:.    ....
+00004a20: 0a20 2064 6566 205f 5f73 6574 5f5f 2873  .  def __set__(s
+00004a30: 656c 662c 206f 626a 2c20 7661 6c75 6529  elf, obj, value)
+00004a40: 202d 3e20 4e6f 6e65 3a0a 2020 2020 2e2e   -> None:.    ..
+00004a50: 2e0a 0a20 2064 6566 205f 5f64 656c 6574  ...  def __delet
+00004a60: 655f 5f28 7365 6c66 2c20 6f62 6a29 202d  e__(self, obj) -
+00004a70: 3e20 4e6f 6e65 3a0a 2020 2020 2e2e 2e0a  > None:.    ....
+00004a80: 0a20 2064 6566 205f 5f73 6574 5f6e 616d  .  def __set_nam
+00004a90: 655f 5f28 7365 6c66 2c20 6f77 6e65 722c  e__(self, owner,
+00004aa0: 206e 616d 6529 202d 3e20 4e6f 6e65 3a0a   name) -> None:.
+00004ab0: 2020 2020 2e2e 2e0a 0a0a 636c 6173 7320      ......class 
+00004ac0: 4465 7363 7269 7074 6f72 5772 6170 7065  DescriptorWrappe
+00004ad0: 723a 0a20 2070 6173 730a 0a0a 6465 6620  r:.  pass...def 
+00004ae0: 6372 6561 7465 5f64 6573 6372 6970 746f  create_descripto
+00004af0: 725f 7772 6170 7065 7228 6465 7363 7269  r_wrapper(descri
+00004b00: 7074 6f72 3a20 4465 7363 7269 7074 6f72  ptor: Descriptor
+00004b10: 293a 0a20 2022 2222 4372 6561 7465 7320  ):.  """Creates 
+00004b20: 6120 6465 7363 7269 7074 6f72 2077 7261  a descriptor wra
+00004b30: 7070 6572 2074 6861 7420 6361 6c6c 7320  pper that calls 
+00004b40: 6120 6765 745f 666e 206f 6e20 7468 6520  a get_fn on the 
+00004b50: 6465 7363 7269 7074 6f72 2e22 2222 0a0a  descriptor."""..
+00004b60: 2020 636c 6173 7320 5f44 6573 6372 6970    class _Descrip
+00004b70: 746f 7257 7261 7070 6572 2844 6573 6372  torWrapper(Descr
+00004b80: 6970 746f 7257 7261 7070 6572 293a 0a20  iptorWrapper):. 
+00004b90: 2020 2022 2222 4120 6465 7363 7269 7074     """A descript
+00004ba0: 6f72 2074 6861 7420 6361 6e20 7772 6170  or that can wrap
+00004bb0: 2061 6e79 2064 6573 6372 6970 746f 7222   any descriptor"
+00004bc0: 2222 0a0a 2020 2020 6966 2068 6173 6174  ""..    if hasat
+00004bd0: 7472 2864 6573 6372 6970 746f 722c 2027  tr(descriptor, '
+00004be0: 5f5f 6973 6162 7374 7261 6374 6d65 7468  __isabstractmeth
+00004bf0: 6f64 5f5f 2729 3a0a 2020 2020 2020 5f5f  od__'):.      __
+00004c00: 6973 6162 7374 7261 6374 6d65 7468 6f64  isabstractmethod
+00004c10: 5f5f 203d 2064 6573 6372 6970 746f 722e  __ = descriptor.
+00004c20: 5f5f 6973 6162 7374 7261 6374 6d65 7468  __isabstractmeth
+00004c30: 6f64 5f5f 0a0a 2020 2020 6465 6620 5f5f  od__..    def __
+00004c40: 696e 6974 5f5f 2873 656c 662c 2077 7261  init__(self, wra
+00004c50: 7070 6564 3a20 4465 7363 7269 7074 6f72  pped: Descriptor
+00004c60: 293a 0a20 2020 2020 2073 656c 662e 7772  ):.      self.wr
+00004c70: 6170 7065 6420 3d20 7772 6170 7065 640a  apped = wrapped.
+00004c80: 0a20 2020 2023 2063 6f6e 6469 7469 6f6e  .    # condition
+00004c90: 616c 6c79 2064 6566 696e 6520 6465 7363  ally define desc
+00004ca0: 7269 7074 6f72 206d 6574 686f 6473 0a20  riptor methods. 
+00004cb0: 2020 2069 6620 6861 7361 7474 7228 6465     if hasattr(de
+00004cc0: 7363 7269 7074 6f72 2c20 275f 5f67 6574  scriptor, '__get
+00004cd0: 5f5f 2729 3a0a 0a20 2020 2020 2064 6566  __'):..      def
+00004ce0: 205f 5f67 6574 5f5f 2873 656c 662c 202a   __get__(self, *
+00004cf0: 6172 6773 2c20 2a2a 6b77 6172 6773 293a  args, **kwargs):
+00004d00: 0a20 2020 2020 2020 2023 2068 6572 6520  .        # here 
+00004d10: 7765 2077 696c 6c20 6361 7463 6820 696e  we will catch in
+00004d20: 7465 726e 616c 2041 7474 7269 6275 7465  ternal Attribute
+00004d30: 4572 726f 7220 616e 6420 7265 2d72 6169  Error and re-rai
+00004d40: 7365 2069 7420 6173 2061 0a20 2020 2020  se it as a.     
+00004d50: 2020 2023 206d 6f72 6520 696e 666f 726d     # more inform
+00004d60: 6174 6976 6520 616e 6420 636f 7272 6563  ative and correc
+00004d70: 7420 6572 726f 7220 6d65 7373 6167 652e  t error message.
+00004d80: 0a20 2020 2020 2020 2074 7279 3a0a 2020  .        try:.  
+00004d90: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+00004da0: 656c 662e 7772 6170 7065 642e 5f5f 6765  elf.wrapped.__ge
+00004db0: 745f 5f28 2a61 7267 732c 202a 2a6b 7761  t__(*args, **kwa
+00004dc0: 7267 7329 0a20 2020 2020 2020 2065 7863  rgs).        exc
+00004dd0: 6570 7420 4174 7472 6962 7574 6545 7272  ept AttributeErr
+00004de0: 6f72 2061 7320 653a 0a20 2020 2020 2020  or as e:.       
+00004df0: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
+00004e00: 4465 7363 7269 7074 6f72 4174 7472 6962  DescriptorAttrib
+00004e10: 7574 6545 7272 6f72 2829 2066 726f 6d20  uteError() from 
+00004e20: 650a 0a20 2020 2069 6620 6861 7361 7474  e..    if hasatt
+00004e30: 7228 6465 7363 7269 7074 6f72 2c20 275f  r(descriptor, '_
+00004e40: 5f73 6574 5f5f 2729 3a0a 0a20 2020 2020  _set__'):..     
+00004e50: 2064 6566 205f 5f73 6574 5f5f 2873 656c   def __set__(sel
+00004e60: 662c 202a 6172 6773 2c20 2a2a 6b77 6172  f, *args, **kwar
+00004e70: 6773 293a 0a20 2020 2020 2020 2072 6574  gs):.        ret
+00004e80: 7572 6e20 7365 6c66 2e77 7261 7070 6564  urn self.wrapped
+00004e90: 2e5f 5f73 6574 5f5f 282a 6172 6773 2c20  .__set__(*args, 
+00004ea0: 2a2a 6b77 6172 6773 290a 0a20 2020 2069  **kwargs)..    i
+00004eb0: 6620 6861 7361 7474 7228 6465 7363 7269  f hasattr(descri
+00004ec0: 7074 6f72 2c20 275f 5f64 656c 6574 655f  ptor, '__delete_
+00004ed0: 5f27 293a 0a0a 2020 2020 2020 6465 6620  _'):..      def 
+00004ee0: 5f5f 6465 6c65 7465 5f5f 2873 656c 662c  __delete__(self,
+00004ef0: 202a 6172 6773 2c20 2a2a 6b77 6172 6773   *args, **kwargs
+00004f00: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur
+00004f10: 6e20 7365 6c66 2e77 7261 7070 6564 2e5f  n self.wrapped._
+00004f20: 5f64 656c 6574 655f 5f28 2a61 7267 732c  _delete__(*args,
+00004f30: 202a 2a6b 7761 7267 7329 0a0a 2020 2020   **kwargs)..    
+00004f40: 6966 2068 6173 6174 7472 2864 6573 6372  if hasattr(descr
+00004f50: 6970 746f 722c 2027 5f5f 7365 745f 6e61  iptor, '__set_na
+00004f60: 6d65 5f5f 2729 3a0a 0a20 2020 2020 2064  me__'):..      d
+00004f70: 6566 205f 5f73 6574 5f6e 616d 655f 5f28  ef __set_name__(
+00004f80: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
+00004f90: 7761 7267 7329 3a0a 2020 2020 2020 2020  wargs):.        
+00004fa0: 7365 6c66 2e77 7261 7070 6564 2e5f 5f73  self.wrapped.__s
+00004fb0: 6574 5f6e 616d 655f 5f28 2a61 7267 732c  et_name__(*args,
+00004fc0: 202a 2a6b 7761 7267 7329 0a0a 2020 2020   **kwargs)..    
+00004fd0: 6465 6620 5f5f 6765 7461 7474 725f 5f28  def __getattr__(
+00004fe0: 7365 6c66 2c20 6e61 6d65 293a 0a20 2020  self, name):.   
+00004ff0: 2020 2072 6574 7572 6e20 6765 7461 7474     return getatt
+00005000: 7228 7365 6c66 2e77 7261 7070 6564 2c20  r(self.wrapped, 
+00005010: 6e61 6d65 290a 0a20 2072 6574 7572 6e20  name)..  return 
+00005020: 5f44 6573 6372 6970 746f 7257 7261 7070  _DescriptorWrapp
+00005030: 6572 2864 6573 6372 6970 746f 7229 0a0a  er(descriptor)..
+00005040: 0a23 2042 6173 6520 4d6f 6475 6c65 2064  .# Base Module d
+00005050: 6566 696e 6974 696f 6e2e 0a23 202d 2d2d  efinition..# ---
+00005060: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00005070: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00005080: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00005090: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000050a0: 2d2d 2d2d 2d2d 2d2d 2d2d 0a0a 0a23 2054  ----------...# T
+000050b0: 6865 204d 6f64 756c 6542 6173 6520 636c  he ModuleBase cl
+000050c0: 6173 7320 6973 2063 7265 6174 6564 206f  ass is created o
+000050d0: 6e6c 7920 746f 206d 616b 6520 7374 6174  nly to make stat
+000050e0: 6963 2061 6e61 6c79 7a65 7273 2068 6170  ic analyzers hap
+000050f0: 7079 0a23 206d 6169 6e6c 7920 7079 7479  py.# mainly pyty
+00005100: 7065 2061 6e64 2070 7972 6967 6874 2e20  pe and pyright. 
+00005110: 536f 6d65 206e 6f74 6573 3a0a 2320 2a20  Some notes:.# * 
+00005120: 7079 7269 6768 7420 2863 6f72 7265 6374  pyright (correct
+00005130: 6c79 2920 636f 6d70 6c61 696e 7320 7468  ly) complains th
+00005140: 6174 204d 6f64 756c 6520 6974 7365 6c66  at Module itself
+00005150: 2069 7320 6e6f 7420 6120 6461 7461 636c   is not a datacl
+00005160: 6173 732c 2065 7665 6e0a 2320 2020 7468  ass, even.#   th
+00005170: 6f75 6768 2061 6c6c 2069 7473 2073 7562  ough all its sub
+00005180: 636c 6173 7365 7320 616e 6420 696e 7461  classes and inta
+00005190: 6e63 6573 2041 5245 2064 6174 6163 6c61  nces ARE datacla
+000051a0: 7373 6573 2e20 4265 6361 7573 6520 7468  sses. Because th
+000051b0: 6572 6520 6973 206e 6f0a 2320 2020 7761  ere is no.#   wa
+000051c0: 7920 746f 2061 6e6e 6f74 6174 6520 7468  y to annotate th
+000051d0: 6973 2069 6e20 6120 7761 7920 7468 6174  is in a way that
+000051e0: 2070 7972 6967 6874 2075 6e64 6572 7374   pyright underst
+000051f0: 616e 6473 2c20 7765 2063 7265 6174 6520  ands, we create 
+00005200: 610a 2320 2020 4d6f 6475 6c65 4261 7365  a.#   ModuleBase
+00005210: 2063 6c61 7373 2064 6563 6f72 6174 6564   class decorated
+00005220: 2077 6974 6820 6064 6174 6163 6c61 7373   with `dataclass
+00005230: 5f74 7261 6e73 666f 726d 6020 7375 6368  _transform` such
+00005240: 2074 6861 7420 7079 7269 6768 740a 2320   that pyright.# 
+00005250: 2020 7468 696e 6b73 204d 6f64 756c 6520    thinks Module 
+00005260: 6973 2061 2064 6174 6163 6c61 7373 2028  is a dataclass (
+00005270: 696e 2072 6561 6c69 7479 206f 6e6c 7920  in reality only 
+00005280: 7375 6263 6c61 7373 6573 2061 7265 2069  subclasses are i
+00005290: 6e73 7461 6e74 6961 7465 640a 2320 2020  nstantiated.#   
+000052a0: 736f 2074 6869 7320 6973 2066 696e 6529  so this is fine)
+000052b0: 2e0a 2320 2a20 5468 6520 605f 5f64 6174  ..# * The `__dat
+000052c0: 6163 6c61 7373 5f66 6965 6c64 735f 5f60  aclass_fields__`
+000052d0: 2061 7474 7269 6275 7465 2069 7320 6e65   attribute is ne
+000052e0: 6564 6564 2062 6563 6175 7365 2070 7974  eded because pyt
+000052f0: 7970 6520 7365 656d 7320 746f 0a23 2020  ype seems to.#  
+00005300: 206e 6f74 2075 6e64 6572 7374 616e 6420   not understand 
+00005310: 7468 6520 6064 6174 6163 6c61 7373 5f74  the `dataclass_t
+00005320: 7261 6e73 666f 726d 6020 6465 636f 7261  ransform` decora
+00005330: 746f 722c 2074 6865 7265 666f 7265 2077  tor, therefore w
+00005340: 6520 6e65 6564 0a23 2020 2074 6f20 6164  e need.#   to ad
+00005350: 6420 7468 6520 6174 7472 6962 7574 6520  d the attribute 
+00005360: 6d61 6e75 616c 6c79 2e0a 2320 2a20 4f74  manually..# * Ot
+00005370: 6865 7220 6174 7472 6962 7574 6573 2061  her attributes a
+00005380: 7265 2061 6e6e 6f74 6174 6564 2066 6f72  re annotated for
+00005390: 2063 6f6d 706c 6574 656e 6573 732e 2042   completeness. B
+000053a0: 6563 6175 7365 2077 6520 6172 6520 7573  ecause we are us
+000053b0: 696e 670a 2320 2020 7468 6520 6069 6620  ing.#   the `if 
+000053c0: 7479 7069 6e67 2e54 5950 455f 4348 4543  typing.TYPE_CHEC
+000053d0: 4b49 4e47 6020 7061 7474 6572 6e2c 2074  KING` pattern, t
+000053e0: 6865 7365 2061 6e6e 6f74 6174 696f 6e73  hese annotations
+000053f0: 2061 7265 206e 6f74 2070 7265 7365 6e74   are not present
+00005400: 0a23 2020 2061 7420 7275 6e74 696d 6520  .#   at runtime 
+00005410: 736f 2074 6865 7920 646f 6e27 7420 6166  so they don't af
+00005420: 6665 6374 2074 6865 2064 6174 6163 6c61  fect the datacla
+00005430: 7373 2062 6568 6176 696f 722e 0a40 6461  ss behavior..@da
+00005440: 7461 636c 6173 735f 7472 616e 7366 6f72  taclass_transfor
+00005450: 6d28 290a 636c 6173 7320 4d6f 6475 6c65  m().class Module
+00005460: 4261 7365 3a0a 2020 6966 2074 7970 696e  Base:.  if typin
+00005470: 672e 5459 5045 5f43 4845 434b 494e 473a  g.TYPE_CHECKING:
+00005480: 0a20 2020 2073 636f 7065 3a20 4f70 7469  .    scope: Opti
+00005490: 6f6e 616c 5b53 636f 7065 5d0a 2020 2020  onal[Scope].    
+000054a0: 5f73 7461 7465 3a20 5f4d 6f64 756c 6549  _state: _ModuleI
+000054b0: 6e74 6572 6e61 6c53 7461 7465 0a20 2020  nternalState.   
+000054c0: 205f 7061 7265 6e74 5f72 6566 3a20 556e   _parent_ref: Un
+000054d0: 696f 6e5b 274d 6f64 756c 6527 2c20 7765  ion['Module', we
+000054e0: 616b 7265 662e 5265 6665 7265 6e63 6554  akref.ReferenceT
+000054f0: 7970 655b 274d 6f64 756c 6527 5d2c 204e  ype['Module'], N
+00005500: 6f6e 655d 0a20 2020 2070 6172 656e 743a  one].    parent:
+00005510: 2055 6e69 6f6e 5b27 4d6f 6475 6c65 272c   Union['Module',
+00005520: 205f 5365 6e74 696e 656c 2c20 4e6f 6e65   _Sentinel, None
+00005530: 5d0a 2020 2020 5f5f 6461 7461 636c 6173  ].    __dataclas
+00005540: 735f 6669 656c 6473 5f5f 3a20 4469 6374  s_fields__: Dict
+00005550: 5b73 7472 2c20 6461 7461 636c 6173 7365  [str, dataclasse
+00005560: 732e 4669 656c 645d 0a0a 0a63 6c61 7373  s.Field]...class
+00005570: 204d 6f64 756c 6528 4d6f 6475 6c65 4261   Module(ModuleBa
+00005580: 7365 293a 0a20 2022 2222 4261 7365 2063  se):.  """Base c
+00005590: 6c61 7373 2066 6f72 2061 6c6c 206e 6575  lass for all neu
+000055a0: 7261 6c20 6e65 7477 6f72 6b20 6d6f 6475  ral network modu
+000055b0: 6c65 732e 204c 6179 6572 7320 616e 6420  les. Layers and 
+000055c0: 6d6f 6465 6c73 2073 686f 756c 6420 7375  models should su
+000055d0: 6263 6c61 7373 2074 6869 7320 636c 6173  bclass this clas
+000055e0: 732e 0a0a 2020 416c 6c20 466c 6178 204d  s...  All Flax M
+000055f0: 6f64 756c 6573 2061 7265 2050 7974 686f  odules are Pytho
+00005600: 6e20 332e 370a 2020 6064 6174 6163 6c61  n 3.7.  `datacla
+00005610: 7373 6573 203c 6874 7470 733a 2f2f 646f  sses <https://do
+00005620: 6373 2e70 7974 686f 6e2e 6f72 672f 332f  cs.python.org/3/
+00005630: 6c69 6272 6172 792f 6461 7461 636c 6173  library/dataclas
+00005640: 7365 732e 6874 6d6c 3e60 5f2e 2053 696e  ses.html>`_. Sin
+00005650: 6365 0a20 2064 6174 6163 6c61 7373 6573  ce.  dataclasses
+00005660: 2074 616b 6520 6f76 6572 2060 605f 5f69   take over ``__i
+00005670: 6e69 745f 5f60 602c 2079 6f75 2073 686f  nit__``, you sho
+00005680: 756c 6420 696e 7374 6561 6420 6f76 6572  uld instead over
+00005690: 7269 6465 203a 6d65 7468 3a60 7365 7475  ride :meth:`setu
+000056a0: 7060 2c0a 2020 7768 6963 6820 6973 2061  p`,.  which is a
+000056b0: 7574 6f6d 6174 6963 616c 6c79 2063 616c  utomatically cal
+000056c0: 6c65 6420 746f 2069 6e69 7469 616c 697a  led to initializ
+000056d0: 6520 7468 6520 6d6f 6475 6c65 2e0a 0a20  e the module... 
+000056e0: 204d 6f64 756c 6573 2063 616e 2063 6f6e   Modules can con
+000056f0: 7461 696e 2073 7562 6d6f 6475 6c65 732c  tain submodules,
+00005700: 2061 6e64 2069 6e20 7468 6973 2077 6179   and in this way
+00005710: 2063 616e 2062 6520 6e65 7374 6564 2069   can be nested i
+00005720: 6e20 6120 7472 6565 0a20 2073 7472 7563  n a tree.  struc
+00005730: 7475 7265 2e20 5375 626d 6f64 656c 7320  ture. Submodels 
+00005740: 6361 6e20 6265 2061 7373 6967 6e65 6420  can be assigned 
+00005750: 6173 2072 6567 756c 6172 2061 7474 7269  as regular attri
+00005760: 6275 7465 7320 696e 7369 6465 2074 6865  butes inside the
+00005770: 0a20 203a 6d65 7468 3a60 7365 7475 7060  .  :meth:`setup`
+00005780: 206d 6574 686f 642e 0a0a 2020 596f 7520   method...  You 
+00005790: 6361 6e20 6465 6669 6e65 2061 7262 6974  can define arbit
+000057a0: 7261 7279 2022 666f 7277 6172 6420 7061  rary "forward pa
+000057b0: 7373 2220 6d65 7468 6f64 7320 6f6e 2079  ss" methods on y
+000057c0: 6f75 7220 4d6f 6475 6c65 2073 7562 636c  our Module subcl
+000057d0: 6173 732e 0a20 2057 6869 6c65 206e 6f20  ass..  While no 
+000057e0: 6d65 7468 6f64 7320 6172 6520 7370 6563  methods are spec
+000057f0: 6961 6c2d 6361 7365 642c 2060 605f 5f63  ial-cased, ``__c
+00005800: 616c 6c5f 5f60 6020 6973 2061 2070 6f70  all__`` is a pop
+00005810: 756c 6172 2063 686f 6963 6520 6265 6361  ular choice beca
+00005820: 7573 650a 2020 6974 2061 6c6c 6f77 7320  use.  it allows 
+00005830: 796f 7520 746f 2075 7365 206d 6f64 756c  you to use modul
+00005840: 6520 696e 7374 616e 6365 7320 6173 2069  e instances as i
+00005850: 6620 7468 6579 2061 7265 2066 756e 6374  f they are funct
+00005860: 696f 6e73 3a3a 0a0a 2020 2020 6672 6f6d  ions::..    from
+00005870: 2066 6c61 7820 696d 706f 7274 206c 696e   flax import lin
+00005880: 656e 2061 7320 6e6e 0a0a 2020 2020 636c  en as nn..    cl
+00005890: 6173 7320 4d6f 6475 6c65 286e 6e2e 4d6f  ass Module(nn.Mo
+000058a0: 6475 6c65 293a 0a20 2020 2020 2066 6561  dule):.      fea
+000058b0: 7475 7265 733a 2054 7570 6c65 5b69 6e74  tures: Tuple[int
+000058c0: 2c20 2e2e 2e5d 203d 2028 3136 2c20 3429  , ...] = (16, 4)
+000058d0: 0a0a 2020 2020 2020 6465 6620 7365 7475  ..      def setu
+000058e0: 7028 7365 6c66 293a 0a20 2020 2020 2020  p(self):.       
+000058f0: 2073 656c 662e 6465 6e73 6531 203d 2044   self.dense1 = D
+00005900: 656e 7365 2873 656c 662e 6665 6174 7572  ense(self.featur
+00005910: 6573 5b30 5d29 0a20 2020 2020 2020 2073  es[0]).        s
+00005920: 656c 662e 6465 6e73 6532 203d 2044 656e  elf.dense2 = Den
+00005930: 7365 2873 656c 662e 6665 6174 7572 6573  se(self.features
+00005940: 5b31 5d29 0a0a 2020 2020 2020 6465 6620  [1])..      def 
+00005950: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
+00005960: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur
+00005970: 6e20 7365 6c66 2e64 656e 7365 3228 6e6e  n self.dense2(nn
+00005980: 2e72 656c 7528 7365 6c66 2e64 656e 7365  .relu(self.dense
+00005990: 3128 7829 2929 0a0a 2020 4f70 7469 6f6e  1(x)))..  Option
+000059a0: 616c 6c79 2c20 666f 7220 6d6f 7265 2063  ally, for more c
+000059b0: 6f6e 6369 7365 206d 6f64 756c 6520 696d  oncise module im
+000059c0: 706c 656d 656e 7461 7469 6f6e 7320 7768  plementations wh
+000059d0: 6572 6520 7375 626d 6f64 756c 6573 0a20  ere submodules. 
+000059e0: 2064 6566 696e 6974 696f 6e73 2061 7265   definitions are
+000059f0: 2063 6f2d 6c6f 6361 7465 6420 7769 7468   co-located with
+00005a00: 2074 6865 6972 2075 7361 6765 2c20 796f   their usage, yo
+00005a10: 7520 6361 6e20 7573 6520 7468 650a 2020  u can use the.  
+00005a20: 3a6d 6574 683a 6063 6f6d 7061 6374 6020  :meth:`compact` 
+00005a30: 7772 6170 7065 722e 0a20 2022 2222 0a0a  wrapper..  """..
+00005a40: 2020 6966 2074 7970 696e 672e 5459 5045    if typing.TYPE
+00005a50: 5f43 4845 434b 494e 473a 0a0a 2020 2020  _CHECKING:..    
+00005a60: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
+00005a70: 662c 202a 6172 6773 2c20 2a2a 6b77 6172  f, *args, **kwar
+00005a80: 6773 293a 0a20 2020 2020 2023 2074 6869  gs):.      # thi
+00005a90: 7320 7374 7562 206d 616b 6573 2073 7572  s stub makes sur
+00005aa0: 6520 7079 7479 7065 2061 6363 6570 7473  e pytype accepts
+00005ab0: 2063 6f6e 7374 7275 6374 6f72 2061 7267   constructor arg
+00005ac0: 756d 656e 7473 2e0a 2020 2020 2020 7061  uments..      pa
+00005ad0: 7373 0a0a 2020 2020 6465 6620 5f5f 6361  ss..    def __ca
+00005ae0: 6c6c 5f5f 2873 656c 662c 202a 6172 6773  ll__(self, *args
+00005af0: 2c20 2a2a 6b77 6172 6773 2920 2d3e 2041  , **kwargs) -> A
+00005b00: 6e79 3a0a 2020 2020 2020 2320 7468 6973  ny:.      # this
+00005b10: 2073 7475 6220 616c 6c6f 7773 2070 7974   stub allows pyt
+00005b20: 7970 6520 746f 2061 6363 6570 7420 4d6f  ype to accept Mo
+00005b30: 6475 6c65 7320 6173 2043 616c 6c61 626c  dules as Callabl
+00005b40: 6573 2e0a 2020 2020 2020 7061 7373 0a0a  es..      pass..
+00005b50: 2020 4063 6c61 7373 6d65 7468 6f64 0a20    @classmethod. 
+00005b60: 2064 6566 205f 5f69 6e69 745f 7375 6263   def __init_subc
+00005b70: 6c61 7373 5f5f 2863 6c73 2c20 6b77 5f6f  lass__(cls, kw_o
+00005b80: 6e6c 793a 2062 6f6f 6c20 3d20 4661 6c73  nly: bool = Fals
+00005b90: 652c 202a 2a6b 7761 7267 733a 2041 6e79  e, **kwargs: Any
+00005ba0: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    "
+00005bb0: 2222 4175 746f 6d61 7469 6361 6c6c 7920  ""Automatically 
+00005bc0: 696e 6974 6961 6c69 7a65 7320 616c 6c20  initializes all 
+00005bd0: 7375 6263 6c61 7373 6573 2061 7320 6375  subclasses as cu
+00005be0: 7374 6f6d 2064 6174 6163 6c61 7373 6573  stom dataclasses
+00005bf0: 2e22 2222 0a20 2020 2073 7570 6572 2829  .""".    super()
+00005c00: 2e5f 5f69 6e69 745f 7375 6263 6c61 7373  .__init_subclass
+00005c10: 5f5f 282a 2a6b 7761 7267 7329 0a20 2020  __(**kwargs).   
+00005c20: 2023 2041 6c6c 2046 6c61 7820 4d6f 6475   # All Flax Modu
+00005c30: 6c65 7320 6172 6520 6461 7461 636c 6173  les are dataclas
+00005c40: 7365 732e 2020 5765 2066 6f72 6365 2074  ses.  We force t
+00005c50: 6869 7320 636f 6e76 656e 7469 6f6e 2073  his convention s
+00005c60: 696e 6365 0a20 2020 2023 2069 7420 656e  ince.    # it en
+00005c70: 636f 7572 6167 6573 2074 6865 2073 7461  courages the sta
+00005c80: 7465 6c65 7373 2062 6568 6176 696f 7220  teless behavior 
+00005c90: 6e65 6564 6564 2074 6f20 636c 6f6e 6520  needed to clone 
+00005ca0: 6d6f 6475 6c65 2069 6e73 7461 6e63 6573  module instances
+00005cb0: 2066 6f72 0a20 2020 2023 2066 756e 6374   for.    # funct
+00005cc0: 696f 6e61 6c20 7472 616e 7366 6f72 6d61  ional transforma
+00005cd0: 7469 6f6e 2e20 2049 6e73 7465 6164 206f  tion.  Instead o
+00005ce0: 6620 7573 696e 6720 6120 7079 7468 6f6e  f using a python
+00005cf0: 206d 6574 6163 6c61 7373 2c20 7765 0a20   metaclass, we. 
+00005d00: 2020 2023 2061 7574 6f6d 6174 6963 616c     # automatical
+00005d10: 6c79 2074 7261 6e73 666f 726d 204d 6f64  ly transform Mod
+00005d20: 756c 6573 2069 6e74 6f20 6461 7461 636c  ules into datacl
+00005d30: 6173 7365 7320 6174 2073 7562 636c 6173  asses at subclas
+00005d40: 7320 6372 6561 7469 6f6e 0a20 2020 2023  s creation.    #
+00005d50: 2074 696d 652c 2061 6e64 2077 6520 7365   time, and we se
+00005d60: 7420 7468 6520 6c61 7374 2064 6174 6163  t the last datac
+00005d70: 6c61 7373 2061 7267 756d 656e 7473 2074  lass arguments t
+00005d80: 6f20 6070 6172 656e 7460 2061 6e64 2060  o `parent` and `
+00005d90: 6e61 6d65 602e 0a20 2020 2063 6c73 2e5f  name`..    cls._
+00005da0: 6375 7374 6f6d 697a 6564 5f64 6174 6163  customized_datac
+00005db0: 6c61 7373 5f74 7261 6e73 666f 726d 286b  lass_transform(k
+00005dc0: 775f 6f6e 6c79 290a 2020 2020 2320 5765  w_only).    # We
+00005dd0: 2077 7261 7020 7573 6572 2d64 6566 696e   wrap user-defin
+00005de0: 6564 206d 6574 686f 6473 2069 6e63 6c75  ed methods inclu
+00005df0: 6469 6e67 2073 6574 7570 2061 6e64 205f  ding setup and _
+00005e00: 5f63 616c 6c5f 5f20 746f 2065 6e66 6f72  _call__ to enfor
+00005e10: 6365 0a20 2020 2023 2061 206e 756d 6265  ce.    # a numbe
+00005e20: 7220 6f66 2064 6966 6665 7265 6e74 2063  r of different c
+00005e30: 6865 636b 7320 616e 6420 746f 2070 726f  hecks and to pro
+00005e40: 7669 6465 2063 6c65 6172 2065 7272 6f72  vide clear error
+00005e50: 206d 6573 7361 6765 732e 0a20 2020 2063   messages..    c
+00005e60: 6c73 2e5f 7665 7269 6679 5f73 696e 676c  ls._verify_singl
+00005e70: 655f 6f72 5f6e 6f5f 636f 6d70 6163 7428  e_or_no_compact(
+00005e80: 290a 2020 2020 636c 732e 5f77 7261 705f  ).    cls._wrap_
+00005e90: 6d6f 6475 6c65 5f61 7474 7269 6275 7465  module_attribute
+00005ea0: 7328 290a 2020 2020 2320 5365 7420 656d  s().    # Set em
+00005eb0: 7074 7920 636c 6173 7320 6465 6661 756c  pty class defaul
+00005ec0: 7473 2e0a 2020 2020 636c 732e 5f73 7461  ts..    cls._sta
+00005ed0: 7465 203d 205f 756e 696e 6974 6961 6c69  te = _uninitiali
+00005ee0: 7a65 645f 6d6f 6475 6c65 5f69 6e74 6572  zed_module_inter
+00005ef0: 6e61 6c5f 7374 6174 6520 2023 2074 7970  nal_state  # typ
+00005f00: 653a 2069 676e 6f72 655b 6174 7472 2d64  e: ignore[attr-d
+00005f10: 6566 696e 6564 5d0a 2020 2020 636c 732e  efined].    cls.
+00005f20: 7363 6f70 653a 204f 7074 696f 6e61 6c5b  scope: Optional[
+00005f30: 5363 6f70 655d 203d 204e 6f6e 6520 2023  Scope] = None  #
+00005f40: 2074 7970 653a 2069 676e 6f72 650a 2020   type: ignore.  
+00005f50: 2020 2320 4861 6e64 6c65 7320 7765 616b    # Handles weak
+00005f60: 2072 6566 6572 656e 6369 6e67 206f 6620   referencing of 
+00005f70: 7061 7265 6e74 204d 6f64 756c 6573 2074  parent Modules t
+00005f80: 6f20 7072 6576 656e 7420 7265 6665 7265  o prevent refere
+00005f90: 6e63 6520 6379 636c 6573 2e0a 2020 2020  nce cycles..    
+00005fa0: 636c 732e 5f70 6172 656e 745f 7265 6620  cls._parent_ref 
+00005fb0: 3d20 4e6f 6e65 2020 2320 7479 7065 3a20  = None  # type: 
+00005fc0: 6967 6e6f 7265 5b61 7474 722d 6465 6669  ignore[attr-defi
+00005fd0: 6e65 645d 0a20 2020 2063 6c73 2e70 6172  ned].    cls.par
+00005fe0: 656e 7420 3d20 5061 7265 6e74 4465 7363  ent = ParentDesc
+00005ff0: 7269 7074 6f72 2829 2020 2320 7479 7065  riptor()  # type
+00006000: 3a20 6967 6e6f 7265 5b61 7373 6967 6e6d  : ignore[assignm
+00006010: 656e 745d 0a0a 2020 4063 6c61 7373 6d65  ent]..  @classme
+00006020: 7468 6f64 0a20 2064 6566 205f 6375 7374  thod.  def _cust
+00006030: 6f6d 697a 6564 5f64 6174 6163 6c61 7373  omized_dataclass
+00006040: 5f74 7261 6e73 666f 726d 2863 6c73 2c20  _transform(cls, 
+00006050: 6b77 5f6f 6e6c 793a 2062 6f6f 6c29 3a0a  kw_only: bool):.
+00006060: 2020 2020 2222 2254 7261 6e73 666f 726d      """Transform
+00006070: 7320 6063 6c73 6020 696e 746f 2061 2064  s `cls` into a d
+00006080: 6174 6163 6c61 7373 2c20 7769 7468 2063  ataclass, with c
+00006090: 7573 746f 6d20 6164 6469 7469 6f6e 616c  ustom additional
+000060a0: 2062 6568 6176 696f 722e 0a0a 2020 2020   behavior...    
+000060b0: 312e 2049 6e6a 6563 7420 6070 6172 656e  1. Inject `paren
+000060c0: 7460 2061 6e64 2060 6e61 6d65 6020 6669  t` and `name` fi
+000060d0: 656c 6473 2e20 2028 4966 2074 6865 7920  elds.  (If they 
+000060e0: 6172 6520 616c 7265 6164 7920 7072 6573  are already pres
+000060f0: 656e 742c 0a20 2020 2020 2020 7468 656e  ent,.       then
+00006100: 2063 6865 636b 2074 6861 7420 7468 6579   check that they
+00006110: 2068 6176 6520 7468 6520 6578 7065 6374   have the expect
+00006120: 6564 2074 7970 6573 2e29 0a20 2020 2032  ed types.).    2
+00006130: 2e20 5365 7420 636f 6d70 6172 652c 2068  . Set compare, h
+00006140: 6173 682c 2061 6e64 2072 6570 7220 746f  ash, and repr to
+00006150: 2046 616c 7365 2066 6f72 206e 6f6e 2d69   False for non-i
+00006160: 6e69 7420 6669 656c 6473 2e0a 2020 2020  nit fields..    
+00006170: 332e 2047 656e 6572 6174 6520 6120 6861  3. Generate a ha
+00006180: 7368 2066 756e 6374 696f 6e20 2869 6620  sh function (if 
+00006190: 6e6f 7420 7072 6f76 6964 6564 2062 7920  not provided by 
+000061a0: 636c 7329 2e0a 2020 2020 2222 220a 2020  cls)..    """.  
+000061b0: 2020 2320 4368 6563 6b20 7265 7365 7276    # Check reserv
+000061c0: 6564 2061 7474 7269 6275 7465 7320 6861  ed attributes ha
+000061d0: 7665 2065 7870 6563 7465 6420 7479 7065  ve expected type
+000061e0: 2061 6e6e 6f74 6174 696f 6e73 2e0a 2020   annotations..  
+000061f0: 2020 616e 6e6f 7461 7469 6f6e 7320 3d20    annotations = 
+00006200: 6469 6374 2863 6c73 2e5f 5f64 6963 745f  dict(cls.__dict_
+00006210: 5f2e 6765 7428 275f 5f61 6e6e 6f74 6174  _.get('__annotat
+00006220: 696f 6e73 5f5f 272c 207b 7d29 290a 2020  ions__', {})).  
+00006230: 2020 6966 2061 6e6e 6f74 6174 696f 6e73    if annotations
+00006240: 2e67 6574 2827 7061 7265 6e74 272c 205f  .get('parent', _
+00006250: 5061 7265 6e74 5479 7065 2920 213d 205f  ParentType) != _
+00006260: 5061 7265 6e74 5479 7065 3a0a 2020 2020  ParentType:.    
+00006270: 2020 7261 6973 6520 6572 726f 7273 2e52    raise errors.R
+00006280: 6573 6572 7665 644d 6f64 756c 6541 7474  eservedModuleAtt
+00006290: 7269 6275 7465 4572 726f 7228 616e 6e6f  ributeError(anno
+000062a0: 7461 7469 6f6e 7329 0a20 2020 2069 6620  tations).    if 
+000062b0: 616e 6e6f 7461 7469 6f6e 732e 6765 7428  annotations.get(
+000062c0: 276e 616d 6527 2c20 7374 7229 206e 6f74  'name', str) not
+000062d0: 2069 6e20 2827 7374 7227 2c20 7374 722c   in ('str', str,
+000062e0: 204f 7074 696f 6e61 6c5b 7374 725d 293a   Optional[str]):
+000062f0: 0a20 2020 2020 2072 6169 7365 2065 7272  .      raise err
+00006300: 6f72 732e 5265 7365 7276 6564 4d6f 6475  ors.ReservedModu
+00006310: 6c65 4174 7472 6962 7574 6545 7272 6f72  leAttributeError
+00006320: 2861 6e6e 6f74 6174 696f 6e73 290a 0a20  (annotations).. 
+00006330: 2020 2023 2061 6e79 206e 6f6e 2d69 6e69     # any non-ini
+00006340: 7420 6669 656c 6420 7769 6c6c 206f 6e6c  t field will onl
+00006350: 7920 6265 2073 6574 2069 6e20 7365 7475  y be set in setu
+00006360: 700a 2020 2020 2320 4475 7269 6e67 205f  p.    # During _
+00006370: 5f68 6173 685f 5f20 616e 6420 5f5f 6571  _hash__ and __eq
+00006380: 5f5f 2074 6865 2066 6965 6c64 2069 7320  __ the field is 
+00006390: 6e6f 7420 7365 7420 7965 740a 2020 2020  not set yet.    
+000063a0: 2320 736f 2069 7420 7368 6f75 6c64 206e  # so it should n
+000063b0: 6f74 2062 6520 7573 6564 2069 6e20 636f  ot be used in co
+000063c0: 6d70 6172 652c 2068 6173 6820 6f72 2072  mpare, hash or r
+000063d0: 6570 722e 0a20 2020 2066 6f72 2066 6965  epr..    for fie
+000063e0: 6c64 2069 6e20 616e 6e6f 7461 7469 6f6e  ld in annotation
+000063f0: 733a 0a20 2020 2020 2066 6965 6c64 5f6d  s:.      field_m
+00006400: 6574 6120 3d20 6765 7461 7474 7228 636c  eta = getattr(cl
+00006410: 732c 2066 6965 6c64 2c20 4e6f 6e65 290a  s, field, None).
+00006420: 2020 2020 2020 6966 2069 7369 6e73 7461        if isinsta
+00006430: 6e63 6528 6669 656c 645f 6d65 7461 2c20  nce(field_meta, 
+00006440: 6461 7461 636c 6173 7365 732e 4669 656c  dataclasses.Fiel
+00006450: 6429 2061 6e64 206e 6f74 2066 6965 6c64  d) and not field
+00006460: 5f6d 6574 612e 696e 6974 3a0a 2020 2020  _meta.init:.    
+00006470: 2020 2020 6669 656c 645f 6d65 7461 2e63      field_meta.c
+00006480: 6f6d 7061 7265 203d 2046 616c 7365 0a20  ompare = False. 
+00006490: 2020 2020 2020 2066 6965 6c64 5f6d 6574         field_met
+000064a0: 612e 6861 7368 203d 2046 616c 7365 0a20  a.hash = False. 
+000064b0: 2020 2020 2020 2066 6965 6c64 5f6d 6574         field_met
+000064c0: 612e 7265 7072 203d 2046 616c 7365 0a0a  a.repr = False..
+000064d0: 2020 2020 6578 7472 615f 6669 656c 6473      extra_fields
+000064e0: 203d 205b 0a20 2020 2020 2020 2028 0a20   = [.        (. 
+000064f0: 2020 2020 2020 2020 2020 2027 7061 7265             'pare
+00006500: 6e74 272c 0a20 2020 2020 2020 2020 2020  nt',.           
+00006510: 205f 5061 7265 6e74 5479 7065 2c0a 2020   _ParentType,.  
+00006520: 2020 2020 2020 2020 2020 6b77 5f6f 6e6c            kw_onl
+00006530: 795f 6461 7461 636c 6173 7365 732e 6669  y_dataclasses.fi
+00006540: 656c 6428 0a20 2020 2020 2020 2020 2020  eld(.           
+00006550: 2020 2020 2072 6570 723d 4661 6c73 652c       repr=False,
+00006560: 2064 6566 6175 6c74 3d5f 756e 7370 6563   default=_unspec
+00006570: 6966 6965 645f 7061 7265 6e74 2c20 6b77  ified_parent, kw
+00006580: 5f6f 6e6c 793d 5472 7565 0a20 2020 2020  _only=True.     
+00006590: 2020 2020 2020 2029 2c0a 2020 2020 2020         ),.      
+000065a0: 2020 292c 0a20 2020 2020 2020 2028 0a20    ),.        (. 
+000065b0: 2020 2020 2020 2020 2020 2027 6e61 6d65             'name
+000065c0: 272c 0a20 2020 2020 2020 2020 2020 204f  ',.            O
+000065d0: 7074 696f 6e61 6c5b 7374 725d 2c0a 2020  ptional[str],.  
+000065e0: 2020 2020 2020 2020 2020 6b77 5f6f 6e6c            kw_onl
+000065f0: 795f 6461 7461 636c 6173 7365 732e 6669  y_dataclasses.fi
+00006600: 656c 6428 6465 6661 756c 743d 4e6f 6e65  eld(default=None
+00006610: 2c20 6b77 5f6f 6e6c 793d 5472 7565 292c  , kw_only=True),
+00006620: 0a20 2020 2020 2020 2029 2c0a 2020 2020  .        ),.    
+00006630: 5d0a 0a20 2020 2069 6620 6b77 5f6f 6e6c  ]..    if kw_onl
+00006640: 793a 0a20 2020 2020 2069 6620 7475 706c  y:.      if tupl
+00006650: 6528 7379 732e 7665 7273 696f 6e5f 696e  e(sys.version_in
+00006660: 666f 295b 3a33 5d20 3e3d 2028 332c 2031  fo)[:3] >= (3, 1
+00006670: 302c 2030 293a 0a20 2020 2020 2020 2066  0, 0):.        f
+00006680: 6f72 206e 616d 652c 2061 6e6e 6f74 6174  or name, annotat
+00006690: 696f 6e2c 2064 6566 6175 6c74 2069 6e20  ion, default in 
+000066a0: 6578 7472 615f 6669 656c 6473 3a20 2023  extra_fields:  #
+000066b0: 2070 7974 7970 653a 2064 6973 6162 6c65   pytype: disable
+000066c0: 3d69 6e76 616c 6964 2d61 6e6e 6f74 6174  =invalid-annotat
+000066d0: 696f 6e0a 2020 2020 2020 2020 2020 7365  ion.          se
+000066e0: 7461 7474 7228 636c 732c 206e 616d 652c  tattr(cls, name,
+000066f0: 2064 6566 6175 6c74 290a 2020 2020 2020   default).      
+00006700: 2020 2020 636c 732e 5f5f 616e 6e6f 7461      cls.__annota
+00006710: 7469 6f6e 735f 5f5b 6e61 6d65 5d20 3d20  tions__[name] = 
+00006720: 616e 6e6f 7461 7469 6f6e 0a20 2020 2020  annotation.     
+00006730: 2020 2064 6174 6163 6c61 7373 6573 2e64     dataclasses.d
+00006740: 6174 6163 6c61 7373 2820 2023 2074 7970  ataclass(  # typ
+00006750: 653a 2069 676e 6f72 655b 6361 6c6c 2d6f  e: ignore[call-o
+00006760: 7665 726c 6f61 645d 0a20 2020 2020 2020  verload].       
+00006770: 2020 2020 2075 6e73 6166 655f 6861 7368       unsafe_hash
+00006780: 3d27 5f5f 6861 7368 5f5f 2720 6e6f 7420  ='__hash__' not 
+00006790: 696e 2063 6c73 2e5f 5f64 6963 745f 5f2c  in cls.__dict__,
+000067a0: 0a20 2020 2020 2020 2020 2020 2072 6570  .            rep
+000067b0: 723d 4661 6c73 652c 0a20 2020 2020 2020  r=False,.       
+000067c0: 2020 2020 206b 775f 6f6e 6c79 3d54 7275       kw_only=Tru
+000067d0: 652c 0a20 2020 2020 2020 2029 2863 6c73  e,.        )(cls
+000067e0: 290a 2020 2020 2020 656c 7365 3a0a 2020  ).      else:.  
+000067f0: 2020 2020 2020 7261 6973 6520 5479 7065        raise Type
+00006800: 4572 726f 7228 2760 6b77 5f6f 6e6c 7960  Error('`kw_only`
+00006810: 2069 7320 6e6f 7420 6176 6169 6c61 626c   is not availabl
+00006820: 6520 6265 666f 7265 2050 7920 332e 3130  e before Py 3.10
+00006830: 2e27 290a 2020 2020 656c 7365 3a0a 2020  .').    else:.  
+00006840: 2020 2020 2320 4e6f 7720 6170 706c 7920      # Now apply 
+00006850: 6461 7461 636c 6173 7320 7472 616e 7366  dataclass transf
+00006860: 6f72 6d20 2877 6869 6368 206f 7065 7261  orm (which opera
+00006870: 7465 7320 696e 2d70 6c61 6365 292e 0a20  tes in-place).. 
+00006880: 2020 2020 2023 2044 6f20 6765 6e65 7261       # Do genera
+00006890: 7465 2061 2068 6173 6820 6675 6e63 7469  te a hash functi
+000068a0: 6f6e 206f 6e6c 7920 6966 206e 6f74 2070  on only if not p
+000068b0: 726f 7669 6465 6420 6279 2074 6865 2063  rovided by the c
+000068c0: 6c61 7373 2e0a 2020 2020 2020 6b77 5f6f  lass..      kw_o
+000068d0: 6e6c 795f 6461 7461 636c 6173 7365 732e  nly_dataclasses.
+000068e0: 6461 7461 636c 6173 7328 0a20 2020 2020  dataclass(.     
+000068f0: 2020 2020 2063 6c73 2c0a 2020 2020 2020       cls,.      
+00006900: 2020 2020 756e 7361 6665 5f68 6173 683d      unsafe_hash=
+00006910: 275f 5f68 6173 685f 5f27 206e 6f74 2069  '__hash__' not i
+00006920: 6e20 636c 732e 5f5f 6469 6374 5f5f 2c0a  n cls.__dict__,.
+00006930: 2020 2020 2020 2020 2020 7265 7072 3d46            repr=F
+00006940: 616c 7365 2c0a 2020 2020 2020 2020 2020  alse,.          
+00006950: 6578 7472 615f 6669 656c 6473 3d65 7874  extra_fields=ext
+00006960: 7261 5f66 6965 6c64 732c 0a20 2020 2020  ra_fields,.     
+00006970: 2029 2020 2320 7079 7479 7065 3a20 6469   )  # pytype: di
+00006980: 7361 626c 653d 7772 6f6e 672d 6b65 7977  sable=wrong-keyw
+00006990: 6f72 642d 6172 6773 0a0a 2020 2020 636c  ord-args..    cl
+000069a0: 732e 5f5f 6861 7368 5f5f 203d 205f 7772  s.__hash__ = _wr
+000069b0: 6170 5f68 6173 6828 636c 732e 5f5f 6861  ap_hash(cls.__ha
+000069c0: 7368 5f5f 2920 2023 2074 7970 653a 2069  sh__)  # type: i
+000069d0: 676e 6f72 655b 6d65 7468 6f64 2d61 7373  gnore[method-ass
+000069e0: 6967 6e5d 0a0a 2020 4063 6c61 7373 6d65  ign]..  @classme
+000069f0: 7468 6f64 0a20 2064 6566 205f 7665 7269  thod.  def _veri
+00006a00: 6679 5f73 696e 676c 655f 6f72 5f6e 6f5f  fy_single_or_no_
+00006a10: 636f 6d70 6163 7428 636c 7329 3a0a 2020  compact(cls):.  
+00006a20: 2020 2222 2253 7461 7469 6361 6c6c 7920    """Statically 
+00006a30: 7665 7269 6669 6573 2074 6861 7420 6174  verifies that at
+00006a40: 206d 6f73 7420 6120 7369 6e67 6c65 206d   most a single m
+00006a50: 6574 686f 6420 6973 206c 6162 656c 6c65  ethod is labelle
+00006a60: 6420 636f 6d70 6163 742e 2222 220a 2020  d compact.""".  
+00006a70: 2020 6d65 7468 6f64 7320 3d20 5b6d 5b30    methods = [m[0
+00006a80: 5d20 666f 7220 6d20 696e 2069 6e73 7065  ] for m in inspe
+00006a90: 6374 2e67 6574 6d65 6d62 6572 7328 636c  ct.getmembers(cl
+00006aa0: 732c 2070 7265 6469 6361 7465 3d63 616c  s, predicate=cal
+00006ab0: 6c61 626c 6529 5d0a 2020 2020 6e5f 636f  lable)].    n_co
+00006ac0: 6d70 6163 745f 666e 7320 3d20 6c65 6e28  mpact_fns = len(
+00006ad0: 0a20 2020 2020 2020 205b 0a20 2020 2020  .        [.     
+00006ae0: 2020 2020 2020 206d 6574 686f 645f 6e61         method_na
+00006af0: 6d65 0a20 2020 2020 2020 2020 2020 2066  me.            f
+00006b00: 6f72 206d 6574 686f 645f 6e61 6d65 2069  or method_name i
+00006b10: 6e20 6d65 7468 6f64 730a 2020 2020 2020  n methods.      
+00006b20: 2020 2020 2020 6966 2068 6173 6174 7472        if hasattr
+00006b30: 2867 6574 6174 7472 2863 6c73 2c20 6d65  (getattr(cls, me
+00006b40: 7468 6f64 5f6e 616d 6529 2c20 2763 6f6d  thod_name), 'com
+00006b50: 7061 6374 2729 0a20 2020 2020 2020 205d  pact').        ]
+00006b60: 0a20 2020 2029 0a20 2020 2069 6620 6e5f  .    ).    if n_
+00006b70: 636f 6d70 6163 745f 666e 7320 3e20 313a  compact_fns > 1:
+00006b80: 0a20 2020 2020 2072 6169 7365 2065 7272  .      raise err
+00006b90: 6f72 732e 4d75 6c74 6970 6c65 4d65 7468  ors.MultipleMeth
+00006ba0: 6f64 7343 6f6d 7061 6374 4572 726f 7228  odsCompactError(
+00006bb0: 290a 0a20 2040 636c 6173 736d 6574 686f  )..  @classmetho
+00006bc0: 640a 2020 6465 6620 5f77 7261 705f 6d6f  d.  def _wrap_mo
+00006bd0: 6475 6c65 5f61 7474 7269 6275 7465 7328  dule_attributes(
+00006be0: 636c 7329 3a0a 2020 2020 2222 2257 7261  cls):.    """Wra
+00006bf0: 7073 2075 7365 722d 6465 6669 6e65 6420  ps user-defined 
+00006c00: 6e6f 6e2d 696e 6865 7269 7465 6420 6d65  non-inherited me
+00006c10: 7468 6f64 7320 616e 6420 6465 7363 7269  thods and descri
+00006c20: 7074 6f72 7320 7769 7468 2073 7461 7465  ptors with state
+00006c30: 0a20 2020 206d 616e 6167 656d 656e 7420  .    management 
+00006c40: 6675 6e63 7469 6f6e 732e 0a20 2020 2022  functions..    "
+00006c50: 2222 0a20 2020 2023 2077 7261 7020 6d65  "".    # wrap me
+00006c60: 7468 6f64 730a 2020 2020 6d65 7468 6f64  thods.    method
+00006c70: 5f65 7863 6c75 7369 6f6e 7320 3d20 5b66  _exclusions = [f
+00006c80: 2e6e 616d 6520 666f 7220 6620 696e 2064  .name for f in d
+00006c90: 6174 6163 6c61 7373 6573 2e66 6965 6c64  ataclasses.field
+00006ca0: 7328 636c 7329 5d20 2b20 5b0a 2020 2020  s(cls)] + [.    
+00006cb0: 2020 2020 275f 5f65 715f 5f27 2c0a 2020      '__eq__',.  
+00006cc0: 2020 2020 2020 275f 5f72 6570 725f 5f27        '__repr__'
+00006cd0: 2c0a 2020 2020 2020 2020 275f 5f69 6e69  ,.        '__ini
+00006ce0: 745f 5f27 2c0a 2020 2020 2020 2020 275f  t__',.        '_
+00006cf0: 5f68 6173 685f 5f27 2c0a 2020 2020 2020  _hash__',.      
+00006d00: 2020 275f 5f70 6f73 745f 696e 6974 5f5f    '__post_init__
+00006d10: 272c 0a20 2020 205d 0a20 2020 2066 6f72  ',.    ].    for
+00006d20: 206b 6579 2069 6e20 5f67 6574 5f6c 6f63   key in _get_loc
+00006d30: 616c 5f6d 6574 686f 645f 6e61 6d65 7328  al_method_names(
+00006d40: 636c 732c 2065 7863 6c75 6465 3d6d 6574  cls, exclude=met
+00006d50: 686f 645f 6578 636c 7573 696f 6e73 293a  hod_exclusions):
+00006d60: 0a20 2020 2020 206d 6574 686f 6420 3d20  .      method = 
+00006d70: 6765 7461 7474 7228 636c 732c 206b 6579  getattr(cls, key
+00006d80: 290a 2020 2020 2020 6966 2068 6173 6174  ).      if hasat
+00006d90: 7472 286d 6574 686f 642c 2027 6e6f 7772  tr(method, 'nowr
+00006da0: 6170 2729 3a0a 2020 2020 2020 2020 636f  ap'):.        co
+00006db0: 6e74 696e 7565 0a20 2020 2020 2073 6574  ntinue.      set
+00006dc0: 6174 7472 2863 6c73 2c20 6b65 792c 2077  attr(cls, key, w
+00006dd0: 7261 705f 6d65 7468 6f64 5f6f 6e63 6528  rap_method_once(
+00006de0: 6d65 7468 6f64 2929 0a0a 2020 2020 2320  method))..    # 
+00006df0: 7772 6170 2064 6573 6372 6970 746f 7273  wrap descriptors
+00006e00: 0a20 2020 2064 6573 6372 6970 746f 725f  .    descriptor_
+00006e10: 6578 636c 7573 696f 6e73 203d 205b 662e  exclusions = [f.
+00006e20: 6e61 6d65 2066 6f72 2066 2069 6e20 6461  name for f in da
+00006e30: 7461 636c 6173 7365 732e 6669 656c 6473  taclasses.fields
+00006e40: 2863 6c73 295d 202b 205b 0a20 2020 2020  (cls)] + [.     
+00006e50: 2020 2027 7061 7265 6e74 272c 0a20 2020     'parent',.   
+00006e60: 2020 2020 2027 5f5f 6469 6374 5f5f 272c       '__dict__',
+00006e70: 0a20 2020 205d 0a20 2020 2066 6f72 206b  .    ].    for k
+00006e80: 6579 2069 6e20 5f67 6574 5f6c 6f63 616c  ey in _get_local
+00006e90: 5f64 6573 6372 6970 746f 725f 6e61 6d65  _descriptor_name
+00006ea0: 7328 636c 732c 2064 6573 6372 6970 746f  s(cls, descripto
+00006eb0: 725f 6578 636c 7573 696f 6e73 293a 0a20  r_exclusions):. 
+00006ec0: 2020 2020 2023 2064 6f6e 2774 2075 7365       # don't use
+00006ed0: 2067 6574 6174 7472 2068 6572 652c 2073   getattr here, s
+00006ee0: 696e 6365 2069 7420 7769 6c6c 2063 616c  ince it will cal
+00006ef0: 6c20 7468 6520 6465 7363 7269 7074 6f72  l the descriptor
+00006f00: 0a20 2020 2020 2064 6573 6372 6970 746f  .      descripto
+00006f10: 7220 3d20 636c 732e 5f5f 6469 6374 5f5f  r = cls.__dict__
+00006f20: 5b6b 6579 5d0a 2020 2020 2020 6966 2068  [key].      if h
+00006f30: 6173 6174 7472 2864 6573 6372 6970 746f  asattr(descripto
+00006f40: 722c 2027 6e6f 7772 6170 2729 3a0a 2020  r, 'nowrap'):.  
+00006f50: 2020 2020 2020 636f 6e74 696e 7565 0a20        continue. 
+00006f60: 2020 2020 2073 6574 6174 7472 2863 6c73       setattr(cls
+00006f70: 2c20 6b65 792c 2077 7261 705f 6465 7363  , key, wrap_desc
+00006f80: 7269 7074 6f72 5f6f 6e63 6528 6465 7363  riptor_once(desc
+00006f90: 7269 7074 6f72 2929 0a20 2020 2072 6574  riptor)).    ret
+00006fa0: 7572 6e20 636c 730a 0a20 2064 6566 205f  urn cls..  def _
+00006fb0: 6361 6c6c 5f77 7261 7070 6564 5f6d 6574  call_wrapped_met
+00006fc0: 686f 6428 7365 6c66 2c20 6675 6e2c 2061  hod(self, fun, a
+00006fd0: 7267 732c 206b 7761 7267 7329 3a0a 2020  rgs, kwargs):.  
+00006fe0: 2020 2222 2220 2243 616c 6c73 2061 2077    """ "Calls a w
+00006ff0: 7261 7070 6564 206d 6574 686f 642e 0a0a  rapped method...
+00007000: 2020 2020 5468 6973 2066 756e 6374 696f      This functio
+00007010: 6e20 6973 2072 6573 706f 6e73 6962 6c65  n is responsible
+00007020: 2066 6f72 2073 6574 7469 6e67 2075 7020   for setting up 
+00007030: 7468 6520 7468 7265 6164 206c 6f63 616c  the thread local
+00007040: 2073 7461 7465 0a20 2020 2063 6f72 7265   state.    corre
+00007050: 6374 6c79 2062 6566 6f72 6520 6361 6c6c  ctly before call
+00007060: 696e 6720 7468 6520 6d65 7468 6f64 2061  ing the method a
+00007070: 6e64 2063 6c65 616e 696e 6720 7570 2061  nd cleaning up a
+00007080: 6674 6572 7761 7264 732e 0a20 2020 2054  fterwards..    T
+00007090: 6869 7320 696e 636c 7564 6573 2073 746f  his includes sto
+000070a0: 7269 6e67 2069 6e74 6572 6d65 6469 6174  ring intermediat
+000070b0: 6573 2c20 7365 7475 7020 6f66 2074 6865  es, setup of the
+000070c0: 2063 6f6d 7061 6374 2073 636f 7065 2c0a   compact scope,.
+000070d0: 2020 2020 616e 6420 6d61 6b69 6e67 2073      and making s
+000070e0: 7572 6520 7365 7475 7020 6973 2063 616c  ure setup is cal
+000070f0: 6c65 6420 6265 666f 7265 2061 6e79 206f  led before any o
+00007100: 7468 6572 206d 6574 686f 642e 0a0a 2020  ther method...  
+00007110: 2020 4172 6773 3a0a 2020 2020 2020 6675    Args:.      fu
+00007120: 6e3a 2054 6865 2077 7261 7070 6564 206d  n: The wrapped m
+00007130: 6574 686f 642e 0a20 2020 2020 2061 7267  ethod..      arg
+00007140: 733a 204e 616d 6564 2061 7267 756d 656e  s: Named argumen
+00007150: 7473 2070 6173 7365 6420 746f 2060 6066  ts passed to ``f
+00007160: 756e 6060 2e0a 2020 2020 2020 6b77 6172  un``..      kwar
+00007170: 6773 3a20 4b65 7977 6f72 6420 6172 6775  gs: Keyword argu
+00007180: 6d65 6e74 7320 7061 7373 6564 2074 6f20  ments passed to 
+00007190: 6060 6675 6e60 602e 0a0a 2020 2020 5265  ``fun``...    Re
+000071a0: 7475 726e 733a 0a20 2020 2020 2054 6865  turns:.      The
+000071b0: 2072 6573 756c 7473 206f 6620 6361 6c6c   results of call
+000071c0: 696e 6720 6060 6675 6e60 602e 0a20 2020  ing ``fun``..   
+000071d0: 2022 2222 0a20 2020 2069 735f 636f 6d70   """.    is_comp
+000071e0: 6163 745f 6d65 7468 6f64 203d 2068 6173  act_method = has
+000071f0: 6174 7472 2866 756e 2c20 2763 6f6d 7061  attr(fun, 'compa
+00007200: 6374 2729 0a20 2020 2066 756e 5f6e 616d  ct').    fun_nam
+00007210: 6520 3d20 6765 7461 7474 7228 6675 6e2c  e = getattr(fun,
+00007220: 2027 5f5f 6e61 6d65 5f5f 272c 2027 756e   '__name__', 'un
+00007230: 6e61 6d65 645f 6675 6e63 7469 6f6e 2729  named_function')
+00007240: 0a20 2020 2069 735f 7365 7475 705f 6d65  .    is_setup_me
+00007250: 7468 6f64 203d 2066 756e 5f6e 616d 6520  thod = fun_name 
+00007260: 3d3d 2027 7365 7475 7027 0a20 2020 2061  == 'setup'.    a
+00007270: 6464 5f63 616c 6c5f 696e 666f 203d 206e  dd_call_info = n
+00007280: 6f74 2069 735f 7365 7475 705f 6d65 7468  ot is_setup_meth
+00007290: 6f64 2061 6e64 206c 656e 285f 636f 6e74  od and len(_cont
+000072a0: 6578 742e 6361 6c6c 5f69 6e66 6f5f 7374  ext.call_info_st
+000072b0: 6163 6b29 203e 2030 0a20 2020 2023 2057  ack) > 0.    # W
+000072c0: 6520 6c61 7a69 6c79 2063 616c 6c20 7365  e lazily call se
+000072d0: 7475 7028 2920 6f6e 6c79 2077 6865 6e20  tup() only when 
+000072e0: 6e65 6564 6564 2e0a 2020 2020 6966 2069  needed..    if i
+000072f0: 735f 7365 7475 705f 6d65 7468 6f64 3a0a  s_setup_method:.
+00007300: 2020 2020 2020 6966 2073 656c 662e 7363        if self.sc
+00007310: 6f70 6520 6973 204e 6f6e 653a 0a20 2020  ope is None:.   
+00007320: 2020 2020 2072 6169 7365 2065 7272 6f72       raise error
+00007330: 732e 4361 6c6c 5365 7475 7055 6e62 6f75  s.CallSetupUnbou
+00007340: 6e64 4d6f 6475 6c65 4572 726f 7228 290a  ndModuleError().
+00007350: 2020 2020 2020 6973 5f72 6563 7572 7265        is_recurre
+00007360: 6e74 203d 2073 656c 662e 5f73 7461 7465  nt = self._state
+00007370: 2e69 6e5f 7365 7475 700a 2020 2020 2020  .in_setup.      
+00007380: 7365 6c66 2e5f 7374 6174 652e 696e 5f73  self._state.in_s
+00007390: 6574 7570 203d 2054 7275 650a 2020 2020  etup = True.    
+000073a0: 656c 7365 3a0a 2020 2020 2020 7365 6c66  else:.      self
+000073b0: 2e5f 7472 795f 7365 7475 7028 290a 0a20  ._try_setup().. 
+000073c0: 2020 2069 6620 6973 5f63 6f6d 7061 6374     if is_compact
+000073d0: 5f6d 6574 686f 643a 0a20 2020 2020 2069  _method:.      i
+000073e0: 6620 7365 6c66 2e73 636f 7065 2069 7320  f self.scope is 
+000073f0: 4e6f 6e65 3a0a 2020 2020 2020 2020 7261  None:.        ra
+00007400: 6973 6520 6572 726f 7273 2e43 616c 6c43  ise errors.CallC
+00007410: 6f6d 7061 6374 556e 626f 756e 644d 6f64  ompactUnboundMod
+00007420: 756c 6545 7272 6f72 2829 0a20 2020 2020  uleError().     
+00007430: 2069 735f 7265 6375 7272 656e 7420 3d20   is_recurrent = 
+00007440: 7365 6c66 2e5f 7374 6174 652e 696e 5f63  self._state.in_c
+00007450: 6f6d 7061 6374 5f6d 6574 686f 640a 2020  ompact_method.  
+00007460: 2020 2020 7365 6c66 2e5f 7374 6174 652e      self._state.
+00007470: 696e 5f63 6f6d 7061 6374 5f6d 6574 686f  in_compact_metho
+00007480: 6420 3d20 5472 7565 0a20 2020 205f 636f  d = True.    _co
+00007490: 6e74 6578 742e 6d6f 6475 6c65 5f73 7461  ntext.module_sta
+000074a0: 636b 2e61 7070 656e 6428 7365 6c66 290a  ck.append(self).
+000074b0: 2020 2020 7472 793a 0a20 2020 2020 2023      try:.      #
+000074c0: 2067 6574 2063 616c 6c20 696e 666f 0a20   get call info. 
+000074d0: 2020 2020 2069 6620 6164 645f 6361 6c6c       if add_call
+000074e0: 5f69 6e66 6f3a 0a20 2020 2020 2020 2061  _info:.        a
+000074f0: 7373 6572 7420 7365 6c66 2e73 636f 7065  ssert self.scope
+00007500: 2069 7320 6e6f 7420 4e6f 6e65 0a20 2020   is not None.   
+00007510: 2020 2020 2063 616c 6c5f 696e 6465 7820       call_index 
+00007520: 3d20 5f63 6f6e 7465 7874 2e63 616c 6c5f  = _context.call_
+00007530: 696e 666f 5f73 7461 636b 5b2d 315d 2e67  info_stack[-1].g
+00007540: 6574 5f63 616c 6c5f 696e 6465 7828 7365  et_call_index(se
+00007550: 6c66 290a 2020 2020 2020 2020 7363 6f70  lf).        scop
+00007560: 655f 7061 7468 203d 206a 6178 2e74 7265  e_path = jax.tre
+00007570: 655f 7574 696c 2e74 7265 655f 6d61 7028  e_util.tree_map(
+00007580: 5f66 6978 5f70 6174 685f 7061 7274 2c20  _fix_path_part, 
+00007590: 7365 6c66 2e73 636f 7065 2e70 6174 6829  self.scope.path)
+000075a0: 0a0a 2020 2020 2020 2320 6361 6c6c 206d  ..      # call m
+000075b0: 6574 686f 640a 2020 2020 2020 6966 205f  ethod.      if _
+000075c0: 7573 655f 6e61 6d65 645f 6361 6c6c 3a0a  use_named_call:.
+000075d0: 2020 2020 2020 2020 7769 7468 206a 6178          with jax
+000075e0: 2e6e 616d 6564 5f73 636f 7065 285f 6465  .named_scope(_de
+000075f0: 7269 7665 5f70 726f 6669 6c69 6e67 5f6e  rive_profiling_n
+00007600: 616d 6528 7365 6c66 2c20 6675 6e29 293a  ame(self, fun)):
+00007610: 0a20 2020 2020 2020 2020 2079 203d 2066  .          y = f
+00007620: 756e 2873 656c 662c 202a 6172 6773 2c20  un(self, *args, 
+00007630: 2a2a 6b77 6172 6773 290a 2020 2020 2020  **kwargs).      
+00007640: 656c 7365 3a0a 2020 2020 2020 2020 7920  else:.        y 
+00007650: 3d20 6675 6e28 7365 6c66 2c20 2a61 7267  = fun(self, *arg
+00007660: 732c 202a 2a6b 7761 7267 7329 0a0a 2020  s, **kwargs)..  
+00007670: 2020 2020 6966 205f 636f 6e74 6578 742e      if _context.
+00007680: 6361 7074 7572 655f 7374 6163 6b3a 0a20  capture_stack:. 
+00007690: 2020 2020 2020 2066 696c 7465 725f 666e         filter_fn
+000076a0: 203d 205f 636f 6e74 6578 742e 6361 7074   = _context.capt
+000076b0: 7572 655f 7374 6163 6b5b 2d31 5d0a 2020  ure_stack[-1].  
+000076c0: 2020 2020 2020 6966 2066 696c 7465 725f        if filter_
+000076d0: 666e 2061 6e64 2066 696c 7465 725f 666e  fn and filter_fn
+000076e0: 2873 656c 662c 2066 756e 5f6e 616d 6529  (self, fun_name)
+000076f0: 3a0a 2020 2020 2020 2020 2020 7365 6c66  :.          self
+00007700: 2e73 6f77 2827 696e 7465 726d 6564 6961  .sow('intermedia
+00007710: 7465 7327 2c20 6675 6e5f 6e61 6d65 2c20  tes', fun_name, 
+00007720: 7929 0a20 2020 2020 2069 6620 6164 645f  y).      if add_
+00007730: 6361 6c6c 5f69 6e66 6f3a 0a20 2020 2020  call_info:.     
+00007740: 2020 205f 6172 6773 2c20 5f6b 7761 7267     _args, _kwarg
+00007750: 732c 205f 7920 3d20 666c 6178 2e6c 696e  s, _y = flax.lin
+00007760: 656e 2e73 756d 6d61 7279 2e5f 7265 7072  en.summary._repr
+00007770: 6573 656e 745f 7472 6565 280a 2020 2020  esent_tree(.    
+00007780: 2020 2020 2020 2020 2861 7267 732c 206b          (args, k
+00007790: 7761 7267 732c 2079 290a 2020 2020 2020  wargs, y).      
+000077a0: 2020 290a 2020 2020 2020 2020 5f63 6f6e    ).        _con
+000077b0: 7465 7874 2e63 616c 6c5f 696e 666f 5f73  text.call_info_s
+000077c0: 7461 636b 5b2d 315d 2e63 616c 6c73 2e61  tack[-1].calls.a
+000077d0: 7070 656e 6428 0a20 2020 2020 2020 2020  ppend(.         
+000077e0: 2020 205f 4361 6c6c 496e 666f 280a 2020     _CallInfo(.  
+000077f0: 2020 2020 2020 2020 2020 2020 2020 6361                ca
+00007800: 6c6c 5f69 6e64 6578 2c0a 2020 2020 2020  ll_index,.      
+00007810: 2020 2020 2020 2020 2020 7363 6f70 655f            scope_
+00007820: 7061 7468 2c0a 2020 2020 2020 2020 2020  path,.          
+00007830: 2020 2020 2020 7479 7065 2873 656c 6629        type(self)
+00007840: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00007850: 2020 6675 6e2e 5f5f 6e61 6d65 5f5f 2c0a    fun.__name__,.
+00007860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007870: 5f61 7267 732c 0a20 2020 2020 2020 2020  _args,.         
+00007880: 2020 2020 2020 205f 6b77 6172 6773 2c0a         _kwargs,.
+00007890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000078a0: 5f79 2c0a 2020 2020 2020 2020 2020 2020  _y,.            
+000078b0: 290a 2020 2020 2020 2020 290a 2020 2020  ).        ).    
+000078c0: 2020 7265 7475 726e 2079 0a20 2020 2066    return y.    f
+000078d0: 696e 616c 6c79 3a0a 2020 2020 2020 5f63  inally:.      _c
+000078e0: 6f6e 7465 7874 2e6d 6f64 756c 655f 7374  ontext.module_st
+000078f0: 6163 6b2e 706f 7028 290a 2020 2020 2020  ack.pop().      
+00007900: 6966 2069 735f 636f 6d70 6163 745f 6d65  if is_compact_me
+00007910: 7468 6f64 3a0a 2020 2020 2020 2020 6f62  thod:.        ob
+00007920: 6a65 6374 2e5f 5f73 6574 6174 7472 5f5f  ject.__setattr__
+00007930: 2873 656c 662c 2027 7363 6f70 6527 2c20  (self, 'scope', 
+00007940: 7365 6c66 2e73 636f 7065 2e72 6577 6f75  self.scope.rewou
+00007950: 6e64 2829 290a 2020 2020 2020 2320 7365  nd()).      # se
+00007960: 7475 7020 6f72 2063 6f6d 7061 6374 2063  tup or compact c
+00007970: 616c 6c73 2063 616e 2062 6520 7265 6375  alls can be recu
+00007980: 7272 656e 7420 666f 7220 6578 616d 706c  rrent for exampl
+00007990: 6520 6475 6520 746f 2073 7570 6572 2063  e due to super c
+000079a0: 616c 6c73 0a20 2020 2020 2023 2072 6573  alls.      # res
+000079b0: 6574 7469 6e67 2074 6865 2073 7461 7465  etting the state
+000079c0: 2077 6f75 6c64 2063 6175 7365 2069 7320   would cause is 
+000079d0: 636f 6d70 6163 742f 7365 7475 7020 6d65  compact/setup me
+000079e0: 7468 6f64 0a20 2020 2020 2023 2074 6f20  thod.      # to 
+000079f0: 6265 2073 6574 2074 6f20 4661 6c73 6520  be set to False 
+00007a00: 7072 656d 6174 7572 656c 792e 0a20 2020  prematurely..   
+00007a10: 2020 2069 6620 2869 735f 636f 6d70 6163     if (is_compac
+00007a20: 745f 6d65 7468 6f64 206f 7220 6973 5f73  t_method or is_s
+00007a30: 6574 7570 5f6d 6574 686f 6429 2061 6e64  etup_method) and
+00007a40: 206e 6f74 2069 735f 7265 6375 7272 656e   not is_recurren
+00007a50: 743a 0a20 2020 2020 2020 2073 656c 662e  t:.        self.
+00007a60: 5f73 7461 7465 2e72 6573 6574 2829 0a0a  _state.reset()..
+00007a70: 2020 6465 6620 5f5f 7365 7461 7474 725f    def __setattr_
+00007a80: 5f28 7365 6c66 2c20 6e61 6d65 3a20 7374  _(self, name: st
+00007a90: 722c 2076 616c 3a20 416e 7929 3a0a 2020  r, val: Any):.  
+00007aa0: 2020 2222 2253 6574 7320 616e 2061 7474    """Sets an att
+00007ab0: 7269 6275 7465 206f 6e20 7468 6973 204d  ribute on this M
+00007ac0: 6f64 756c 652e 0a0a 2020 2020 5765 206f  odule...    We o
+00007ad0: 7665 726c 6f61 6420 7365 7461 7474 7220  verload setattr 
+00007ae0: 736f 6c65 6c79 2074 6f20 7375 7070 6f72  solely to suppor
+00007af0: 7420 7079 7468 6f6e 6963 206e 616d 696e  t pythonic namin
+00007b00: 6720 7669 6120 6173 7369 676e 6d65 6e74  g via assignment
+00007b10: 206f 660a 2020 2020 7375 626d 6f64 756c   of.    submodul
+00007b20: 6573 2069 6e20 7468 6520 7370 6563 6961  es in the specia
+00007b30: 6c20 3a6d 6574 683a 6073 6574 7570 6020  l :meth:`setup` 
+00007b40: 6675 6e63 7469 6f6e 3a3a 0a0a 2020 2020  function::..    
+00007b50: 2020 7365 6c66 2e73 7562 6d6f 6475 6c65    self.submodule
+00007b60: 5f6e 616d 6520 3d20 4d79 4d6f 6475 6c65  _name = MyModule
+00007b70: 282e 2e2e 290a 0a20 2020 2057 6520 616c  (...)..    We al
+00007b80: 736f 2073 7570 706f 7274 206c 6973 7473  so support lists
+00007b90: 2061 6e64 206f 7468 6572 2067 656e 6572   and other gener
+00007ba0: 616c 2070 7974 7265 6573 2c20 652e 672e  al pytrees, e.g.
+00007bb0: 3a3a 0a0a 2020 2020 2020 7365 6c66 2e73  ::..      self.s
+00007bc0: 7562 6d6f 6475 6c65 7320 3d20 5b4d 794d  ubmodules = [MyM
+00007bd0: 6f64 756c 6530 282e 2e29 2c20 4d79 4d6f  odule0(..), MyMo
+00007be0: 6475 6c65 3128 2e2e 292c 202e 2e2e 5d0a  dule1(..), ...].
+00007bf0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
+00007c00: 206e 616d 653a 2041 7474 7269 6275 7465   name: Attribute
+00007c10: 2074 6f20 7365 742e 0a20 2020 2020 2076   to set..      v
+00007c20: 616c 3a20 5661 6c75 6520 6f66 2074 6865  al: Value of the
+00007c30: 2061 7474 7269 6275 7465 2e0a 2020 2020   attribute..    
+00007c40: 2222 220a 2020 2020 6669 656c 6473 203d  """.    fields =
+00007c50: 2073 656c 662e 5f5f 6461 7461 636c 6173   self.__dataclas
+00007c60: 735f 6669 656c 6473 5f5f 2020 2320 7079  s_fields__  # py
+00007c70: 7479 7065 3a20 6469 7361 626c 653d 6174  type: disable=at
+00007c80: 7472 6962 7574 652d 6572 726f 720a 2020  tribute-error.  
+00007c90: 2020 6973 5f64 6174 6163 6c61 7373 5f61    is_dataclass_a
+00007ca0: 7474 7220 3d20 6e61 6d65 2069 6e20 6669  ttr = name in fi
+00007cb0: 656c 6473 2061 6e64 2066 6965 6c64 735b  elds and fields[
+00007cc0: 6e61 6d65 5d2e 696e 6974 0a0a 2020 2020  name].init..    
+00007cd0: 6966 206e 6f74 2073 656c 662e 5f73 7461  if not self._sta
+00007ce0: 7465 2e69 6e5f 7365 7475 703a 0a20 2020  te.in_setup:.   
+00007cf0: 2020 2069 6620 6e6f 7420 7365 6c66 2e5f     if not self._
+00007d00: 7374 6174 652e 6973 5f69 6e69 7469 616c  state.is_initial
+00007d10: 697a 6564 3a0a 2020 2020 2020 2020 2320  ized:.        # 
+00007d20: 5365 7474 696e 6720 6174 7472 6962 7574  Setting attribut
+00007d30: 6573 2062 6566 6f72 6520 656e 6420 6f66  es before end of
+00007d40: 204d 6f64 756c 652e 5f5f 706f 7374 5f69   Module.__post_i
+00007d50: 6e69 745f 5f28 290a 2020 2020 2020 2020  nit__().        
+00007d60: 6f62 6a65 6374 2e5f 5f73 6574 6174 7472  object.__setattr
+00007d70: 5f5f 2873 656c 662c 206e 616d 652c 2076  __(self, name, v
+00007d80: 616c 290a 2020 2020 2020 2020 7265 7475  al).        retu
+00007d90: 726e 0a20 2020 2020 2065 6c73 653a 0a20  rn.      else:. 
+00007da0: 2020 2020 2020 2023 2057 6527 7265 2070         # We're p
+00007db0: 6173 7420 616c 6c20 696e 6974 6961 6c69  ast all initiali
+00007dc0: 7a61 7469 6f6e 2061 6e64 2073 6574 7570  zation and setup
+00007dd0: 206c 6f67 6963 3a0a 2020 2020 2020 2020   logic:.        
+00007de0: 2320 5261 6973 6573 2061 2054 7970 6545  # Raises a TypeE
+00007df0: 7272 6f72 206a 7573 7420 6c69 6b65 2066  rror just like f
+00007e00: 726f 7a65 6e20 7079 7468 6f6e 2064 6174  rozen python dat
+00007e10: 6163 6c61 7373 6573 2e0a 2020 2020 2020  aclasses..      
+00007e20: 2020 7261 6973 6520 6572 726f 7273 2e53    raise errors.S
+00007e30: 6574 4174 7472 6962 7574 6546 726f 7a65  etAttributeFroze
+00007e40: 6e4d 6f64 756c 6545 7272 6f72 280a 2020  nModuleError(.  
+00007e50: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00007e60: 5f63 6c61 7373 5f5f 2e5f 5f6e 616d 655f  _class__.__name_
+00007e70: 5f2c 206e 616d 652c 2076 616c 0a20 2020  _, name, val.   
+00007e80: 2020 2020 2029 0a0a 2020 2020 2320 5765       )..    # We
+00007e90: 2772 6520 696e 7369 6465 2074 6865 2073  're inside the s
+00007ea0: 6574 7570 2829 206d 6574 686f 643a 0a20  etup() method:. 
+00007eb0: 2020 2069 6620 6973 5f64 6174 6163 6c61     if is_datacla
+00007ec0: 7373 5f61 7474 723a 0a20 2020 2020 2023  ss_attr:.      #
+00007ed0: 2054 6865 7365 206e 616d 6573 2061 7265   These names are
+00007ee0: 2073 7065 6369 6669 6564 2061 7320 6461   specified as da
+00007ef0: 7461 636c 6173 7320 6669 656c 6473 2e20  taclass fields. 
+00007f00: 5468 6579 2073 686f 756c 6420 6e6f 7420  They should not 
+00007f10: 6265 0a20 2020 2020 2023 2069 6e69 7469  be.      # initi
+00007f20: 616c 697a 6564 2077 6974 6869 6e20 7468  alized within th
+00007f30: 6520 7365 7475 7028 2920 6d65 7468 6f64  e setup() method
+00007f40: 2c20 6275 7420 6361 6e20 6265 206d 6f64  , but can be mod
+00007f50: 6966 6965 6420 6672 6565 6c79 0a20 2020  ified freely.   
+00007f60: 2020 2023 2062 6566 6f72 6520 6974 2e0a     # before it..
+00007f70: 2020 2020 2020 7261 6973 6520 6572 726f        raise erro
+00007f80: 7273 2e53 6574 4174 7472 6962 7574 6549  rs.SetAttributeI
+00007f90: 6e4d 6f64 756c 6553 6574 7570 4572 726f  nModuleSetupErro
+00007fa0: 7228 290a 0a20 2020 2023 2056 616c 7565  r()..    # Value
+00007fb0: 7320 2874 6861 7420 6d61 7920 6265 2076  s (that may be v
+00007fc0: 6172 6961 626c 6573 206f 7220 7375 626d  ariables or subm
+00007fd0: 6f64 756c 6573 2920 6172 6520 6265 696e  odules) are bein
+00007fe0: 6720 6465 6669 6e65 6420 616e 640a 2020  g defined and.  
+00007ff0: 2020 2320 6174 7461 6368 6564 2069 6e20    # attached in 
+00008000: 7365 7475 7028 292c 2077 6520 7275 6e20  setup(), we run 
+00008010: 736f 6d65 2065 7874 7261 206c 6f67 6963  some extra logic
+00008020: 2069 6e20 7468 6174 2063 6173 652e 0a20   in that case.. 
+00008030: 2020 2073 656c 662e 5f72 6567 6973 7465     self._registe
+00008040: 725f 7375 626d 6f64 756c 6573 286e 616d  r_submodules(nam
+00008050: 652c 2076 616c 290a 0a20 2064 6566 205f  e, val)..  def _
+00008060: 5f67 6574 6174 7472 5f5f 2873 656c 662c  _getattr__(self,
+00008070: 206e 616d 653a 2073 7472 2920 2d3e 2041   name: str) -> A
+00008080: 6e79 3a0a 2020 2020 2222 2243 616c 6c20  ny:.    """Call 
+00008090: 7365 7475 7028 2920 6265 666f 7265 2067  setup() before g
+000080a0: 6574 7469 6e67 2061 6e79 2073 6574 7570  etting any setup
+000080b0: 2d64 6566 696e 6564 2061 7474 7269 6275  -defined attribu
+000080c0: 7465 732e 2222 220a 2020 2020 2320 5765  tes.""".    # We
+000080d0: 2064 6f6e 2774 2077 616e 7420 746f 2072   don't want to r
+000080e0: 6574 7572 6e20 616e 7974 6869 6e67 2066  eturn anything f
+000080f0: 6f72 2070 7974 686f 6e20 636f 7079 202f  or python copy /
+00008100: 2070 6963 6b6c 6520 6d65 7468 6f64 732e   pickle methods.
+00008110: 0a20 2020 2069 6620 6e61 6d65 2069 6e20  .    if name in 
+00008120: 5f55 4e44 4546 494e 4544 5f43 4f50 595f  _UNDEFINED_COPY_
+00008130: 5049 434b 4c45 5f4d 4554 484f 4453 3a0a  PICKLE_METHODS:.
+00008140: 2020 2020 2020 7261 6973 6520 4174 7472        raise Attr
+00008150: 6962 7574 6545 7272 6f72 2829 0a20 2020  ibuteError().   
+00008160: 2073 656c 662e 5f74 7279 5f73 6574 7570   self._try_setup
+00008170: 2829 0a20 2020 2069 6620 6e61 6d65 2069  ().    if name i
+00008180: 6e20 7365 6c66 2e5f 5f64 6963 745f 5f3a  n self.__dict__:
+00008190: 0a20 2020 2020 2072 6574 7572 6e20 7365  .      return se
+000081a0: 6c66 2e5f 5f64 6963 745f 5f5b 6e61 6d65  lf.__dict__[name
+000081b0: 5d0a 2020 2020 656c 7365 3a0a 2020 2020  ].    else:.    
+000081c0: 2020 6d73 6720 3d20 6627 227b 7365 6c66    msg = f'"{self
+000081d0: 2e5f 5f63 6c61 7373 5f5f 2e5f 5f6e 616d  .__class__.__nam
+000081e0: 655f 5f7d 2220 6f62 6a65 6374 2068 6173  e__}" object has
+000081f0: 206e 6f20 6174 7472 6962 7574 6520 227b   no attribute "{
+00008200: 6e61 6d65 7d22 2e27 0a20 2020 2020 2069  name}".'.      i
+00008210: 6620 7365 6c66 2e73 636f 7065 2069 7320  f self.scope is 
+00008220: 4e6f 6e65 3a0a 2020 2020 2020 2020 6d73  None:.        ms
+00008230: 6720 2b3d 2028 0a20 2020 2020 2020 2020  g += (.         
+00008240: 2020 2066 2720 4966 2022 7b6e 616d 657d     f' If "{name}
+00008250: 2220 6973 2064 6566 696e 6564 2069 6e20  " is defined in 
+00008260: 5c27 2e73 6574 7570 2829 5c27 2c20 7265  \'.setup()\', re
+00008270: 6d65 6d62 6572 2074 6865 7365 2066 6965  member these fie
+00008280: 6c64 7320 270a 2020 2020 2020 2020 2020  lds '.          
+00008290: 2020 2261 7265 206f 6e6c 7920 6163 6365    "are only acce
+000082a0: 7373 6962 6c65 2066 726f 6d20 696e 7369  ssible from insi
+000082b0: 6465 2027 696e 6974 2720 6f72 2027 6170  de 'init' or 'ap
+000082c0: 706c 7927 2e22 0a20 2020 2020 2020 2029  ply'.".        )
+000082d0: 0a20 2020 2020 2072 6169 7365 2041 7474  .      raise Att
+000082e0: 7269 6275 7465 4572 726f 7228 6d73 6729  ributeError(msg)
+000082f0: 0a0a 2020 6465 6620 5f5f 6469 725f 5f28  ..  def __dir__(
+00008300: 7365 6c66 2920 2d3e 204c 6973 745b 7374  self) -> List[st
+00008310: 725d 3a0a 2020 2020 2222 2243 616c 6c20  r]:.    """Call 
+00008320: 7365 7475 7028 2920 6265 666f 7265 206c  setup() before l
+00008330: 6973 7469 6e67 2061 7474 7269 6275 7465  isting attribute
+00008340: 732e 2222 220a 2020 2020 7365 6c66 2e5f  s.""".    self._
+00008350: 7472 795f 7365 7475 7028 290a 2020 2020  try_setup().    
+00008360: 7265 7475 726e 206f 626a 6563 742e 5f5f  return object.__
+00008370: 6469 725f 5f28 7365 6c66 2920 2023 2074  dir__(self)  # t
+00008380: 7970 653a 2069 676e 6f72 650a 0a20 2064  ype: ignore..  d
+00008390: 6566 205f 5f70 6f73 745f 696e 6974 5f5f  ef __post_init__
+000083a0: 2873 656c 6629 202d 3e20 4e6f 6e65 3a0a  (self) -> None:.
+000083b0: 2020 2020 2320 444f 204e 4f54 2052 454d      # DO NOT REM
+000083c0: 4f56 4520 2d20 4d61 726b 6572 2066 6f72  OVE - Marker for
+000083d0: 2069 6e74 6572 6e61 6c20 6c6f 6767 696e   internal loggin
+000083e0: 672e 0a20 2020 2023 2049 6e20 6461 7461  g..    # In data
+000083f0: 636c 6173 7365 732c 205f 5f69 6e69 745f  classes, __init_
+00008400: 5f20 6973 206f 7665 7272 6964 6465 6e20  _ is overridden 
+00008410: 746f 2070 726f 6365 7373 2064 6174 6163  to process datac
+00008420: 6c61 7373 2061 7267 756d 656e 7473 2c0a  lass arguments,.
+00008430: 2020 2020 2320 616e 6420 5f5f 706f 7374      # and __post
+00008440: 5f69 6e69 745f 5f20 6973 2063 616c 6c65  _init__ is calle
+00008450: 6420 696d 6d65 6469 6174 656c 7920 6166  d immediately af
+00008460: 7465 7277 6172 6473 2e20 4865 7265 2c20  terwards. Here, 
+00008470: 6465 7065 6e64 696e 6720 6f6e 2074 6865  depending on the
+00008480: 0a20 2020 2023 2074 7970 6520 6f66 2060  .    # type of `
+00008490: 7061 7265 6e74 6020 7061 7373 6564 2074  parent` passed t
+000084a0: 6f20 696e 6974 6961 6c69 7a65 2074 6865  o initialize the
+000084b0: 204d 6f64 756c 652c 2077 6520 6569 7468   Module, we eith
+000084c0: 6572 2064 6566 6572 0a20 2020 2023 2069  er defer.    # i
+000084d0: 6e69 7469 616c 697a 6174 696f 6e2c 2061  nitialization, a
+000084e0: 7474 6163 6820 7468 6973 204d 6f64 756c  ttach this Modul
+000084f0: 6520 6173 2061 2073 7562 6d6f 6475 6c65  e as a submodule
+00008500: 206f 6620 6120 7061 7265 6e74 2c20 6f72   of a parent, or
+00008510: 2062 696e 640a 2020 2020 2320 7468 6973   bind.    # this
+00008520: 204d 6f64 756c 6520 6174 2074 6865 2074   Module at the t
+00008530: 6f70 2d6c 6576 656c 2074 6f20 7661 7269  op-level to vari
+00008540: 6162 6c65 7320 616e 6420 726e 6773 2e0a  ables and rngs..
+00008550: 0a20 2020 206f 626a 6563 742e 5f5f 7365  .    object.__se
+00008560: 7461 7474 725f 5f28 7365 6c66 2c20 275f  tattr__(self, '_
+00008570: 6964 272c 2075 7569 6428 2929 0a20 2020  id', uuid()).   
+00008580: 206f 626a 6563 742e 5f5f 7365 7461 7474   object.__setatt
+00008590: 725f 5f28 7365 6c66 2c20 275f 7374 6174  r__(self, '_stat
+000085a0: 6527 2c20 5f4d 6f64 756c 6549 6e74 6572  e', _ModuleInter
+000085b0: 6e61 6c53 7461 7465 2829 290a 0a20 2020  nalState())..   
+000085c0: 2023 2054 7970 6963 616c 6c79 2077 6520   # Typically we 
+000085d0: 7365 7420 7468 6520 7061 7265 6e74 2062  set the parent b
+000085e0: 6173 6564 206f 6e20 7468 6520 6479 6e61  ased on the dyna
+000085f0: 6d69 6320 6d6f 6475 6c65 2063 6f6e 7465  mic module conte
+00008600: 7874 2e0a 2020 2020 6966 2073 656c 662e  xt..    if self.
+00008610: 7061 7265 6e74 2069 7320 5f75 6e73 7065  parent is _unspe
+00008620: 6369 6669 6564 5f70 6172 656e 743a 2020  cified_parent:  
+00008630: 2320 7079 7479 7065 3a20 6469 7361 626c  # pytype: disabl
+00008640: 653d 6174 7472 6962 7574 652d 6572 726f  e=attribute-erro
+00008650: 720a 2020 2020 2020 6f62 6a65 6374 2e5f  r.      object._
+00008660: 5f73 6574 6174 7472 5f5f 2873 656c 662c  _setattr__(self,
+00008670: 2027 7061 7265 6e74 272c 205f 636f 6e74   'parent', _cont
+00008680: 6578 742e 6d6f 6475 6c65 5f73 7461 636b  ext.module_stack
+00008690: 5b2d 315d 290a 0a20 2020 2023 2049 6e69  [-1])..    # Ini
+000086a0: 7469 616c 697a 6174 696f 6e20 6973 2064  tialization is d
+000086b0: 6566 6572 7265 6420 666f 7220 746f 7020  eferred for top 
+000086c0: 6c65 7665 6c20 4d6f 6475 6c65 7320 6f72  level Modules or
+000086d0: 2061 6e79 206f 7468 6572 2022 6f72 7068   any other "orph
+000086e0: 616e 220a 2020 2020 2320 4d6f 6475 6c65  an".    # Module
+000086f0: 7320 756e 7469 6c20 6174 7461 6368 6d65  s until attachme
+00008700: 6e74 2062 7920 5f5f 7365 7461 7474 725f  nt by __setattr_
+00008710: 5f20 692e 652e 204d 794d 6f64 756c 6528  _ i.e. MyModule(
+00008720: 2e2e 2e2c 2070 6172 656e 743d 4e6f 6e65  ..., parent=None
+00008730: 290a 2020 2020 6966 2073 656c 662e 7061  ).    if self.pa
+00008740: 7265 6e74 2069 7320 4e6f 6e65 3a0a 2020  rent is None:.  
+00008750: 2020 2020 7265 7475 726e 0a0a 2020 2020      return..    
+00008760: 2320 5265 6769 7374 6572 2073 7562 6d6f  # Register submo
+00008770: 6475 6c65 206f 6e20 7061 7265 6e74 204d  dule on parent M
+00008780: 6f64 756c 652e 0a20 2020 2069 6620 6973  odule..    if is
+00008790: 696e 7374 616e 6365 2873 656c 662e 7061  instance(self.pa
+000087a0: 7265 6e74 2c20 4d6f 6475 6c65 293a 0a20  rent, Module):. 
+000087b0: 2020 2020 2023 2057 6865 6e20 696e 6974       # When init
+000087c0: 6961 6c69 7a69 6e67 2061 6e20 756e 6e61  ializing an unna
+000087d0: 6d65 6420 4d6f 6475 6c65 2069 6e73 6964  med Module insid
+000087e0: 6520 7365 7475 7028 290a 2020 2020 2020  e setup().      
+000087f0: 2320 696e 6974 6961 6c69 7a61 7469 6f6e  # initialization
+00008800: 2069 7320 6465 6665 7272 6564 2075 6e74   is deferred unt
+00008810: 696c 2061 7474 6163 686d 656e 7420 6279  il attachment by
+00008820: 205f 5f73 6574 6174 7472 5f5f 0a20 2020   __setattr__.   
+00008830: 2020 2023 2069 2e65 2e20 7365 6c66 2e6d     # i.e. self.m
+00008840: 796d 6f64 756c 6520 3d20 4d79 4d6f 6475  ymodule = MyModu
+00008850: 6c65 282e 2e2e 290a 2020 2020 2020 7365  le(...).      se
+00008860: 6c66 2e6e 616d 653a 204f 7074 696f 6e61  lf.name: Optiona
+00008870: 6c5b 7374 725d 0a20 2020 2020 2069 6620  l[str].      if 
+00008880: 7365 6c66 2e70 6172 656e 742e 5f73 7461  self.parent._sta
+00008890: 7465 2e69 6e5f 7365 7475 7020 616e 6420  te.in_setup and 
+000088a0: 7365 6c66 2e6e 616d 6520 6973 204e 6f6e  self.name is Non
+000088b0: 653a 2020 2320 7079 7479 7065 3a20 6469  e:  # pytype: di
+000088c0: 7361 626c 653d 6174 7472 6962 7574 652d  sable=attribute-
+000088d0: 6572 726f 720a 2020 2020 2020 2020 7265  error.        re
+000088e0: 7475 726e 0a20 2020 2020 2069 6620 6e6f  turn.      if no
+000088f0: 7420 7365 6c66 2e70 6172 656e 742e 5f69  t self.parent._i
+00008900: 6e69 7469 616c 697a 6174 696f 6e5f 616c  nitialization_al
+00008910: 6c6f 7765 643a 0a20 2020 2020 2020 2072  lowed:.        r
+00008920: 6169 7365 2065 7272 6f72 732e 4173 7369  aise errors.Assi
+00008930: 676e 5375 624d 6f64 756c 6545 7272 6f72  gnSubModuleError
+00008940: 2873 656c 662e 5f5f 636c 6173 735f 5f2e  (self.__class__.
+00008950: 5f5f 6e61 6d65 5f5f 290a 2020 2020 2020  __name__).      
+00008960: 2320 4175 746f 6e61 6d69 6e67 206f 6620  # Autonaming of 
+00008970: 7375 626d 6f64 756c 6573 2e0a 2020 2020  submodules..    
+00008980: 2020 6966 2073 656c 662e 6e61 6d65 2069    if self.name i
+00008990: 7320 4e6f 6e65 3a20 2023 2070 7974 7970  s None:  # pytyp
+000089a0: 653a 2064 6973 6162 6c65 3d61 7474 7269  e: disable=attri
+000089b0: 6275 7465 2d65 7272 6f72 0a20 2020 2020  bute-error.     
+000089c0: 2020 2070 7265 6669 7820 3d20 6627 7b73     prefix = f'{s
+000089d0: 656c 662e 5f5f 636c 6173 735f 5f2e 5f5f  elf.__class__.__
+000089e0: 6e61 6d65 5f5f 7d27 0a20 2020 2020 2020  name__}'.       
+000089f0: 2063 7572 736f 7220 3d20 7365 6c66 2e70   cursor = self.p
+00008a00: 6172 656e 742e 5f73 7461 7465 2e61 7574  arent._state.aut
+00008a10: 6f6e 616d 655f 6375 7273 6f72 2e67 6574  oname_cursor.get
+00008a20: 2870 7265 6669 782c 2030 290a 2020 2020  (prefix, 0).    
+00008a30: 2020 2020 7365 6c66 2e6e 616d 6520 3d20      self.name = 
+00008a40: 6627 7b70 7265 6669 787d 5f7b 6375 7273  f'{prefix}_{curs
+00008a50: 6f72 7d27 0a20 2020 2020 2020 2073 656c  or}'.        sel
+00008a60: 662e 7061 7265 6e74 2e5f 7374 6174 652e  f.parent._state.
+00008a70: 6175 746f 6e61 6d65 5f63 7572 736f 725b  autoname_cursor[
+00008a80: 7072 6566 6978 5d20 3d20 6375 7273 6f72  prefix] = cursor
+00008a90: 202b 2031 0a20 2020 2020 2023 2041 6c6c   + 1.      # All
+00008aa0: 6f77 2073 636f 7065 2061 6c69 6173 696e  ow scope aliasin
+00008ab0: 6720 756e 6465 7220 7472 616e 7366 6f72  g under transfor
+00008ac0: 6d73 2066 6f72 2073 7562 6d6f 6475 6c65  ms for submodule
+00008ad0: 7320 6465 6669 6e65 6420 696e 2073 6574  s defined in set
+00008ae0: 7570 2e0a 2020 2020 2020 7265 7573 655f  up..      reuse_
+00008af0: 7363 6f70 6573 203d 2028 0a20 2020 2020  scopes = (.     
+00008b00: 2020 2020 2073 656c 662e 7061 7265 6e74       self.parent
+00008b10: 2e5f 7374 6174 652e 696e 5f73 6574 7570  ._state.in_setup
+00008b20: 0a20 2020 2020 2020 2020 2061 6e64 2073  .          and s
+00008b30: 656c 662e 7061 7265 6e74 2e5f 7374 6174  elf.parent._stat
+00008b40: 652e 7365 7475 705f 6361 6c6c 6564 203d  e.setup_called =
+00008b50: 3d20 5365 7475 7053 7461 7465 2e54 5241  = SetupState.TRA
+00008b60: 4e53 464f 524d 4544 0a20 2020 2020 2029  NSFORMED.      )
+00008b70: 0a20 2020 2020 2023 2050 6572 666f 726d  .      # Perform
+00008b80: 206e 616d 652d 636f 6c6c 6973 696f 6e20   name-collision 
+00008b90: 6368 6563 6b2e 0a20 2020 2020 2069 6620  check..      if 
+00008ba0: 7365 6c66 2e70 6172 656e 742e 5f6e 616d  self.parent._nam
+00008bb0: 655f 7461 6b65 6e28 7365 6c66 2e6e 616d  e_taken(self.nam
+00008bc0: 652c 2073 656c 662c 2072 6575 7365 5f73  e, self, reuse_s
+00008bd0: 636f 7065 733d 7265 7573 655f 7363 6f70  copes=reuse_scop
+00008be0: 6573 293a 0a20 2020 2020 2020 2070 6172  es):.        par
+00008bf0: 656e 745f 636c 6173 7320 3d20 7365 6c66  ent_class = self
+00008c00: 2e70 6172 656e 742e 5f5f 636c 6173 735f  .parent.__class_
+00008c10: 5f2e 5f5f 6e61 6d65 5f5f 0a20 2020 2020  _.__name__.     
+00008c20: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
+00008c30: 4e61 6d65 496e 5573 6545 7272 6f72 2827  NameInUseError('
+00008c40: 7375 626d 6f64 756c 6527 2c20 7365 6c66  submodule', self
+00008c50: 2e6e 616d 652c 2070 6172 656e 745f 636c  .name, parent_cl
+00008c60: 6173 7329 0a20 2020 2020 2023 2046 696e  ass).      # Fin
+00008c70: 616c 697a 6520 6174 7461 6368 6d65 6e74  alize attachment
+00008c80: 2074 6f20 7061 7265 6e74 2061 6e64 2073   to parent and s
+00008c90: 636f 7065 2069 6e69 7469 616c 697a 6174  cope initializat
+00008ca0: 696f 6e2e 0a20 2020 2020 2073 656c 662e  ion..      self.
+00008cb0: 7061 7265 6e74 2e5f 7374 6174 652e 6368  parent._state.ch
+00008cc0: 696c 6472 656e 5b73 656c 662e 6e61 6d65  ildren[self.name
+00008cd0: 5d20 3d20 7365 6c66 0a20 2020 2020 2061  ] = self.      a
+00008ce0: 7373 6572 7420 7365 6c66 2e70 6172 656e  ssert self.paren
+00008cf0: 742e 7363 6f70 6520 6973 206e 6f74 204e  t.scope is not N
+00008d00: 6f6e 650a 2020 2020 2020 6f62 6a65 6374  one.      object
+00008d10: 2e5f 5f73 6574 6174 7472 5f5f 280a 2020  .__setattr__(.  
+00008d20: 2020 2020 2020 2020 7365 6c66 2c20 2773          self, 's
+00008d30: 636f 7065 272c 2073 656c 662e 7061 7265  cope', self.pare
+00008d40: 6e74 2e73 636f 7065 2e70 7573 6828 7365  nt.scope.push(se
+00008d50: 6c66 2e6e 616d 652c 2072 6575 7365 3d72  lf.name, reuse=r
+00008d60: 6575 7365 5f73 636f 7065 7329 0a20 2020  euse_scopes).   
+00008d70: 2020 2029 0a0a 2020 2020 2320 546f 702d     )..    # Top-
+00008d80: 6c65 7665 6c20 696e 766f 6361 7469 6f6e  level invocation
+00008d90: 2077 6974 6820 6120 6675 6e63 7469 6f6e   with a function
+00008da0: 616c 2053 636f 7065 2e0a 2020 2020 656c  al Scope..    el
+00008db0: 6966 2069 7369 6e73 7461 6e63 6528 7365  if isinstance(se
+00008dc0: 6c66 2e70 6172 656e 742c 2053 636f 7065  lf.parent, Scope
+00008dd0: 293a 0a20 2020 2020 206f 626a 6563 742e  ):.      object.
+00008de0: 5f5f 7365 7461 7474 725f 5f28 7365 6c66  __setattr__(self
+00008df0: 2c20 2773 636f 7065 272c 2073 656c 662e  , 'scope', self.
+00008e00: 7061 7265 6e74 290a 2020 2020 656c 7365  parent).    else
+00008e10: 3a0a 2020 2020 2020 7261 6973 6520 5661  :.      raise Va
+00008e20: 6c75 6545 7272 6f72 2827 7061 7265 6e74  lueError('parent
+00008e30: 206d 7573 7420 6265 204e 6f6e 652c 204d   must be None, M
+00008e40: 6f64 756c 6520 6f72 2053 636f 7065 2729  odule or Scope')
+00008e50: 0a0a 2020 2020 2320 6561 6765 726c 7920  ..    # eagerly 
+00008e60: 6269 6e64 2073 7562 6d6f 6475 6c65 7320  bind submodules 
+00008e70: 6966 2073 636f 7065 2069 7320 6176 6169  if scope is avai
+00008e80: 6c61 626c 650a 2020 2020 6966 2073 656c  lable.    if sel
+00008e90: 662e 7363 6f70 6520 6973 206e 6f74 204e  f.scope is not N
+00008ea0: 6f6e 653a 0a20 2020 2020 2066 6f72 2066  one:.      for f
+00008eb0: 6965 6c64 2069 6e20 6461 7461 636c 6173  ield in dataclas
+00008ec0: 7365 732e 6669 656c 6473 2873 656c 6629  ses.fields(self)
+00008ed0: 3a0a 2020 2020 2020 2020 6966 2066 6965  :.        if fie
+00008ee0: 6c64 2e6e 616d 6520 6e6f 7420 696e 2028  ld.name not in (
+00008ef0: 2770 6172 656e 7427 2c20 276e 616d 6527  'parent', 'name'
+00008f00: 2920 616e 6420 6669 656c 642e 696e 6974  ) and field.init
+00008f10: 3a0a 2020 2020 2020 2020 2020 7365 6c66  :.          self
+00008f20: 2e5f 7265 6769 7374 6572 5f73 7562 6d6f  ._register_submo
+00008f30: 6475 6c65 7328 6669 656c 642e 6e61 6d65  dules(field.name
+00008f40: 2c20 6765 7461 7474 7228 7365 6c66 2c20  , getattr(self, 
+00008f50: 6669 656c 642e 6e61 6d65 2929 0a0a 2020  field.name))..  
+00008f60: 2020 7365 6c66 2e5f 7374 6174 652e 6973    self._state.is
+00008f70: 5f69 6e69 7469 616c 697a 6564 203d 2054  _initialized = T
+00008f80: 7275 650a 0a20 2064 6566 205f 5f72 6570  rue..  def __rep
+00008f90: 725f 5f28 7365 6c66 2920 2d3e 2073 7472  r__(self) -> str
+00008fa0: 3a0a 2020 2020 7265 7475 726e 205f 6d6f  :.    return _mo
+00008fb0: 6475 6c65 5f72 6570 7228 7365 6c66 290a  dule_repr(self).
+00008fc0: 0a20 2064 6566 2073 6574 7570 2873 656c  .  def setup(sel
+00008fd0: 6629 202d 3e20 4e6f 6e65 3a0a 2020 2020  f) -> None:.    
+00008fe0: 2222 2249 6e69 7469 616c 697a 6573 2061  """Initializes a
+00008ff0: 204d 6f64 756c 6520 6c61 7a69 6c79 2028   Module lazily (
+00009000: 7369 6d69 6c61 7220 746f 2061 206c 617a  similar to a laz
+00009010: 7920 6060 5f5f 696e 6974 5f5f 6060 292e  y ``__init__``).
+00009020: 0a0a 2020 2020 6060 7365 7475 7060 6020  ..    ``setup`` 
+00009030: 6973 2063 616c 6c65 6420 6f6e 6365 206c  is called once l
+00009040: 617a 696c 7920 6f6e 2061 206d 6f64 756c  azily on a modul
+00009050: 6520 696e 7374 616e 6365 2077 6865 6e20  e instance when 
+00009060: 6120 6d6f 6475 6c65 0a20 2020 2069 7320  a module.    is 
+00009070: 626f 756e 642c 2069 6d6d 6564 6961 7465  bound, immediate
+00009080: 6c79 2062 6566 6f72 6520 616e 7920 6f74  ly before any ot
+00009090: 6865 7220 6d65 7468 6f64 7320 6c69 6b65  her methods like
+000090a0: 2060 605f 5f63 616c 6c5f 5f60 6020 6172   ``__call__`` ar
+000090b0: 650a 2020 2020 696e 766f 6b65 642c 206f  e.    invoked, o
+000090c0: 7220 6265 666f 7265 2061 2060 6073 6574  r before a ``set
+000090d0: 7570 6060 2d64 6566 696e 6564 2061 7474  up``-defined att
+000090e0: 7269 6275 7465 206f 6e20 6073 656c 6660  ribute on `self`
+000090f0: 2069 7320 6163 6365 7373 6564 2e0a 0a20   is accessed... 
+00009100: 2020 2054 6869 7320 6361 6e20 6861 7070     This can happ
+00009110: 656e 2069 6e20 7468 7265 6520 6361 7365  en in three case
+00009120: 733a 0a0a 2020 2020 2020 312e 2049 6d6d  s:..      1. Imm
+00009130: 6564 6961 7465 6c79 2077 6865 6e20 696e  ediately when in
+00009140: 766f 6b69 6e67 203a 6d65 7468 3a60 6170  voking :meth:`ap
+00009150: 706c 7960 2c20 3a6d 6574 683a 6069 6e69  ply`, :meth:`ini
+00009160: 7460 206f 720a 2020 2020 2020 2020 203a  t` or.         :
+00009170: 6d65 7468 3a60 696e 6974 5f61 6e64 5f6f  meth:`init_and_o
+00009180: 7574 7075 7460 2e0a 0a20 2020 2020 2032  utput`...      2
+00009190: 2e20 4f6e 6365 2074 6865 206d 6f64 756c  . Once the modul
+000091a0: 6520 6973 2067 6976 656e 2061 206e 616d  e is given a nam
+000091b0: 6520 6279 2062 6569 6e67 2061 7373 6967  e by being assig
+000091c0: 6e65 6420 746f 2061 6e20 6174 7472 6962  ned to an attrib
+000091d0: 7574 6520 6f66 0a20 2020 2020 2020 2020  ute of.         
+000091e0: 616e 6f74 6865 7220 6d6f 6475 6c65 2069  another module i
+000091f0: 6e73 6964 6520 7468 6520 6f74 6865 7220  nside the other 
+00009200: 6d6f 6475 6c65 2773 2060 6073 6574 7570  module's ``setup
+00009210: 6060 206d 6574 686f 640a 2020 2020 2020  `` method.      
+00009220: 2020 2028 7365 6520 3a6d 6574 683a 605f     (see :meth:`_
+00009230: 5f73 6574 6174 7472 5f5f 6029 3a3a 0a0a  _setattr__`)::..
+00009240: 2020 2020 2020 2020 2020 2063 6c61 7373             class
+00009250: 204d 794d 6f64 756c 6528 6e6e 2e4d 6f64   MyModule(nn.Mod
+00009260: 756c 6529 3a0a 2020 2020 2020 2020 2020  ule):.          
+00009270: 2020 2064 6566 2073 6574 7570 2873 656c     def setup(sel
+00009280: 6629 3a0a 2020 2020 2020 2020 2020 2020  f):.            
+00009290: 2020 2073 7562 6d6f 6475 6c65 203d 2043     submodule = C
+000092a0: 6f6e 7628 2e2e 2e29 0a0a 2020 2020 2020  onv(...)..      
+000092b0: 2020 2020 2020 2020 2023 2041 6363 6573           # Acces
+000092c0: 7369 6e67 2060 7375 626d 6f64 756c 6560  sing `submodule`
+000092d0: 2061 7474 7269 6275 7465 7320 646f 6573   attributes does
+000092e0: 206e 6f74 2079 6574 2077 6f72 6b20 6865   not yet work he
+000092f0: 7265 2e0a 0a20 2020 2020 2020 2020 2020  re...           
+00009300: 2020 2020 2320 5468 6520 666f 6c6c 6f77      # The follow
+00009310: 696e 6720 6c69 6e65 2069 6e76 6f6b 6573  ing line invokes
+00009320: 2060 7365 6c66 2e5f 5f73 6574 6174 7472   `self.__setattr
+00009330: 5f5f 602c 2077 6869 6368 2067 6976 6573  __`, which gives
+00009340: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00009350: 2320 6073 7562 6d6f 6475 6c65 6020 7468  # `submodule` th
+00009360: 6520 6e61 6d65 2022 636f 6e76 3122 2e0a  e name "conv1"..
+00009370: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00009380: 656c 662e 636f 6e76 3120 3d20 7375 626d  elf.conv1 = subm
+00009390: 6f64 756c 650a 0a20 2020 2020 2020 2020  odule..         
+000093a0: 2020 2020 2020 2320 4163 6365 7373 696e        # Accessin
+000093b0: 6720 6073 7562 6d6f 6475 6c65 6020 6174  g `submodule` at
+000093c0: 7472 6962 7574 6573 206f 7220 6d65 7468  tributes or meth
+000093d0: 6f64 7320 6973 206e 6f77 2073 6166 6520  ods is now safe 
+000093e0: 616e 640a 2020 2020 2020 2020 2020 2020  and.            
+000093f0: 2020 2023 2065 6974 6865 7220 6361 7573     # either caus
+00009400: 6573 2073 6574 7570 2829 2074 6f20 6265  es setup() to be
+00009410: 2063 616c 6c65 6420 6f6e 6365 2e0a 0a20   called once... 
+00009420: 2020 2020 2033 2e20 4f6e 6365 2061 206d       3. Once a m
+00009430: 6f64 756c 6520 6973 2063 6f6e 7374 7275  odule is constru
+00009440: 6374 6564 2069 6e73 6964 6520 6120 6d65  cted inside a me
+00009450: 7468 6f64 2077 7261 7070 6564 2077 6974  thod wrapped wit
+00009460: 680a 2020 2020 2020 2020 203a 6d65 7468  h.         :meth
+00009470: 3a60 636f 6d70 6163 7460 2c20 696d 6d65  :`compact`, imme
+00009480: 6469 6174 656c 7920 6265 666f 7265 2061  diately before a
+00009490: 6e6f 7468 6572 206d 6574 686f 6420 6973  nother method is
+000094a0: 2063 616c 6c65 6420 6f72 0a20 2020 2020   called or.     
+000094b0: 2020 2020 6060 7365 7475 7060 6020 6465      ``setup`` de
+000094c0: 6669 6e65 6420 6174 7472 6962 7574 6520  fined attribute 
+000094d0: 6973 2061 6363 6573 7365 642e 0a20 2020  is accessed..   
+000094e0: 2022 2222 0a20 2020 2070 6173 730a 0a20   """.    pass.. 
+000094f0: 2064 6566 205f 7265 6769 7374 6572 5f73   def _register_s
+00009500: 7562 6d6f 6475 6c65 7328 7365 6c66 2c20  ubmodules(self, 
+00009510: 6e61 6d65 2c20 7661 6c29 3a0a 2020 2020  name, val):.    
+00009520: 2222 2252 6567 6973 7465 7273 2061 2073  """Registers a s
+00009530: 7562 6d6f 6475 6c65 2e22 2222 0a20 2020  ubmodule.""".   
+00009540: 2061 7373 6572 7420 7365 6c66 2e73 636f   assert self.sco
+00009550: 7065 2c20 2754 7279 696e 6720 746f 2072  pe, 'Trying to r
+00009560: 6567 6973 7465 7220 7375 626d 6f64 756c  egister submodul
+00009570: 6573 206f 6e20 756e 626f 756e 6420 7363  es on unbound sc
+00009580: 6f70 652e 270a 2020 2020 726f 6f74 203d  ope.'.    root =
+00009590: 2073 656c 662e 7363 6f70 652e 726f 6f74   self.scope.root
+000095a0: 0a20 2020 2063 6163 6865 203d 205f 6361  .    cache = _ca
+000095b0: 6368 6573 2e67 6574 2872 6f6f 742c 2077  ches.get(root, w
+000095c0: 6561 6b72 6566 2e57 6561 6b56 616c 7565  eakref.WeakValue
+000095d0: 4469 6374 696f 6e61 7279 2829 290a 2020  Dictionary()).  
+000095e0: 2020 5f63 6163 6865 735b 726f 6f74 5d20    _caches[root] 
+000095f0: 3d20 6361 6368 650a 2020 2020 7175 6575  = cache.    queu
+00009600: 6520 3d20 5b5d 0a20 2020 2070 7265 7365  e = [].    prese
+00009610: 7276 655f 6164 6f70 7465 645f 6e61 6d65  rve_adopted_name
+00009620: 7320 3d20 636f 6e66 6967 2e66 6c61 785f  s = config.flax_
+00009630: 7072 6573 6572 7665 5f61 646f 7074 6564  preserve_adopted
+00009640: 5f6e 616d 6573 0a20 2020 2069 6620 6861  _names.    if ha
+00009650: 7361 7474 7228 7479 7065 2873 656c 6629  sattr(type(self)
+00009660: 2c20 2770 7265 7365 7276 655f 6164 6f70  , 'preserve_adop
+00009670: 7465 645f 6e61 6d65 7327 293a 0a20 2020  ted_names'):.   
+00009680: 2020 2070 7265 7365 7276 655f 6164 6f70     preserve_adop
+00009690: 7465 645f 6e61 6d65 7320 3d20 7479 7065  ted_names = type
+000096a0: 2873 656c 6629 2e70 7265 7365 7276 655f  (self).preserve_
+000096b0: 6164 6f70 7465 645f 6e61 6d65 730a 0a20  adopted_names.. 
+000096c0: 2020 2064 6566 2061 646f 7074 5f61 7474     def adopt_att
+000096d0: 725f 6d6f 6475 6c65 7328 6361 6368 652c  r_modules(cache,
+000096e0: 2071 7565 7565 2c20 7375 6666 6978 2c20   queue, suffix, 
+000096f0: 7375 6276 616c 7565 293a 0a20 2020 2020  subvalue):.     
+00009700: 2069 6620 6973 696e 7374 616e 6365 2873   if isinstance(s
+00009710: 7562 7661 6c75 652c 204d 6f64 756c 6529  ubvalue, Module)
+00009720: 3a0a 2020 2020 2020 2020 6164 6f70 7465  :.        adopte
+00009730: 645f 6e61 6d65 203d 204e 6f6e 650a 2020  d_name = None.  
+00009740: 2020 2020 2020 6966 2073 7562 7661 6c75        if subvalu
+00009750: 652e 7061 7265 6e74 2069 7320 4e6f 6e65  e.parent is None
+00009760: 3a0a 2020 2020 2020 2020 2020 2320 5072  :.          # Pr
+00009770: 6573 6572 7665 2073 6861 7269 6e67 2d62  eserve sharing-b
+00009780: 792d 7265 6665 7265 6e63 6520 7265 6c61  y-reference rela
+00009790: 7469 6f6e 7368 6970 7320 6475 7269 6e67  tionships during
+000097a0: 2061 646f 7074 696f 6e0a 2020 2020 2020   adoption.      
+000097b0: 2020 2020 2320 7669 6120 6361 6368 6520      # via cache 
+000097c0: 6b65 7965 6420 6f6e 2075 6e69 7175 6520  keyed on unique 
+000097d0: 696e 7374 616e 6365 2069 6473 2e0a 2020  instance ids..  
+000097e0: 2020 2020 2020 2020 6b65 7920 3d20 7375          key = su
+000097f0: 6276 616c 7565 2e5f 6964 0a20 2020 2020  bvalue._id.     
+00009800: 2020 2020 2023 204d 6f64 756c 6520 7761       # Module wa
+00009810: 7320 7061 7373 6564 2066 726f 6d20 6f75  s passed from ou
+00009820: 7473 6964 652e 2049 7420 6e65 6564 7320  tside. It needs 
+00009830: 746f 2062 6520 636c 6f6e 6564 2e0a 2020  to be cloned..  
+00009840: 2020 2020 2020 2020 2320 4f75 7473 6964          # Outsid
+00009850: 6520 6d6f 6475 6c65 7320 6172 6520 6e61  e modules are na
+00009860: 6d65 6420 6279 2061 7474 6163 686d 656e  med by attachmen
+00009870: 742c 206e 6f74 2061 6e20 6f75 7465 7220  t, not an outer 
+00009880: 6e61 6d65 2c0a 2020 2020 2020 2020 2020  name,.          
+00009890: 2320 554e 4c45 5353 2077 6527 7265 2075  # UNLESS we're u
+000098a0: 7369 6e67 206e 6577 2061 646f 7074 6564  sing new adopted
+000098b0: 206e 616d 6520 706f 6c69 6379 2c20 696e   name policy, in
+000098c0: 2077 6869 6368 2063 6173 6520 616e 2065   which case an e
+000098d0: 7869 7374 696e 670a 2020 2020 2020 2020  xisting.        
+000098e0: 2020 2320 6e61 6d65 2077 696c 6c20 6265    # name will be
+000098f0: 2075 7365 642c 2061 7320 6973 206f 6674   used, as is oft
+00009900: 656e 2073 7570 706c 6965 6420 6279 2063  en supplied by c
+00009910: 6f6e 6669 6720 7379 7374 656d 732e 0a20  onfig systems.. 
+00009920: 2020 2020 2020 2020 2069 6620 7072 6573           if pres
+00009930: 6572 7665 5f61 646f 7074 6564 5f6e 616d  erve_adopted_nam
+00009940: 6573 3a0a 2020 2020 2020 2020 2020 2020  es:.            
+00009950: 6164 6f70 7465 645f 6e61 6d65 203d 206f  adopted_name = o
+00009960: 626a 6563 742e 5f5f 6765 7461 7474 7269  bject.__getattri
+00009970: 6275 7465 5f5f 2873 7562 7661 6c75 652c  bute__(subvalue,
+00009980: 2027 6e61 6d65 2729 0a20 2020 2020 2020   'name').       
+00009990: 2020 2069 6620 6b65 7920 696e 2063 6163     if key in cac
+000099a0: 6865 3a0a 2020 2020 2020 2020 2020 2020  he:.            
+000099b0: 7375 6276 616c 7565 203d 2063 6163 6865  subvalue = cache
+000099c0: 5b6b 6579 5d0a 2020 2020 2020 2020 2020  [key].          
+000099d0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+000099e0: 2020 7375 6276 616c 7565 203d 2073 7562    subvalue = sub
+000099f0: 7661 6c75 652e 636c 6f6e 6528 6e61 6d65  value.clone(name
+00009a00: 3d4e 6f6e 6529 0a20 2020 2020 2020 2020  =None).         
+00009a10: 2020 2063 6163 6865 5b6b 6579 5d20 3d20     cache[key] = 
+00009a20: 7375 6276 616c 7565 0a20 2020 2020 2020  subvalue.       
+00009a30: 2069 6620 7375 6276 616c 7565 2e6e 616d   if subvalue.nam
+00009a40: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     
+00009a50: 2020 2020 206f 626a 6563 742e 5f5f 7365       object.__se
+00009a60: 7461 7474 725f 5f28 7375 6276 616c 7565  tattr__(subvalue
+00009a70: 2c20 2770 6172 656e 7427 2c20 7365 6c66  , 'parent', self
+00009a80: 290a 2020 2020 2020 2020 2020 6966 2061  ).          if a
+00009a90: 646f 7074 6564 5f6e 616d 6520 6973 204e  dopted_name is N
+00009aa0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           
+00009ab0: 2061 646f 7074 6564 5f6e 616d 6520 3d20   adopted_name = 
+00009ac0: 6627 7b6e 616d 657d 7b73 7566 6669 787d  f'{name}{suffix}
+00009ad0: 270a 2020 2020 2020 2020 2020 6f62 6a65  '.          obje
+00009ae0: 6374 2e5f 5f73 6574 6174 7472 5f5f 2873  ct.__setattr__(s
+00009af0: 7562 7661 6c75 652c 2027 6e61 6d65 272c  ubvalue, 'name',
+00009b00: 2061 646f 7074 6564 5f6e 616d 6529 0a20   adopted_name). 
+00009b10: 2020 2020 2020 2020 2071 7565 7565 2e61           queue.a
+00009b20: 7070 656e 6428 7375 6276 616c 7565 290a  ppend(subvalue).
+00009b30: 2020 2020 2020 7265 7475 726e 2073 7562        return sub
+00009b40: 7661 6c75 650a 0a20 2020 2076 616c 203d  value..    val =
+00009b50: 205f 6672 6565 7a65 5f61 7474 7228 0a20   _freeze_attr(. 
+00009b60: 2020 2020 2020 205f 6d61 705f 6f76 6572         _map_over
+00009b70: 5f6d 6f64 756c 6573 5f69 6e5f 7472 6565  _modules_in_tree
+00009b80: 280a 2020 2020 2020 2020 2020 2020 6675  (.            fu
+00009b90: 6e63 746f 6f6c 732e 7061 7274 6961 6c28  nctools.partial(
+00009ba0: 6164 6f70 745f 6174 7472 5f6d 6f64 756c  adopt_attr_modul
+00009bb0: 6573 2c20 6361 6368 652c 2071 7565 7565  es, cache, queue
+00009bc0: 292c 2076 616c 0a20 2020 2020 2020 2029  ), val.        )
+00009bd0: 0a20 2020 2029 0a20 2020 206f 626a 6563  .    ).    objec
+00009be0: 742e 5f5f 7365 7461 7474 725f 5f28 7365  t.__setattr__(se
+00009bf0: 6c66 2c20 6e61 6d65 2c20 7661 6c29 0a20  lf, name, val). 
+00009c00: 2020 2066 6f72 2078 2069 6e20 7175 6575     for x in queu
+00009c10: 653a 0a20 2020 2020 2078 2e5f 5f70 6f73  e:.      x.__pos
+00009c20: 745f 696e 6974 5f5f 2829 0a0a 2020 6465  t_init__()..  de
+00009c30: 6620 5f74 7279 5f73 6574 7570 2873 656c  f _try_setup(sel
+00009c40: 662c 2073 6861 6c6c 6f77 3a20 626f 6f6c  f, shallow: bool
+00009c50: 203d 2046 616c 7365 2920 2d3e 204e 6f6e   = False) -> Non
+00009c60: 653a 0a20 2020 2022 2222 5472 6965 7320  e:.    """Tries 
+00009c70: 746f 2073 6574 7570 206d 6f64 756c 6520  to setup module 
+00009c80: 6966 2073 636f 7065 2069 7320 6176 6169  if scope is avai
+00009c90: 6c61 626c 6520 616e 6420 7365 7475 7020  lable and setup 
+00009ca0: 6861 7320 6e6f 7420 6265 656e 2063 616c  has not been cal
+00009cb0: 6c65 6420 7965 742e 2222 220a 2020 2020  led yet.""".    
+00009cc0: 6966 2028 0a20 2020 2020 2020 2073 656c  if (.        sel
+00009cd0: 662e 7363 6f70 650a 2020 2020 2020 2020  f.scope.        
+00009ce0: 616e 6420 6e6f 7420 7365 6c66 2e5f 7374  and not self._st
+00009cf0: 6174 652e 696e 5f73 6574 7570 0a20 2020  ate.in_setup.   
+00009d00: 2020 2020 2061 6e64 2073 656c 662e 5f73       and self._s
+00009d10: 7461 7465 2e73 6574 7570 5f63 616c 6c65  tate.setup_calle
+00009d20: 6420 213d 2053 6574 7570 5374 6174 652e  d != SetupState.
+00009d30: 444f 4e45 0a20 2020 2029 3a0a 2020 2020  DONE.    ):.    
+00009d40: 2020 7472 793a 0a20 2020 2020 2020 2073    try:.        s
+00009d50: 656c 662e 5f73 7461 7465 2e69 6e5f 7365  elf._state.in_se
+00009d60: 7475 7020 3d20 5472 7565 0a20 2020 2020  tup = True.     
+00009d70: 2020 2023 2041 2073 6861 6c6c 6f77 2073     # A shallow s
+00009d80: 6574 7570 2077 696c 6c20 6f6e 6c79 2072  etup will only r
+00009d90: 6567 6973 7465 7220 6174 7472 6962 7574  egister attribut
+00009da0: 6520 7375 626d 6f64 756c 6573 2062 7574  e submodules but
+00009db0: 2069 7420 646f 6573 0a20 2020 2020 2020   it does.       
+00009dc0: 2023 206e 6f74 2063 616c 6c20 7468 6520   # not call the 
+00009dd0: 7573 6572 2773 2073 6574 7570 2e20 5468  user's setup. Th
+00009de0: 6973 2061 766f 6964 7320 7275 6e6e 696e  is avoids runnin
+00009df0: 6720 6265 666f 7265 2061 0a20 2020 2020  g before a.     
+00009e00: 2020 2023 2074 7261 6e73 666f 726d 6174     # transformat
+00009e10: 696f 6e2e 0a20 2020 2020 2020 2066 6f72  ion..        for
+00009e20: 2066 6965 6c64 2069 6e20 6461 7461 636c   field in datacl
+00009e30: 6173 7365 732e 6669 656c 6473 2873 656c  asses.fields(sel
+00009e40: 6629 3a0a 2020 2020 2020 2020 2020 6966  f):.          if
+00009e50: 2066 6965 6c64 2e6e 616d 6520 6e6f 7420   field.name not 
+00009e60: 696e 2028 2770 6172 656e 7427 2c20 276e  in ('parent', 'n
+00009e70: 616d 6527 2920 616e 6420 6669 656c 642e  ame') and field.
+00009e80: 696e 6974 3a0a 2020 2020 2020 2020 2020  init:.          
+00009e90: 2020 7365 6c66 2e5f 7265 6769 7374 6572    self._register
+00009ea0: 5f73 7562 6d6f 6475 6c65 7328 6669 656c  _submodules(fiel
+00009eb0: 642e 6e61 6d65 2c20 6765 7461 7474 7228  d.name, getattr(
+00009ec0: 7365 6c66 2c20 6669 656c 642e 6e61 6d65  self, field.name
+00009ed0: 2929 0a20 2020 2020 2020 2069 6620 6e6f  )).        if no
+00009ee0: 7420 7368 616c 6c6f 773a 0a20 2020 2020  t shallow:.     
+00009ef0: 2020 2020 2073 656c 662e 7365 7475 7028       self.setup(
+00009f00: 290a 2020 2020 2020 2020 2320 5765 2072  ).        # We r
+00009f10: 756e 2073 7461 7469 6320 6368 6563 6b73  un static checks
+00009f20: 2061 6273 7472 6163 746c 7920 6f6e 6365   abstractly once
+00009f30: 2066 6f72 2073 6574 7570 2062 6566 6f72   for setup befor
+00009f40: 6520 616e 7920 7472 616e 7366 6f72 6d73  e any transforms
+00009f50: 0a20 2020 2020 2020 2023 2074 6f20 6465  .        # to de
+00009f60: 7465 6374 206e 616d 6520 636f 6c6c 6973  tect name collis
+00009f70: 696f 6e73 2061 6e64 206f 7468 6572 2070  ions and other p
+00009f80: 7974 686f 6e20 6572 726f 7273 2e0a 2020  ython errors..  
+00009f90: 2020 2020 2020 656c 6966 2073 656c 662e        elif self.
+00009fa0: 5f73 7461 7465 2e73 6574 7570 5f63 616c  _state.setup_cal
+00009fb0: 6c65 6420 3d3d 2053 6574 7570 5374 6174  led == SetupStat
+00009fc0: 652e 4e45 573a 0a20 2020 2020 2020 2020  e.NEW:.         
+00009fd0: 2073 656c 662e 5f76 616c 6964 6174 655f   self._validate_
+00009fe0: 7365 7475 7028 290a 2020 2020 2020 6669  setup().      fi
+00009ff0: 6e61 6c6c 793a 0a20 2020 2020 2020 2073  nally:.        s
+0000a000: 656c 662e 5f73 7461 7465 2e69 6e5f 7365  elf._state.in_se
+0000a010: 7475 7020 3d20 4661 6c73 650a 2020 2020  tup = False.    
+0000a020: 2020 2020 6966 206e 6f74 2073 6861 6c6c      if not shall
+0000a030: 6f77 3a0a 2020 2020 2020 2020 2020 7365  ow:.          se
+0000a040: 6c66 2e5f 7374 6174 652e 7365 7475 705f  lf._state.setup_
+0000a050: 6361 6c6c 6564 203d 2053 6574 7570 5374  called = SetupSt
+0000a060: 6174 652e 444f 4e45 0a0a 2020 6465 6620  ate.DONE..  def 
+0000a070: 5f76 616c 6964 6174 655f 7365 7475 7028  _validate_setup(
+0000a080: 7365 6c66 2920 2d3e 204e 6f6e 653a 0a20  self) -> None:. 
+0000a090: 2020 2022 2222 4162 7374 7261 6374 6c79     """Abstractly
+0000a0a0: 2065 7661 6c75 6174 6573 2073 6574 7570   evaluates setup
+0000a0b0: 206f 6e6c 7920 746f 2072 756e 2073 7461   only to run sta
+0000a0c0: 7469 6320 6368 6563 6b73 2e22 2222 0a0a  tic checks."""..
+0000a0d0: 2020 2020 6465 6620 7275 6e5f 7365 7475      def run_setu
+0000a0e0: 705f 6f6e 6c79 2878 293a 0a20 2020 2020  p_only(x):.     
+0000a0f0: 2077 7261 7070 6564 5f69 6420 3d20 7772   wrapped_id = wr
+0000a100: 6170 5f6d 6574 686f 645f 6f6e 6365 286c  ap_method_once(l
+0000a110: 616d 6264 6120 6d2c 2078 3a20 7829 0a20  ambda m, x: x). 
+0000a120: 2020 2020 2077 6974 6820 5465 7374 5363       with TestSc
+0000a130: 6f70 6528 7b7d 2c20 726e 6773 3d7b 7d2c  ope({}, rngs={},
+0000a140: 206d 7574 6162 6c65 3d54 7275 6529 2e74   mutable=True).t
+0000a150: 656d 706f 7261 7279 2829 2061 7320 726f  emporary() as ro
+0000a160: 6f74 3a0a 2020 2020 2020 2020 7265 7475  ot:.        retu
+0000a170: 726e 2077 7261 7070 6564 5f69 6428 7365  rn wrapped_id(se
+0000a180: 6c66 2e63 6c6f 6e65 2870 6172 656e 743d  lf.clone(parent=
+0000a190: 726f 6f74 292c 2078 290a 0a20 2020 205f  root), x)..    _
+0000a1a0: 203d 206a 6178 2e65 7661 6c5f 7368 6170   = jax.eval_shap
+0000a1b0: 6528 7275 6e5f 7365 7475 705f 6f6e 6c79  e(run_setup_only
+0000a1c0: 2c20 3029 0a0a 2020 6465 6620 5f6e 616d  , 0)..  def _nam
+0000a1d0: 655f 7461 6b65 6e28 0a20 2020 2020 2073  e_taken(.      s
+0000a1e0: 656c 662c 0a20 2020 2020 206e 616d 653a  elf,.      name:
+0000a1f0: 2073 7472 2c0a 2020 2020 2020 6d6f 6475   str,.      modu
+0000a200: 6c65 3a20 4f70 7469 6f6e 616c 5b27 4d6f  le: Optional['Mo
+0000a210: 6475 6c65 275d 203d 204e 6f6e 652c 0a20  dule'] = None,. 
+0000a220: 2020 2020 2072 6575 7365 5f73 636f 7065       reuse_scope
+0000a230: 733a 2062 6f6f 6c20 3d20 4661 6c73 652c  s: bool = False,
+0000a240: 0a20 2020 2020 2063 6f6c 6c65 6374 696f  .      collectio
+0000a250: 6e3a 204f 7074 696f 6e61 6c5b 7374 725d  n: Optional[str]
+0000a260: 203d 204e 6f6e 652c 0a20 2029 202d 3e20   = None,.  ) -> 
+0000a270: 626f 6f6c 3a0a 2020 2020 6173 7365 7274  bool:.    assert
+0000a280: 2073 656c 662e 7363 6f70 6520 6973 206e   self.scope is n
+0000a290: 6f74 204e 6f6e 650a 2020 2020 6966 2072  ot None.    if r
+0000a2a0: 6575 7365 5f73 636f 7065 733a 0a20 2020  euse_scopes:.   
+0000a2b0: 2020 2072 6574 7572 6e20 4661 6c73 650a     return False.
+0000a2c0: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
+0000a2d0: 7363 6f70 652e 6e61 6d65 5f72 6573 6572  scope.name_reser
+0000a2e0: 7665 6428 6e61 6d65 2c20 636f 6c6c 6563  ved(name, collec
+0000a2f0: 7469 6f6e 290a 0a20 2040 7072 6f70 6572  tion)..  @proper
+0000a300: 7479 0a20 2064 6566 205f 696e 6974 6961  ty.  def _initia
+0000a310: 6c69 7a61 7469 6f6e 5f61 6c6c 6f77 6564  lization_allowed
+0000a320: 2873 656c 6629 3a0a 2020 2020 7265 7475  (self):.    retu
+0000a330: 726e 2028 0a20 2020 2020 2020 206e 6f74  rn (.        not
+0000a340: 2073 656c 662e 5f73 7461 7465 2e69 735f   self._state.is_
+0000a350: 696e 6974 6961 6c69 7a65 6420 2023 2061  initialized  # a
+0000a360: 6c6c 6f77 2065 6167 6572 2061 7474 6163  llow eager attac
+0000a370: 686d 656e 7420 696e 2070 6f73 742d 696e  hment in post-in
+0000a380: 6974 0a20 2020 2020 2020 206f 7220 7365  it.        or se
+0000a390: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
+0000a3a0: 7570 0a20 2020 2020 2020 206f 7220 7365  up.        or se
+0000a3b0: 6c66 2e5f 7374 6174 652e 696e 5f63 6f6d  lf._state.in_com
+0000a3c0: 7061 6374 5f6d 6574 686f 640a 2020 2020  pact_method.    
+0000a3d0: 290a 0a20 2064 6566 2063 6c6f 6e65 280a  )..  def clone(.
+0000a3e0: 2020 2020 2020 7365 6c66 3a20 4d2c 0a20        self: M,. 
+0000a3f0: 2020 2020 202a 2c0a 2020 2020 2020 7061       *,.      pa
+0000a400: 7265 6e74 3a20 4f70 7469 6f6e 616c 5b55  rent: Optional[U
+0000a410: 6e69 6f6e 5b53 636f 7065 2c20 274d 6f64  nion[Scope, 'Mod
+0000a420: 756c 6527 5d5d 203d 204e 6f6e 652c 0a20  ule']] = None,. 
+0000a430: 2020 2020 205f 6465 6570 5f63 6c6f 6e65       _deep_clone
+0000a440: 3a20 556e 696f 6e5b 626f 6f6c 2c20 7765  : Union[bool, we
+0000a450: 616b 7265 662e 5765 616b 5661 6c75 6544  akref.WeakValueD
+0000a460: 6963 7469 6f6e 6172 795d 203d 2046 616c  ictionary] = Fal
+0000a470: 7365 2c0a 2020 2020 2020 2a2a 7570 6461  se,.      **upda
+0000a480: 7465 732c 0a20 2029 202d 3e20 4d3a 0a20  tes,.  ) -> M:. 
+0000a490: 2020 2022 2222 4372 6561 7465 7320 6120     """Creates a 
+0000a4a0: 636c 6f6e 6520 6f66 2074 6869 7320 4d6f  clone of this Mo
+0000a4b0: 6475 6c65 2c20 7769 7468 206f 7074 696f  dule, with optio
+0000a4c0: 6e61 6c6c 7920 7570 6461 7465 6420 6172  nally updated ar
+0000a4d0: 6775 6d65 6e74 732e 0a0a 2020 2020 4172  guments...    Ar
+0000a4e0: 6773 3a0a 2020 2020 2020 7061 7265 6e74  gs:.      parent
+0000a4f0: 3a20 5468 6520 7061 7265 6e74 206f 6620  : The parent of 
+0000a500: 7468 6520 636c 6f6e 652e 2054 6865 2063  the clone. The c
+0000a510: 6c6f 6e65 2077 696c 6c20 6861 7665 206e  lone will have n
+0000a520: 6f20 7061 7265 6e74 2069 6620 6e6f 0a20  o parent if no. 
+0000a530: 2020 2020 2020 2065 7870 6c69 6369 7420         explicit 
+0000a540: 7061 7265 6e74 2069 7320 7370 6563 6966  parent is specif
+0000a550: 6965 642e 0a20 2020 2020 205f 6465 6570  ied..      _deep
+0000a560: 5f63 6c6f 6e65 3a20 4120 626f 6f6c 6561  _clone: A boolea
+0000a570: 6e20 6f72 2061 2077 6561 6b20 7661 6c75  n or a weak valu
+0000a580: 6520 6469 6374 696f 6e61 7279 2074 6f20  e dictionary to 
+0000a590: 636f 6e74 726f 6c20 6465 6570 2063 6c6f  control deep clo
+0000a5a0: 6e69 6e67 0a20 2020 2020 2020 206f 6620  ning.        of 
+0000a5b0: 7375 626d 6f64 756c 6573 2e20 4966 2054  submodules. If T
+0000a5c0: 7275 652c 2073 7562 6d6f 6475 6c65 7320  rue, submodules 
+0000a5d0: 7769 6c6c 2062 6520 636c 6f6e 6564 2072  will be cloned r
+0000a5e0: 6563 7572 7369 7665 6c79 2e20 4966 2061  ecursively. If a
+0000a5f0: 0a20 2020 2020 2020 2077 6561 6b20 7661  .        weak va
+0000a600: 6c75 6520 6469 6374 696f 6e61 7279 2069  lue dictionary i
+0000a610: 7320 7061 7373 6564 2c20 6974 2077 696c  s passed, it wil
+0000a620: 6c20 6265 2075 7365 6420 746f 2063 6163  l be used to cac
+0000a630: 6865 2063 6c6f 6e65 640a 2020 2020 2020  he cloned.      
+0000a640: 2020 7375 626d 6f64 756c 6573 2e20 5468    submodules. Th
+0000a650: 6973 2066 6c61 6720 6973 2075 7365 6420  is flag is used 
+0000a660: 6279 2069 6e69 742f 6170 706c 792f 6269  by init/apply/bi
+0000a670: 6e64 2074 6f20 6176 6f69 6420 7363 6f70  nd to avoid scop
+0000a680: 650a 2020 2020 2020 2020 6c65 616b 6167  e.        leakag
+0000a690: 652e 0a20 2020 2020 202a 2a75 7064 6174  e..      **updat
+0000a6a0: 6573 3a20 4174 7472 6962 7574 6520 7570  es: Attribute up
+0000a6b0: 6461 7465 732e 0a20 2020 2052 6574 7572  dates..    Retur
+0000a6c0: 6e73 3a0a 2020 2020 2020 4120 636c 6f6e  ns:.      A clon
+0000a6d0: 6520 6f66 2074 6865 2074 6869 7320 4d6f  e of the this Mo
+0000a6e0: 6475 6c65 2077 6974 6820 7468 6520 7570  dule with the up
+0000a6f0: 6461 7465 6420 6174 7472 6962 7574 6573  dated attributes
+0000a700: 2061 6e64 2070 6172 656e 742e 0a20 2020   and parent..   
+0000a710: 2022 2222 0a20 2020 2061 7474 7273 203d   """.    attrs =
+0000a720: 207b 0a20 2020 2020 2020 2066 2e6e 616d   {.        f.nam
+0000a730: 653a 2067 6574 6174 7472 2873 656c 662c  e: getattr(self,
+0000a740: 2066 2e6e 616d 6529 0a20 2020 2020 2020   f.name).       
+0000a750: 2066 6f72 2066 2069 6e20 6461 7461 636c   for f in datacl
+0000a760: 6173 7365 732e 6669 656c 6473 2873 656c  asses.fields(sel
+0000a770: 6629 0a20 2020 2020 2020 2069 6620 662e  f).        if f.
+0000a780: 696e 6974 0a20 2020 207d 0a0a 2020 2020  init.    }..    
+0000a790: 6174 7472 732e 7570 6461 7465 2870 6172  attrs.update(par
+0000a7a0: 656e 743d 7061 7265 6e74 2c20 2a2a 7570  ent=parent, **up
+0000a7b0: 6461 7465 7329 0a0a 2020 2020 2320 4865  dates)..    # He
+0000a7c0: 7265 2077 6520 696d 706c 656d 656e 7420  re we implement 
+0000a7d0: 6465 6570 2063 6c6f 6e69 6e67 206f 6620  deep cloning of 
+0000a7e0: 7375 626d 6f64 756c 6573 2c20 7468 6973  submodules, this
+0000a7f0: 2069 7320 6e65 6365 7373 6172 7920 746f   is necessary to
+0000a800: 2061 766f 6964 2073 636f 7065 206c 6561   avoid scope lea
+0000a810: 6b61 6765 0a20 2020 2023 2066 726f 6d20  kage.    # from 
+0000a820: 6578 7465 726e 616c 2073 7562 6d6f 6475  external submodu
+0000a830: 6c65 7320 696e 746f 2069 6e69 742f 6170  les into init/ap
+0000a840: 706c 792f 6269 6e64 2077 6869 6c65 2070  ply/bind while p
+0000a850: 7265 7365 7276 696e 6720 7368 6172 696e  reserving sharin
+0000a860: 672d 6279 2d72 6566 6572 656e 6365 0a20  g-by-reference. 
+0000a870: 2020 2023 2072 656c 6174 696f 6e73 6869     # relationshi
+0000a880: 7073 2062 6574 7765 656e 2073 7562 6d6f  ps between submo
+0000a890: 6475 6c65 732e 0a20 2020 2069 6620 5f64  dules..    if _d
+0000a8a0: 6565 705f 636c 6f6e 6520 213d 2046 616c  eep_clone != Fal
+0000a8b0: 7365 3a0a 2020 2020 2020 2320 5765 2075  se:.      # We u
+0000a8c0: 7365 2061 2077 6561 6b20 7661 6c75 6520  se a weak value 
+0000a8d0: 6469 6374 696f 6e61 7279 2074 6f20 6361  dictionary to ca
+0000a8e0: 6368 6520 636c 6f6e 6564 2073 7562 6d6f  che cloned submo
+0000a8f0: 6475 6c65 732e 2057 6865 6e20 6120 7368  dules. When a sh
+0000a900: 6172 6564 0a20 2020 2020 2023 2073 7562  ared.      # sub
+0000a910: 6d6f 6475 6c65 2069 7320 636c 6f6e 6564  module is cloned
+0000a920: 2c20 6974 7320 6f6e 6c79 2063 6c6f 6e65  , its only clone
+0000a930: 6420 6f6e 6365 2065 6c73 6520 6974 7320  d once else its 
+0000a940: 6665 7463 6865 6420 6672 6f6d 2074 6865  fetched from the
+0000a950: 2063 6163 6865 2e0a 2020 2020 2020 6361   cache..      ca
+0000a960: 6368 6520 3d20 280a 2020 2020 2020 2020  che = (.        
+0000a970: 2020 7765 616b 7265 662e 5765 616b 5661    weakref.WeakVa
+0000a980: 6c75 6544 6963 7469 6f6e 6172 7928 290a  lueDictionary().
+0000a990: 2020 2020 2020 2020 2020 6966 2069 7369            if isi
+0000a9a0: 6e73 7461 6e63 6528 5f64 6565 705f 636c  nstance(_deep_cl
+0000a9b0: 6f6e 652c 2062 6f6f 6c29 0a20 2020 2020  one, bool).     
+0000a9c0: 2020 2020 2065 6c73 6520 5f64 6565 705f       else _deep_
+0000a9d0: 636c 6f6e 650a 2020 2020 2020 290a 0a20  clone.      ).. 
+0000a9e0: 2020 2020 2064 6566 2063 6c6f 6e65 5f66       def clone_f
+0000a9f0: 6e28 6d3a 204d 6f64 756c 6529 202d 3e20  n(m: Module) -> 
+0000aa00: 4d6f 6475 6c65 3a0a 2020 2020 2020 2020  Module:.        
+0000aa10: 6966 2068 6173 6174 7472 286d 2c20 275f  if hasattr(m, '_
+0000aa20: 6964 2729 3a0a 2020 2020 2020 2020 2020  id'):.          
+0000aa30: 6b65 7920 3d20 6d2e 5f69 640a 2020 2020  key = m._id.    
+0000aa40: 2020 2020 2020 6966 206b 6579 2069 6e20        if key in 
+0000aa50: 6361 6368 653a 0a20 2020 2020 2020 2020  cache:.         
+0000aa60: 2020 2072 6574 7572 6e20 6361 6368 655b     return cache[
+0000aa70: 6b65 795d 0a20 2020 2020 2020 2020 2065  key].          e
+0000aa80: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+0000aa90: 2063 6c6f 6e65 203d 206d 2e63 6c6f 6e65   clone = m.clone
+0000aaa0: 285f 6465 6570 5f63 6c6f 6e65 3d63 6163  (_deep_clone=cac
+0000aab0: 6865 290a 2020 2020 2020 2020 2020 2020  he).            
+0000aac0: 6361 6368 655b 6b65 795d 203d 2063 6c6f  cache[key] = clo
+0000aad0: 6e65 0a20 2020 2020 2020 2020 2020 2072  ne.            r
+0000aae0: 6574 7572 6e20 636c 6f6e 650a 2020 2020  eturn clone.    
+0000aaf0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0000ab00: 2020 2020 2320 4966 2074 6865 206d 6f64      # If the mod
+0000ab10: 756c 6520 646f 6573 6e27 7420 6861 7665  ule doesn't have
+0000ab20: 2061 6e20 5f69 6420 6174 7472 6962 7574   an _id attribut
+0000ab30: 6520 6974 2063 6f75 6c64 2062 6520 6120  e it could be a 
+0000ab40: 6d6f 636b 206f 626a 6563 740a 2020 2020  mock object.    
+0000ab50: 2020 2020 2020 2320 736f 2077 6520 7265        # so we re
+0000ab60: 7475 726e 2069 7420 6173 2069 732e 0a20  turn it as is.. 
+0000ab70: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+0000ab80: 6d0a 0a20 2020 2020 2023 205f 6d61 705f  m..      # _map_
+0000ab90: 7375 626d 6f64 756c 6573 2077 696c 6c20  submodules will 
+0000aba0: 6d61 7020 6f76 6572 2061 6c6c 2073 7562  map over all sub
+0000abb0: 6d6f 6475 6c65 7320 696e 7369 6465 2061  modules inside a
+0000abc0: 7474 7273 0a20 2020 2020 2023 2076 616c  ttrs.      # val
+0000abd0: 7565 2068 6572 6520 6361 6e20 6265 2061  ue here can be a
+0000abe0: 6e79 2070 7974 7265 652c 206e 6f6e 2d6d  ny pytree, non-m
+0000abf0: 6f64 756c 6520 7661 6c75 6573 2061 7265  odule values are
+0000ac00: 2069 676e 6f72 6564 0a20 2020 2020 2066   ignored.      f
+0000ac10: 6f72 2066 6965 6c64 5f6e 616d 652c 2076  or field_name, v
+0000ac20: 616c 7565 2069 6e20 6174 7472 732e 6974  alue in attrs.it
+0000ac30: 656d 7328 293a 0a20 2020 2020 2020 2061  ems():.        a
+0000ac40: 7474 7273 5b66 6965 6c64 5f6e 616d 655d  ttrs[field_name]
+0000ac50: 203d 205f 6d61 705f 7375 626d 6f64 756c   = _map_submodul
+0000ac60: 6573 2863 6c6f 6e65 5f66 6e2c 2076 616c  es(clone_fn, val
+0000ac70: 7565 290a 0a20 2020 206d 6f64 756c 6520  ue)..    module 
+0000ac80: 3d20 7365 6c66 2e5f 5f63 6c61 7373 5f5f  = self.__class__
+0000ac90: 282a 2a61 7474 7273 290a 0a20 2020 2072  (**attrs)..    r
+0000aca0: 6574 7572 6e20 6d6f 6475 6c65 0a0a 2020  eturn module..  
+0000acb0: 6465 6620 7661 7269 6162 6c65 280a 2020  def variable(.  
+0000acc0: 2020 2020 7365 6c66 2c0a 2020 2020 2020      self,.      
+0000acd0: 636f 6c3a 2073 7472 2c0a 2020 2020 2020  col: str,.      
+0000ace0: 6e61 6d65 3a20 7374 722c 0a20 2020 2020  name: str,.     
+0000acf0: 2069 6e69 745f 666e 3a20 4f70 7469 6f6e   init_fn: Option
+0000ad00: 616c 5b43 616c 6c61 626c 655b 2e2e 2e2c  al[Callable[...,
+0000ad10: 2041 6e79 5d5d 203d 204e 6f6e 652c 0a20   Any]] = None,. 
+0000ad20: 2020 2020 202a 696e 6974 5f61 7267 732c       *init_args,
+0000ad30: 0a20 2020 2020 2075 6e62 6f78 3a20 626f  .      unbox: bo
+0000ad40: 6f6c 203d 2054 7275 652c 0a20 2029 202d  ol = True,.  ) -
+0000ad50: 3e20 5661 7269 6162 6c65 3a0a 2020 2020  > Variable:.    
+0000ad60: 2222 2244 6563 6c61 7265 7320 616e 6420  """Declares and 
+0000ad70: 7265 7475 726e 7320 6120 7661 7269 6162  returns a variab
+0000ad80: 6c65 2069 6e20 7468 6973 204d 6f64 756c  le in this Modul
+0000ad90: 652e 0a0a 2020 2020 5365 6520 3a6d 6f64  e...    See :mod
+0000ada0: 3a60 666c 6178 2e63 6f72 652e 7661 7269  :`flax.core.vari
+0000adb0: 6162 6c65 7360 2066 6f72 206d 6f72 6520  ables` for more 
+0000adc0: 696e 666f 726d 6174 696f 6e2e 2053 6565  information. See
+0000add0: 2061 6c73 6f20 3a6d 6574 683a 6070 6172   also :meth:`par
+0000ade0: 616d 600a 2020 2020 666f 7220 6120 7368  am`.    for a sh
+0000adf0: 6f72 7468 616e 6420 7761 7920 746f 2064  orthand way to d
+0000ae00: 6566 696e 6520 7265 6164 2d6f 6e6c 7920  efine read-only 
+0000ae10: 7661 7269 6162 6c65 7320 696e 2074 6865  variables in the
+0000ae20: 2022 7061 7261 6d73 220a 2020 2020 636f   "params".    co
+0000ae30: 6c6c 6563 7469 6f6e 2e0a 0a20 2020 2043  llection...    C
+0000ae40: 6f6e 7472 6172 7920 746f 203a 6d65 7468  ontrary to :meth
+0000ae50: 3a60 7061 7261 6d60 2c20 616c 6c20 6172  :`param`, all ar
+0000ae60: 6775 6d65 6e74 7320 7061 7373 696e 6720  guments passing 
+0000ae70: 7573 696e 6720 6069 6e69 745f 666e 6020  using `init_fn` 
+0000ae80: 7368 6f75 6c64 2062 650a 2020 2020 7061  should be.    pa
+0000ae90: 7373 6564 206f 6e20 6578 706c 6963 6974  ssed on explicit
+0000aea0: 6c79 3a3a 0a0a 2020 2020 2020 6b65 7920  ly::..      key 
+0000aeb0: 3d20 7365 6c66 2e6d 616b 655f 726e 6728  = self.make_rng(
+0000aec0: 2773 7461 7473 2729 0a20 2020 2020 206d  'stats').      m
+0000aed0: 6561 6e20 3d20 7365 6c66 2e76 6172 6961  ean = self.varia
+0000aee0: 626c 6528 2773 7461 7473 272c 2027 6d65  ble('stats', 'me
+0000aef0: 616e 272c 206c 6563 756e 5f6e 6f72 6d61  an', lecun_norma
+0000af00: 6c28 292c 206b 6579 2c20 2832 2c20 3229  l(), key, (2, 2)
+0000af10: 290a 0a20 2020 2049 6e20 7468 6520 6578  )..    In the ex
+0000af20: 616d 706c 6520 6162 6f76 652c 2074 6865  ample above, the
+0000af30: 2066 756e 6374 696f 6e20 606c 6563 756e   function `lecun
+0000af40: 5f6e 6f72 6d61 6c60 2065 7870 6563 7473  _normal` expects
+0000af50: 2074 776f 2061 7267 756d 656e 7473 3a0a   two arguments:.
+0000af60: 2020 2020 606b 6579 6020 616e 6420 6073      `key` and `s
+0000af70: 6861 7065 602c 2061 6e64 2062 6f74 6820  hape`, and both 
+0000af80: 6861 7665 2074 6f20 6265 2070 6173 7365  have to be passe
+0000af90: 6420 6f6e 2e20 5468 6520 5052 4e47 2066  d on. The PRNG f
+0000afa0: 6f72 2060 7374 6174 7360 2068 6173 0a20  or `stats` has. 
+0000afb0: 2020 2074 6f20 6265 2070 726f 7669 6465     to be provide
+0000afc0: 6420 6578 706c 6963 6974 6c79 2077 6865  d explicitly whe
+0000afd0: 6e20 6361 6c6c 696e 6720 3a6d 6574 683a  n calling :meth:
+0000afe0: 6069 6e69 7460 2061 6e64 203a 6d65 7468  `init` and :meth
+0000aff0: 3a60 6170 706c 7960 2e0a 0a20 2020 2041  :`apply`...    A
+0000b000: 7267 733a 0a20 2020 2020 2063 6f6c 3a20  rgs:.      col: 
+0000b010: 5468 6520 7661 7269 6162 6c65 2063 6f6c  The variable col
+0000b020: 6c65 6374 696f 6e20 6e61 6d65 2e0a 2020  lection name..  
+0000b030: 2020 2020 6e61 6d65 3a20 5468 6520 7661      name: The va
+0000b040: 7269 6162 6c65 206e 616d 652e 0a20 2020  riable name..   
+0000b050: 2020 2069 6e69 745f 666e 3a20 5468 6520     init_fn: The 
+0000b060: 6675 6e63 7469 6f6e 2074 6861 7420 7769  function that wi
+0000b070: 6c6c 2062 6520 6361 6c6c 6564 2074 6f20  ll be called to 
+0000b080: 636f 6d70 7574 6520 7468 6520 696e 6974  compute the init
+0000b090: 6961 6c20 7661 6c75 650a 2020 2020 2020  ial value.      
+0000b0a0: 2020 6f66 2074 6869 7320 7661 7269 6162    of this variab
+0000b0b0: 6c65 2e20 5468 6973 2066 756e 6374 696f  le. This functio
+0000b0c0: 6e20 7769 6c6c 206f 6e6c 7920 6265 2063  n will only be c
+0000b0d0: 616c 6c65 6420 7468 6520 6669 7273 7420  alled the first 
+0000b0e0: 7469 6d65 0a20 2020 2020 2020 2074 6869  time.        thi
+0000b0f0: 7320 7661 7269 6162 6c65 2069 7320 7573  s variable is us
+0000b100: 6564 2069 6e20 7468 6973 206d 6f64 756c  ed in this modul
+0000b110: 652e 2049 6620 4e6f 6e65 2c20 7468 6520  e. If None, the 
+0000b120: 7661 7269 6162 6c65 206d 7573 740a 2020  variable must.  
+0000b130: 2020 2020 2020 616c 7265 6164 7920 6265        already be
+0000b140: 2069 6e69 7469 616c 697a 6564 206f 7468   initialized oth
+0000b150: 6572 7769 7365 2061 6e20 6572 726f 7220  erwise an error 
+0000b160: 6973 2072 6169 7365 642e 0a20 2020 2020  is raised..     
+0000b170: 202a 696e 6974 5f61 7267 733a 2054 6865   *init_args: The
+0000b180: 2061 7267 756d 656e 7473 2074 6f20 7061   arguments to pa
+0000b190: 7373 2074 6f20 696e 6974 5f66 6e2e 0a20  ss to init_fn.. 
+0000b1a0: 2020 2020 2075 6e62 6f78 3a20 4966 2054       unbox: If T
+0000b1b0: 7275 652c 2060 6041 7869 734d 6574 6164  rue, ``AxisMetad
+0000b1c0: 6174 6160 6020 696e 7374 616e 6365 7320  ata`` instances 
+0000b1d0: 6172 6520 7265 706c 6163 6564 2062 7920  are replaced by 
+0000b1e0: 7468 6569 7220 756e 626f 7865 640a 2020  their unboxed.  
+0000b1f0: 2020 2020 2020 7661 6c75 652c 2073 6565        value, see
+0000b200: 2060 6066 6c61 782e 6e6e 2e6d 6574 612e   ``flax.nn.meta.
+0000b210: 756e 626f 7860 6020 2864 6566 6175 6c74  unbox`` (default
+0000b220: 3a20 5472 7565 292e 0a0a 2020 2020 5265  : True)...    Re
+0000b230: 7475 726e 733a 0a20 2020 2020 2041 203a  turns:.      A :
+0000b240: 636c 6173 733a 6066 6c61 782e 636f 7265  class:`flax.core
+0000b250: 2e76 6172 6961 626c 6573 2e56 6172 6961  .variables.Varia
+0000b260: 626c 6560 2074 6861 7420 6361 6e20 6265  ble` that can be
+0000b270: 2072 6561 6420 6f72 2073 6574 2076 6961   read or set via
+0000b280: 0a20 2020 2020 2022 2e76 616c 7565 2220  .      ".value" 
+0000b290: 6174 7472 6962 7574 652e 2054 6872 6f77  attribute. Throw
+0000b2a0: 7320 616e 2065 7272 6f72 2069 6620 7468  s an error if th
+0000b2b0: 6520 7661 7269 6162 6c65 2065 7869 7374  e variable exist
+0000b2c0: 7320 616c 7265 6164 792e 0a20 2020 2022  s already..    "
+0000b2d0: 2222 0a20 2020 2069 6620 6e6f 7420 7365  "".    if not se
+0000b2e0: 6c66 2e5f 696e 6974 6961 6c69 7a61 7469  lf._initializati
+0000b2f0: 6f6e 5f61 6c6c 6f77 6564 3a0a 2020 2020  on_allowed:.    
+0000b300: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+0000b310: 6f72 280a 2020 2020 2020 2020 2020 2756  or(.          'V
+0000b320: 6172 6961 626c 6573 206d 7573 7420 6265  ariables must be
+0000b330: 2069 6e69 7469 616c 697a 6564 2069 6e20   initialized in 
+0000b340: 6073 6574 7570 2829 6020 6f72 2069 6e20  `setup()` or in 
+0000b350: 6120 6d65 7468 6f64 2027 0a20 2020 2020  a method '.     
+0000b360: 2020 2020 2027 7772 6170 7065 6420 696e       'wrapped in
+0000b370: 2060 4063 6f6d 7061 6374 6027 0a20 2020   `@compact`'.   
+0000b380: 2020 2029 0a20 2020 2069 6620 7365 6c66     ).    if self
+0000b390: 2e5f 6e61 6d65 5f74 616b 656e 286e 616d  ._name_taken(nam
+0000b3a0: 652c 2063 6f6c 6c65 6374 696f 6e3d 636f  e, collection=co
+0000b3b0: 6c29 3a0a 2020 2020 2020 7261 6973 6520  l):.      raise 
+0000b3c0: 6572 726f 7273 2e4e 616d 6549 6e55 7365  errors.NameInUse
+0000b3d0: 4572 726f 7228 2776 6172 6961 626c 6527  Error('variable'
+0000b3e0: 2c20 6e61 6d65 2c20 7365 6c66 2e5f 5f63  , name, self.__c
+0000b3f0: 6c61 7373 5f5f 2e5f 5f6e 616d 655f 5f29  lass__.__name__)
+0000b400: 0a20 2020 2061 7373 6572 7420 7365 6c66  .    assert self
+0000b410: 2e73 636f 7065 2069 7320 6e6f 7420 4e6f  .scope is not No
+0000b420: 6e65 0a20 2020 2076 203d 2073 656c 662e  ne.    v = self.
+0000b430: 7363 6f70 652e 7661 7269 6162 6c65 2863  scope.variable(c
+0000b440: 6f6c 2c20 6e61 6d65 2c20 696e 6974 5f66  ol, name, init_f
+0000b450: 6e2c 202a 696e 6974 5f61 7267 732c 2075  n, *init_args, u
+0000b460: 6e62 6f78 3d75 6e62 6f78 290a 2020 2020  nbox=unbox).    
+0000b470: 7365 6c66 2e5f 7374 6174 652e 6368 696c  self._state.chil
+0000b480: 6472 656e 5b6e 616d 655d 203d 2063 6f6c  dren[name] = col
+0000b490: 0a20 2020 2072 6574 7572 6e20 760a 0a20  .    return v.. 
+0000b4a0: 2064 6566 2070 6172 616d 280a 2020 2020   def param(.    
+0000b4b0: 2020 7365 6c66 2c20 6e61 6d65 3a20 7374    self, name: st
+0000b4c0: 722c 2069 6e69 745f 666e 3a20 4361 6c6c  r, init_fn: Call
+0000b4d0: 6162 6c65 5b2e 2e2e 2c20 545d 2c20 2a69  able[..., T], *i
+0000b4e0: 6e69 745f 6172 6773 2c20 756e 626f 783a  nit_args, unbox:
+0000b4f0: 2062 6f6f 6c20 3d20 5472 7565 0a20 2029   bool = True.  )
+0000b500: 202d 3e20 543a 0a20 2020 2022 2222 4465   -> T:.    """De
+0000b510: 636c 6172 6573 2061 6e64 2072 6574 7572  clares and retur
+0000b520: 6e73 2061 2070 6172 616d 6574 6572 2069  ns a parameter i
+0000b530: 6e20 7468 6973 204d 6f64 756c 652e 0a0a  n this Module...
+0000b540: 2020 2020 5061 7261 6d65 7465 7273 2061      Parameters a
+0000b550: 7265 2072 6561 642d 6f6e 6c79 2076 6172  re read-only var
+0000b560: 6961 626c 6573 2069 6e20 7468 6520 636f  iables in the co
+0000b570: 6c6c 6563 7469 6f6e 206e 616d 6564 2022  llection named "
+0000b580: 7061 7261 6d73 222e 2053 6565 0a20 2020  params". See.   
+0000b590: 203a 6d6f 643a 6066 6c61 782e 636f 7265   :mod:`flax.core
+0000b5a0: 2e76 6172 6961 626c 6573 6020 666f 7220  .variables` for 
+0000b5b0: 6d6f 7265 2064 6574 6169 6c73 206f 6e20  more details on 
+0000b5c0: 7661 7269 6162 6c65 732e 0a0a 2020 2020  variables...    
+0000b5d0: 5468 6520 6669 7273 7420 6172 6775 6d65  The first argume
+0000b5e0: 6e74 206f 6620 6069 6e69 745f 666e 6020  nt of `init_fn` 
+0000b5f0: 6973 2061 7373 756d 6564 2074 6f20 6265  is assumed to be
+0000b600: 2061 2050 524e 4720 6b65 792c 2077 6869   a PRNG key, whi
+0000b610: 6368 2069 730a 2020 2020 7072 6f76 6964  ch is.    provid
+0000b620: 6564 2061 7574 6f6d 6174 6963 616c 6c79  ed automatically
+0000b630: 2061 6e64 2064 6f65 7320 6e6f 7420 6861   and does not ha
+0000b640: 7665 2074 6f20 6265 2070 6173 7365 6420  ve to be passed 
+0000b650: 7573 696e 6720 6069 6e69 745f 6172 6773  using `init_args
+0000b660: 603a 3a0a 0a20 2020 2020 206d 6561 6e20  `::..      mean 
+0000b670: 3d20 7365 6c66 2e70 6172 616d 2827 6d65  = self.param('me
+0000b680: 616e 272c 206c 6563 756e 5f6e 6f72 6d61  an', lecun_norma
+0000b690: 6c28 292c 2028 322c 2032 2929 0a0a 2020  l(), (2, 2))..  
+0000b6a0: 2020 496e 2074 6865 2065 7861 6d70 6c65    In the example
+0000b6b0: 2061 626f 7665 2c20 7468 6520 6675 6e63   above, the func
+0000b6c0: 7469 6f6e 2060 6c65 6375 6e5f 6e6f 726d  tion `lecun_norm
+0000b6d0: 616c 6020 6578 7065 6374 7320 7477 6f20  al` expects two 
+0000b6e0: 6172 6775 6d65 6e74 733a 0a20 2020 2060  arguments:.    `
+0000b6f0: 6b65 7960 2061 6e64 2060 7368 6170 6560  key` and `shape`
+0000b700: 2c20 6275 7420 6f6e 6c79 2060 7368 6170  , but only `shap
+0000b710: 6560 2068 6173 2074 6f20 6265 2070 726f  e` has to be pro
+0000b720: 7669 6465 6420 6578 706c 6963 6974 6c79  vided explicitly
+0000b730: 3b20 606b 6579 600a 2020 2020 6973 2073  ; `key`.    is s
+0000b740: 6574 2061 7574 6f6d 6174 6963 616c 6c79  et automatically
+0000b750: 2075 7369 6e67 2074 6865 2050 524e 4720   using the PRNG 
+0000b760: 666f 7220 6070 6172 616d 7360 2074 6861  for `params` tha
+0000b770: 7420 6973 2070 6173 7365 6420 7768 656e  t is passed when
+0000b780: 0a20 2020 2069 6e69 7469 616c 697a 696e  .    initializin
+0000b790: 6720 7468 6520 6d6f 6475 6c65 2075 7369  g the module usi
+0000b7a0: 6e67 203a 6d65 7468 3a60 696e 6974 602e  ng :meth:`init`.
+0000b7b0: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
+0000b7c0: 2020 6e61 6d65 3a20 5468 6520 7061 7261    name: The para
+0000b7d0: 6d65 7465 7220 6e61 6d65 2e0a 2020 2020  meter name..    
+0000b7e0: 2020 696e 6974 5f66 6e3a 2054 6865 2066    init_fn: The f
+0000b7f0: 756e 6374 696f 6e20 7468 6174 2077 696c  unction that wil
+0000b800: 6c20 6265 2063 616c 6c65 6420 746f 2063  l be called to c
+0000b810: 6f6d 7075 7465 2074 6865 2069 6e69 7469  ompute the initi
+0000b820: 616c 2076 616c 7565 0a20 2020 2020 2020  al value.       
+0000b830: 206f 6620 7468 6973 2076 6172 6961 626c   of this variabl
+0000b840: 652e 2054 6869 7320 6675 6e63 7469 6f6e  e. This function
+0000b850: 2077 696c 6c20 6f6e 6c79 2062 6520 6361   will only be ca
+0000b860: 6c6c 6564 2074 6865 2066 6972 7374 2074  lled the first t
+0000b870: 696d 650a 2020 2020 2020 2020 7468 6973  ime.        this
+0000b880: 2070 6172 616d 6574 6572 2069 7320 7573   parameter is us
+0000b890: 6564 2069 6e20 7468 6973 206d 6f64 756c  ed in this modul
+0000b8a0: 652e 0a20 2020 2020 202a 696e 6974 5f61  e..      *init_a
+0000b8b0: 7267 733a 2054 6865 2061 7267 756d 656e  rgs: The argumen
+0000b8c0: 7473 2074 6f20 7061 7373 2074 6f20 696e  ts to pass to in
+0000b8d0: 6974 5f66 6e2e 0a20 2020 2020 2075 6e62  it_fn..      unb
+0000b8e0: 6f78 3a20 4966 2054 7275 652c 2060 6041  ox: If True, ``A
+0000b8f0: 7869 734d 6574 6164 6174 6160 6020 696e  xisMetadata`` in
+0000b900: 7374 616e 6365 7320 6172 6520 7265 706c  stances are repl
+0000b910: 6163 6564 2062 7920 7468 6569 7220 756e  aced by their un
+0000b920: 626f 7865 640a 2020 2020 2020 2020 7661  boxed.        va
+0000b930: 6c75 652c 2073 6565 2060 6066 6c61 782e  lue, see ``flax.
+0000b940: 6e6e 2e6d 6574 612e 756e 626f 7860 6020  nn.meta.unbox`` 
+0000b950: 2864 6566 6175 6c74 3a20 5472 7565 292e  (default: True).
+0000b960: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
+0000b970: 2020 2020 2054 6865 2076 616c 7565 206f       The value o
+0000b980: 6620 7468 6520 696e 6974 6961 6c69 7a65  f the initialize
+0000b990: 6420 7061 7261 6d65 7465 722e 2054 6872  d parameter. Thr
+0000b9a0: 6f77 7320 616e 2065 7272 6f72 2069 6620  ows an error if 
+0000b9b0: 7468 6520 7061 7261 6d65 7465 720a 2020  the parameter.  
+0000b9c0: 2020 2020 6578 6973 7473 2061 6c72 6561      exists alrea
+0000b9d0: 6479 2e0a 2020 2020 2222 220a 2020 2020  dy..    """.    
+0000b9e0: 6966 206e 6f74 2073 656c 662e 5f69 6e69  if not self._ini
+0000b9f0: 7469 616c 697a 6174 696f 6e5f 616c 6c6f  tialization_allo
+0000ba00: 7765 643a 0a20 2020 2020 2072 6169 7365  wed:.      raise
+0000ba10: 2056 616c 7565 4572 726f 7228 0a20 2020   ValueError(.   
+0000ba20: 2020 2020 2020 2027 5061 7261 6d65 7465         'Paramete
+0000ba30: 7273 206d 7573 7420 6265 2069 6e69 7469  rs must be initi
+0000ba40: 616c 697a 6564 2069 6e20 6073 6574 7570  alized in `setup
+0000ba50: 2829 6020 6f72 2069 6e20 6120 6d65 7468  ()` or in a meth
+0000ba60: 6f64 2027 0a20 2020 2020 2020 2020 2027  od '.          '
+0000ba70: 7772 6170 7065 6420 696e 2060 4063 6f6d  wrapped in `@com
+0000ba80: 7061 6374 6027 0a20 2020 2020 2029 0a20  pact`'.      ). 
+0000ba90: 2020 2069 6620 7365 6c66 2e5f 6e61 6d65     if self._name
+0000baa0: 5f74 616b 656e 286e 616d 652c 2063 6f6c  _taken(name, col
+0000bab0: 6c65 6374 696f 6e3d 2770 6172 616d 7327  lection='params'
+0000bac0: 293a 0a20 2020 2020 2072 6169 7365 2065  ):.      raise e
+0000bad0: 7272 6f72 732e 4e61 6d65 496e 5573 6545  rrors.NameInUseE
+0000bae0: 7272 6f72 2827 7061 7261 6d27 2c20 6e61  rror('param', na
+0000baf0: 6d65 2c20 7365 6c66 2e5f 5f63 6c61 7373  me, self.__class
+0000bb00: 5f5f 2e5f 5f6e 616d 655f 5f29 0a20 2020  __.__name__).   
+0000bb10: 2061 7373 6572 7420 7365 6c66 2e73 636f   assert self.sco
+0000bb20: 7065 2069 7320 6e6f 7420 4e6f 6e65 0a20  pe is not None. 
+0000bb30: 2020 2076 203d 2073 656c 662e 7363 6f70     v = self.scop
+0000bb40: 652e 7061 7261 6d28 6e61 6d65 2c20 696e  e.param(name, in
+0000bb50: 6974 5f66 6e2c 202a 696e 6974 5f61 7267  it_fn, *init_arg
+0000bb60: 732c 2075 6e62 6f78 3d75 6e62 6f78 290a  s, unbox=unbox).
+0000bb70: 2020 2020 7365 6c66 2e5f 7374 6174 652e      self._state.
+0000bb80: 6368 696c 6472 656e 5b6e 616d 655d 203d  children[name] =
+0000bb90: 2027 7061 7261 6d73 270a 2020 2020 7265   'params'.    re
+0000bba0: 7475 726e 2076 0a0a 2020 6465 6620 6861  turn v..  def ha
+0000bbb0: 735f 7661 7269 6162 6c65 2873 656c 662c  s_variable(self,
+0000bbc0: 2063 6f6c 3a20 7374 722c 206e 616d 653a   col: str, name:
+0000bbd0: 2073 7472 2920 2d3e 2062 6f6f 6c3a 0a20   str) -> bool:. 
+0000bbe0: 2020 2022 2222 4368 6563 6b73 2069 6620     """Checks if 
+0000bbf0: 6120 7661 7269 6162 6c65 206f 6620 6769  a variable of gi
+0000bc00: 7665 6e20 636f 6c6c 6563 7469 6f6e 2061  ven collection a
+0000bc10: 6e64 206e 616d 6520 6578 6973 7473 2069  nd name exists i
+0000bc20: 6e20 7468 6973 204d 6f64 756c 652e 0a0a  n this Module...
+0000bc30: 2020 2020 5365 6520 3a6d 6f64 3a60 666c      See :mod:`fl
+0000bc40: 6178 2e63 6f72 652e 7661 7269 6162 6c65  ax.core.variable
+0000bc50: 7360 2066 6f72 206d 6f72 6520 6578 706c  s` for more expl
+0000bc60: 616e 6174 696f 6e20 6f6e 2076 6172 6961  anation on varia
+0000bc70: 626c 6573 2061 6e64 0a20 2020 2063 6f6c  bles and.    col
+0000bc80: 6c65 6374 696f 6e73 2e0a 0a20 2020 2041  lections...    A
+0000bc90: 7267 733a 0a20 2020 2020 2063 6f6c 3a20  rgs:.      col: 
+0000bca0: 5468 6520 7661 7269 6162 6c65 2063 6f6c  The variable col
+0000bcb0: 6c65 6374 696f 6e20 6e61 6d65 2e0a 2020  lection name..  
+0000bcc0: 2020 2020 6e61 6d65 3a20 5468 6520 6e61      name: The na
+0000bcd0: 6d65 206f 6620 7468 6520 7661 7269 6162  me of the variab
+0000bce0: 6c65 2e0a 2020 2020 5265 7475 726e 733a  le..    Returns:
+0000bcf0: 0a20 2020 2020 2054 7275 6520 6966 2074  .      True if t
+0000bd00: 6865 2076 6172 6961 626c 6520 6578 6973  he variable exis
+0000bd10: 7473 2e0a 2020 2020 2222 220a 2020 2020  ts..    """.    
+0000bd20: 6966 2073 656c 662e 7363 6f70 6520 6973  if self.scope is
+0000bd30: 204e 6f6e 653a 0a20 2020 2020 2072 6169   None:.      rai
+0000bd40: 7365 2056 616c 7565 4572 726f 7228 2243  se ValueError("C
+0000bd50: 616e 2774 2061 6363 6573 7320 7661 7269  an't access vari
+0000bd60: 6162 6c65 7320 6f6e 2075 6e62 6f75 6e64  ables on unbound
+0000bd70: 206d 6f64 756c 6573 2229 0a20 2020 2072   modules").    r
+0000bd80: 6574 7572 6e20 7365 6c66 2e73 636f 7065  eturn self.scope
+0000bd90: 2e68 6173 5f76 6172 6961 626c 6528 636f  .has_variable(co
+0000bda0: 6c2c 206e 616d 6529 0a0a 2020 6465 6620  l, name)..  def 
+0000bdb0: 6973 5f6d 7574 6162 6c65 5f63 6f6c 6c65  is_mutable_colle
+0000bdc0: 6374 696f 6e28 7365 6c66 2c20 636f 6c3a  ction(self, col:
+0000bdd0: 2073 7472 2920 2d3e 2062 6f6f 6c3a 0a20   str) -> bool:. 
+0000bde0: 2020 2022 2222 5265 7475 726e 7320 7472     """Returns tr
+0000bdf0: 7565 2069 6620 7468 6520 636f 6c6c 6563  ue if the collec
+0000be00: 7469 6f6e 2060 636f 6c60 2069 7320 6d75  tion `col` is mu
+0000be10: 7461 626c 652e 2222 220a 2020 2020 6966  table.""".    if
+0000be20: 2073 656c 662e 7363 6f70 6520 6973 204e   self.scope is N
+0000be30: 6f6e 653a 0a20 2020 2020 2072 6169 7365  one:.      raise
+0000be40: 2056 616c 7565 4572 726f 7228 2243 616e   ValueError("Can
+0000be50: 2774 2063 6865 636b 206d 7574 6162 696c  't check mutabil
+0000be60: 6974 7920 6f6e 2075 6e62 6f75 6e64 206d  ity on unbound m
+0000be70: 6f64 756c 6573 2229 0a20 2020 2072 6574  odules").    ret
+0000be80: 7572 6e20 7365 6c66 2e73 636f 7065 2e69  urn self.scope.i
+0000be90: 735f 6d75 7461 626c 655f 636f 6c6c 6563  s_mutable_collec
+0000bea0: 7469 6f6e 2863 6f6c 290a 0a20 2064 6566  tion(col)..  def
+0000beb0: 2068 6173 5f72 6e67 2873 656c 662c 206e   has_rng(self, n
+0000bec0: 616d 653a 2073 7472 2920 2d3e 2062 6f6f  ame: str) -> boo
+0000bed0: 6c3a 0a20 2020 2022 2222 5265 7475 726e  l:.    """Return
+0000bee0: 7320 7472 7565 2069 6620 6120 5052 4e47  s true if a PRNG
+0000bef0: 5365 7175 656e 6365 2077 6974 6820 6e61  Sequence with na
+0000bf00: 6d65 2060 6e61 6d65 6020 6578 6973 7473  me `name` exists
+0000bf10: 2e22 2222 0a20 2020 2069 6620 7365 6c66  .""".    if self
+0000bf20: 2e73 636f 7065 2069 7320 4e6f 6e65 3a0a  .scope is None:.
+0000bf30: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+0000bf40: 6545 7272 6f72 2822 4361 6e27 7420 7175  eError("Can't qu
+0000bf50: 6572 7920 666f 7220 524e 4773 206f 6e20  ery for RNGs on 
+0000bf60: 756e 626f 756e 6420 6d6f 6475 6c65 7322  unbound modules"
+0000bf70: 290a 2020 2020 7265 7475 726e 2073 656c  ).    return sel
+0000bf80: 662e 7363 6f70 652e 6861 735f 726e 6728  f.scope.has_rng(
+0000bf90: 6e61 6d65 290a 0a20 2064 6566 206d 616b  name)..  def mak
+0000bfa0: 655f 726e 6728 7365 6c66 2c20 6e61 6d65  e_rng(self, name
+0000bfb0: 3a20 7374 7229 202d 3e20 4b65 7941 7272  : str) -> KeyArr
+0000bfc0: 6179 3a0a 2020 2020 2222 2252 6574 7572  ay:.    """Retur
+0000bfd0: 6e73 2061 206e 6577 2052 4e47 206b 6579  ns a new RNG key
+0000bfe0: 2066 726f 6d20 6120 6769 7665 6e20 524e   from a given RN
+0000bff0: 4720 7365 7175 656e 6365 2066 6f72 2074  G sequence for t
+0000c000: 6869 7320 4d6f 6475 6c65 2e0a 0a20 2020  his Module...   
+0000c010: 2054 6865 206e 6577 2052 4e47 206b 6579   The new RNG key
+0000c020: 2069 7320 7370 6c69 7420 6672 6f6d 2074   is split from t
+0000c030: 6865 2070 7265 7669 6f75 7320 6f6e 652e  he previous one.
+0000c040: 2054 6875 732c 2065 7665 7279 2063 616c   Thus, every cal
+0000c050: 6c20 746f 0a20 2020 2060 6d61 6b65 5f72  l to.    `make_r
+0000c060: 6e67 6020 7265 7475 726e 7320 6120 6e65  ng` returns a ne
+0000c070: 7720 524e 4720 6b65 792c 2077 6869 6c65  w RNG key, while
+0000c080: 2073 7469 6c6c 2067 7561 7261 6e74 6565   still guarantee
+0000c090: 696e 6720 6675 6c6c 0a20 2020 2072 6570  ing full.    rep
+0000c0a0: 726f 6475 6369 6269 6c69 7479 2e0a 0a20  roducibility... 
+0000c0b0: 2020 2054 4f44 4f3a 204c 696e 6b20 746f     TODO: Link to
+0000c0c0: 2046 6c61 7820 524e 4720 6465 7369 676e   Flax RNG design
+0000c0d0: 206e 6f74 652e 0a0a 2020 2020 4172 6773   note...    Args
+0000c0e0: 3a0a 2020 2020 2020 6e61 6d65 3a20 5468  :.      name: Th
+0000c0f0: 6520 524e 4720 7365 7175 656e 6365 206e  e RNG sequence n
+0000c100: 616d 652e 0a20 2020 2052 6574 7572 6e73  ame..    Returns
+0000c110: 3a0a 2020 2020 2020 5468 6520 6e65 776c  :.      The newl
+0000c120: 7920 6765 6e65 7261 7465 6420 524e 4720  y generated RNG 
+0000c130: 6b65 792e 0a20 2020 2022 2222 0a20 2020  key..    """.   
+0000c140: 2069 6620 7365 6c66 2e73 636f 7065 2069   if self.scope i
+0000c150: 7320 4e6f 6e65 3a0a 2020 2020 2020 7261  s None:.      ra
+0000c160: 6973 6520 5661 6c75 6545 7272 6f72 2822  ise ValueError("
+0000c170: 4361 6e27 7420 7573 6520 524e 4773 206f  Can't use RNGs o
+0000c180: 6e20 756e 626f 756e 6420 6d6f 6475 6c65  n unbound module
+0000c190: 7322 290a 2020 2020 7265 7475 726e 2073  s").    return s
+0000c1a0: 656c 662e 7363 6f70 652e 6d61 6b65 5f72  elf.scope.make_r
+0000c1b0: 6e67 286e 616d 6529 0a0a 2020 6465 6620  ng(name)..  def 
+0000c1c0: 6973 5f69 6e69 7469 616c 697a 696e 6728  is_initializing(
+0000c1d0: 7365 6c66 2920 2d3e 2062 6f6f 6c3a 0a20  self) -> bool:. 
+0000c1e0: 2020 2022 2222 5265 7475 726e 7320 5472     """Returns Tr
+0000c1f0: 7565 2069 6620 7275 6e6e 696e 6720 756e  ue if running un
+0000c200: 6465 7220 7365 6c66 2e69 6e69 7428 2e2e  der self.init(..
+0000c210: 2e29 206f 7220 6e6e 2e69 6e69 7428 2e2e  .) or nn.init(..
+0000c220: 2e29 2829 2e0a 0a20 2020 2054 6869 7320  .)()...    This 
+0000c230: 6973 2061 2068 656c 7065 7220 6d65 7468  is a helper meth
+0000c240: 6f64 2074 6f20 6861 6e64 6c65 2074 6865  od to handle the
+0000c250: 2063 6f6d 6d6f 6e20 6361 7365 206f 6620   common case of 
+0000c260: 7369 6d70 6c65 2069 6e69 7469 616c 697a  simple initializ
+0000c270: 6174 696f 6e0a 2020 2020 7768 6572 6520  ation.    where 
+0000c280: 7765 2077 6973 6820 746f 2068 6176 6520  we wish to have 
+0000c290: 7365 7475 7020 6c6f 6769 6320 6f63 6375  setup logic occu
+0000c2a0: 7220 7768 656e 206f 6e6c 7920 6361 6c6c  r when only call
+0000c2b0: 6564 2075 6e64 6572 0a20 2020 2060 606d  ed under.    ``m
+0000c2c0: 6f64 756c 652e 696e 6974 6060 206f 7220  odule.init`` or 
+0000c2d0: 6060 6e6e 2e69 6e69 7460 602e 2020 466f  ``nn.init``.  Fo
+0000c2e0: 7220 6d6f 7265 2063 6f6d 706c 6963 6174  r more complicat
+0000c2f0: 6564 206d 756c 7469 2d70 6861 7365 0a20  ed multi-phase. 
+0000c300: 2020 2069 6e69 7469 616c 697a 6174 696f     initializatio
+0000c310: 6e20 7363 656e 6172 696f 7320 6974 2069  n scenarios it i
+0000c320: 7320 6265 7474 6572 2074 6f20 7465 7374  s better to test
+0000c330: 2066 6f72 2074 6865 206d 7574 6162 696c   for the mutabil
+0000c340: 6974 7920 6f66 0a20 2020 2070 6172 7469  ity of.    parti
+0000c350: 6375 6c61 7220 7661 7269 6162 6c65 2063  cular variable c
+0000c360: 6f6c 6c65 6374 696f 6e73 206f 7220 666f  ollections or fo
+0000c370: 7220 7468 6520 7072 6573 656e 6365 206f  r the presence o
+0000c380: 6620 7061 7274 6963 756c 6172 0a20 2020  f particular.   
+0000c390: 2076 6172 6961 626c 6573 2074 6861 7420   variables that 
+0000c3a0: 706f 7465 6e74 6961 6c6c 7920 6e65 6564  potentially need
+0000c3b0: 2074 6f20 6265 2069 6e69 7469 616c 697a   to be initializ
+0000c3c0: 6564 2e0a 2020 2020 2222 220a 2020 2020  ed..    """.    
+0000c3d0: 6966 2073 656c 662e 7363 6f70 6520 6973  if self.scope is
+0000c3e0: 204e 6f6e 653a 0a20 2020 2020 2072 6169   None:.      rai
+0000c3f0: 7365 2056 616c 7565 4572 726f 7228 2243  se ValueError("C
+0000c400: 616e 2774 2063 6865 636b 2069 6620 7275  an't check if ru
+0000c410: 6e6e 696e 6720 756e 6465 7220 696e 6974  nning under init
+0000c420: 2829 206f 6e20 756e 626f 756e 6420 6d6f  () on unbound mo
+0000c430: 6475 6c65 7322 290a 2020 2020 7265 7475  dules").    retu
+0000c440: 726e 2073 656c 662e 7363 6f70 652e 6765  rn self.scope.ge
+0000c450: 745f 666c 6167 2827 696e 6974 6961 6c69  t_flag('initiali
+0000c460: 7a69 6e67 272c 2046 616c 7365 290a 0a20  zing', False).. 
+0000c470: 2064 6566 205f 6d6f 6475 6c65 5f63 6865   def _module_che
+0000c480: 636b 7328 7365 6c66 293a 0a20 2020 2022  cks(self):.    "
+0000c490: 2222 5275 6e20 7374 616e 6461 7264 2072  ""Run standard r
+0000c4a0: 756e 7469 6d65 2063 6865 636b 732e 2222  untime checks.""
+0000c4b0: 220a 0a20 2020 2069 6620 6e6f 7420 6973  "..    if not is
+0000c4c0: 696e 7374 616e 6365 2873 656c 662c 204d  instance(self, M
+0000c4d0: 6f64 756c 6529 3a0a 2020 2020 2020 7261  odule):.      ra
+0000c4e0: 6973 6520 6572 726f 7273 2e49 6e76 616c  ise errors.Inval
+0000c4f0: 6964 496e 7374 616e 6365 4d6f 6475 6c65  idInstanceModule
+0000c500: 4572 726f 7228 290a 0a20 2020 206f 7665  Error()..    ove
+0000c510: 7272 6964 6465 6e5f 706f 7374 5f69 6e69  rridden_post_ini
+0000c520: 7420 3d20 7365 6c66 2e5f 5f70 6f73 745f  t = self.__post_
+0000c530: 696e 6974 5f5f 2021 3d20 4d6f 6475 6c65  init__ != Module
+0000c540: 2e5f 5f70 6f73 745f 696e 6974 5f5f 0a20  .__post_init__. 
+0000c550: 2020 2069 6620 6f76 6572 7269 6464 656e     if overridden
+0000c560: 5f70 6f73 745f 696e 6974 2061 6e64 206e  _post_init and n
+0000c570: 6f74 2068 6173 6174 7472 2873 656c 662c  ot hasattr(self,
+0000c580: 2027 5f69 6427 293a 0a20 2020 2020 2072   '_id'):.      r
+0000c590: 6169 7365 2065 7272 6f72 732e 496e 636f  aise errors.Inco
+0000c5a0: 7272 6563 7450 6f73 7449 6e69 744f 7665  rrectPostInitOve
+0000c5b0: 7272 6964 6545 7272 6f72 2829 0a0a 2020  rrideError()..  
+0000c5c0: 4074 7261 6365 6261 636b 5f75 7469 6c2e  @traceback_util.
+0000c5d0: 6170 695f 626f 756e 6461 7279 0a20 2064  api_boundary.  d
+0000c5e0: 6566 2062 696e 6428 0a20 2020 2020 2073  ef bind(.      s
+0000c5f0: 656c 663a 204d 2c0a 2020 2020 2020 7661  elf: M,.      va
+0000c600: 7269 6162 6c65 733a 2056 6172 6961 626c  riables: Variabl
+0000c610: 6544 6963 742c 0a20 2020 2020 202a 6172  eDict,.      *ar
+0000c620: 6773 2c0a 2020 2020 2020 726e 6773 3a20  gs,.      rngs: 
+0000c630: 4f70 7469 6f6e 616c 5b52 4e47 5365 7175  Optional[RNGSequ
+0000c640: 656e 6365 735d 203d 204e 6f6e 652c 0a20  ences] = None,. 
+0000c650: 2020 2020 206d 7574 6162 6c65 3a20 436f       mutable: Co
+0000c660: 6c6c 6563 7469 6f6e 4669 6c74 6572 203d  llectionFilter =
+0000c670: 2046 616c 7365 2c0a 2020 2920 2d3e 204d   False,.  ) -> M
+0000c680: 3a0a 2020 2020 2222 2243 7265 6174 6573  :.    """Creates
+0000c690: 2061 6e20 696e 7465 7261 6374 6976 6520   an interactive 
+0000c6a0: 4d6f 6475 6c65 2069 6e73 7461 6e63 6520  Module instance 
+0000c6b0: 6279 2062 696e 6469 6e67 2076 6172 6961  by binding varia
+0000c6c0: 626c 6573 2061 6e64 2052 4e47 732e 0a0a  bles and RNGs...
+0000c6d0: 2020 2020 6060 6269 6e64 6060 2070 726f      ``bind`` pro
+0000c6e0: 7669 6465 7320 616e 2022 696e 7465 7261  vides an "intera
+0000c6f0: 6374 6976 6522 2069 6e73 7461 6e63 6520  ctive" instance 
+0000c700: 6f66 2061 204d 6f64 756c 6520 6469 7265  of a Module dire
+0000c710: 6374 6c79 2077 6974 686f 7574 0a20 2020  ctly without.   
+0000c720: 2074 7261 6e73 666f 726d 696e 6720 6120   transforming a 
+0000c730: 6675 6e63 7469 6f6e 2077 6974 6820 6060  function with ``
+0000c740: 6170 706c 7960 602e 2054 6869 7320 6973  apply``. This is
+0000c750: 2070 6172 7469 6375 6c61 726c 7920 7573   particularly us
+0000c760: 6566 756c 2066 6f72 0a20 2020 2064 6562  eful for.    deb
+0000c770: 7567 6769 6e67 2061 6e64 2069 6e74 6572  ugging and inter
+0000c780: 6163 7469 7665 2075 7365 2063 6173 6573  active use cases
+0000c790: 206c 696b 6520 6e6f 7465 626f 6f6b 7320   like notebooks 
+0000c7a0: 7768 6572 6520 6120 6675 6e63 7469 6f6e  where a function
+0000c7b0: 2077 6f75 6c64 0a20 2020 206c 696d 6974   would.    limit
+0000c7c0: 2074 6865 2061 6269 6c69 7479 2074 6f20   the ability to 
+0000c7d0: 7370 6c69 7420 7570 2063 6f64 6520 696e  split up code in
+0000c7e0: 746f 2064 6966 6665 7265 6e74 2063 656c  to different cel
+0000c7f0: 6c73 2e0a 0a20 2020 204f 6e63 6520 7468  ls...    Once th
+0000c800: 6520 7661 7269 6162 6c65 7320 2861 6e64  e variables (and
+0000c810: 206f 7074 696f 6e61 6c6c 7920 524e 4773   optionally RNGs
+0000c820: 2920 6172 6520 626f 756e 6420 746f 2061  ) are bound to a
+0000c830: 2060 604d 6f64 756c 6560 6020 6974 0a20   ``Module`` it. 
+0000c840: 2020 2062 6563 6f6d 6573 2061 2073 7461     becomes a sta
+0000c850: 7465 6675 6c20 6f62 6a65 6374 2e20 4e6f  teful object. No
+0000c860: 7465 2074 6861 7420 6964 696f 6d61 7469  te that idiomati
+0000c870: 6320 4a41 5820 6973 2066 756e 6374 696f  c JAX is functio
+0000c880: 6e61 6c20 616e 640a 2020 2020 7468 6572  nal and.    ther
+0000c890: 6566 6f72 6520 616e 2069 6e74 6572 6163  efore an interac
+0000c8a0: 7469 7665 2069 6e73 7461 6e63 6520 646f  tive instance do
+0000c8b0: 6573 206e 6f74 206d 6978 2077 656c 6c20  es not mix well 
+0000c8c0: 7769 7468 2076 616e 696c 6c61 204a 4158  with vanilla JAX
+0000c8d0: 2041 5049 732e 0a20 2020 2060 6062 696e   APIs..    ``bin
+0000c8e0: 6428 2960 6020 7368 6f75 6c64 206f 6e6c  d()`` should onl
+0000c8f0: 7920 6265 2075 7365 6420 666f 7220 696e  y be used for in
+0000c900: 7465 7261 6374 6976 6520 6578 7065 7269  teractive experi
+0000c910: 6d65 6e74 6174 696f 6e2c 2061 6e64 2069  mentation, and i
+0000c920: 6e20 616c 6c0a 2020 2020 6f74 6865 7220  n all.    other 
+0000c930: 6361 7365 7320 7765 2073 7472 6f6e 676c  cases we strongl
+0000c940: 7920 656e 636f 7572 6167 6520 7573 6572  y encourage user
+0000c950: 7320 746f 2075 7365 2060 6061 7070 6c79  s to use ``apply
+0000c960: 2829 6060 2069 6e73 7465 6164 2e0a 0a20  ()`` instead... 
+0000c970: 2020 2045 7861 6d70 6c65 3a3a 0a0a 2020     Example::..  
+0000c980: 2020 2020 696d 706f 7274 206a 6178 0a20      import jax. 
+0000c990: 2020 2020 2069 6d70 6f72 7420 6a61 782e       import jax.
+0000c9a0: 6e75 6d70 7920 6173 206a 6e70 0a20 2020  numpy as jnp.   
+0000c9b0: 2020 2069 6d70 6f72 7420 666c 6178 2e6c     import flax.l
+0000c9c0: 696e 656e 2061 7320 6e6e 0a0a 2020 2020  inen as nn..    
+0000c9d0: 2020 636c 6173 7320 4175 746f 456e 636f    class AutoEnco
+0000c9e0: 6465 7228 6e6e 2e4d 6f64 756c 6529 3a0a  der(nn.Module):.
+0000c9f0: 2020 2020 2020 2020 6465 6620 7365 7475          def setu
+0000ca00: 7028 7365 6c66 293a 0a20 2020 2020 2020  p(self):.       
+0000ca10: 2020 2073 656c 662e 656e 636f 6465 7220     self.encoder 
+0000ca20: 3d20 6e6e 2e44 656e 7365 2833 290a 2020  = nn.Dense(3).  
+0000ca30: 2020 2020 2020 2020 7365 6c66 2e64 6563          self.dec
+0000ca40: 6f64 6572 203d 206e 6e2e 4465 6e73 6528  oder = nn.Dense(
+0000ca50: 3529 0a0a 2020 2020 2020 2020 6465 6620  5)..        def 
+0000ca60: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
+0000ca70: 293a 0a20 2020 2020 2020 2020 2072 6574  ):.          ret
+0000ca80: 7572 6e20 7365 6c66 2e64 6563 6f64 6572  urn self.decoder
+0000ca90: 2873 656c 662e 656e 636f 6465 7228 7829  (self.encoder(x)
+0000caa0: 290a 0a20 2020 2020 2078 203d 206a 6e70  )..      x = jnp
+0000cab0: 2e6f 6e65 7328 2831 362c 2039 2929 0a20  .ones((16, 9)). 
+0000cac0: 2020 2020 2061 6520 3d20 4175 746f 456e       ae = AutoEn
+0000cad0: 636f 6465 7228 290a 2020 2020 2020 7661  coder().      va
+0000cae0: 7269 6162 6c65 7320 3d20 6165 2e69 6e69  riables = ae.ini
+0000caf0: 7428 6a61 782e 7261 6e64 6f6d 2e50 524e  t(jax.random.PRN
+0000cb00: 474b 6579 2830 292c 2078 290a 2020 2020  GKey(0), x).    
+0000cb10: 2020 6d6f 6465 6c20 3d20 6165 2e62 696e    model = ae.bin
+0000cb20: 6428 7661 7269 6162 6c65 7329 0a20 2020  d(variables).   
+0000cb30: 2020 207a 203d 206d 6f64 656c 2e65 6e63     z = model.enc
+0000cb40: 6f64 6572 2878 290a 2020 2020 2020 785f  oder(x).      x_
+0000cb50: 7265 636f 6e73 7472 7563 7465 6420 3d20  reconstructed = 
+0000cb60: 6d6f 6465 6c2e 6465 636f 6465 7228 7a29  model.decoder(z)
+0000cb70: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
+0000cb80: 2020 7661 7269 6162 6c65 733a 2041 2064    variables: A d
+0000cb90: 6963 7469 6f6e 6172 7920 636f 6e74 6169  ictionary contai
+0000cba0: 6e69 6e67 2076 6172 6961 626c 6573 206b  ning variables k
+0000cbb0: 6579 6564 2062 7920 7661 7269 6162 6c65  eyed by variable
+0000cbc0: 0a20 2020 2020 2020 2063 6f6c 6c65 6374  .        collect
+0000cbd0: 696f 6e73 2e20 5365 6520 3a6d 6f64 3a60  ions. See :mod:`
+0000cbe0: 666c 6178 2e63 6f72 652e 7661 7269 6162  flax.core.variab
+0000cbf0: 6c65 7360 2066 6f72 206d 6f72 6520 6465  les` for more de
+0000cc00: 7461 696c 730a 2020 2020 2020 2020 6162  tails.        ab
+0000cc10: 6f75 7420 7661 7269 6162 6c65 732e 0a20  out variables.. 
+0000cc20: 2020 2020 202a 6172 6773 3a20 4e61 6d65       *args: Name
+0000cc30: 6420 6172 6775 6d65 6e74 7320 286e 6f74  d arguments (not
+0000cc40: 2075 7365 6429 2e0a 2020 2020 2020 726e   used)..      rn
+0000cc50: 6773 3a20 6120 6469 6374 206f 6620 5052  gs: a dict of PR
+0000cc60: 4e47 4b65 7973 2074 6f20 696e 6974 6961  NGKeys to initia
+0000cc70: 6c69 7a65 2074 6865 2050 524e 4720 7365  lize the PRNG se
+0000cc80: 7175 656e 6365 732e 0a20 2020 2020 206d  quences..      m
+0000cc90: 7574 6162 6c65 3a20 4361 6e20 6265 2062  utable: Can be b
+0000cca0: 6f6f 6c2c 2073 7472 2c20 6f72 206c 6973  ool, str, or lis
+0000ccb0: 742e 2053 7065 6369 6669 6573 2077 6869  t. Specifies whi
+0000ccc0: 6368 2063 6f6c 6c65 6374 696f 6e73 2073  ch collections s
+0000ccd0: 686f 756c 6420 6265 0a20 2020 2020 2020  hould be.       
+0000cce0: 2074 7265 6174 6564 2061 7320 6d75 7461   treated as muta
+0000ccf0: 626c 653a 0a20 2020 2020 2020 2020 2060  ble:.          `
+0000cd00: 6062 6f6f 6c60 603a 2061 6c6c 2f6e 6f20  `bool``: all/no 
+0000cd10: 636f 6c6c 6563 7469 6f6e 7320 6172 6520  collections are 
+0000cd20: 6d75 7461 626c 652e 0a20 2020 2020 2020  mutable..       
+0000cd30: 2020 2060 6073 7472 6060 3a20 5468 6520     ``str``: The 
+0000cd40: 6e61 6d65 206f 6620 6120 7369 6e67 6c65  name of a single
+0000cd50: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
+0000cd60: 696f 6e2e 0a20 2020 2020 2020 2020 2060  ion..          `
+0000cd70: 606c 6973 7460 603a 2041 206c 6973 7420  `list``: A list 
+0000cd80: 6f66 206e 616d 6573 206f 6620 6d75 7461  of names of muta
+0000cd90: 626c 6520 636f 6c6c 6563 7469 6f6e 732e  ble collections.
+0000cda0: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
+0000cdb0: 2020 2020 2041 2063 6f70 7920 6f66 2074       A copy of t
+0000cdc0: 6869 7320 696e 7374 616e 6365 2077 6974  his instance wit
+0000cdd0: 6820 626f 756e 6420 7661 7269 6162 6c65  h bound variable
+0000cde0: 7320 616e 6420 524e 4773 2e0a 2020 2020  s and RNGs..    
+0000cdf0: 2222 220a 2020 2020 4d6f 6475 6c65 2e5f  """.    Module._
+0000ce00: 6d6f 6475 6c65 5f63 6865 636b 7328 7365  module_checks(se
+0000ce10: 6c66 290a 0a20 2020 2064 656c 2061 7267  lf)..    del arg
+0000ce20: 730a 2020 2020 7363 6f70 6520 3d20 636f  s.    scope = co
+0000ce30: 7265 2e62 696e 6428 7661 7269 6162 6c65  re.bind(variable
+0000ce40: 732c 2072 6e67 733d 726e 6773 2c20 6d75  s, rngs=rngs, mu
+0000ce50: 7461 626c 653d 6d75 7461 626c 6529 0a20  table=mutable). 
+0000ce60: 2020 2072 6574 7572 6e20 7365 6c66 2e63     return self.c
+0000ce70: 6c6f 6e65 2870 6172 656e 743d 7363 6f70  lone(parent=scop
+0000ce80: 652c 205f 6465 6570 5f63 6c6f 6e65 3d54  e, _deep_clone=T
+0000ce90: 7275 6529 0a0a 2020 6465 6620 756e 6269  rue)..  def unbi
+0000cea0: 6e64 2873 656c 663a 204d 2920 2d3e 2054  nd(self: M) -> T
+0000ceb0: 7570 6c65 5b4d 2c20 5661 7269 6162 6c65  uple[M, Variable
+0000cec0: 4469 6374 5d3a 0a20 2020 2022 2222 5265  Dict]:.    """Re
+0000ced0: 7475 726e 7320 616e 2075 6e62 6f75 6e64  turns an unbound
+0000cee0: 2063 6f70 7920 6f66 2061 204d 6f64 756c   copy of a Modul
+0000cef0: 6520 616e 6420 6974 7320 7661 7269 6162  e and its variab
+0000cf00: 6c65 732e 0a0a 2020 2020 6060 756e 6269  les...    ``unbi
+0000cf10: 6e64 6060 2068 656c 7073 2063 7265 6174  nd`` helps creat
+0000cf20: 6520 6120 7374 6174 656c 6573 7320 7665  e a stateless ve
+0000cf30: 7273 696f 6e20 6f66 2061 2062 6f75 6e64  rsion of a bound
+0000cf40: 204d 6f64 756c 652e 0a0a 2020 2020 416e   Module...    An
+0000cf50: 2065 7861 6d70 6c65 206f 6620 6120 636f   example of a co
+0000cf60: 6d6d 6f6e 2075 7365 2063 6173 653a 2074  mmon use case: t
+0000cf70: 6f20 6578 7472 6163 7420 6120 7375 622d  o extract a sub-
+0000cf80: 4d6f 6475 6c65 2064 6566 696e 6564 2069  Module defined i
+0000cf90: 6e73 6964 650a 2020 2020 6060 7365 7475  nside.    ``setu
+0000cfa0: 7028 2960 6020 616e 6420 6974 7320 636f  p()`` and its co
+0000cfb0: 7272 6573 706f 6e64 696e 6720 7661 7269  rresponding vari
+0000cfc0: 6162 6c65 733a 2031 2920 7465 6d70 6f72  ables: 1) tempor
+0000cfd0: 6172 696c 7920 6060 6269 6e64 6060 2074  arily ``bind`` t
+0000cfe0: 6865 2070 6172 656e 740a 2020 2020 4d6f  he parent.    Mo
+0000cff0: 6475 6c65 3b20 616e 6420 7468 656e 2032  dule; and then 2
+0000d000: 2920 6060 756e 6269 6e64 6060 2074 6865  ) ``unbind`` the
+0000d010: 2064 6573 6972 6564 2073 7562 2d4d 6f64   desired sub-Mod
+0000d020: 756c 652e 2028 5265 6361 6c6c 2074 6861  ule. (Recall tha
+0000d030: 7420 6060 7365 7475 7028 2960 600a 2020  t ``setup()``.  
+0000d040: 2020 6973 206f 6e6c 7920 6361 6c6c 6564    is only called
+0000d050: 2077 6865 6e20 7468 6520 4d6f 6475 6c65   when the Module
+0000d060: 2069 7320 626f 756e 642e 293a 3a0a 0a20   is bound.)::.. 
+0000d070: 2020 2020 2063 6c61 7373 2041 7574 6f45       class AutoE
+0000d080: 6e63 6f64 6572 286e 6e2e 4d6f 6475 6c65  ncoder(nn.Module
+0000d090: 293a 0a20 2020 2020 2020 2064 6566 2073  ):.        def s
+0000d0a0: 6574 7570 2873 656c 6629 3a0a 2020 2020  etup(self):.    
+0000d0b0: 2020 2020 2020 7365 6c66 2e65 6e63 6f64        self.encod
+0000d0c0: 6572 203d 2045 6e63 6f64 6572 2829 0a20  er = Encoder(). 
+0000d0d0: 2020 2020 2020 2020 2073 656c 662e 6465           self.de
+0000d0e0: 636f 6465 7220 3d20 4465 636f 6465 7228  coder = Decoder(
+0000d0f0: 290a 0a20 2020 2020 2020 2064 6566 205f  )..        def _
+0000d100: 5f63 616c 6c5f 5f28 7365 6c66 2c20 7829  _call__(self, x)
+0000d110: 3a0a 2020 2020 2020 2020 2020 7265 7475  :.          retu
+0000d120: 726e 2073 656c 662e 6465 636f 6465 7228  rn self.decoder(
+0000d130: 7365 6c66 2e65 6e63 6f64 6572 2878 2929  self.encoder(x))
+0000d140: 0a0a 2020 2020 2020 6d6f 6475 6c65 203d  ..      module =
+0000d150: 2041 7574 6f45 6e63 6f64 6572 2829 0a20   AutoEncoder(). 
+0000d160: 2020 2020 2076 6172 6961 626c 6573 203d       variables =
+0000d170: 206d 6f64 756c 652e 696e 6974 286a 6178   module.init(jax
+0000d180: 2e72 616e 646f 6d2e 5052 4e47 4b65 7928  .random.PRNGKey(
+0000d190: 3029 2c20 6a6e 702e 6f6e 6573 2828 312c  0), jnp.ones((1,
+0000d1a0: 2037 3834 2929 290a 2020 2020 2020 2e2e   784))).      ..
+0000d1b0: 2e0a 2020 2020 2020 2320 4578 7472 6163  ..      # Extrac
+0000d1c0: 7420 7468 6520 456e 636f 6465 7220 7375  t the Encoder su
+0000d1d0: 622d 4d6f 6475 6c65 2061 6e64 2069 7473  b-Module and its
+0000d1e0: 2076 6172 6961 626c 6573 0a20 2020 2020   variables.     
+0000d1f0: 2065 6e63 6f64 6572 2c20 656e 636f 6465   encoder, encode
+0000d200: 725f 7661 7273 203d 206d 6f64 756c 652e  r_vars = module.
+0000d210: 6269 6e64 2876 6172 6961 626c 6573 292e  bind(variables).
+0000d220: 656e 636f 6465 722e 756e 6269 6e64 2829  encoder.unbind()
+0000d230: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
+0000d240: 2020 2020 2041 2074 7570 6c65 2077 6974       A tuple wit
+0000d250: 6820 616e 2075 6e62 6f75 6e64 2063 6f70  h an unbound cop
+0000d260: 7920 6f66 2074 6869 7320 4d6f 6475 6c65  y of this Module
+0000d270: 2061 6e64 2069 7473 2076 6172 6961 626c   and its variabl
+0000d280: 6573 2e0a 2020 2020 2222 220a 2020 2020  es..    """.    
+0000d290: 4d6f 6475 6c65 2e5f 6d6f 6475 6c65 5f63  Module._module_c
+0000d2a0: 6865 636b 7328 7365 6c66 290a 0a20 2020  hecks(self)..   
+0000d2b0: 2069 6620 7365 6c66 2e73 636f 7065 2069   if self.scope i
+0000d2c0: 7320 4e6f 6e65 3a0a 2020 2020 2020 7261  s None:.      ra
+0000d2d0: 6973 6520 6572 726f 7273 2e43 616c 6c55  ise errors.CallU
+0000d2e0: 6e62 696e 644f 6e55 6e62 6f75 6e64 4d6f  nbindOnUnboundMo
+0000d2f0: 6475 6c65 4572 726f 7228 290a 0a20 2020  duleError()..   
+0000d300: 2076 6172 6961 626c 6573 203d 2073 656c   variables = sel
+0000d310: 662e 7661 7269 6162 6c65 730a 2020 2020  f.variables.    
+0000d320: 6d6f 6475 6c65 203d 2073 656c 662e 636c  module = self.cl
+0000d330: 6f6e 6528 290a 2020 2020 7265 7475 726e  one().    return
+0000d340: 206d 6f64 756c 652c 2076 6172 6961 626c   module, variabl
+0000d350: 6573 0a0a 2020 4074 7261 6365 6261 636b  es..  @traceback
+0000d360: 5f75 7469 6c2e 6170 695f 626f 756e 6461  _util.api_bounda
+0000d370: 7279 0a20 2064 6566 2061 7070 6c79 280a  ry.  def apply(.
+0000d380: 2020 2020 2020 7365 6c66 2c0a 2020 2020        self,.    
+0000d390: 2020 7661 7269 6162 6c65 733a 2056 6172    variables: Var
+0000d3a0: 6961 626c 6544 6963 742c 0a20 2020 2020  iableDict,.     
+0000d3b0: 202a 6172 6773 2c0a 2020 2020 2020 726e   *args,.      rn
+0000d3c0: 6773 3a20 4f70 7469 6f6e 616c 5b52 4e47  gs: Optional[RNG
+0000d3d0: 5365 7175 656e 6365 735d 203d 204e 6f6e  Sequences] = Non
+0000d3e0: 652c 0a20 2020 2020 206d 6574 686f 643a  e,.      method:
+0000d3f0: 2055 6e69 6f6e 5b43 616c 6c61 626c 655b   Union[Callable[
+0000d400: 2e2e 2e2c 2041 6e79 5d2c 2073 7472 2c20  ..., Any], str, 
+0000d410: 4e6f 6e65 5d20 3d20 4e6f 6e65 2c0a 2020  None] = None,.  
+0000d420: 2020 2020 6d75 7461 626c 653a 2043 6f6c      mutable: Col
+0000d430: 6c65 6374 696f 6e46 696c 7465 7220 3d20  lectionFilter = 
+0000d440: 4661 6c73 652c 0a20 2020 2020 2063 6170  False,.      cap
+0000d450: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+0000d460: 6573 3a20 556e 696f 6e5b 0a20 2020 2020  es: Union[.     
+0000d470: 2020 2020 2062 6f6f 6c2c 2043 616c 6c61       bool, Calla
+0000d480: 626c 655b 5b27 4d6f 6475 6c65 272c 2073  ble[['Module', s
+0000d490: 7472 5d2c 2062 6f6f 6c5d 0a20 2020 2020  tr], bool].     
+0000d4a0: 205d 203d 2046 616c 7365 2c0a 2020 2020   ] = False,.    
+0000d4b0: 2020 2a2a 6b77 6172 6773 2c0a 2020 2920    **kwargs,.  ) 
+0000d4c0: 2d3e 2055 6e69 6f6e 5b41 6e79 2c20 5475  -> Union[Any, Tu
+0000d4d0: 706c 655b 416e 792c 2055 6e69 6f6e 5b46  ple[Any, Union[F
+0000d4e0: 726f 7a65 6e56 6172 6961 626c 6544 6963  rozenVariableDic
+0000d4f0: 742c 2044 6963 745b 7374 722c 2041 6e79  t, Dict[str, Any
+0000d500: 5d5d 5d5d 3a0a 2020 2020 2222 2241 7070  ]]]]:.    """App
+0000d510: 6c69 6573 2061 206d 6f64 756c 6520 6d65  lies a module me
+0000d520: 7468 6f64 2074 6f20 7661 7269 6162 6c65  thod to variable
+0000d530: 7320 616e 6420 7265 7475 726e 7320 6f75  s and returns ou
+0000d540: 7470 7574 2061 6e64 206d 6f64 6966 6965  tput and modifie
+0000d550: 6420 7661 7269 6162 6c65 732e 0a0a 2020  d variables...  
+0000d560: 2020 4e6f 7465 2074 6861 7420 606d 6574    Note that `met
+0000d570: 686f 6460 2073 686f 756c 6420 6265 2073  hod` should be s
+0000d580: 6574 2069 6620 6f6e 6520 776f 756c 6420  et if one would 
+0000d590: 6c69 6b65 2074 6f20 6361 6c6c 2060 6170  like to call `ap
+0000d5a0: 706c 7960 206f 6e20 610a 2020 2020 6469  ply` on a.    di
+0000d5b0: 6666 6572 656e 7420 636c 6173 7320 6d65  fferent class me
+0000d5c0: 7468 6f64 2074 6861 6e20 6060 5f5f 6361  thod than ``__ca
+0000d5d0: 6c6c 5f5f 6060 2e20 466f 7220 696e 7374  ll__``. For inst
+0000d5e0: 616e 6365 2c20 7375 7070 6f73 6520 610a  ance, suppose a.
+0000d5f0: 2020 2020 5472 616e 7366 6f72 6d65 7220      Transformer 
+0000d600: 6d6f 6475 6c65 7320 6861 7320 6120 6d65  modules has a me
+0000d610: 7468 6f64 2063 616c 6c65 6420 6065 6e63  thod called `enc
+0000d620: 6f64 6560 2c20 7468 656e 2074 6865 2066  ode`, then the f
+0000d630: 6f6c 6c6f 7769 6e67 2063 616c 6c73 0a20  ollowing calls. 
+0000d640: 2020 2060 6170 706c 7960 206f 6e20 7468     `apply` on th
+0000d650: 6174 206d 6574 686f 643a 3a0a 0a20 2020  at method::..   
+0000d660: 2020 206d 6f64 656c 203d 2054 7261 6e73     model = Trans
+0000d670: 666f 726d 6572 2829 0a20 2020 2020 2065  former().      e
+0000d680: 6e63 6f64 6564 203d 206d 6f64 656c 2e61  ncoded = model.a
+0000d690: 7070 6c79 287b 2770 6172 616d 7327 3a20  pply({'params': 
+0000d6a0: 7061 7261 6d73 7d2c 2078 2c20 6d65 7468  params}, x, meth
+0000d6b0: 6f64 3d54 7261 6e73 666f 726d 6572 2e65  od=Transformer.e
+0000d6c0: 6e63 6f64 6529 0a0a 2020 2020 4966 2061  ncode)..    If a
+0000d6d0: 2066 756e 6374 696f 6e20 696e 7374 616e   function instan
+0000d6e0: 6365 2069 7320 7072 6f76 6964 6564 2c20  ce is provided, 
+0000d6f0: 7468 6520 756e 626f 756e 6420 6675 6e63  the unbound func
+0000d700: 7469 6f6e 2069 7320 7573 6564 2e20 466f  tion is used. Fo
+0000d710: 720a 2020 2020 696e 7374 616e 6365 2c20  r.    instance, 
+0000d720: 7468 6520 6578 616d 706c 6520 6265 6c6f  the example belo
+0000d730: 7720 6973 2065 7175 6976 616c 656e 7420  w is equivalent 
+0000d740: 746f 2074 6865 206f 6e65 2061 626f 7665  to the one above
+0000d750: 3a3a 0a0a 2020 2020 2020 656e 636f 6465  ::..      encode
+0000d760: 6420 3d20 6d6f 6465 6c2e 6170 706c 7928  d = model.apply(
+0000d770: 7b27 7061 7261 6d73 273a 2070 6172 616d  {'params': param
+0000d780: 737d 2c20 782c 206d 6574 686f 643d 6d6f  s}, x, method=mo
+0000d790: 6465 6c2e 656e 636f 6465 290a 0a20 2020  del.encode)..   
+0000d7a0: 2059 6f75 2063 616e 2061 6c73 6f20 7061   You can also pa
+0000d7b0: 7373 2061 2073 7472 696e 6720 746f 2061  ss a string to a
+0000d7c0: 2063 616c 6c61 626c 6520 6174 7472 6962   callable attrib
+0000d7d0: 7574 6520 6f66 2074 6865 206d 6f64 756c  ute of the modul
+0000d7e0: 652e 2046 6f72 0a20 2020 2065 7861 6d70  e. For.    examp
+0000d7f0: 6c65 2c20 7468 6520 7072 6576 696f 7573  le, the previous
+0000d800: 2063 616e 2062 6520 7772 6974 7465 6e20   can be written 
+0000d810: 6173 3a3a 0a0a 2020 2020 2020 656e 636f  as::..      enco
+0000d820: 6465 6420 3d20 6d6f 6465 6c2e 6170 706c  ded = model.appl
+0000d830: 7928 7b27 7061 7261 6d73 273a 2070 6172  y({'params': par
+0000d840: 616d 737d 2c20 782c 206d 6574 686f 643d  ams}, x, method=
+0000d850: 2765 6e63 6f64 6527 290a 0a20 2020 204e  'encode')..    N
+0000d860: 6f74 6520 6060 6d65 7468 6f64 6060 2063  ote ``method`` c
+0000d870: 616e 2061 6c73 6f20 6265 2061 2066 756e  an also be a fun
+0000d880: 6374 696f 6e20 7468 6174 2069 7320 6e6f  ction that is no
+0000d890: 7420 6465 6669 6e65 6420 696e 0a20 2020  t defined in.   
+0000d8a0: 2060 6054 7261 6e73 666f 726d 6572 6060   ``Transformer``
+0000d8b0: 2e20 496e 2074 6861 7420 6361 7365 2c20  . In that case, 
+0000d8c0: 7468 6520 6675 6e63 7469 6f6e 2073 686f  the function sho
+0000d8d0: 756c 6420 6861 7665 2061 7420 6c65 6173  uld have at leas
+0000d8e0: 7420 6f6e 650a 2020 2020 6172 6775 6d65  t one.    argume
+0000d8f0: 6e74 2072 6570 7265 7365 6e74 696e 6720  nt representing 
+0000d900: 616e 2069 6e73 7461 6e63 6520 6f66 2074  an instance of t
+0000d910: 6865 204d 6f64 756c 6520 636c 6173 733a  he Module class:
+0000d920: 3a0a 0a20 2020 2020 2064 6566 206f 7468  :..      def oth
+0000d930: 6572 5f66 6e28 696e 7374 616e 6365 2c20  er_fn(instance, 
+0000d940: 2e2e 2e29 3a0a 2020 2020 2020 2020 696e  ...):.        in
+0000d950: 7374 616e 6365 2e73 6f6d 655f 6d6f 6475  stance.some_modu
+0000d960: 6c65 5f61 7474 7228 2e2e 2e29 0a20 2020  le_attr(...).   
+0000d970: 2020 2020 202e 2e2e 0a0a 2020 2020 2020       .....      
+0000d980: 6d6f 6465 6c2e 6170 706c 7928 7b27 7061  model.apply({'pa
+0000d990: 7261 6d73 273a 2070 6172 616d 737d 2c20  rams': params}, 
+0000d9a0: 782c 206d 6574 686f 643d 6f74 6865 725f  x, method=other_
+0000d9b0: 666e 290a 0a20 2020 2041 7267 733a 0a20  fn)..    Args:. 
+0000d9c0: 2020 2020 2076 6172 6961 626c 6573 3a20       variables: 
+0000d9d0: 4120 6469 6374 696f 6e61 7279 2063 6f6e  A dictionary con
+0000d9e0: 7461 696e 696e 6720 7661 7269 6162 6c65  taining variable
+0000d9f0: 7320 6b65 7965 6420 6279 2076 6172 6961  s keyed by varia
+0000da00: 626c 650a 2020 2020 2020 2020 636f 6c6c  ble.        coll
+0000da10: 6563 7469 6f6e 732e 2053 6565 203a 6d6f  ections. See :mo
+0000da20: 643a 6066 6c61 782e 636f 7265 2e76 6172  d:`flax.core.var
+0000da30: 6961 626c 6573 6020 666f 7220 6d6f 7265  iables` for more
+0000da40: 2064 6574 6169 6c73 0a20 2020 2020 2020   details.       
+0000da50: 2061 626f 7574 2076 6172 6961 626c 6573   about variables
+0000da60: 2e0a 2020 2020 2020 2a61 7267 733a 204e  ..      *args: N
+0000da70: 616d 6564 2061 7267 756d 656e 7473 2070  amed arguments p
+0000da80: 6173 7365 6420 746f 2074 6865 2073 7065  assed to the spe
+0000da90: 6369 6669 6564 2061 7070 6c79 206d 6574  cified apply met
+0000daa0: 686f 642e 0a20 2020 2020 2072 6e67 733a  hod..      rngs:
+0000dab0: 2061 2064 6963 7420 6f66 2050 524e 474b   a dict of PRNGK
+0000dac0: 6579 7320 746f 2069 6e69 7469 616c 697a  eys to initializ
+0000dad0: 6520 7468 6520 5052 4e47 2073 6571 7565  e the PRNG seque
+0000dae0: 6e63 6573 2e0a 2020 2020 2020 2020 5468  nces..        Th
+0000daf0: 6520 2270 6172 616d 7322 2050 524e 4720  e "params" PRNG 
+0000db00: 7365 7175 656e 6365 2069 7320 7573 6564  sequence is used
+0000db10: 2074 6f20 696e 6974 6961 6c69 7a65 2070   to initialize p
+0000db20: 6172 616d 6574 6572 732e 0a20 2020 2020  arameters..     
+0000db30: 206d 6574 686f 643a 2041 2066 756e 6374   method: A funct
+0000db40: 696f 6e20 746f 2063 616c 6c20 6170 706c  ion to call appl
+0000db50: 7920 6f6e 2e20 5468 6973 2069 7320 6765  y on. This is ge
+0000db60: 6e65 7261 6c6c 7920 6120 6675 6e63 7469  nerally a functi
+0000db70: 6f6e 2069 6e20 7468 650a 2020 2020 2020  on in the.      
+0000db80: 2020 6d6f 6475 6c65 2e20 4966 2070 726f    module. If pro
+0000db90: 7669 6465 642c 2061 7070 6c69 6573 2074  vided, applies t
+0000dba0: 6869 7320 6d65 7468 6f64 2e20 4966 206e  his method. If n
+0000dbb0: 6f74 2070 726f 7669 6465 642c 2061 7070  ot provided, app
+0000dbc0: 6c69 6573 2074 6865 0a20 2020 2020 2020  lies the.       
+0000dbd0: 2060 605f 5f63 616c 6c5f 5f60 6020 6d65   ``__call__`` me
+0000dbe0: 7468 6f64 206f 6620 7468 6520 6d6f 6475  thod of the modu
+0000dbf0: 6c65 2e20 4120 7374 7269 6e67 2063 616e  le. A string can
+0000dc00: 2061 6c73 6f20 6265 2070 726f 7669 6465   also be provide
+0000dc10: 6420 746f 0a20 2020 2020 2020 2073 7065  d to.        spe
+0000dc20: 6369 6679 2061 206d 6574 686f 6420 6279  cify a method by
+0000dc30: 206e 616d 652e 0a20 2020 2020 206d 7574   name..      mut
+0000dc40: 6162 6c65 3a20 4361 6e20 6265 2062 6f6f  able: Can be boo
+0000dc50: 6c2c 2073 7472 2c20 6f72 206c 6973 742e  l, str, or list.
+0000dc60: 2053 7065 6369 6669 6573 2077 6869 6368   Specifies which
+0000dc70: 2063 6f6c 6c65 6374 696f 6e73 2073 686f   collections sho
+0000dc80: 756c 6420 6265 0a20 2020 2020 2020 2020  uld be.         
+0000dc90: 2020 2020 2020 7472 6561 7465 6420 6173        treated as
+0000dca0: 206d 7574 6162 6c65 3a20 6060 626f 6f6c   mutable: ``bool
+0000dcb0: 6060 3a20 616c 6c2f 6e6f 2063 6f6c 6c65  ``: all/no colle
+0000dcc0: 6374 696f 6e73 2061 7265 206d 7574 6162  ctions are mutab
+0000dcd0: 6c65 2e0a 2020 2020 2020 2020 2020 2020  le..            
+0000dce0: 2020 2060 6073 7472 6060 3a20 5468 6520     ``str``: The 
+0000dcf0: 6e61 6d65 206f 6620 6120 7369 6e67 6c65  name of a single
+0000dd00: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
+0000dd10: 696f 6e2e 2060 606c 6973 7460 603a 2041  ion. ``list``: A
+0000dd20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000dd30: 6c69 7374 206f 6620 6e61 6d65 7320 6f66  list of names of
+0000dd40: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
+0000dd50: 696f 6e73 2e0a 2020 2020 2020 6361 7074  ions..      capt
+0000dd60: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
+0000dd70: 733a 2049 6620 6054 7275 6560 2c20 6361  s: If `True`, ca
+0000dd80: 7074 7572 6573 2069 6e74 6572 6d65 6469  ptures intermedi
+0000dd90: 6174 6520 7265 7475 726e 2076 616c 7565  ate return value
+0000dda0: 730a 2020 2020 2020 2020 6f66 2061 6c6c  s.        of all
+0000ddb0: 204d 6f64 756c 6573 2069 6e73 6964 6520   Modules inside 
+0000ddc0: 7468 6520 2269 6e74 6572 6d65 6469 6174  the "intermediat
+0000ddd0: 6573 2220 636f 6c6c 6563 7469 6f6e 2e20  es" collection. 
+0000dde0: 4279 2064 6566 6175 6c74 206f 6e6c 790a  By default only.
+0000ddf0: 2020 2020 2020 2020 7468 6520 7265 7475          the retu
+0000de00: 726e 2076 616c 7565 7320 6f66 2061 6c6c  rn values of all
+0000de10: 2060 605f 5f63 616c 6c5f 5f60 6020 6d65   ``__call__`` me
+0000de20: 7468 6f64 7320 6172 6520 7374 6f72 6564  thods are stored
+0000de30: 2e20 4120 6675 6e63 7469 6f6e 2063 616e  . A function can
+0000de40: 0a20 2020 2020 2020 2062 6520 7061 7373  .        be pass
+0000de50: 6564 2074 6f20 6368 616e 6765 2074 6865  ed to change the
+0000de60: 2066 696c 7465 7220 6265 6861 7669 6f72   filter behavior
+0000de70: 2e20 5468 6520 6669 6c74 6572 2066 756e  . The filter fun
+0000de80: 6374 696f 6e20 7461 6b65 730a 2020 2020  ction takes.    
+0000de90: 2020 2020 7468 6520 4d6f 6475 6c65 2069      the Module i
+0000dea0: 6e73 7461 6e63 6520 616e 6420 6d65 7468  nstance and meth
+0000deb0: 6f64 206e 616d 6520 616e 6420 7265 7475  od name and retu
+0000dec0: 726e 7320 6120 626f 6f6c 2069 6e64 6963  rns a bool indic
+0000ded0: 6174 696e 670a 2020 2020 2020 2020 7768  ating.        wh
+0000dee0: 6574 6865 7220 7468 6520 6f75 7470 7574  ether the output
+0000def0: 206f 6620 7468 6174 206d 6574 686f 6420   of that method 
+0000df00: 696e 766f 6361 7469 6f6e 2073 686f 756c  invocation shoul
+0000df10: 6420 6265 2073 746f 7265 642e 0a20 2020  d be stored..   
+0000df20: 2020 202a 2a6b 7761 7267 733a 204b 6579     **kwargs: Key
+0000df30: 776f 7264 2061 7267 756d 656e 7473 2070  word arguments p
+0000df40: 6173 7365 6420 746f 2074 6865 2073 7065  assed to the spe
+0000df50: 6369 6669 6564 2061 7070 6c79 206d 6574  cified apply met
+0000df60: 686f 642e 0a20 2020 2052 6574 7572 6e73  hod..    Returns
+0000df70: 3a0a 2020 2020 2020 4966 2060 606d 7574  :.      If ``mut
+0000df80: 6162 6c65 6060 2069 7320 4661 6c73 652c  able`` is False,
+0000df90: 2072 6574 7572 6e73 206f 7574 7075 742e   returns output.
+0000dfa0: 2049 6620 616e 7920 636f 6c6c 6563 7469   If any collecti
+0000dfb0: 6f6e 7320 6172 650a 2020 2020 2020 6d75  ons are.      mu
+0000dfc0: 7461 626c 652c 2072 6574 7572 6e73 2060  table, returns `
+0000dfd0: 6028 6f75 7470 7574 2c20 7661 7273 2960  `(output, vars)`
+0000dfe0: 602c 2077 6865 7265 2060 6076 6172 7360  `, where ``vars`
+0000dff0: 6020 6172 6520 6973 2061 2064 6963 740a  ` are is a dict.
+0000e000: 2020 2020 2020 6f66 2074 6865 206d 6f64        of the mod
+0000e010: 6966 6965 6420 636f 6c6c 6563 7469 6f6e  ified collection
+0000e020: 732e 0a20 2020 2022 2222 0a20 2020 204d  s..    """.    M
+0000e030: 6f64 756c 652e 5f6d 6f64 756c 655f 6368  odule._module_ch
+0000e040: 6563 6b73 2873 656c 6629 0a0a 2020 2020  ecks(self)..    
+0000e050: 6966 2069 7369 6e73 7461 6e63 6528 6d65  if isinstance(me
+0000e060: 7468 6f64 2c20 7374 7229 3a0a 2020 2020  thod, str):.    
+0000e070: 2020 6174 7472 6962 7574 655f 6e61 6d65    attribute_name
+0000e080: 203d 206d 6574 686f 640a 2020 2020 2020   = method.      
+0000e090: 6d65 7468 6f64 203d 2067 6574 6174 7472  method = getattr
+0000e0a0: 2873 656c 662c 2061 7474 7269 6275 7465  (self, attribute
+0000e0b0: 5f6e 616d 6529 0a20 2020 2020 2069 6620  _name).      if 
+0000e0c0: 6e6f 7420 6361 6c6c 6162 6c65 286d 6574  not callable(met
+0000e0d0: 686f 6429 3a0a 2020 2020 2020 2020 636c  hod):.        cl
+0000e0e0: 6173 735f 6e61 6d65 203d 2074 7970 6528  ass_name = type(
+0000e0f0: 7365 6c66 292e 5f5f 6e61 6d65 5f5f 0a20  self).__name__. 
+0000e100: 2020 2020 2020 2072 6169 7365 2054 7970         raise Typ
+0000e110: 6545 7272 6f72 280a 2020 2020 2020 2020  eError(.        
+0000e120: 2020 2020 6622 277b 636c 6173 735f 6e61      f"'{class_na
+0000e130: 6d65 7d2e 7b61 7474 7269 6275 7465 5f6e  me}.{attribute_n
+0000e140: 616d 657d 2720 6d75 7374 2062 6520 6120  ame}' must be a 
+0000e150: 6361 6c6c 6162 6c65 2c20 676f 7422 0a20  callable, got". 
+0000e160: 2020 2020 2020 2020 2020 2066 2720 7b74             f' {t
+0000e170: 7970 6528 6d65 7468 6f64 297d 2e27 0a20  ype(method)}.'. 
+0000e180: 2020 2020 2020 2029 0a20 2020 2065 6c69         ).    eli
+0000e190: 6620 6d65 7468 6f64 2069 7320 4e6f 6e65  f method is None
+0000e1a0: 3a0a 2020 2020 2020 6d65 7468 6f64 203d  :.      method =
+0000e1b0: 2073 656c 662e 5f5f 6361 6c6c 5f5f 0a20   self.__call__. 
+0000e1c0: 2020 206d 6574 686f 6420 3d20 5f67 6574     method = _get
+0000e1d0: 5f75 6e62 6f75 6e64 5f66 6e28 6d65 7468  _unbound_fn(meth
+0000e1e0: 6f64 290a 2020 2020 7265 7475 726e 2061  od).    return a
+0000e1f0: 7070 6c79 280a 2020 2020 2020 2020 6d65  pply(.        me
+0000e200: 7468 6f64 2c0a 2020 2020 2020 2020 7365  thod,.        se
+0000e210: 6c66 2c0a 2020 2020 2020 2020 6d75 7461  lf,.        muta
+0000e220: 626c 653d 6d75 7461 626c 652c 0a20 2020  ble=mutable,.   
+0000e230: 2020 2020 2063 6170 7475 7265 5f69 6e74       capture_int
+0000e240: 6572 6d65 6469 6174 6573 3d63 6170 7475  ermediates=captu
+0000e250: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
+0000e260: 2c0a 2020 2020 2928 7661 7269 6162 6c65  ,.    )(variable
+0000e270: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
+0000e280: 6773 2c20 726e 6773 3d72 6e67 7329 0a0a  gs, rngs=rngs)..
+0000e290: 2020 4074 7261 6365 6261 636b 5f75 7469    @traceback_uti
+0000e2a0: 6c2e 6170 695f 626f 756e 6461 7279 0a20  l.api_boundary. 
+0000e2b0: 2064 6566 2069 6e69 745f 7769 7468 5f6f   def init_with_o
+0000e2c0: 7574 7075 7428 0a20 2020 2020 2073 656c  utput(.      sel
+0000e2d0: 662c 0a20 2020 2020 2072 6e67 733a 2055  f,.      rngs: U
+0000e2e0: 6e69 6f6e 5b4b 6579 4172 7261 792c 2052  nion[KeyArray, R
+0000e2f0: 4e47 5365 7175 656e 6365 735d 2c0a 2020  NGSequences],.  
+0000e300: 2020 2020 2a61 7267 732c 0a20 2020 2020      *args,.     
+0000e310: 206d 6574 686f 643a 2055 6e69 6f6e 5b43   method: Union[C
+0000e320: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
+0000e330: 5d2c 2073 7472 2c20 4e6f 6e65 5d20 3d20  ], str, None] = 
+0000e340: 4e6f 6e65 2c0a 2020 2020 2020 6d75 7461  None,.      muta
+0000e350: 626c 653a 2043 6f6c 6c65 6374 696f 6e46  ble: CollectionF
+0000e360: 696c 7465 7220 3d20 4465 6e79 4c69 7374  ilter = DenyList
+0000e370: 2827 696e 7465 726d 6564 6961 7465 7327  ('intermediates'
+0000e380: 292c 0a20 2020 2020 2063 6170 7475 7265  ),.      capture
+0000e390: 5f69 6e74 6572 6d65 6469 6174 6573 3a20  _intermediates: 
+0000e3a0: 556e 696f 6e5b 0a20 2020 2020 2020 2020  Union[.         
+0000e3b0: 2062 6f6f 6c2c 2043 616c 6c61 626c 655b   bool, Callable[
+0000e3c0: 5b27 4d6f 6475 6c65 272c 2073 7472 5d2c  ['Module', str],
+0000e3d0: 2062 6f6f 6c5d 0a20 2020 2020 205d 203d   bool].      ] =
+0000e3e0: 2046 616c 7365 2c0a 2020 2020 2020 2a2a   False,.      **
+0000e3f0: 6b77 6172 6773 2c0a 2020 2920 2d3e 2054  kwargs,.  ) -> T
+0000e400: 7570 6c65 5b41 6e79 2c20 556e 696f 6e5b  uple[Any, Union[
+0000e410: 4672 6f7a 656e 5661 7269 6162 6c65 4469  FrozenVariableDi
+0000e420: 6374 2c20 4469 6374 5b73 7472 2c20 416e  ct, Dict[str, An
+0000e430: 795d 5d5d 3a0a 2020 2020 2222 2249 6e69  y]]]:.    """Ini
+0000e440: 7469 616c 697a 6573 2061 206d 6f64 756c  tializes a modul
+0000e450: 6520 6d65 7468 6f64 2077 6974 6820 7661  e method with va
+0000e460: 7269 6162 6c65 7320 616e 6420 7265 7475  riables and retu
+0000e470: 726e 7320 6f75 7470 7574 2061 6e64 206d  rns output and m
+0000e480: 6f64 6966 6965 6420 7661 7269 6162 6c65  odified variable
+0000e490: 732e 0a0a 2020 2020 4172 6773 3a0a 2020  s...    Args:.  
+0000e4a0: 2020 2020 726e 6773 3a20 5468 6520 726e      rngs: The rn
+0000e4b0: 6773 2066 6f72 2074 6865 2076 6172 6961  gs for the varia
+0000e4c0: 626c 6520 636f 6c6c 6563 7469 6f6e 732e  ble collections.
+0000e4d0: 0a20 2020 2020 202a 6172 6773 3a20 4e61  .      *args: Na
+0000e4e0: 6d65 6420 6172 6775 6d65 6e74 7320 7061  med arguments pa
+0000e4f0: 7373 6564 2074 6f20 7468 6520 696e 6974  ssed to the init
+0000e500: 2066 756e 6374 696f 6e2e 0a20 2020 2020   function..     
+0000e510: 206d 6574 686f 643a 2041 6e20 6f70 7469   method: An opti
+0000e520: 6f6e 616c 206d 6574 686f 642e 2049 6620  onal method. If 
+0000e530: 7072 6f76 6964 6564 2c20 6170 706c 6965  provided, applie
+0000e540: 7320 7468 6973 206d 6574 686f 642e 2049  s this method. I
+0000e550: 6620 6e6f 740a 2020 2020 2020 2020 7072  f not.        pr
+0000e560: 6f76 6964 6564 2c20 6170 706c 6965 7320  ovided, applies 
+0000e570: 7468 6520 6060 5f5f 6361 6c6c 5f5f 6060  the ``__call__``
+0000e580: 206d 6574 686f 642e 2041 2073 7472 696e   method. A strin
+0000e590: 6720 6361 6e20 616c 736f 2062 6527 0a20  g can also be'. 
+0000e5a0: 2020 2020 2020 2070 726f 7669 6465 6420         provided 
+0000e5b0: 746f 2073 7065 6369 6679 2061 206d 6574  to specify a met
+0000e5c0: 686f 6420 6279 206e 616d 652e 0a20 2020  hod by name..   
+0000e5d0: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
+0000e5e0: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
+0000e5f0: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
+0000e600: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
+0000e610: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
+0000e620: 2020 2020 2074 7265 6174 6564 2061 7320       treated as 
+0000e630: 6d75 7461 626c 653a 2060 6062 6f6f 6c60  mutable: ``bool`
+0000e640: 603a 2061 6c6c 2f6e 6f20 636f 6c6c 6563  `: all/no collec
+0000e650: 7469 6f6e 7320 6172 6520 6d75 7461 626c  tions are mutabl
+0000e660: 652e 0a20 2020 2020 2020 2060 6073 7472  e..        ``str
+0000e670: 6060 3a20 5468 6520 6e61 6d65 206f 6620  ``: The name of 
+0000e680: 6120 7369 6e67 6c65 206d 7574 6162 6c65  a single mutable
+0000e690: 2063 6f6c 6c65 6374 696f 6e2e 2060 606c   collection. ``l
+0000e6a0: 6973 7460 603a 2041 0a20 2020 2020 2020  ist``: A.       
+0000e6b0: 206c 6973 7420 6f66 206e 616d 6573 206f   list of names o
+0000e6c0: 6620 6d75 7461 626c 6520 636f 6c6c 6563  f mutable collec
+0000e6d0: 7469 6f6e 732e 2042 7920 6465 6661 756c  tions. By defaul
+0000e6e0: 7420 616c 6c20 636f 6c6c 6563 7469 6f6e  t all collection
+0000e6f0: 730a 2020 2020 2020 2020 6578 6365 7074  s.        except
+0000e700: 2022 696e 7465 726d 6564 6961 7465 7322   "intermediates"
+0000e710: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
+0000e720: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
+0000e730: 726d 6564 6961 7465 733a 2049 6620 6054  rmediates: If `T
+0000e740: 7275 6560 2c20 6361 7074 7572 6573 2069  rue`, captures i
+0000e750: 6e74 6572 6d65 6469 6174 6520 7265 7475  ntermediate retu
+0000e760: 726e 2076 616c 7565 730a 2020 2020 2020  rn values.      
+0000e770: 2020 6f66 2061 6c6c 204d 6f64 756c 6573    of all Modules
+0000e780: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
+0000e790: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
+0000e7a0: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
+0000e7b0: 6c74 206f 6e6c 790a 2020 2020 2020 2020  lt only.        
+0000e7c0: 7468 6520 7265 7475 726e 2076 616c 7565  the return value
+0000e7d0: 7320 6f66 2061 6c6c 2060 605f 5f63 616c  s of all ``__cal
+0000e7e0: 6c5f 5f60 6020 6d65 7468 6f64 7320 6172  l__`` methods ar
+0000e7f0: 6520 7374 6f72 6564 2e20 4120 6675 6e63  e stored. A func
+0000e800: 7469 6f6e 2063 616e 0a20 2020 2020 2020  tion can.       
+0000e810: 2062 6520 7061 7373 6564 2074 6f20 6368   be passed to ch
+0000e820: 616e 6765 2074 6865 2066 696c 7465 7220  ange the filter 
+0000e830: 6265 6861 7669 6f72 2e20 5468 6520 6669  behavior. The fi
+0000e840: 6c74 6572 2066 756e 6374 696f 6e20 7461  lter function ta
+0000e850: 6b65 730a 2020 2020 2020 2020 7468 6520  kes.        the 
+0000e860: 4d6f 6475 6c65 2069 6e73 7461 6e63 6520  Module instance 
+0000e870: 616e 6420 6d65 7468 6f64 206e 616d 6520  and method name 
+0000e880: 616e 6420 7265 7475 726e 7320 6120 626f  and returns a bo
+0000e890: 6f6c 2069 6e64 6963 6174 696e 670a 2020  ol indicating.  
+0000e8a0: 2020 2020 2020 7768 6574 6865 7220 7468        whether th
+0000e8b0: 6520 6f75 7470 7574 206f 6620 7468 6174  e output of that
+0000e8c0: 206d 6574 686f 6420 696e 766f 6361 7469   method invocati
+0000e8d0: 6f6e 2073 686f 756c 6420 6265 2073 746f  on should be sto
+0000e8e0: 7265 642e 0a20 2020 2020 202a 2a6b 7761  red..      **kwa
+0000e8f0: 7267 733a 204b 6579 776f 7264 2061 7267  rgs: Keyword arg
+0000e900: 756d 656e 7473 2070 6173 7365 6420 746f  uments passed to
+0000e910: 2074 6865 2069 6e69 7420 6675 6e63 7469   the init functi
+0000e920: 6f6e 2e0a 2020 2020 5265 7475 726e 733a  on..    Returns:
+0000e930: 0a20 2020 2020 2060 286f 7574 7075 742c  .      `(output,
+0000e940: 2076 6172 7329 6060 2c20 7768 6572 6520   vars)``, where 
+0000e950: 6060 7661 7273 6060 2061 7265 2069 7320  ``vars`` are is 
+0000e960: 6120 6469 6374 206f 6620 7468 6520 6d6f  a dict of the mo
+0000e970: 6469 6669 6564 0a20 2020 2020 2063 6f6c  dified.      col
+0000e980: 6c65 6374 696f 6e73 2e0a 2020 2020 2222  lections..    ""
+0000e990: 220a 2020 2020 4d6f 6475 6c65 2e5f 6d6f  ".    Module._mo
+0000e9a0: 6475 6c65 5f63 6865 636b 7328 7365 6c66  dule_checks(self
+0000e9b0: 290a 0a20 2020 2069 6620 6e6f 7420 6973  )..    if not is
+0000e9c0: 696e 7374 616e 6365 2872 6e67 732c 2064  instance(rngs, d
+0000e9d0: 6963 7429 3a0a 2020 2020 2020 6966 206e  ict):.      if n
+0000e9e0: 6f74 2063 6f72 652e 7363 6f70 652e 5f69  ot core.scope._i
+0000e9f0: 735f 7661 6c69 645f 726e 6728 726e 6773  s_valid_rng(rngs
+0000ea00: 293a 0a20 2020 2020 2020 2072 6169 7365  ):.        raise
+0000ea10: 2065 7272 6f72 732e 496e 7661 6c69 6452   errors.InvalidR
+0000ea20: 6e67 4572 726f 7228 0a20 2020 2020 2020  ngError(.       
+0000ea30: 2020 2020 2027 524e 4773 2073 686f 756c       'RNGs shoul
+0000ea40: 6420 6265 206f 6620 7368 6170 6520 2832  d be of shape (2
+0000ea50: 2c29 206f 7220 4b65 7941 7272 6179 2069  ,) or KeyArray i
+0000ea60: 6e20 4d6f 6475 6c65 2027 0a20 2020 2020  n Module '.     
+0000ea70: 2020 2020 2020 2066 277b 7365 6c66 2e5f         f'{self._
+0000ea80: 5f63 6c61 7373 5f5f 2e5f 5f6e 616d 655f  _class__.__name_
+0000ea90: 5f7d 2c20 6275 7420 726e 6773 2061 7265  _}, but rngs are
+0000eaa0: 3a20 7b72 6e67 737d 270a 2020 2020 2020  : {rngs}'.      
+0000eab0: 2020 290a 2020 2020 2020 726e 6773 203d    ).      rngs =
+0000eac0: 207b 2770 6172 616d 7327 3a20 726e 6773   {'params': rngs
+0000ead0: 7d0a 0a20 2020 2069 6620 6973 696e 7374  }..    if isinst
+0000eae0: 616e 6365 286d 6574 686f 642c 2073 7472  ance(method, str
+0000eaf0: 293a 0a20 2020 2020 2061 7474 7269 6275  ):.      attribu
+0000eb00: 7465 5f6e 616d 6520 3d20 6d65 7468 6f64  te_name = method
+0000eb10: 0a20 2020 2020 206d 6574 686f 6420 3d20  .      method = 
+0000eb20: 6765 7461 7474 7228 7365 6c66 2c20 6174  getattr(self, at
+0000eb30: 7472 6962 7574 655f 6e61 6d65 290a 2020  tribute_name).  
+0000eb40: 2020 2020 6966 206e 6f74 2063 616c 6c61      if not calla
+0000eb50: 626c 6528 6d65 7468 6f64 293a 0a20 2020  ble(method):.   
+0000eb60: 2020 2020 2063 6c61 7373 5f6e 616d 6520       class_name 
+0000eb70: 3d20 7479 7065 2873 656c 6629 2e5f 5f6e  = type(self).__n
+0000eb80: 616d 655f 5f0a 2020 2020 2020 2020 7261  ame__.        ra
+0000eb90: 6973 6520 5479 7065 4572 726f 7228 0a20  ise TypeError(. 
+0000eba0: 2020 2020 2020 2020 2020 2066 2227 7b63             f"'{c
+0000ebb0: 6c61 7373 5f6e 616d 657d 2e7b 6174 7472  lass_name}.{attr
+0000ebc0: 6962 7574 655f 6e61 6d65 7d27 206d 7573  ibute_name}' mus
+0000ebd0: 7420 6265 2061 2063 616c 6c61 626c 652c  t be a callable,
+0000ebe0: 2067 6f74 220a 2020 2020 2020 2020 2020   got".          
+0000ebf0: 2020 6627 207b 7479 7065 286d 6574 686f    f' {type(metho
+0000ec00: 6429 7d2e 270a 2020 2020 2020 2020 290a  d)}.'.        ).
+0000ec10: 2020 2020 656c 6966 206d 6574 686f 6420      elif method 
+0000ec20: 6973 204e 6f6e 653a 0a20 2020 2020 206d  is None:.      m
+0000ec30: 6574 686f 6420 3d20 7365 6c66 2e5f 5f63  ethod = self.__c
+0000ec40: 616c 6c5f 5f0a 2020 2020 6d65 7468 6f64  all__.    method
+0000ec50: 203d 205f 6765 745f 756e 626f 756e 645f   = _get_unbound_
+0000ec60: 666e 286d 6574 686f 6429 0a20 2020 2072  fn(method).    r
+0000ec70: 6574 7572 6e20 696e 6974 5f77 6974 685f  eturn init_with_
+0000ec80: 6f75 7470 7574 280a 2020 2020 2020 2020  output(.        
+0000ec90: 6d65 7468 6f64 2c0a 2020 2020 2020 2020  method,.        
+0000eca0: 7365 6c66 2c0a 2020 2020 2020 2020 6d75  self,.        mu
+0000ecb0: 7461 626c 653d 6d75 7461 626c 652c 0a20  table=mutable,. 
+0000ecc0: 2020 2020 2020 2063 6170 7475 7265 5f69         capture_i
+0000ecd0: 6e74 6572 6d65 6469 6174 6573 3d63 6170  ntermediates=cap
+0000ece0: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+0000ecf0: 6573 2c0a 2020 2020 2928 726e 6773 2c20  es,.    )(rngs, 
+0000ed00: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+0000ed10: 0a0a 2020 4074 7261 6365 6261 636b 5f75  ..  @traceback_u
+0000ed20: 7469 6c2e 6170 695f 626f 756e 6461 7279  til.api_boundary
+0000ed30: 0a20 2064 6566 2069 6e69 7428 0a20 2020  .  def init(.   
+0000ed40: 2020 2073 656c 662c 0a20 2020 2020 2072     self,.      r
+0000ed50: 6e67 733a 2055 6e69 6f6e 5b4b 6579 4172  ngs: Union[KeyAr
+0000ed60: 7261 792c 2052 4e47 5365 7175 656e 6365  ray, RNGSequence
+0000ed70: 735d 2c0a 2020 2020 2020 2a61 7267 732c  s],.      *args,
+0000ed80: 0a20 2020 2020 206d 6574 686f 643a 2055  .      method: U
+0000ed90: 6e69 6f6e 5b43 616c 6c61 626c 655b 2e2e  nion[Callable[..
+0000eda0: 2e2c 2041 6e79 5d2c 2073 7472 2c20 4e6f  ., Any], str, No
+0000edb0: 6e65 5d20 3d20 4e6f 6e65 2c0a 2020 2020  ne] = None,.    
+0000edc0: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
+0000edd0: 6374 696f 6e46 696c 7465 7220 3d20 4465  ctionFilter = De
+0000ede0: 6e79 4c69 7374 2827 696e 7465 726d 6564  nyList('intermed
+0000edf0: 6961 7465 7327 292c 0a20 2020 2020 2063  iates'),.      c
+0000ee00: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
+0000ee10: 6174 6573 3a20 556e 696f 6e5b 0a20 2020  ates: Union[.   
+0000ee20: 2020 2020 2020 2062 6f6f 6c2c 2043 616c         bool, Cal
+0000ee30: 6c61 626c 655b 5b27 4d6f 6475 6c65 272c  lable[['Module',
+0000ee40: 2073 7472 5d2c 2062 6f6f 6c5d 0a20 2020   str], bool].   
+0000ee50: 2020 205d 203d 2046 616c 7365 2c0a 2020     ] = False,.  
+0000ee60: 2020 2020 2a2a 6b77 6172 6773 2c0a 2020      **kwargs,.  
+0000ee70: 2920 2d3e 2055 6e69 6f6e 5b46 726f 7a65  ) -> Union[Froze
+0000ee80: 6e56 6172 6961 626c 6544 6963 742c 2044  nVariableDict, D
+0000ee90: 6963 745b 7374 722c 2041 6e79 5d5d 3a0a  ict[str, Any]]:.
+0000eea0: 2020 2020 2222 2249 6e69 7469 616c 697a      """Initializ
+0000eeb0: 6573 2061 206d 6f64 756c 6520 6d65 7468  es a module meth
+0000eec0: 6f64 2077 6974 6820 7661 7269 6162 6c65  od with variable
+0000eed0: 7320 616e 6420 7265 7475 726e 7320 6d6f  s and returns mo
+0000eee0: 6469 6669 6564 2076 6172 6961 626c 6573  dified variables
+0000eef0: 2e0a 0a20 2020 2060 6069 6e69 7460 6020  ...    ``init`` 
+0000ef00: 7461 6b65 7320 6173 2066 6972 7374 2061  takes as first a
+0000ef10: 7267 756d 656e 7420 6569 7468 6572 2061  rgument either a
+0000ef20: 2073 696e 676c 6520 6060 5052 4e47 4b65   single ``PRNGKe
+0000ef30: 7960 602c 206f 7220 6120 6469 6374 696f  y``, or a dictio
+0000ef40: 6e61 7279 206d 6170 7069 6e67 2076 6172  nary mapping var
+0000ef50: 6961 626c 6520 636f 6c6c 6563 7469 6f6e  iable collection
+0000ef60: 7320 6e61 6d65 7320 746f 2074 6865 6972  s names to their
+0000ef70: 2060 6050 524e 474b 6579 7360 602c 2061   ``PRNGKeys``, a
+0000ef80: 6e64 2077 696c 6c20 6361 6c6c 2060 606d  nd will call ``m
+0000ef90: 6574 686f 6460 6020 2877 6869 6368 2069  ethod`` (which i
+0000efa0: 7320 7468 6520 6d6f 6475 6c65 2773 2060  s the module's `
+0000efb0: 605f 5f63 616c 6c5f 5f60 6020 6675 6e63  `__call__`` func
+0000efc0: 7469 6f6e 2062 7920 6465 6661 756c 7429  tion by default)
+0000efd0: 2070 6173 7369 6e67 2060 602a 6172 6773   passing ``*args
+0000efe0: 6060 2061 6e64 2060 602a 2a6b 7761 7267  `` and ``**kwarg
+0000eff0: 7360 602c 2061 6e64 2072 6574 7572 6e73  s``, and returns
+0000f000: 0a20 2020 2061 2064 6963 7469 6f6e 6172  .    a dictionar
+0000f010: 7920 6f66 2069 6e69 7469 616c 697a 6564  y of initialized
+0000f020: 2076 6172 6961 626c 6573 2e0a 0a20 2020   variables...   
+0000f030: 2045 7861 6d70 6c65 3a3a 0a0a 2020 2020   Example::..    
+0000f040: 2020 3e3e 3e20 696d 706f 7274 2066 6c61    >>> import fla
+0000f050: 782e 6c69 6e65 6e20 6173 206e 6e0a 2020  x.linen as nn.  
+0000f060: 2020 2020 3e3e 3e20 696d 706f 7274 206a      >>> import j
+0000f070: 6178 2e6e 756d 7079 2061 7320 6a6e 700a  ax.numpy as jnp.
+0000f080: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
+0000f090: 206a 6178 0a20 2020 2020 202e 2e2e 0a20   jax.      .... 
+0000f0a0: 2020 2020 203e 3e3e 2063 6c61 7373 2046       >>> class F
+0000f0b0: 6f6f 286e 6e2e 4d6f 6475 6c65 293a 0a20  oo(nn.Module):. 
+0000f0c0: 2020 2020 202e 2e2e 2020 2040 6e6e 2e63       ...   @nn.c
+0000f0d0: 6f6d 7061 6374 0a20 2020 2020 202e 2e2e  ompact.      ...
+0000f0e0: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+0000f0f0: 7365 6c66 2c20 782c 2074 7261 696e 293a  self, x, train):
+0000f100: 0a20 2020 2020 202e 2e2e 2020 2020 2078  .      ...     x
+0000f110: 203d 206e 6e2e 4465 6e73 6528 3136 2928   = nn.Dense(16)(
+0000f120: 7829 0a20 2020 2020 202e 2e2e 2020 2020  x).      ...    
+0000f130: 2078 203d 206e 6e2e 4261 7463 684e 6f72   x = nn.BatchNor
+0000f140: 6d28 7573 655f 7275 6e6e 696e 675f 6176  m(use_running_av
+0000f150: 6572 6167 653d 6e6f 7420 7472 6169 6e29  erage=not train)
+0000f160: 2878 290a 2020 2020 2020 2e2e 2e20 2020  (x).      ...   
+0000f170: 2020 7820 3d20 6e6e 2e72 656c 7528 7829    x = nn.relu(x)
+0000f180: 0a20 2020 2020 202e 2e2e 2020 2020 2072  .      ...     r
+0000f190: 6574 7572 6e20 6e6e 2e44 656e 7365 2831  eturn nn.Dense(1
+0000f1a0: 2928 7829 0a20 2020 2020 202e 2e2e 0a20  )(x).      .... 
+0000f1b0: 2020 2020 203e 3e3e 206d 6f64 756c 6520       >>> module 
+0000f1c0: 3d20 466f 6f28 290a 2020 2020 2020 3e3e  = Foo().      >>
+0000f1d0: 3e20 6b65 7920 3d20 6a61 782e 7261 6e64  > key = jax.rand
+0000f1e0: 6f6d 2e50 524e 474b 6579 2830 290a 2020  om.PRNGKey(0).  
+0000f1f0: 2020 2020 3e3e 3e20 7661 7269 6162 6c65      >>> variable
+0000f200: 7320 3d20 6d6f 6475 6c65 2e69 6e69 7428  s = module.init(
+0000f210: 6b65 792c 206a 6e70 2e65 6d70 7479 2828  key, jnp.empty((
+0000f220: 312c 2037 2929 2c20 7472 6169 6e3d 4661  1, 7)), train=Fa
+0000f230: 6c73 6529 0a0a 2020 2020 4966 2079 6f75  lse)..    If you
+0000f240: 2070 6173 7320 6120 7369 6e67 6c65 2060   pass a single `
+0000f250: 6050 524e 474b 6579 6060 2c20 466c 6178  `PRNGKey``, Flax
+0000f260: 2077 696c 6c20 7573 6520 6974 2074 6f20   will use it to 
+0000f270: 6665 6564 2074 6865 2060 6027 7061 7261  feed the ``'para
+0000f280: 6d73 2760 6020 524e 4720 7374 7265 616d  ms'`` RNG stream
+0000f290: 2e0a 2020 2020 4966 2079 6f75 2077 616e  ..    If you wan
+0000f2a0: 7420 746f 2075 7365 2061 2064 6966 6665  t to use a diffe
+0000f2b0: 7265 6e74 2052 4e47 2073 7472 6561 6d20  rent RNG stream 
+0000f2c0: 6f72 206e 6565 6420 746f 2075 7365 206d  or need to use m
+0000f2d0: 756c 7469 706c 6520 7374 7265 616d 732c  ultiple streams,
+0000f2e0: 2079 6f75 206d 7573 7420 7061 7373 2061   you must pass a
+0000f2f0: 0a20 2020 2064 6963 7469 6f6e 6172 7920  .    dictionary 
+0000f300: 6d61 7070 696e 6720 6561 6368 2052 4e47  mapping each RNG
+0000f310: 2073 7472 6561 6d20 6e61 6d65 2074 6f20   stream name to 
+0000f320: 6974 7320 636f 7272 6573 706f 6e64 696e  its correspondin
+0000f330: 6720 6060 5052 4e47 4b65 7960 6020 746f  g ``PRNGKey`` to
+0000f340: 2060 6069 6e69 7460 602e 0a0a 2020 2020   ``init``...    
+0000f350: 4578 616d 706c 653a 3a0a 0a20 2020 2020  Example::..     
+0000f360: 203e 3e3e 2063 6c61 7373 2046 6f6f 286e   >>> class Foo(n
+0000f370: 6e2e 4d6f 6475 6c65 293a 0a20 2020 2020  n.Module):.     
+0000f380: 202e 2e2e 2020 2040 6e6e 2e63 6f6d 7061   ...   @nn.compa
+0000f390: 6374 0a20 2020 2020 202e 2e2e 2020 2064  ct.      ...   d
+0000f3a0: 6566 205f 5f63 616c 6c5f 5f28 7365 6c66  ef __call__(self
+0000f3b0: 2c20 782c 2074 7261 696e 293a 0a20 2020  , x, train):.   
+0000f3c0: 2020 202e 2e2e 2020 2020 2078 203d 206e     ...     x = n
+0000f3d0: 6e2e 4465 6e73 6528 3136 2928 7829 0a20  n.Dense(16)(x). 
+0000f3e0: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
+0000f3f0: 206e 6e2e 4261 7463 684e 6f72 6d28 7573   nn.BatchNorm(us
+0000f400: 655f 7275 6e6e 696e 675f 6176 6572 6167  e_running_averag
+0000f410: 653d 6e6f 7420 7472 6169 6e29 2878 290a  e=not train)(x).
+0000f420: 2020 2020 2020 2e2e 2e20 2020 2020 7820        ...     x 
+0000f430: 3d20 6e6e 2e72 656c 7528 7829 0a20 2020  = nn.relu(x).   
+0000f440: 2020 202e 2e2e 0a20 2020 2020 202e 2e2e     ....      ...
+0000f450: 2020 2020 2023 2041 6464 2067 6175 7373       # Add gauss
+0000f460: 6961 6e20 6e6f 6973 650a 2020 2020 2020  ian noise.      
+0000f470: 2e2e 2e20 2020 2020 6e6f 6973 655f 6b65  ...     noise_ke
+0000f480: 7920 3d20 7365 6c66 2e6d 616b 655f 726e  y = self.make_rn
+0000f490: 6728 276e 6f69 7365 2729 0a20 2020 2020  g('noise').     
+0000f4a0: 202e 2e2e 2020 2020 2078 203d 2078 202b   ...     x = x +
+0000f4b0: 206a 6178 2e72 616e 646f 6d2e 6e6f 726d   jax.random.norm
+0000f4c0: 616c 286e 6f69 7365 5f6b 6579 2c20 782e  al(noise_key, x.
+0000f4d0: 7368 6170 6529 0a20 2020 2020 202e 2e2e  shape).      ...
+0000f4e0: 0a20 2020 2020 202e 2e2e 2020 2020 2072  .      ...     r
+0000f4f0: 6574 7572 6e20 6e6e 2e44 656e 7365 2831  eturn nn.Dense(1
+0000f500: 2928 7829 0a20 2020 2020 202e 2e2e 0a20  )(x).      .... 
+0000f510: 2020 2020 203e 3e3e 206d 6f64 756c 6520       >>> module 
+0000f520: 3d20 466f 6f28 290a 2020 2020 2020 3e3e  = Foo().      >>
+0000f530: 3e20 726e 6773 203d 207b 2770 6172 616d  > rngs = {'param
+0000f540: 7327 3a20 6a61 782e 7261 6e64 6f6d 2e50  s': jax.random.P
+0000f550: 524e 474b 6579 2830 292c 2027 6e6f 6973  RNGKey(0), 'nois
+0000f560: 6527 3a20 6a61 782e 7261 6e64 6f6d 2e50  e': jax.random.P
+0000f570: 524e 474b 6579 2831 297d 0a20 2020 2020  RNGKey(1)}.     
+0000f580: 203e 3e3e 2076 6172 6961 626c 6573 203d   >>> variables =
+0000f590: 206d 6f64 756c 652e 696e 6974 2872 6e67   module.init(rng
+0000f5a0: 732c 206a 6e70 2e65 6d70 7479 2828 312c  s, jnp.empty((1,
+0000f5b0: 2037 2929 2c20 7472 6169 6e3d 4661 6c73   7)), train=Fals
+0000f5c0: 6529 0a0a 2020 2020 4a69 7474 696e 6720  e)..    Jitting 
+0000f5d0: 6069 6e69 7460 2069 6e69 7469 616c 697a  `init` initializ
+0000f5e0: 6573 2061 206d 6f64 656c 206c 617a 696c  es a model lazil
+0000f5f0: 7920 7573 696e 6720 6f6e 6c79 2074 6865  y using only the
+0000f600: 2073 6861 7065 7320 6f66 2074 6865 0a20   shapes of the. 
+0000f610: 2020 2070 726f 7669 6465 6420 6172 6775     provided argu
+0000f620: 6d65 6e74 732c 2061 6e64 2061 766f 6964  ments, and avoid
+0000f630: 7320 636f 6d70 7574 696e 6720 7468 6520  s computing the 
+0000f640: 666f 7277 6172 6420 7061 7373 2077 6974  forward pass wit
+0000f650: 6820 6163 7475 616c 0a20 2020 2076 616c  h actual.    val
+0000f660: 7565 732e 2045 7861 6d70 6c65 3a3a 0a0a  ues. Example::..
+0000f670: 2020 2020 2020 3e3e 3e20 6d6f 6475 6c65        >>> module
+0000f680: 203d 206e 6e2e 4465 6e73 6528 3129 0a20   = nn.Dense(1). 
+0000f690: 2020 2020 203e 3e3e 2069 6e69 745f 6a69       >>> init_ji
+0000f6a0: 7420 3d20 6a61 782e 6a69 7428 6d6f 6475  t = jax.jit(modu
+0000f6b0: 6c65 2e69 6e69 7429 0a20 2020 2020 203e  le.init).      >
+0000f6c0: 3e3e 2076 6172 6961 626c 6573 203d 2069  >> variables = i
+0000f6d0: 6e69 745f 6a69 7428 6a61 782e 7261 6e64  nit_jit(jax.rand
+0000f6e0: 6f6d 2e50 524e 474b 6579 2830 292c 206a  om.PRNGKey(0), j
+0000f6f0: 6e70 2e65 6d70 7479 2828 312c 2037 2929  np.empty((1, 7))
+0000f700: 290a 0a20 2020 2060 6069 6e69 7460 6020  )..    ``init`` 
+0000f710: 6973 2061 206c 6967 6874 2077 7261 7070  is a light wrapp
+0000f720: 6572 206f 7665 7220 6060 6170 706c 7960  er over ``apply`
+0000f730: 602c 2073 6f20 6f74 6865 7220 6060 6170  `, so other ``ap
+0000f740: 706c 7960 6020 6172 6775 6d65 6e74 7320  ply`` arguments 
+0000f750: 6c69 6b65 0a20 2020 2060 606d 6574 686f  like.    ``metho
+0000f760: 6460 602c 2060 606d 7574 6162 6c65 6060  d``, ``mutable``
+0000f770: 2c20 616e 6420 6060 6361 7074 7572 655f  , and ``capture_
+0000f780: 696e 7465 726d 6564 6961 7465 7360 6020  intermediates`` 
+0000f790: 6172 6520 616c 736f 2061 7661 696c 6162  are also availab
+0000f7a0: 6c65 2e0a 0a20 2020 2041 7267 733a 0a20  le...    Args:. 
+0000f7b0: 2020 2020 2072 6e67 733a 2054 6865 2072       rngs: The r
+0000f7c0: 6e67 7320 666f 7220 7468 6520 7661 7269  ngs for the vari
+0000f7d0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
+0000f7e0: 2e0a 2020 2020 2020 2a61 7267 733a 204e  ..      *args: N
+0000f7f0: 616d 6564 2061 7267 756d 656e 7473 2070  amed arguments p
+0000f800: 6173 7365 6420 746f 2074 6865 2069 6e69  assed to the ini
+0000f810: 7420 6675 6e63 7469 6f6e 2e0a 2020 2020  t function..    
+0000f820: 2020 6d65 7468 6f64 3a20 416e 206f 7074    method: An opt
+0000f830: 696f 6e61 6c20 6d65 7468 6f64 2e20 4966  ional method. If
+0000f840: 2070 726f 7669 6465 642c 2061 7070 6c69   provided, appli
+0000f850: 6573 2074 6869 7320 6d65 7468 6f64 2e20  es this method. 
+0000f860: 4966 206e 6f74 0a20 2020 2020 2020 2070  If not.        p
+0000f870: 726f 7669 6465 642c 2061 7070 6c69 6573  rovided, applies
+0000f880: 2074 6865 2060 605f 5f63 616c 6c5f 5f60   the ``__call__`
+0000f890: 6020 6d65 7468 6f64 2e20 4120 7374 7269  ` method. A stri
+0000f8a0: 6e67 2063 616e 2061 6c73 6f20 6265 0a20  ng can also be. 
+0000f8b0: 2020 2020 2020 2070 726f 7669 6465 6420         provided 
+0000f8c0: 746f 2073 7065 6369 6679 2061 206d 6574  to specify a met
+0000f8d0: 686f 6420 6279 206e 616d 652e 0a20 2020  hod by name..   
+0000f8e0: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
+0000f8f0: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
+0000f900: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
+0000f910: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
+0000f920: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
+0000f930: 2020 2020 2074 7265 6174 6564 2061 7320       treated as 
+0000f940: 6d75 7461 626c 653a 2060 6062 6f6f 6c60  mutable: ``bool`
+0000f950: 603a 2061 6c6c 2f6e 6f20 636f 6c6c 6563  `: all/no collec
+0000f960: 7469 6f6e 7320 6172 6520 6d75 7461 626c  tions are mutabl
+0000f970: 652e 0a20 2020 2020 2020 2060 6073 7472  e..        ``str
+0000f980: 6060 3a20 5468 6520 6e61 6d65 206f 6620  ``: The name of 
+0000f990: 6120 7369 6e67 6c65 206d 7574 6162 6c65  a single mutable
+0000f9a0: 2063 6f6c 6c65 6374 696f 6e2e 2060 606c   collection. ``l
+0000f9b0: 6973 7460 603a 2041 0a20 2020 2020 2020  ist``: A.       
+0000f9c0: 206c 6973 7420 6f66 206e 616d 6573 206f   list of names o
+0000f9d0: 6620 6d75 7461 626c 6520 636f 6c6c 6563  f mutable collec
+0000f9e0: 7469 6f6e 732e 2042 7920 6465 6661 756c  tions. By defaul
+0000f9f0: 7420 616c 6c20 636f 6c6c 6563 7469 6f6e  t all collection
+0000fa00: 730a 2020 2020 2020 2020 6578 6365 7074  s.        except
+0000fa10: 2022 696e 7465 726d 6564 6961 7465 7322   "intermediates"
+0000fa20: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
+0000fa30: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
+0000fa40: 726d 6564 6961 7465 733a 2049 6620 6054  rmediates: If `T
+0000fa50: 7275 6560 2c20 6361 7074 7572 6573 2069  rue`, captures i
+0000fa60: 6e74 6572 6d65 6469 6174 6520 7265 7475  ntermediate retu
+0000fa70: 726e 2076 616c 7565 730a 2020 2020 2020  rn values.      
+0000fa80: 2020 6f66 2061 6c6c 204d 6f64 756c 6573    of all Modules
+0000fa90: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
+0000faa0: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
+0000fab0: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
+0000fac0: 6c74 206f 6e6c 790a 2020 2020 2020 2020  lt only.        
+0000fad0: 7468 6520 7265 7475 726e 2076 616c 7565  the return value
+0000fae0: 7320 6f66 2061 6c6c 2060 605f 5f63 616c  s of all ``__cal
+0000faf0: 6c5f 5f60 6020 6d65 7468 6f64 7320 6172  l__`` methods ar
+0000fb00: 6520 7374 6f72 6564 2e20 4120 6675 6e63  e stored. A func
+0000fb10: 7469 6f6e 2063 616e 0a20 2020 2020 2020  tion can.       
+0000fb20: 2062 6520 7061 7373 6564 2074 6f20 6368   be passed to ch
+0000fb30: 616e 6765 2074 6865 2066 696c 7465 7220  ange the filter 
+0000fb40: 6265 6861 7669 6f72 2e20 5468 6520 6669  behavior. The fi
+0000fb50: 6c74 6572 2066 756e 6374 696f 6e20 7461  lter function ta
+0000fb60: 6b65 730a 2020 2020 2020 2020 7468 6520  kes.        the 
+0000fb70: 4d6f 6475 6c65 2069 6e73 7461 6e63 6520  Module instance 
+0000fb80: 616e 6420 6d65 7468 6f64 206e 616d 6520  and method name 
+0000fb90: 616e 6420 7265 7475 726e 7320 6120 626f  and returns a bo
+0000fba0: 6f6c 2069 6e64 6963 6174 696e 670a 2020  ol indicating.  
+0000fbb0: 2020 2020 2020 7768 6574 6865 7220 7468        whether th
+0000fbc0: 6520 6f75 7470 7574 206f 6620 7468 6174  e output of that
+0000fbd0: 206d 6574 686f 6420 696e 766f 6361 7469   method invocati
+0000fbe0: 6f6e 2073 686f 756c 6420 6265 2073 746f  on should be sto
+0000fbf0: 7265 642e 0a20 2020 2020 202a 2a6b 7761  red..      **kwa
+0000fc00: 7267 733a 204b 6579 776f 7264 2061 7267  rgs: Keyword arg
+0000fc10: 756d 656e 7473 2070 6173 7365 6420 746f  uments passed to
+0000fc20: 2074 6865 2069 6e69 7420 6675 6e63 7469   the init functi
+0000fc30: 6f6e 2e0a 2020 2020 5265 7475 726e 733a  on..    Returns:
+0000fc40: 0a20 2020 2020 2054 6865 2069 6e69 7469  .      The initi
+0000fc50: 616c 697a 6564 2076 6172 6961 626c 6520  alized variable 
+0000fc60: 6469 6374 2e0a 2020 2020 2222 220a 2020  dict..    """.  
+0000fc70: 2020 4d6f 6475 6c65 2e5f 6d6f 6475 6c65    Module._module
+0000fc80: 5f63 6865 636b 7328 7365 6c66 290a 0a20  _checks(self).. 
+0000fc90: 2020 205f 2c20 765f 6f75 7420 3d20 7365     _, v_out = se
+0000fca0: 6c66 2e69 6e69 745f 7769 7468 5f6f 7574  lf.init_with_out
+0000fcb0: 7075 7428 0a20 2020 2020 2020 2072 6e67  put(.        rng
+0000fcc0: 732c 0a20 2020 2020 2020 202a 6172 6773  s,.        *args
+0000fcd0: 2c0a 2020 2020 2020 2020 6d65 7468 6f64  ,.        method
+0000fce0: 3d6d 6574 686f 642c 0a20 2020 2020 2020  =method,.       
+0000fcf0: 206d 7574 6162 6c65 3d6d 7574 6162 6c65   mutable=mutable
+0000fd00: 2c0a 2020 2020 2020 2020 6361 7074 7572  ,.        captur
+0000fd10: 655f 696e 7465 726d 6564 6961 7465 733d  e_intermediates=
+0000fd20: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+0000fd30: 6961 7465 732c 0a20 2020 2020 2020 202a  iates,.        *
+0000fd40: 2a6b 7761 7267 732c 0a20 2020 2029 0a20  *kwargs,.    ). 
+0000fd50: 2020 2072 6574 7572 6e20 765f 6f75 740a     return v_out.
+0000fd60: 0a20 2040 7472 6163 6562 6163 6b5f 7574  .  @traceback_ut
+0000fd70: 696c 2e61 7069 5f62 6f75 6e64 6172 790a  il.api_boundary.
+0000fd80: 2020 6465 6620 6c61 7a79 5f69 6e69 7428    def lazy_init(
+0000fd90: 0a20 2020 2020 2073 656c 662c 0a20 2020  .      self,.   
+0000fda0: 2020 2072 6e67 733a 2055 6e69 6f6e 5b4b     rngs: Union[K
+0000fdb0: 6579 4172 7261 792c 2052 4e47 5365 7175  eyArray, RNGSequ
+0000fdc0: 656e 6365 735d 2c0a 2020 2020 2020 2a61  ences],.      *a
+0000fdd0: 7267 732c 0a20 2020 2020 206d 6574 686f  rgs,.      metho
+0000fde0: 643a 204f 7074 696f 6e61 6c5b 4361 6c6c  d: Optional[Call
+0000fdf0: 6162 6c65 5b2e 2e2e 2c20 416e 795d 5d20  able[..., Any]] 
+0000fe00: 3d20 4e6f 6e65 2c0a 2020 2020 2020 6d75  = None,.      mu
+0000fe10: 7461 626c 653a 2043 6f6c 6c65 6374 696f  table: Collectio
+0000fe20: 6e46 696c 7465 7220 3d20 4465 6e79 4c69  nFilter = DenyLi
+0000fe30: 7374 2827 696e 7465 726d 6564 6961 7465  st('intermediate
+0000fe40: 7327 292c 0a20 2020 2020 202a 2a6b 7761  s'),.      **kwa
+0000fe50: 7267 732c 0a20 2029 202d 3e20 4672 6f7a  rgs,.  ) -> Froz
+0000fe60: 656e 5661 7269 6162 6c65 4469 6374 3a0a  enVariableDict:.
+0000fe70: 2020 2020 2222 2249 6e69 7469 616c 697a      """Initializ
+0000fe80: 6573 2061 206d 6f64 756c 6520 7769 7468  es a module with
+0000fe90: 6f75 7420 636f 6d70 7574 696e 6720 6f6e  out computing on
+0000fea0: 2061 6e20 6163 7475 616c 2069 6e70 7574   an actual input
+0000feb0: 2e0a 0a20 2020 206c 617a 795f 696e 6974  ...    lazy_init
+0000fec0: 2077 696c 6c20 696e 6974 6961 6c69 7a65   will initialize
+0000fed0: 2074 6865 2076 6172 6961 626c 6573 2077   the variables w
+0000fee0: 6974 686f 7574 2064 6f69 6e67 2075 6e6e  ithout doing unn
+0000fef0: 6563 6573 7361 7279 2063 6f6d 7075 7465  ecessary compute
+0000ff00: 2e0a 2020 2020 5468 6520 696e 7075 7420  ..    The input 
+0000ff10: 6461 7461 2073 686f 756c 6420 6265 2070  data should be p
+0000ff20: 6173 7365 6420 6173 2061 2060 606a 6178  assed as a ``jax
+0000ff30: 2e53 6861 7065 4474 7970 6553 7472 7563  .ShapeDtypeStruc
+0000ff40: 7460 6020 7768 6963 6820 7370 6563 6966  t`` which specif
+0000ff50: 6965 730a 2020 2020 7468 6520 7368 6170  ies.    the shap
+0000ff60: 6520 616e 6420 6474 7970 6520 6f66 2074  e and dtype of t
+0000ff70: 6865 2069 6e70 7574 2062 7574 206e 6f20  he input but no 
+0000ff80: 636f 6e63 7265 7465 2064 6174 612e 0a0a  concrete data...
+0000ff90: 2020 2020 4578 616d 706c 653a 3a0a 0a20      Example::.. 
+0000ffa0: 2020 2020 206d 6f64 656c 203d 206e 6e2e       model = nn.
+0000ffb0: 4465 6e73 6528 6665 6174 7572 6573 3d32  Dense(features=2
+0000ffc0: 3536 290a 2020 2020 2020 7661 7269 6162  56).      variab
+0000ffd0: 6c65 7320 3d20 6d6f 6465 6c2e 6c61 7a79  les = model.lazy
+0000ffe0: 5f69 6e69 7428 726e 672c 206a 6178 2e53  _init(rng, jax.S
+0000fff0: 6861 7065 4474 7970 6553 7472 7563 7428  hapeDtypeStruct(
+00010000: 2831 2c20 3132 3829 2c20 6a6e 702e 666c  (1, 128), jnp.fl
+00010010: 6f61 7433 3229 290a 0a20 2020 2054 6865  oat32))..    The
+00010020: 2061 7267 7320 616e 6420 6b77 6172 6773   args and kwargs
+00010030: 2061 7267 7320 7061 7373 6564 2074 6f20   args passed to 
+00010040: 6060 6c61 7a79 5f69 6e69 7460 6020 6361  ``lazy_init`` ca
+00010050: 6e20 6265 2061 206d 6978 206f 660a 2020  n be a mix of.  
+00010060: 2020 636f 6e63 7265 7465 2028 6a61 7820    concrete (jax 
+00010070: 6172 7261 7973 2c20 7363 616c 6172 732c  arrays, scalars,
+00010080: 2062 6f6f 6c73 2920 616e 6420 6162 7374   bools) and abst
+00010090: 7261 6374 2028 5368 6170 6544 7479 7065  ract (ShapeDtype
+000100a0: 5374 7275 6374 2920 7661 6c75 6573 2e0a  Struct) values..
+000100b0: 2020 2020 436f 6e63 7265 7465 2076 616c      Concrete val
+000100c0: 7565 7320 6172 6520 6f6e 6c79 206e 6563  ues are only nec
+000100d0: 6573 7361 7279 2066 6f72 2061 7267 756d  essary for argum
+000100e0: 656e 7473 2074 6861 7420 6166 6665 6374  ents that affect
+000100f0: 0a20 2020 2074 6865 2069 6e69 7469 616c  .    the initial
+00010100: 697a 6174 696f 6e20 6f66 2076 6172 6961  ization of varia
+00010110: 626c 6573 2e20 466f 7220 6578 616d 706c  bles. For exampl
+00010120: 652c 2074 6865 206d 6f64 656c 206d 6967  e, the model mig
+00010130: 6874 2065 7870 6563 740a 2020 2020 6120  ht expect.    a 
+00010140: 6b65 7977 6f72 6420 6172 6720 7468 6174  keyword arg that
+00010150: 2065 6e61 626c 6573 2f64 6973 6162 6c65   enables/disable
+00010160: 7320 6120 7375 6270 6172 7420 6f66 2074  s a subpart of t
+00010170: 6865 206d 6f64 656c 2e0a 2020 2020 496e  he model..    In
+00010180: 2074 6869 7320 6361 7365 2c20 616e 2065   this case, an e
+00010190: 7870 6c69 6369 7420 7661 6c75 6520 2854  xplicit value (T
+000101a0: 7275 652f 466c 6173 6529 2073 686f 756c  rue/Flase) shoul
+000101b0: 6420 6265 2070 6173 7365 6420 6f74 6865  d be passed othe
+000101c0: 7277 6973 650a 2020 2020 6060 6c61 7a79  rwise.    ``lazy
+000101d0: 5f69 6e69 7460 6020 6361 6e6e 6f74 2069  _init`` cannot i
+000101e0: 6e66 6572 2077 6869 6368 2076 6172 6961  nfer which varia
+000101f0: 626c 6573 2073 686f 756c 6420 6265 2069  bles should be i
+00010200: 6e69 7469 616c 697a 6564 2e0a 0a20 2020  nitialized...   
+00010210: 2041 7267 733a 0a20 2020 2020 2072 6e67   Args:.      rng
+00010220: 733a 2054 6865 2072 6e67 7320 666f 7220  s: The rngs for 
+00010230: 7468 6520 7661 7269 6162 6c65 2063 6f6c  the variable col
+00010240: 6c65 6374 696f 6e73 2e0a 2020 2020 2020  lections..      
+00010250: 2a61 7267 733a 2061 7267 756d 656e 7473  *args: arguments
+00010260: 2070 6173 7365 6420 746f 2074 6865 2069   passed to the i
+00010270: 6e69 7420 6675 6e63 7469 6f6e 2e0a 2020  nit function..  
+00010280: 2020 2020 6d65 7468 6f64 3a20 416e 206f      method: An o
+00010290: 7074 696f 6e61 6c20 6d65 7468 6f64 2e20  ptional method. 
+000102a0: 4966 2070 726f 7669 6465 642c 2061 7070  If provided, app
+000102b0: 6c69 6573 2074 6869 7320 6d65 7468 6f64  lies this method
+000102c0: 2e20 4966 206e 6f74 0a20 2020 2020 2020  . If not.       
+000102d0: 2070 726f 7669 6465 642c 2061 7070 6c69   provided, appli
+000102e0: 6573 2074 6865 2060 605f 5f63 616c 6c5f  es the ``__call_
+000102f0: 5f60 6020 6d65 7468 6f64 2e0a 2020 2020  _`` method..    
+00010300: 2020 6d75 7461 626c 653a 2043 616e 2062    mutable: Can b
+00010310: 6520 626f 6f6c 2c20 7374 722c 206f 7220  e bool, str, or 
+00010320: 6c69 7374 2e20 5370 6563 6966 6965 7320  list. Specifies 
+00010330: 7768 6963 6820 636f 6c6c 6563 7469 6f6e  which collection
+00010340: 7320 7368 6f75 6c64 2062 650a 2020 2020  s should be.    
+00010350: 2020 2020 7472 6561 7465 6420 6173 206d      treated as m
+00010360: 7574 6162 6c65 3a20 6060 626f 6f6c 6060  utable: ``bool``
+00010370: 3a20 616c 6c2f 6e6f 2063 6f6c 6c65 6374  : all/no collect
+00010380: 696f 6e73 2061 7265 206d 7574 6162 6c65  ions are mutable
+00010390: 2e0a 2020 2020 2020 2020 6060 7374 7260  ..        ``str`
+000103a0: 603a 2054 6865 206e 616d 6520 6f66 2061  `: The name of a
+000103b0: 2073 696e 676c 6520 6d75 7461 626c 6520   single mutable 
+000103c0: 636f 6c6c 6563 7469 6f6e 2e20 6060 6c69  collection. ``li
+000103d0: 7374 6060 3a20 410a 2020 2020 2020 2020  st``: A.        
+000103e0: 6c69 7374 206f 6620 6e61 6d65 7320 6f66  list of names of
+000103f0: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
+00010400: 696f 6e73 2e20 4279 2064 6566 6175 6c74  ions. By default
+00010410: 2061 6c6c 2063 6f6c 6c65 6374 696f 6e73   all collections
+00010420: 0a20 2020 2020 2020 2065 7863 6570 7420  .        except 
+00010430: 2269 6e74 6572 6d65 6469 6174 6573 2220  "intermediates" 
+00010440: 6172 6520 6d75 7461 626c 652e 0a20 2020  are mutable..   
+00010450: 2020 202a 2a6b 7761 7267 733a 204b 6579     **kwargs: Key
+00010460: 776f 7264 2061 7267 756d 656e 7473 2070  word arguments p
+00010470: 6173 7365 6420 746f 2074 6865 2069 6e69  assed to the ini
+00010480: 7420 6675 6e63 7469 6f6e 2e0a 2020 2020  t function..    
+00010490: 5265 7475 726e 733a 0a20 2020 2020 2054  Returns:.      T
+000104a0: 6865 2069 6e69 7469 616c 697a 6564 2076  he initialized v
+000104b0: 6172 6961 626c 6520 6469 6374 2e0a 2020  ariable dict..  
+000104c0: 2020 2222 220a 2020 2020 4d6f 6475 6c65    """.    Module
+000104d0: 2e5f 6d6f 6475 6c65 5f63 6865 636b 7328  ._module_checks(
+000104e0: 7365 6c66 290a 0a20 2020 2064 6566 206c  self)..    def l
+000104f0: 617a 795f 7772 6170 7065 7228 726e 6773  azy_wrapper(rngs
+00010500: 2c20 2a61 7267 732c 202a 2a6b 7761 7267  , *args, **kwarg
+00010510: 7329 3a0a 2020 2020 2020 7265 7475 726e  s):.      return
+00010520: 2073 656c 662e 696e 6974 2872 6e67 732c   self.init(rngs,
+00010530: 202a 6172 6773 2c20 6d65 7468 6f64 3d6d   *args, method=m
+00010540: 6574 686f 642c 206d 7574 6162 6c65 3d6d  ethod, mutable=m
+00010550: 7574 6162 6c65 2c20 2a2a 6b77 6172 6773  utable, **kwargs
+00010560: 290a 0a20 2020 2072 6574 7572 6e20 7061  )..    return pa
+00010570: 7274 6961 6c5f 6576 616c 2e6c 617a 795f  rtial_eval.lazy_
+00010580: 696e 6974 286c 617a 795f 7772 6170 7065  init(lazy_wrappe
+00010590: 7229 2872 6e67 732c 202a 6172 6773 2c20  r)(rngs, *args, 
+000105a0: 2a2a 6b77 6172 6773 290a 0a20 2040 7072  **kwargs)..  @pr
+000105b0: 6f70 6572 7479 0a20 2064 6566 2076 6172  operty.  def var
+000105c0: 6961 626c 6573 2873 656c 6629 202d 3e20  iables(self) -> 
+000105d0: 5661 7269 6162 6c65 4469 6374 3a0a 2020  VariableDict:.  
+000105e0: 2020 2222 2252 6574 7572 6e73 2074 6865    """Returns the
+000105f0: 2076 6172 6961 626c 6573 2069 6e20 7468   variables in th
+00010600: 6973 206d 6f64 756c 652e 2222 220a 2020  is module.""".  
+00010610: 2020 6966 2073 656c 662e 7363 6f70 6520    if self.scope 
+00010620: 6973 204e 6f6e 653a 0a20 2020 2020 2072  is None:.      r
+00010630: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+00010640: 2243 616e 2774 2061 6363 6573 7320 7661  "Can't access va
+00010650: 7269 6162 6c65 7320 6f6e 2075 6e62 6f75  riables on unbou
+00010660: 6e64 206d 6f64 756c 6573 2229 0a20 2020  nd modules").   
+00010670: 2072 6574 7572 6e20 7365 6c66 2e73 636f   return self.sco
+00010680: 7065 2e76 6172 6961 626c 6573 2829 0a0a  pe.variables()..
+00010690: 2020 6465 6620 6765 745f 7661 7269 6162    def get_variab
+000106a0: 6c65 2873 656c 662c 2063 6f6c 3a20 7374  le(self, col: st
+000106b0: 722c 206e 616d 653a 2073 7472 2c20 6465  r, name: str, de
+000106c0: 6661 756c 743a 204f 7074 696f 6e61 6c5b  fault: Optional[
+000106d0: 545d 203d 204e 6f6e 6529 202d 3e20 543a  T] = None) -> T:
+000106e0: 0a20 2020 2022 2222 5265 7472 6965 7665  .    """Retrieve
+000106f0: 7320 7468 6520 7661 6c75 6520 6f66 2061  s the value of a
+00010700: 2056 6172 6961 626c 652e 0a0a 2020 2020   Variable...    
+00010710: 4172 6773 3a0a 2020 2020 2020 636f 6c3a  Args:.      col:
+00010720: 2074 6865 2076 6172 6961 626c 6520 636f   the variable co
+00010730: 6c6c 6563 7469 6f6e 2e0a 2020 2020 2020  llection..      
+00010740: 6e61 6d65 3a20 7468 6520 6e61 6d65 206f  name: the name o
+00010750: 6620 7468 6520 7661 7269 6162 6c65 2e0a  f the variable..
+00010760: 2020 2020 2020 6465 6661 756c 743a 2074        default: t
+00010770: 6865 2064 6566 6175 6c74 2076 616c 7565  he default value
+00010780: 2074 6f20 7265 7475 726e 2069 6620 7468   to return if th
+00010790: 6520 7661 7269 6162 6c65 2064 6f65 7320  e variable does 
+000107a0: 6e6f 7420 6578 6973 7420 696e 0a20 2020  not exist in.   
+000107b0: 2020 2020 2074 6869 7320 7363 6f70 652e       this scope.
+000107c0: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
+000107d0: 2020 2020 2054 6865 2076 616c 7565 206f       The value o
+000107e0: 6620 7468 6520 696e 7075 7420 7661 7269  f the input vari
+000107f0: 6162 6c65 2c20 6f66 2074 6865 2064 6566  able, of the def
+00010800: 6175 6c74 2076 616c 7565 2069 6620 7468  ault value if th
+00010810: 6520 7661 7269 6162 6c65 0a20 2020 2020  e variable.     
+00010820: 2064 6f65 736e 2774 2065 7869 7374 2069   doesn't exist i
+00010830: 6e20 7468 6973 2073 636f 7065 2e0a 2020  n this scope..  
+00010840: 2020 2222 220a 2020 2020 6966 2073 656c    """.    if sel
+00010850: 662e 7363 6f70 6520 6973 204e 6f6e 653a  f.scope is None:
+00010860: 0a20 2020 2020 2072 6169 7365 2056 616c  .      raise Val
+00010870: 7565 4572 726f 7228 2243 616e 2774 2061  ueError("Can't a
+00010880: 6363 6573 7320 7661 7269 6162 6c65 7320  ccess variables 
+00010890: 6f6e 2075 6e62 6f75 6e64 206d 6f64 756c  on unbound modul
+000108a0: 6573 2229 0a20 2020 2072 6574 7572 6e20  es").    return 
+000108b0: 7365 6c66 2e73 636f 7065 2e67 6574 5f76  self.scope.get_v
+000108c0: 6172 6961 626c 6528 636f 6c2c 206e 616d  ariable(col, nam
+000108d0: 652c 2064 6566 6175 6c74 290a 0a20 2064  e, default)..  d
+000108e0: 6566 2070 7574 5f76 6172 6961 626c 6528  ef put_variable(
+000108f0: 7365 6c66 2c20 636f 6c3a 2073 7472 2c20  self, col: str, 
+00010900: 6e61 6d65 3a20 7374 722c 2076 616c 7565  name: str, value
+00010910: 3a20 416e 7929 3a0a 2020 2020 2222 2255  : Any):.    """U
+00010920: 7064 6174 6573 2074 6865 2076 616c 7565  pdates the value
+00010930: 206f 6620 7468 6520 6769 7665 6e20 7661   of the given va
+00010940: 7269 6162 6c65 2069 6620 6974 2069 7320  riable if it is 
+00010950: 6d75 7461 626c 652c 206f 7220 616e 2065  mutable, or an e
+00010960: 7272 6f72 206f 7468 6572 7769 7365 2e0a  rror otherwise..
+00010970: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
+00010980: 2063 6f6c 3a20 7468 6520 7661 7269 6162   col: the variab
+00010990: 6c65 2063 6f6c 6c65 6374 696f 6e2e 0a20  le collection.. 
+000109a0: 2020 2020 206e 616d 653a 2074 6865 206e       name: the n
+000109b0: 616d 6520 6f66 2074 6865 2076 6172 6961  ame of the varia
+000109c0: 626c 652e 0a20 2020 2020 2076 616c 7565  ble..      value
+000109d0: 3a20 7468 6520 6e65 7720 7661 6c75 6520  : the new value 
+000109e0: 6f66 2074 6865 2076 6172 6961 626c 652e  of the variable.
+000109f0: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    """.    if 
+00010a00: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
+00010a10: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
+00010a20: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
+00010a30: 7420 6163 6365 7373 2076 6172 6961 626c  t access variabl
+00010a40: 6573 206f 6e20 756e 626f 756e 6420 6d6f  es on unbound mo
+00010a50: 6475 6c65 7322 290a 2020 2020 7365 6c66  dules").    self
+00010a60: 2e73 636f 7065 2e70 7574 5f76 6172 6961  .scope.put_varia
+00010a70: 626c 6528 636f 6c2c 206e 616d 652c 2076  ble(col, name, v
+00010a80: 616c 7565 290a 0a20 2040 6f76 6572 6c6f  alue)..  @overlo
+00010a90: 6164 0a20 2064 6566 2073 6f77 2873 656c  ad.  def sow(sel
+00010aa0: 662c 2063 6f6c 3a20 7374 722c 206e 616d  f, col: str, nam
+00010ab0: 653a 2073 7472 2c20 7661 6c75 653a 2041  e: str, value: A
+00010ac0: 6e79 2920 2d3e 2062 6f6f 6c3a 0a20 2020  ny) -> bool:.   
+00010ad0: 202e 2e2e 0a0a 2020 406f 7665 726c 6f61   .....  @overloa
+00010ae0: 640a 2020 6465 6620 736f 7728 0a20 2020  d.  def sow(.   
+00010af0: 2020 2073 656c 662c 0a20 2020 2020 2063     self,.      c
+00010b00: 6f6c 3a20 7374 722c 0a20 2020 2020 206e  ol: str,.      n
+00010b10: 616d 653a 2073 7472 2c0a 2020 2020 2020  ame: str,.      
+00010b20: 7661 6c75 653a 2054 2c0a 2020 2020 2020  value: T,.      
+00010b30: 7265 6475 6365 5f66 6e3a 2043 616c 6c61  reduce_fn: Calla
+00010b40: 626c 655b 5b4b 2c20 545d 2c20 4b5d 203d  ble[[K, T], K] =
+00010b50: 2074 7570 6c65 5f72 6564 7563 652c 0a20   tuple_reduce,. 
+00010b60: 2020 2020 2069 6e69 745f 666e 3a20 4361       init_fn: Ca
+00010b70: 6c6c 6162 6c65 5b5b 5d2c 204b 5d20 3d20  llable[[], K] = 
+00010b80: 7475 706c 655f 696e 6974 2c20 2023 2074  tuple_init,  # t
+00010b90: 7970 653a 2069 676e 6f72 650a 2020 2920  ype: ignore.  ) 
+00010ba0: 2d3e 2062 6f6f 6c3a 0a20 2020 202e 2e2e  -> bool:.    ...
+00010bb0: 0a0a 2020 6465 6620 736f 7728 0a20 2020  ..  def sow(.   
+00010bc0: 2020 2073 656c 662c 0a20 2020 2020 2063     self,.      c
+00010bd0: 6f6c 3a20 7374 722c 0a20 2020 2020 206e  ol: str,.      n
+00010be0: 616d 653a 2073 7472 2c0a 2020 2020 2020  ame: str,.      
+00010bf0: 7661 6c75 653a 2054 2c0a 2020 2020 2020  value: T,.      
+00010c00: 7265 6475 6365 5f66 6e3a 2043 616c 6c61  reduce_fn: Calla
+00010c10: 626c 655b 5b4b 2c20 545d 2c20 4b5d 203d  ble[[K, T], K] =
+00010c20: 2074 7570 6c65 5f72 6564 7563 652c 0a20   tuple_reduce,. 
+00010c30: 2020 2020 2069 6e69 745f 666e 3a20 4361       init_fn: Ca
+00010c40: 6c6c 6162 6c65 5b5b 5d2c 204b 5d20 3d20  llable[[], K] = 
+00010c50: 7475 706c 655f 696e 6974 2c20 2023 2074  tuple_init,  # t
+00010c60: 7970 653a 2069 676e 6f72 650a 2020 2920  ype: ignore.  ) 
+00010c70: 2d3e 2062 6f6f 6c3a 0a20 2020 2022 2222  -> bool:.    """
+00010c80: 5374 6f72 6573 2061 2076 616c 7565 2069  Stores a value i
+00010c90: 6e20 6120 636f 6c6c 6563 7469 6f6e 2e0a  n a collection..
+00010ca0: 0a20 2020 2043 6f6c 6c65 6374 696f 6e73  .    Collections
+00010cb0: 2063 616e 2062 6520 7573 6564 2074 6f20   can be used to 
+00010cc0: 636f 6c6c 6563 7420 696e 7465 726d 6564  collect intermed
+00010cd0: 6961 7465 2076 616c 7565 7320 7769 7468  iate values with
+00010ce0: 6f75 740a 2020 2020 7468 6520 6f76 6572  out.    the over
+00010cf0: 6865 6164 206f 6620 6578 706c 6963 6974  head of explicit
+00010d00: 6c79 2070 6173 7369 6e67 2061 2063 6f6e  ly passing a con
+00010d10: 7461 696e 6572 2074 6872 6f75 6768 2065  tainer through e
+00010d20: 6163 6820 4d6f 6475 6c65 2063 616c 6c2e  ach Module call.
+00010d30: 0a0a 2020 2020 4966 2074 6865 2074 6172  ..    If the tar
+00010d40: 6765 7420 636f 6c6c 6563 7469 6f6e 2069  get collection i
+00010d50: 7320 6e6f 7420 6d75 7461 626c 6520 6073  s not mutable `s
+00010d60: 6f77 6020 6265 6861 7665 7320 6c69 6b65  ow` behaves like
+00010d70: 2061 206e 6f2d 6f70 0a20 2020 2061 6e64   a no-op.    and
+00010d80: 2072 6574 7572 6e73 2060 4661 6c73 6560   returns `False`
+00010d90: 2e0a 0a20 2020 2045 7861 6d70 6c65 3a3a  ...    Example::
+00010da0: 0a0a 2020 2020 2020 696d 706f 7274 206a  ..      import j
+00010db0: 6178 0a20 2020 2020 2069 6d70 6f72 7420  ax.      import 
+00010dc0: 6a61 782e 6e75 6d70 7920 6173 206a 6e70  jax.numpy as jnp
+00010dd0: 0a20 2020 2020 2069 6d70 6f72 7420 666c  .      import fl
+00010de0: 6178 2e6c 696e 656e 2061 7320 6e6e 0a0a  ax.linen as nn..
+00010df0: 2020 2020 2020 636c 6173 7320 466f 6f28        class Foo(
+00010e00: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
+00010e10: 2020 2020 406e 6e2e 636f 6d70 6163 740a      @nn.compact.
+00010e20: 2020 2020 2020 2020 6465 6620 5f5f 6361          def __ca
+00010e30: 6c6c 5f5f 2873 656c 662c 2078 293a 0a20  ll__(self, x):. 
+00010e40: 2020 2020 2020 2020 2068 203d 206e 6e2e           h = nn.
+00010e50: 4465 6e73 6528 3429 2878 290a 2020 2020  Dense(4)(x).    
+00010e60: 2020 2020 2020 7365 6c66 2e73 6f77 2827        self.sow('
+00010e70: 696e 7465 726d 6564 6961 7465 7327 2c20  intermediates', 
+00010e80: 2768 272c 2068 290a 2020 2020 2020 2020  'h', h).        
+00010e90: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
+00010ea0: 6528 3229 2868 290a 0a20 2020 2020 2078  e(2)(h)..      x
+00010eb0: 203d 206a 6e70 2e6f 6e65 7328 2831 362c   = jnp.ones((16,
+00010ec0: 2039 2929 0a20 2020 2020 206d 6f64 656c   9)).      model
+00010ed0: 203d 2046 6f6f 2829 0a20 2020 2020 2076   = Foo().      v
+00010ee0: 6172 6961 626c 6573 203d 206d 6f64 656c  ariables = model
+00010ef0: 2e69 6e69 7428 6a61 782e 7261 6e64 6f6d  .init(jax.random
+00010f00: 2e50 524e 474b 6579 2830 292c 2078 290a  .PRNGKey(0), x).
+00010f10: 2020 2020 2020 792c 2073 7461 7465 203d        y, state =
+00010f20: 206d 6f64 656c 2e61 7070 6c79 2876 6172   model.apply(var
+00010f30: 6961 626c 6573 2c20 782c 206d 7574 6162  iables, x, mutab
+00010f40: 6c65 3d5b 2769 6e74 6572 6d65 6469 6174  le=['intermediat
+00010f50: 6573 275d 290a 2020 2020 2020 7072 696e  es']).      prin
+00010f60: 7428 7374 6174 655b 2769 6e74 6572 6d65  t(state['interme
+00010f70: 6469 6174 6573 275d 2920 2023 207b 2768  diates'])  # {'h
+00010f80: 273a 2028 2e2e 2e2c 297d 0a0a 2020 2020  ': (...,)}..    
+00010f90: 4279 2064 6566 6175 6c74 2074 6865 2076  By default the v
+00010fa0: 616c 7565 7320 6172 6520 7374 6f72 6564  alues are stored
+00010fb0: 2069 6e20 6120 7475 706c 6520 616e 6420   in a tuple and 
+00010fc0: 6561 6368 2073 746f 7265 6420 7661 6c75  each stored valu
+00010fd0: 650a 2020 2020 6973 2061 7070 656e 6465  e.    is appende
+00010fe0: 6420 6174 2074 6865 2065 6e64 2e20 5468  d at the end. Th
+00010ff0: 6973 2077 6179 2061 6c6c 2069 6e74 6572  is way all inter
+00011000: 6d65 6469 6174 6573 2063 616e 2062 6520  mediates can be 
+00011010: 7472 6163 6b65 6420 7768 656e 0a20 2020  tracked when.   
+00011020: 2074 6865 2073 616d 6520 6d6f 6475 6c65   the same module
+00011030: 2069 7320 6361 6c6c 6564 206d 756c 7469   is called multi
+00011040: 706c 6520 7469 6d65 732e 2041 6c74 6572  ple times. Alter
+00011050: 6e61 7469 7665 6c79 2c20 6120 6375 7374  natively, a cust
+00011060: 6f6d 0a20 2020 2069 6e69 742f 7265 6475  om.    init/redu
+00011070: 6365 2066 756e 6374 696f 6e20 6361 6e20  ce function can 
+00011080: 6265 2070 6173 7365 643a 3a0a 0a20 2020  be passed::..   
+00011090: 2020 2063 6c61 7373 2046 6f6f 3228 6e6e     class Foo2(nn
+000110a0: 2e4d 6f64 756c 6529 3a0a 2020 2020 2020  .Module):.      
+000110b0: 2020 406e 6e2e 636f 6d70 6163 740a 2020    @nn.compact.  
+000110c0: 2020 2020 2020 6465 6620 5f5f 6361 6c6c        def __call
+000110d0: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
+000110e0: 2020 2020 2020 2069 6e69 745f 666e 203d         init_fn =
+000110f0: 206c 616d 6264 613a 2030 0a20 2020 2020   lambda: 0.     
+00011100: 2020 2020 2072 6564 7563 655f 666e 203d       reduce_fn =
+00011110: 206c 616d 6264 6120 612c 2062 3a20 6120   lambda a, b: a 
+00011120: 2b20 620a 2020 2020 2020 2020 2020 7365  + b.          se
+00011130: 6c66 2e73 6f77 2827 696e 7465 726d 6564  lf.sow('intermed
+00011140: 6961 7465 7327 2c20 2768 272c 2078 2c0a  iates', 'h', x,.
+00011150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011160: 2020 2069 6e69 745f 666e 3d69 6e69 745f     init_fn=init_
+00011170: 666e 2c20 7265 6475 6365 5f66 6e3d 7265  fn, reduce_fn=re
+00011180: 6475 6365 5f66 6e29 0a20 2020 2020 2020  duce_fn).       
+00011190: 2020 2073 656c 662e 736f 7728 2769 6e74     self.sow('int
+000111a0: 6572 6d65 6469 6174 6573 272c 2027 6827  ermediates', 'h'
+000111b0: 2c20 7820 2a20 322c 0a20 2020 2020 2020  , x * 2,.       
+000111c0: 2020 2020 2020 2020 2020 2020 696e 6974              init
+000111d0: 5f66 6e3d 696e 6974 5f66 6e2c 2072 6564  _fn=init_fn, red
+000111e0: 7563 655f 666e 3d72 6564 7563 655f 666e  uce_fn=reduce_fn
+000111f0: 290a 2020 2020 2020 2020 2020 7265 7475  ).          retu
+00011200: 726e 2078 0a0a 2020 2020 2020 6d6f 6465  rn x..      mode
+00011210: 6c20 3d20 466f 6f32 2829 0a20 2020 2020  l = Foo2().     
+00011220: 2076 6172 6961 626c 6573 203d 206d 6f64   variables = mod
+00011230: 656c 2e69 6e69 7428 6a61 782e 7261 6e64  el.init(jax.rand
+00011240: 6f6d 2e50 524e 474b 6579 2830 292c 2078  om.PRNGKey(0), x
+00011250: 290a 2020 2020 2020 792c 2073 7461 7465  ).      y, state
+00011260: 203d 206d 6f64 656c 2e61 7070 6c79 2876   = model.apply(v
+00011270: 6172 6961 626c 6573 2c20 6a6e 702e 6f6e  ariables, jnp.on
+00011280: 6573 2828 312c 2031 2929 2c20 6d75 7461  es((1, 1)), muta
+00011290: 626c 653d 5b27 696e 7465 726d 6564 6961  ble=['intermedia
+000112a0: 7465 7327 5d29 0a20 2020 2020 2070 7269  tes']).      pri
+000112b0: 6e74 2873 7461 7465 5b27 696e 7465 726d  nt(state['interm
+000112c0: 6564 6961 7465 7327 5d29 2020 2320 3d3d  ediates'])  # ==
+000112d0: 3e20 7b27 6827 3a20 5b5b 332e 5d5d 7d0a  > {'h': [[3.]]}.
+000112e0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
+000112f0: 2063 6f6c 3a20 5468 6520 6e61 6d65 206f   col: The name o
+00011300: 6620 7468 6520 7661 7269 6162 6c65 2063  f the variable c
+00011310: 6f6c 6c65 6374 696f 6e2e 0a20 2020 2020  ollection..     
+00011320: 206e 616d 653a 2054 6865 206e 616d 6520   name: The name 
+00011330: 6f66 2074 6865 2076 6172 6961 626c 652e  of the variable.
+00011340: 0a20 2020 2020 2076 616c 7565 3a20 5468  .      value: Th
+00011350: 6520 7661 6c75 6520 6f66 2074 6865 2076  e value of the v
+00011360: 6172 6961 626c 652e 0a20 2020 2020 2072  ariable..      r
+00011370: 6564 7563 655f 666e 3a20 5468 6520 6675  educe_fn: The fu
+00011380: 6e63 7469 6f6e 2075 7365 6420 746f 2063  nction used to c
+00011390: 6f6d 6269 6e65 2074 6865 2065 7869 7374  ombine the exist
+000113a0: 696e 6720 7661 6c75 6520 7769 7468 0a20  ing value with. 
+000113b0: 2020 2020 2020 2074 6865 206e 6577 2076         the new v
+000113c0: 616c 7565 2e20 5468 6520 6465 6661 756c  alue. The defaul
+000113d0: 7420 6973 2074 6f20 6170 7065 6e64 2074  t is to append t
+000113e0: 6865 2076 616c 7565 2074 6f20 6120 7475  he value to a tu
+000113f0: 706c 652e 0a20 2020 2020 2069 6e69 745f  ple..      init_
+00011400: 666e 3a20 466f 7220 7468 6520 6669 7273  fn: For the firs
+00011410: 7420 7661 6c75 6520 7374 6f72 6564 2c20  t value stored, 
+00011420: 6072 6564 7563 655f 666e 6020 7769 6c6c  `reduce_fn` will
+00011430: 2062 6520 7061 7373 6564 0a20 2020 2020   be passed.     
+00011440: 2020 2074 6865 2072 6573 756c 7420 6f66     the result of
+00011450: 2060 696e 6974 5f66 6e60 2074 6f67 6574   `init_fn` toget
+00011460: 6865 7220 7769 7468 2074 6865 2076 616c  her with the val
+00011470: 7565 2074 6f20 6265 2073 746f 7265 642e  ue to be stored.
+00011480: 0a20 2020 2020 2020 2054 6865 2064 6566  .        The def
+00011490: 6175 6c74 2069 7320 616e 2065 6d70 7479  ault is an empty
+000114a0: 2074 7570 6c65 2e0a 0a20 2020 2052 6574   tuple...    Ret
+000114b0: 7572 6e73 3a0a 2020 2020 2020 6054 7275  urns:.      `Tru
+000114c0: 6560 2069 6620 7468 6520 7661 6c75 6520  e` if the value 
+000114d0: 6861 7320 6265 656e 2073 746f 7265 6420  has been stored 
+000114e0: 7375 6363 6573 7366 756c 6c79 2c20 6046  successfully, `F
+000114f0: 616c 7365 6020 6f74 6865 7277 6973 652e  alse` otherwise.
+00011500: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    """.    if 
+00011510: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
+00011520: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
+00011530: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
+00011540: 7420 7374 6f72 6520 7661 7269 6162 6c65  t store variable
+00011550: 7320 6f6e 2075 6e62 6f75 6e64 206d 6f64  s on unbound mod
+00011560: 756c 6573 2229 0a20 2020 2069 6620 6e6f  ules").    if no
+00011570: 7420 7365 6c66 2e73 636f 7065 2e69 735f  t self.scope.is_
+00011580: 6d75 7461 626c 655f 636f 6c6c 6563 7469  mutable_collecti
+00011590: 6f6e 2863 6f6c 293a 0a20 2020 2020 2072  on(col):.      r
+000115a0: 6574 7572 6e20 4661 6c73 650a 2020 2020  eturn False.    
+000115b0: 6966 2073 656c 662e 7363 6f70 652e 6861  if self.scope.ha
+000115c0: 735f 7661 7269 6162 6c65 2863 6f6c 2c20  s_variable(col, 
+000115d0: 6e61 6d65 293a 0a20 2020 2020 2078 7320  name):.      xs 
+000115e0: 3d20 7365 6c66 2e73 636f 7065 2e67 6574  = self.scope.get
+000115f0: 5f76 6172 6961 626c 6528 636f 6c2c 206e  _variable(col, n
+00011600: 616d 6529 0a20 2020 2065 6c73 653a 0a20  ame).    else:. 
+00011610: 2020 2020 2073 656c 662e 7363 6f70 652e       self.scope.
+00011620: 7265 7365 7276 6528 6e61 6d65 2c20 636f  reserve(name, co
+00011630: 6c29 0a20 2020 2020 2073 656c 662e 5f73  l).      self._s
+00011640: 7461 7465 2e63 6869 6c64 7265 6e5b 6e61  tate.children[na
+00011650: 6d65 5d20 3d20 636f 6c0a 2020 2020 2020  me] = col.      
+00011660: 7873 203d 2069 6e69 745f 666e 2829 0a20  xs = init_fn(). 
+00011670: 2020 2078 7320 3d20 7265 6475 6365 5f66     xs = reduce_f
+00011680: 6e28 7873 2c20 7661 6c75 6529 0a20 2020  n(xs, value).   
+00011690: 2073 656c 662e 7363 6f70 652e 7075 745f   self.scope.put_
+000116a0: 7661 7269 6162 6c65 2863 6f6c 2c20 6e61  variable(col, na
+000116b0: 6d65 2c20 7873 290a 2020 2020 7265 7475  me, xs).    retu
+000116c0: 726e 2054 7275 650a 0a20 2064 6566 2070  rn True..  def p
+000116d0: 6572 7475 7262 280a 2020 2020 2020 7365  erturb(.      se
+000116e0: 6c66 2c20 6e61 6d65 3a20 7374 722c 2076  lf, name: str, v
+000116f0: 616c 7565 3a20 542c 2063 6f6c 6c65 6374  alue: T, collect
+00011700: 696f 6e3a 2073 7472 203d 2027 7065 7274  ion: str = 'pert
+00011710: 7572 6261 7469 6f6e 7327 0a20 2029 202d  urbations'.  ) -
+00011720: 3e20 543a 0a20 2020 2022 2222 4164 6420  > T:.    """Add 
+00011730: 616e 207a 6572 6f2d 7661 6c75 6520 7661  an zero-value va
+00011740: 7269 6162 6c65 2028 2770 6572 7475 7262  riable ('perturb
+00011750: 6174 696f 6e27 2920 746f 2074 6865 2069  ation') to the i
+00011760: 6e74 6572 6d65 6469 6174 6520 7661 6c75  ntermediate valu
+00011770: 652e 0a0a 2020 2020 5468 6520 6772 6164  e...    The grad
+00011780: 6965 6e74 206f 6620 6076 616c 7565 6020  ient of `value` 
+00011790: 776f 756c 6420 6265 2074 6865 2073 616d  would be the sam
+000117a0: 6520 6173 2074 6865 2067 7261 6469 656e  e as the gradien
+000117b0: 7420 6f66 2074 6869 730a 2020 2020 7065  t of this.    pe
+000117c0: 7274 7572 6261 7469 6f6e 2076 6172 6961  rturbation varia
+000117d0: 626c 652e 2054 6865 7265 666f 7265 2c20  ble. Therefore, 
+000117e0: 6966 2079 6f75 2064 6566 696e 6520 796f  if you define yo
+000117f0: 7572 206c 6f73 7320 6675 6e63 7469 6f6e  ur loss function
+00011800: 2077 6974 680a 2020 2020 626f 7468 2070   with.    both p
+00011810: 6172 616d 7320 616e 6420 7065 7274 7572  arams and pertur
+00011820: 6261 7469 6f6e 7320 6173 2073 7461 6e64  bations as stand
+00011830: 616c 6f6e 6520 6172 6775 6d65 6e74 732c  alone arguments,
+00011840: 2079 6f75 2063 616e 2067 6574 2074 6865   you can get the
+00011850: 0a20 2020 2069 6e74 6572 6d65 6469 6174  .    intermediat
+00011860: 6520 6772 6164 6965 6e74 7320 6f66 2060  e gradients of `
+00011870: 7661 6c75 6560 2062 7920 7275 6e6e 696e  value` by runnin
+00011880: 6720 606a 6178 2e67 7261 6460 206f 6e20  g `jax.grad` on 
+00011890: 7468 6520 7065 7274 7572 6261 7469 6f6e  the perturbation
+000118a0: 0a20 2020 2061 7267 756d 656e 742e 0a0a  .    argument...
+000118b0: 2020 2020 4e6f 7465 3a20 7468 6973 2069      Note: this i
+000118c0: 7320 616e 2065 7870 6572 696d 656e 7461  s an experimenta
+000118d0: 6c20 4150 4920 616e 6420 6d61 7920 6265  l API and may be
+000118e0: 2074 7765 616b 6564 206c 6174 6572 2066   tweaked later f
+000118f0: 6f72 2062 6574 7465 720a 2020 2020 7065  or better.    pe
+00011900: 7266 6f72 6d61 6e63 6520 616e 6420 7573  rformance and us
+00011910: 6162 696c 6974 792e 0a20 2020 2041 7420  ability..    At 
+00011920: 6974 7320 6375 7272 656e 7420 7374 6167  its current stag
+00011930: 652c 2069 7420 6372 6561 7465 7320 6578  e, it creates ex
+00011940: 7472 6120 6475 6d6d 7920 7661 7269 6162  tra dummy variab
+00011950: 6c65 7320 7468 6174 206f 6363 7570 6965  les that occupie
+00011960: 7320 6578 7472 610a 2020 2020 6d65 6d6f  s extra.    memo
+00011970: 7279 2073 7061 6365 2e20 5573 6520 6974  ry space. Use it
+00011980: 206f 6e6c 7920 746f 2064 6562 7567 2067   only to debug g
+00011990: 7261 6469 656e 7473 2069 6e20 7472 6169  radients in trai
+000119a0: 6e69 6e67 2e0a 0a20 2020 2045 7861 6d70  ning...    Examp
+000119b0: 6c65 3a3a 0a0a 2020 2020 2020 696d 706f  le::..      impo
+000119c0: 7274 206a 6178 0a20 2020 2020 2069 6d70  rt jax.      imp
+000119d0: 6f72 7420 6a61 782e 6e75 6d70 7920 6173  ort jax.numpy as
+000119e0: 206a 6e70 0a20 2020 2020 2069 6d70 6f72   jnp.      impor
+000119f0: 7420 666c 6178 2e6c 696e 656e 2061 7320  t flax.linen as 
+00011a00: 6e6e 0a0a 2020 2020 2020 636c 6173 7320  nn..      class 
+00011a10: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
+00011a20: 2020 2020 2020 2020 2020 406e 6e2e 636f            @nn.co
+00011a30: 6d70 6163 740a 2020 2020 2020 2020 2020  mpact.          
+00011a40: 6465 6620 5f5f 6361 6c6c 5f5f 2873 656c  def __call__(sel
+00011a50: 662c 2078 293a 0a20 2020 2020 2020 2020  f, x):.         
+00011a60: 2020 2020 2078 203d 206e 6e2e 4465 6e73       x = nn.Dens
+00011a70: 6528 3329 2878 290a 2020 2020 2020 2020  e(3)(x).        
+00011a80: 2020 2020 2020 7820 3d20 7365 6c66 2e70        x = self.p
+00011a90: 6572 7475 7262 2827 6465 6e73 6533 272c  erturb('dense3',
+00011aa0: 2078 290a 2020 2020 2020 2020 2020 2020   x).            
+00011ab0: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
+00011ac0: 6528 3229 2878 290a 0a20 2020 2020 2064  e(2)(x)..      d
+00011ad0: 6566 206c 6f73 7328 7061 7261 6d73 2c20  ef loss(params, 
+00011ae0: 7065 7274 7572 6261 7469 6f6e 732c 2069  perturbations, i
+00011af0: 6e70 7574 732c 2074 6172 6765 7473 293a  nputs, targets):
+00011b00: 0a20 2020 2020 2020 2076 6172 6961 626c  .        variabl
+00011b10: 6573 203d 207b 2770 6172 616d 7327 3a20  es = {'params': 
+00011b20: 7061 7261 6d73 2c20 2770 6572 7475 7262  params, 'perturb
+00011b30: 6174 696f 6e73 273a 2070 6572 7475 7262  ations': perturb
+00011b40: 6174 696f 6e73 7d0a 2020 2020 2020 2020  ations}.        
+00011b50: 7072 6564 7320 3d20 6d6f 6465 6c2e 6170  preds = model.ap
+00011b60: 706c 7928 7661 7269 6162 6c65 732c 2069  ply(variables, i
+00011b70: 6e70 7574 7329 0a20 2020 2020 2020 2072  nputs).        r
+00011b80: 6574 7572 6e20 6a6e 702e 7371 7561 7265  eturn jnp.square
+00011b90: 2870 7265 6473 202d 2074 6172 6765 7473  (preds - targets
+00011ba0: 292e 6d65 616e 2829 0a0a 2020 2020 2020  ).mean()..      
+00011bb0: 7820 3d20 6a6e 702e 6f6e 6573 2828 322c  x = jnp.ones((2,
+00011bc0: 2039 2929 0a20 2020 2020 2079 203d 206a   9)).      y = j
+00011bd0: 6e70 2e6f 6e65 7328 2832 2c20 3229 290a  np.ones((2, 2)).
+00011be0: 2020 2020 2020 6d6f 6465 6c20 3d20 466f        model = Fo
+00011bf0: 6f28 290a 2020 2020 2020 7661 7269 6162  o().      variab
+00011c00: 6c65 7320 3d20 6d6f 6465 6c2e 696e 6974  les = model.init
+00011c10: 286a 6178 2e72 616e 646f 6d2e 5052 4e47  (jax.random.PRNG
+00011c20: 4b65 7928 3029 2c20 7829 0a20 2020 2020  Key(0), x).     
+00011c30: 2069 6e74 6d5f 6772 6164 7320 3d20 6a61   intm_grads = ja
+00011c40: 782e 6772 6164 286c 6f73 732c 2061 7267  x.grad(loss, arg
+00011c50: 6e75 6d73 3d31 2928 7661 7269 6162 6c65  nums=1)(variable
+00011c60: 735b 2770 6172 616d 7327 5d2c 2076 6172  s['params'], var
+00011c70: 6961 626c 6573 5b27 7065 7274 7572 6261  iables['perturba
+00011c80: 7469 6f6e 7327 5d2c 2078 2c20 7929 0a20  tions'], x, y). 
+00011c90: 2020 2020 2070 7269 6e74 2869 6e74 6d5f       print(intm_
+00011ca0: 6772 6164 735b 2764 656e 7365 3327 5d29  grads['dense3'])
+00011cb0: 2023 203d 3d3e 205b 5b2d 312e 3435 3639   # ==> [[-1.4569
+00011cc0: 3234 2020 202d 302e 3434 3333 3235 3337  24   -0.44332537
+00011cd0: 2020 302e 3032 3432 3238 3437 5d0a 2020    0.02422847].  
+00011ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011d00: 2320 2020 2020 205b 2d31 2e34 3536 3932  #      [-1.45692
+00011d10: 3420 2020 2d30 2e34 3433 3332 3533 3720  4   -0.44332537 
+00011d20: 2030 2e30 3234 3232 3834 375d 5d0a 0a20   0.02422847]].. 
+00011d30: 2020 2049 6620 7065 7274 7572 6261 7469     If perturbati
+00011d40: 6f6e 7320 6172 6520 6e6f 7420 7061 7373  ons are not pass
+00011d50: 6564 2074 6f20 6061 7070 6c79 602c 2060  ed to `apply`, `
+00011d60: 7065 7274 7572 6260 2062 6568 6176 6573  perturb` behaves
+00011d70: 206c 696b 6520 6120 6e6f 2d6f 700a 2020   like a no-op.  
+00011d80: 2020 736f 2079 6f75 2063 616e 2065 6173    so you can eas
+00011d90: 696c 7920 6469 7361 626c 6520 7468 6520  ily disable the 
+00011da0: 6265 6861 7669 6f72 2077 6865 6e20 6e6f  behavior when no
+00011db0: 7420 6e65 6564 6564 3a3a 0a0a 2020 2020  t needed::..    
+00011dc0: 2020 6d6f 6465 6c2e 6170 706c 7928 7b27    model.apply({'
+00011dd0: 7061 7261 6d73 273a 2070 6172 616d 732c  params': params,
+00011de0: 2027 7065 7274 7572 6261 7469 6f6e 7327   'perturbations'
+00011df0: 3a20 7065 7274 7572 6261 7469 6f6e 737d  : perturbations}
+00011e00: 2c20 7829 2023 2077 6f72 6b73 2061 7320  , x) # works as 
+00011e10: 6578 7065 6374 6564 0a20 2020 2020 206d  expected.      m
+00011e20: 6f64 656c 2e61 7070 6c79 287b 2770 6172  odel.apply({'par
+00011e30: 616d 7327 3a20 7061 7261 6d73 7d2c 2078  ams': params}, x
+00011e40: 2920 2320 6265 6861 7665 7320 6c69 6b65  ) # behaves like
+00011e50: 2061 206e 6f2d 6f70 0a0a 2020 2020 2222   a no-op..    ""
+00011e60: 220a 0a20 2020 2064 6566 205f 726f 6f74  "..    def _root
+00011e70: 5f68 6173 5f63 6f6c 6c65 6374 696f 6e28  _has_collection(
+00011e80: 293a 0a20 2020 2020 2022 2222 5265 7475  ):.      """Retu
+00011e90: 726e 7320 5472 7565 2069 6620 7468 6520  rns True if the 
+00011ea0: 726f 6f74 2073 636f 7065 2068 6173 2074  root scope has t
+00011eb0: 6865 2063 6f6c 6c65 6374 696f 6e2e 2222  he collection.""
+00011ec0: 220a 2020 2020 2020 6173 7365 7274 2073  ".      assert s
+00011ed0: 656c 662e 7363 6f70 6520 6973 206e 6f74  elf.scope is not
+00011ee0: 204e 6f6e 650a 2020 2020 2020 7265 7475   None.      retu
+00011ef0: 726e 2063 6f6c 6c65 6374 696f 6e20 696e  rn collection in
+00011f00: 2073 656c 662e 7363 6f70 652e 726f 6f74   self.scope.root
+00011f10: 2e5f 7661 7269 6162 6c65 730a 0a20 2020  ._variables..   
+00011f20: 2023 2077 6520 7769 6c6c 206f 6e6c 7920   # we will only 
+00011f30: 6164 6420 7468 6520 7065 7274 7572 6261  add the perturba
+00011f40: 7469 6f6e 2076 6172 6961 626c 6520 6966  tion variable if
+00011f50: 2074 6865 2063 6f6c 6c65 6374 696f 6e20   the collection 
+00011f60: 6973 206d 7574 6162 6c65 0a20 2020 2023  is mutable.    #
+00011f70: 2028 652e 672e 2064 7572 696e 6720 6069   (e.g. during `i
+00011f80: 6e69 7460 2920 6f72 2069 6620 7468 6520  nit`) or if the 
+00011f90: 636f 6c6c 6563 7469 6f6e 2077 6173 2070  collection was p
+00011fa0: 6173 7365 6420 746f 2060 6170 706c 7960  assed to `apply`
+00011fb0: 2028 636f 6e74 6169 6e65 6420 696e 0a20   (contained in. 
+00011fc0: 2020 2023 2074 6865 2072 6f6f 7420 7363     # the root sc
+00011fd0: 6f70 6529 2e0a 2020 2020 6966 2073 656c  ope)..    if sel
+00011fe0: 662e 6973 5f6d 7574 6162 6c65 5f63 6f6c  f.is_mutable_col
+00011ff0: 6c65 6374 696f 6e28 636f 6c6c 6563 7469  lection(collecti
+00012000: 6f6e 2920 6f72 205f 726f 6f74 5f68 6173  on) or _root_has
+00012010: 5f63 6f6c 6c65 6374 696f 6e28 293a 0a20  _collection():. 
+00012020: 2020 2020 2076 616c 7565 202b 3d20 7365       value += se
+00012030: 6c66 2e76 6172 6961 626c 6528 636f 6c6c  lf.variable(coll
+00012040: 6563 7469 6f6e 2c20 6e61 6d65 2c20 6c61  ection, name, la
+00012050: 6d62 6461 3a20 6a6e 702e 7a65 726f 735f  mbda: jnp.zeros_
+00012060: 6c69 6b65 2876 616c 7565 2929 2e76 616c  like(value)).val
+00012070: 7565 2020 2320 7479 7065 3a20 6967 6e6f  ue  # type: igno
+00012080: 7265 0a20 2020 2072 6574 7572 6e20 7661  re.    return va
+00012090: 6c75 650a 0a20 2064 6566 2074 6162 756c  lue..  def tabul
+000120a0: 6174 6528 0a20 2020 2020 2073 656c 662c  ate(.      self,
+000120b0: 0a20 2020 2020 2072 6e67 733a 2055 6e69  .      rngs: Uni
+000120c0: 6f6e 5b4b 6579 4172 7261 792c 2052 4e47  on[KeyArray, RNG
+000120d0: 5365 7175 656e 6365 735d 2c0a 2020 2020  Sequences],.    
+000120e0: 2020 2a61 7267 732c 0a20 2020 2020 2064    *args,.      d
+000120f0: 6570 7468 3a20 4f70 7469 6f6e 616c 5b69  epth: Optional[i
+00012100: 6e74 5d20 3d20 4e6f 6e65 2c0a 2020 2020  nt] = None,.    
+00012110: 2020 7368 6f77 5f72 6570 6561 7465 643a    show_repeated:
+00012120: 2062 6f6f 6c20 3d20 4661 6c73 652c 0a20   bool = False,. 
+00012130: 2020 2020 206d 7574 6162 6c65 3a20 436f       mutable: Co
+00012140: 6c6c 6563 7469 6f6e 4669 6c74 6572 203d  llectionFilter =
+00012150: 2054 7275 652c 0a20 2020 2020 2063 6f6e   True,.      con
+00012160: 736f 6c65 5f6b 7761 7267 733a 204f 7074  sole_kwargs: Opt
+00012170: 696f 6e61 6c5b 4d61 7070 696e 675b 7374  ional[Mapping[st
+00012180: 722c 2041 6e79 5d5d 203d 204e 6f6e 652c  r, Any]] = None,
+00012190: 0a20 2020 2020 2074 6162 6c65 5f6b 7761  .      table_kwa
+000121a0: 7267 733a 204d 6170 7069 6e67 5b73 7472  rgs: Mapping[str
+000121b0: 2c20 416e 795d 203d 204d 6170 7069 6e67  , Any] = Mapping
+000121c0: 5072 6f78 7954 7970 6528 7b7d 292c 0a20  ProxyType({}),. 
+000121d0: 2020 2020 2063 6f6c 756d 6e5f 6b77 6172       column_kwar
+000121e0: 6773 3a20 4d61 7070 696e 675b 7374 722c  gs: Mapping[str,
+000121f0: 2041 6e79 5d20 3d20 4d61 7070 696e 6750   Any] = MappingP
+00012200: 726f 7879 5479 7065 287b 7d29 2c0a 2020  roxyType({}),.  
+00012210: 2020 2020 2a2a 6b77 6172 6773 2c0a 2020      **kwargs,.  
+00012220: 2920 2d3e 2073 7472 3a0a 2020 2020 2222  ) -> str:.    ""
+00012230: 2243 7265 6174 6573 2061 2073 756d 6d61  "Creates a summa
+00012240: 7279 206f 6620 7468 6520 4d6f 6475 6c65  ry of the Module
+00012250: 2072 6570 7265 7365 6e74 6564 2061 7320   represented as 
+00012260: 6120 7461 626c 652e 0a0a 2020 2020 5468  a table...    Th
+00012270: 6973 206d 6574 686f 6420 6861 7320 7468  is method has th
+00012280: 6520 7361 6d65 2073 6967 6e61 7475 7265  e same signature
+00012290: 2061 6e64 2069 6e74 6572 6e61 6c6c 7920   and internally 
+000122a0: 6361 6c6c 7320 604d 6f64 756c 652e 696e  calls `Module.in
+000122b0: 6974 602c 0a20 2020 2062 7574 2069 6e73  it`,.    but ins
+000122c0: 7465 6164 206f 6620 7265 7475 726e 696e  tead of returnin
+000122d0: 6720 7468 6520 7661 7269 6162 6c65 732c  g the variables,
+000122e0: 2069 7420 7265 7475 726e 7320 7468 6520   it returns the 
+000122f0: 7374 7269 6e67 2073 756d 6d61 7269 7a69  string summarizi
+00012300: 6e67 0a20 2020 2074 6865 204d 6f64 756c  ng.    the Modul
+00012310: 6520 696e 2061 2074 6162 6c65 2e20 6074  e in a table. `t
+00012320: 6162 756c 6174 6560 2075 7365 7320 606a  abulate` uses `j
+00012330: 6178 2e65 7661 6c5f 7368 6170 6560 2074  ax.eval_shape` t
+00012340: 6f20 7275 6e20 7468 6520 666f 7277 6172  o run the forwar
+00012350: 640a 2020 2020 636f 6d70 7574 6174 696f  d.    computatio
+00012360: 6e20 7769 7468 6f75 7420 636f 6e73 756d  n without consum
+00012370: 696e 6720 616e 7920 464c 4f50 7320 6f72  ing any FLOPs or
+00012380: 2061 6c6c 6f63 6174 696e 6720 6d65 6d6f   allocating memo
+00012390: 7279 2e0a 0a20 2020 2041 6464 6974 696f  ry...    Additio
+000123a0: 6e61 6c20 6172 6775 6d65 6e74 7320 6361  nal arguments ca
+000123b0: 6e20 6265 2070 6173 7365 6420 696e 746f  n be passed into
+000123c0: 2074 6865 2060 636f 6e73 6f6c 655f 6b77   the `console_kw
+000123d0: 6172 6773 6020 6172 6775 6d65 6e74 2c20  args` argument, 
+000123e0: 666f 7220 6578 616d 706c 652c 0a20 2020  for example,.   
+000123f0: 2060 7b27 7769 6474 6827 3a20 3132 307d   `{'width': 120}
+00012400: 602e 2046 6f72 2061 2066 756c 6c20 6c69  `. For a full li
+00012410: 7374 206f 6620 6063 6f6e 736f 6c65 5f6b  st of `console_k
+00012420: 7761 7267 7360 2061 7267 756d 656e 7473  wargs` arguments
+00012430: 2c20 7365 653a 0a20 2020 2068 7474 7073  , see:.    https
+00012440: 3a2f 2f72 6963 682e 7265 6164 7468 6564  ://rich.readthed
+00012450: 6f63 732e 696f 2f65 6e2f 7374 6162 6c65  ocs.io/en/stable
+00012460: 2f72 6566 6572 656e 6365 2f63 6f6e 736f  /reference/conso
+00012470: 6c65 2e68 746d 6c23 7269 6368 2e63 6f6e  le.html#rich.con
+00012480: 736f 6c65 2e43 6f6e 736f 6c65 0a0a 2020  sole.Console..  
+00012490: 2020 4578 616d 706c 653a 3a0a 0a20 2020    Example::..   
+000124a0: 2020 2069 6d70 6f72 7420 6a61 780a 2020     import jax.  
+000124b0: 2020 2020 696d 706f 7274 206a 6178 2e6e      import jax.n
+000124c0: 756d 7079 2061 7320 6a6e 700a 2020 2020  umpy as jnp.    
+000124d0: 2020 696d 706f 7274 2066 6c61 782e 6c69    import flax.li
+000124e0: 6e65 6e20 6173 206e 6e0a 0a20 2020 2020  nen as nn..     
+000124f0: 2063 6c61 7373 2046 6f6f 286e 6e2e 4d6f   class Foo(nn.Mo
+00012500: 6475 6c65 293a 0a20 2020 2020 2020 2020  dule):.         
+00012510: 2040 6e6e 2e63 6f6d 7061 6374 0a20 2020   @nn.compact.   
+00012520: 2020 2020 2020 2064 6566 205f 5f63 616c         def __cal
+00012530: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
+00012540: 2020 2020 2020 2020 2020 2020 6820 3d20              h = 
+00012550: 6e6e 2e44 656e 7365 2834 2928 7829 0a20  nn.Dense(4)(x). 
+00012560: 2020 2020 2020 2020 2020 2020 2072 6574               ret
+00012570: 7572 6e20 6e6e 2e44 656e 7365 2832 2928  urn nn.Dense(2)(
+00012580: 6829 0a0a 2020 2020 2020 7820 3d20 6a6e  h)..      x = jn
+00012590: 702e 6f6e 6573 2828 3136 2c20 3929 290a  p.ones((16, 9)).
+000125a0: 0a20 2020 2020 2070 7269 6e74 2846 6f6f  .      print(Foo
+000125b0: 2829 2e74 6162 756c 6174 6528 6a61 782e  ().tabulate(jax.
+000125c0: 7261 6e64 6f6d 2e50 524e 474b 6579 2830  random.PRNGKey(0
+000125d0: 292c 2078 2929 0a0a 0a20 2020 2054 6869  ), x))...    Thi
+000125e0: 7320 6769 7665 7320 7468 6520 666f 6c6c  s gives the foll
+000125f0: 6f77 696e 6720 6f75 7470 7574 3a3a 0a0a  owing output::..
+00012600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012620: 2020 2020 2020 466f 6f20 5375 6d6d 6172        Foo Summar
+00012630: 790a 2020 2020 2020 e294 8fe2 9481 e294  y.      ........
+00012640: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00012650: e294 81e2 9481 e294 b3e2 9481 e294 81e2  ................
+00012660: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012670: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
+00012680: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00012690: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000126a0: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
+000126b0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000126c0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000126d0: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
+000126e0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000126f0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012700: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00012710: e294 81e2 9481 e294 930a 2020 2020 2020  ..........      
+00012720: e294 8320 7061 7468 2020 2020 e294 8320  ... path    ... 
+00012730: 6d6f 6475 6c65 20e2 9483 2069 6e70 7574  module ... input
+00012740: 7320 2020 2020 2020 20e2 9483 206f 7574  s        ... out
+00012750: 7075 7473 2020 2020 2020 20e2 9483 2070  puts       ... p
+00012760: 6172 616d 7320 2020 2020 2020 2020 2020  arams           
+00012770: 2020 2020 e294 830a 2020 2020 2020 e294      ....      ..
+00012780: a1e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00012790: e294 81e2 9481 e294 81e2 9481 e295 87e2  ................
+000127a0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000127b0: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
+000127c0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000127d0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000127e0: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
+000127f0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00012800: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012810: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
+00012820: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00012830: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00012840: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00012850: e294 81e2 9481 e294 81e2 9481 e294 a90a  ................
+00012860: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00012870: 2020 e294 8220 466f 6f20 2020 20e2 9482    ... Foo    ...
+00012880: 2066 6c6f 6174 3332 5b31 362c 395d 20e2   float32[16,9] .
+00012890: 9482 2066 6c6f 6174 3332 5b31 362c 325d  .. float32[16,2]
+000128a0: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+000128b0: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
+000128c0: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
+000128d0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000128e0: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
+000128f0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012900: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012910: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012920: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012930: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012940: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012950: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012960: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012970: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012980: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012990: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000129a0: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
+000129b0: 4465 6e73 655f 3020 e294 8220 4465 6e73  Dense_0 ... Dens
+000129c0: 6520 20e2 9482 2066 6c6f 6174 3332 5b31  e  ... float32[1
+000129d0: 362c 395d 20e2 9482 2066 6c6f 6174 3332  6,9] ... float32
+000129e0: 5b31 362c 345d 20e2 9482 2062 6961 733a  [16,4] ... bias:
+000129f0: 2066 6c6f 6174 3332 5b34 5d20 2020 2020   float32[4]     
+00012a00: e294 820a 2020 2020 2020 e294 8220 2020  ....      ...   
+00012a10: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
 00012a20: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
 00012a30: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-00012a40: 2020 2020 2020 2020 2020 2020 e294 820a              ....
-00012a50: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00012a60: 2020 e294 8220 2020 2020 2020 20e2 9482    ...        ...
-00012a70: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
+00012a40: 2020 2020 20e2 9482 206b 6572 6e65 6c3a       ... kernel:
+00012a50: 2066 6c6f 6174 3332 5b39 2c34 5d20 e294   float32[9,4] ..
+00012a60: 820a 2020 2020 2020 e294 8220 2020 2020  ..      ...     
+00012a70: 2020 2020 e294 8220 2020 2020 2020 20e2      ...        .
 00012a80: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
-00012a90: 20e2 9482 2031 3020 2834 3020 4229 2020   ... 10 (40 B)  
-00012aa0: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
-00012ab0: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
-00012ac0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012ad0: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
-00012ae0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012af0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012b00: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012b10: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012b20: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012b30: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012b40: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00012b50: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012b60: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012b70: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012b80: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012b90: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
-00012ba0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-00012bb0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-00012bc0: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00012bd0: 2054 6f74 616c 20e2 9482 2035 3020 2832   Total ... 50 (2
-00012be0: 3030 2042 2920 2020 2020 2020 2020 2020  00 B)           
-00012bf0: e294 820a 2020 2020 2020 e294 94e2 9480  ....      ......
-00012c00: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012c10: 9480 e294 80e2 9480 e294 b4e2 9480 e294  ................
-00012c20: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012c30: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
-00012c40: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012c50: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012c60: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
-00012c70: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012c80: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012c90: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
-00012ca0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00012cb0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00012cc0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00012cd0: 9480 e294 80e2 9480 e294 980a 0a20 2020  .............   
-00012ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012cf0: 2020 2020 2020 2020 2054 6f74 616c 2050           Total P
-00012d00: 6172 616d 6574 6572 733a 2035 3020 2832  arameters: 50 (2
-00012d10: 3030 2042 290a 0a20 2020 202a 2a4e 6f74  00 B)..    **Not
-00012d20: 652a 2a3a 2072 6f77 7320 6f72 6465 7220  e**: rows order 
-00012d30: 696e 2074 6865 2074 6162 6c65 2064 6f65  in the table doe
-00012d40: 7320 6e6f 7420 7265 7072 6573 656e 7420  s not represent 
-00012d50: 6578 6563 7574 696f 6e20 6f72 6465 722c  execution order,
-00012d60: 0a20 2020 2069 6e73 7465 6164 2069 7420  .    instead it 
-00012d70: 616c 6967 6e73 2077 6974 6820 7468 6520  aligns with the 
-00012d80: 6f72 6465 7220 6f66 206b 6579 7320 696e  order of keys in
-00012d90: 2060 7661 7269 6162 6c65 7360 2077 6869   `variables` whi
-00012da0: 6368 2061 7265 2073 6f72 7465 640a 2020  ch are sorted.  
-00012db0: 2020 616c 7068 6162 6574 6963 616c 6c79    alphabetically
-00012dc0: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   
-00012dd0: 2020 2072 6e67 733a 2054 6865 2072 6e67     rngs: The rng
-00012de0: 7320 666f 7220 7468 6520 7661 7269 6162  s for the variab
-00012df0: 6c65 2063 6f6c 6c65 6374 696f 6e73 2061  le collections a
-00012e00: 7320 7061 7373 6564 2074 6f20 604d 6f64  s passed to `Mod
-00012e10: 756c 652e 696e 6974 602e 0a20 2020 2020  ule.init`..     
-00012e20: 202a 6172 6773 3a20 5468 6520 6172 6775   *args: The argu
-00012e30: 6d65 6e74 7320 746f 2074 6865 2066 6f72  ments to the for
-00012e40: 7761 7264 2063 6f6d 7075 7461 7469 6f6e  ward computation
-00012e50: 2e0a 2020 2020 2020 6465 7074 683a 2063  ..      depth: c
-00012e60: 6f6e 7472 6f6c 7320 686f 7720 6d61 6e79  ontrols how many
-00012e70: 2073 7562 6d6f 6475 6c65 2064 6565 7020   submodule deep 
-00012e80: 7468 6520 7375 6d6d 6172 7920 6361 6e20  the summary can 
-00012e90: 676f 2e20 4279 2064 6566 6175 6c74 2069  go. By default i
-00012ea0: 7473 0a20 2020 2020 2020 2060 4e6f 6e65  ts.        `None
-00012eb0: 6020 7768 6963 6820 6d65 616e 7320 6e6f  ` which means no
-00012ec0: 206c 696d 6974 2e20 4966 2061 2073 7562   limit. If a sub
-00012ed0: 6d6f 6475 6c65 2069 7320 6e6f 7420 7368  module is not sh
-00012ee0: 6f77 6e20 6265 6361 7573 6520 6f66 2074  own because of t
-00012ef0: 6865 0a20 2020 2020 2020 2064 6570 7468  he.        depth
-00012f00: 206c 696d 6974 2c20 6974 7320 7061 7261   limit, its para
-00012f10: 6d65 7465 7220 636f 756e 7420 616e 6420  meter count and 
-00012f20: 6279 7465 7320 7769 6c6c 2062 6520 6164  bytes will be ad
-00012f30: 6465 6420 746f 2074 6865 2072 6f77 206f  ded to the row o
-00012f40: 6620 6974 730a 2020 2020 2020 2020 6669  f its.        fi
-00012f50: 7273 7420 7368 6f77 6e20 616e 6365 7374  rst shown ancest
-00012f60: 6f72 2073 7563 6820 7468 6174 2074 6865  or such that the
-00012f70: 2073 756d 206f 6620 616c 6c20 726f 7773   sum of all rows
-00012f80: 2061 6c77 6179 7320 6164 6473 2075 7020   always adds up 
-00012f90: 746f 2074 6865 0a20 2020 2020 2020 2074  to the.        t
-00012fa0: 6f74 616c 206e 756d 6265 7220 6f66 2070  otal number of p
-00012fb0: 6172 616d 6574 6572 7320 6f66 2074 6865  arameters of the
-00012fc0: 204d 6f64 756c 652e 0a20 2020 2020 2073   Module..      s
-00012fd0: 686f 775f 7265 7065 6174 6564 3a20 4966  how_repeated: If
-00012fe0: 2060 5472 7565 602c 2072 6570 6561 7465   `True`, repeate
-00012ff0: 6420 6361 6c6c 7320 746f 2074 6865 2073  d calls to the s
-00013000: 616d 6520 6d6f 6475 6c65 2077 696c 6c20  ame module will 
-00013010: 6265 2073 686f 776e 0a20 2020 2020 2020  be shown.       
-00013020: 2069 6e20 7468 6520 7461 626c 652c 206f   in the table, o
-00013030: 7468 6572 7769 7365 206f 6e6c 7920 7468  therwise only th
-00013040: 6520 6669 7273 7420 6361 6c6c 2077 696c  e first call wil
-00013050: 6c20 6265 2073 686f 776e 2e20 4465 6661  l be shown. Defa
-00013060: 756c 7420 6973 0a20 2020 2020 2020 2060  ult is.        `
-00013070: 4661 6c73 6560 2e0a 2020 2020 2020 6d75  False`..      mu
-00013080: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
-00013090: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
-000130a0: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
-000130b0: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
-000130c0: 6f75 6c64 2062 650a 2020 2020 2020 2020  ould be.        
-000130d0: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
-000130e0: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
-000130f0: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
-00013100: 2061 7265 206d 7574 6162 6c65 2e20 6060   are mutable. ``
-00013110: 7374 7260 603a 2054 6865 0a20 2020 2020  str``: The.     
-00013120: 2020 206e 616d 6520 6f66 2061 2073 696e     name of a sin
-00013130: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
-00013140: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
-00013150: 3a20 4120 6c69 7374 206f 6620 6e61 6d65  : A list of name
-00013160: 7320 6f66 206d 7574 6162 6c65 0a20 2020  s of mutable.   
-00013170: 2020 2020 2063 6f6c 6c65 6374 696f 6e73       collections
-00013180: 2e20 4279 2064 6566 6175 6c74 2061 6c6c  . By default all
-00013190: 2063 6f6c 6c65 6374 696f 6e73 2065 7863   collections exc
-000131a0: 6570 7420 2769 6e74 6572 6d65 6469 6174  ept 'intermediat
-000131b0: 6573 2720 6172 650a 2020 2020 2020 2020  es' are.        
-000131c0: 6d75 7461 626c 652e 0a20 2020 2020 2063  mutable..      c
-000131d0: 6f6e 736f 6c65 5f6b 7761 7267 733a 2041  onsole_kwargs: A
-000131e0: 6e20 6f70 7469 6f6e 616c 2064 6963 7469  n optional dicti
-000131f0: 6f6e 6172 7920 7769 7468 2061 6464 6974  onary with addit
-00013200: 696f 6e61 6c20 6b65 7977 6f72 6420 6172  ional keyword ar
-00013210: 6775 6d65 6e74 7320 7468 6174 0a20 2020  guments that.   
-00013220: 2020 2020 2061 7265 2070 6173 7365 6420       are passed 
-00013230: 746f 2060 7269 6368 2e63 6f6e 736f 6c65  to `rich.console
-00013240: 2e43 6f6e 736f 6c65 6020 7768 656e 2072  .Console` when r
-00013250: 656e 6465 7269 6e67 2074 6865 2074 6162  endering the tab
-00013260: 6c65 2e20 4465 6661 756c 7420 6172 6775  le. Default argu
-00013270: 6d65 6e74 730a 2020 2020 2020 2020 6172  ments.        ar
-00013280: 6520 607b 2766 6f72 6365 5f74 6572 6d69  e `{'force_termi
-00013290: 6e61 6c27 3a20 5472 7565 2c20 2766 6f72  nal': True, 'for
-000132a0: 6365 5f6a 7570 7974 6572 273a 2046 616c  ce_jupyter': Fal
-000132b0: 7365 7d60 2e0a 2020 2020 2020 2a2a 6b77  se}`..      **kw
-000132c0: 6172 6773 3a20 6b65 7977 6f72 6420 6172  args: keyword ar
-000132d0: 6775 6d65 6e74 7320 746f 2070 6173 7320  guments to pass 
-000132e0: 746f 2074 6865 2066 6f72 7761 7264 2063  to the forward c
-000132f0: 6f6d 7075 7461 7469 6f6e 2e0a 0a20 2020  omputation...   
-00013300: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-00013310: 4120 7374 7269 6e67 2073 756d 6d61 7269  A string summari
-00013320: 7a69 6e67 2074 6865 204d 6f64 756c 652e  zing the Module.
-00013330: 0a20 2020 2022 2222 0a20 2020 2066 726f  .    """.    fro
-00013340: 6d20 666c 6178 2e6c 696e 656e 2069 6d70  m flax.linen imp
-00013350: 6f72 7420 7375 6d6d 6172 790a 0a20 2020  ort summary..   
-00013360: 2074 6162 756c 6174 655f 666e 203d 2073   tabulate_fn = s
-00013370: 756d 6d61 7279 2e74 6162 756c 6174 6528  ummary.tabulate(
-00013380: 7365 6c66 2c20 726e 6773 2c20 6465 7074  self, rngs, dept
-00013390: 683d 6465 7074 682c 0a20 2020 2020 2020  h=depth,.       
-000133a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000133b0: 2020 2020 2020 2020 2020 2020 7368 6f77              show
-000133c0: 5f72 6570 6561 7465 643d 7368 6f77 5f72  _repeated=show_r
-000133d0: 6570 6561 7465 642c 206d 7574 6162 6c65  epeated, mutable
-000133e0: 3d6d 7574 6162 6c65 2c0a 2020 2020 2020  =mutable,.      
-000133f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013400: 2020 2020 2020 2020 2020 2020 2063 6f6e               con
-00013410: 736f 6c65 5f6b 7761 7267 733d 636f 6e73  sole_kwargs=cons
-00013420: 6f6c 655f 6b77 6172 6773 290a 2020 2020  ole_kwargs).    
-00013430: 7265 7475 726e 2074 6162 756c 6174 655f  return tabulate_
-00013440: 666e 282a 6172 6773 2c20 2a2a 6b77 6172  fn(*args, **kwar
-00013450: 6773 290a 0a0a 5f50 6172 656e 7454 7970  gs)..._ParentTyp
-00013460: 6520 3d20 556e 696f 6e5b 5479 7065 5b4d  e = Union[Type[M
-00013470: 6f64 756c 655d 2c20 5479 7065 5b53 636f  odule], Type[Sco
-00013480: 7065 5d2c 2054 7970 655b 5f53 656e 7469  pe], Type[_Senti
-00013490: 6e65 6c5d 2c20 4e6f 6e65 5d0a 0a64 6566  nel], None]..def
-000134a0: 206d 6572 6765 5f70 6172 616d 286e 616d   merge_param(nam
-000134b0: 653a 2073 7472 2c20 613a 204f 7074 696f  e: str, a: Optio
-000134c0: 6e61 6c5b 545d 2c20 623a 204f 7074 696f  nal[T], b: Optio
-000134d0: 6e61 6c5b 545d 2920 2d3e 2054 3a0a 2020  nal[T]) -> T:.  
-000134e0: 2222 224d 6572 6765 7320 636f 6e73 7472  """Merges constr
-000134f0: 7563 7469 6f6e 2d20 616e 6420 6361 6c6c  uction- and call
-00013500: 2d74 696d 6520 6172 6775 6d65 6e74 2e0a  -time argument..
-00013510: 0a20 2054 6869 7320 6973 2061 2075 7469  .  This is a uti
-00013520: 6c69 7479 2066 6f72 2073 7570 706f 7274  lity for support
-00013530: 696e 6720 6120 7061 7474 6572 6e20 7768  ing a pattern wh
-00013540: 6572 6520 6120 4d6f 6475 6c65 2068 7970  ere a Module hyp
-00013550: 6572 7061 7261 6d65 7465 720a 2020 6361  erparameter.  ca
-00013560: 6e20 6265 2070 6173 7365 6420 6569 7468  n be passed eith
-00013570: 6572 2074 6f20 6060 5f5f 696e 6974 5f5f  er to ``__init__
-00013580: 6060 206f 7220 6060 5f5f 6361 6c6c 5f5f  `` or ``__call__
-00013590: 6060 2c20 616e 6420 7468 6520 7661 6c75  ``, and the valu
-000135a0: 6520 7468 6174 2069 730a 2020 6e6f 7420  e that is.  not 
-000135b0: 604e 6f6e 6560 2077 696c 6c20 6265 2075  `None` will be u
-000135c0: 7365 642e 0a0a 2020 4578 616d 706c 653a  sed...  Example:
-000135d0: 3a0a 0a20 2020 2063 6c61 7373 2046 6f6f  :..    class Foo
-000135e0: 286e 6e2e 4d6f 6475 6c65 293a 0a20 2020  (nn.Module):.   
-000135f0: 2020 2074 7261 696e 3a20 4f70 7469 6f6e     train: Option
-00013600: 616c 5b62 6f6f 6c5d 203d 204e 6f6e 650a  al[bool] = None.
-00013610: 0a20 2020 2020 2064 6566 205f 5f63 616c  .      def __cal
-00013620: 6c5f 5f28 7365 6c66 2c20 7472 6169 6e3a  l__(self, train:
-00013630: 204f 7074 696f 6e61 6c5b 626f 6f6c 5d20   Optional[bool] 
-00013640: 3d20 4e6f 6e65 293a 0a20 2020 2020 2020  = None):.       
-00013650: 2074 7261 696e 203d 206e 6e2e 6d65 7267   train = nn.merg
-00013660: 655f 7061 7261 6d28 2774 7261 696e 272c  e_param('train',
-00013670: 2073 656c 662e 7472 6169 6e2c 2074 7261   self.train, tra
-00013680: 696e 290a 0a20 2041 6e20 6572 726f 7220  in)..  An error 
-00013690: 6973 2074 6872 6f77 6e20 7768 656e 2062  is thrown when b
-000136a0: 6f74 6820 6172 6775 6d65 6e74 7320 6172  oth arguments ar
-000136b0: 6520 604e 6f6e 6560 206f 7220 626f 7468  e `None` or both
-000136c0: 2076 616c 7565 7320 6172 6520 6e6f 740a   values are not.
-000136d0: 2020 604e 6f6e 6560 2e0a 0a20 2041 7267    `None`...  Arg
-000136e0: 733a 0a20 2020 206e 616d 653a 2074 6865  s:.    name: the
-000136f0: 206e 616d 6520 6f66 2074 6865 2070 6172   name of the par
-00013700: 616d 6574 6572 2e20 5573 6564 2066 6f72  ameter. Used for
-00013710: 2065 7272 6f72 206d 6573 7361 6765 732e   error messages.
-00013720: 0a20 2020 2061 3a20 6f70 7469 6f6e 2061  .    a: option a
-00013730: 0a20 2020 2062 3a20 6f70 7469 6f6e 2062  .    b: option b
-00013740: 0a20 2052 6574 7572 6e73 3a0a 2020 2020  .  Returns:.    
-00013750: 6120 6f72 2062 2077 6869 6368 6576 6572  a or b whichever
-00013760: 2069 7320 6e6f 7420 604e 6f6e 6560 2e0a   is not `None`..
-00013770: 0a20 2022 2222 0a20 2069 6620 6120 6973  .  """.  if a is
-00013780: 204e 6f6e 6520 616e 6420 6220 6973 204e   None and b is N
-00013790: 6f6e 653a 0a20 2020 2072 6169 7365 2056  one:.    raise V
-000137a0: 616c 7565 4572 726f 7228 6627 5061 7261  alueError(f'Para
-000137b0: 6d65 7465 7220 227b 6e61 6d65 7d22 206d  meter "{name}" m
-000137c0: 7573 7420 6265 2070 6173 7365 6420 746f  ust be passed to
-000137d0: 2074 6865 2063 6f6e 7374 7275 6374 6f72   the constructor
-000137e0: 206f 7220 6174 2063 616c 6c20 7469 6d65   or at call time
-000137f0: 2e27 290a 2020 6966 2061 2069 7320 6e6f  .').  if a is no
-00013800: 7420 4e6f 6e65 2061 6e64 2062 2069 7320  t None and b is 
-00013810: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 7261  not None:.    ra
-00013820: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
-00013830: 2750 6172 616d 6574 6572 2022 7b6e 616d  'Parameter "{nam
-00013840: 657d 2220 7761 7320 7061 7373 6564 2074  e}" was passed t
-00013850: 6f20 7468 6520 636f 6e73 7472 7563 746f  o the constructo
-00013860: 7220 616e 6420 6174 2063 616c 6c20 7469  r and at call ti
-00013870: 6d65 2e27 0a20 2020 2020 2020 2020 2020  me.'.           
-00013880: 2020 2020 2020 2020 2020 2720 5368 6f75            ' Shou
-00013890: 6c64 2062 6520 7061 7373 6564 206a 7573  ld be passed jus
-000138a0: 7420 6f6e 6365 2e27 290a 2020 6966 2061  t once.').  if a
-000138b0: 2069 7320 4e6f 6e65 3a0a 2020 2020 6173   is None:.    as
-000138c0: 7365 7274 2062 2069 7320 6e6f 7420 4e6f  sert b is not No
-000138d0: 6e65 0a20 2020 2072 6574 7572 6e20 620a  ne.    return b.
-000138e0: 2020 7265 7475 726e 2061 0a0a 0a40 7472    return a...@tr
-000138f0: 6163 6562 6163 6b5f 7574 696c 2e61 7069  aceback_util.api
-00013900: 5f62 6f75 6e64 6172 790a 6465 6620 6170  _boundary.def ap
-00013910: 706c 7928 666e 3a20 4361 6c6c 6162 6c65  ply(fn: Callable
-00013920: 5b2e 2e2e 2c20 416e 795d 2c20 6d6f 6475  [..., Any], modu
-00013930: 6c65 3a20 4d6f 6475 6c65 2c0a 2020 2020  le: Module,.    
-00013940: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
-00013950: 6f6c 6c65 6374 696f 6e46 696c 7465 7220  ollectionFilter 
-00013960: 3d20 4661 6c73 652c 0a20 2020 2020 2020  = False,.       
-00013970: 2020 2063 6170 7475 7265 5f69 6e74 6572     capture_inter
-00013980: 6d65 6469 6174 6573 3a20 556e 696f 6e5b  mediates: Union[
-00013990: 626f 6f6c 2c20 4361 6c6c 6162 6c65 5b5b  bool, Callable[[
-000139a0: 4d6f 6475 6c65 2c20 7374 725d 2c20 626f  Module, str], bo
-000139b0: 6f6c 5d5d 203d 2046 616c 7365 2c0a 2020  ol]] = False,.  
-000139c0: 2020 2020 2020 2020 2920 2d3e 2043 616c          ) -> Cal
-000139d0: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d3a  lable[..., Any]:
-000139e0: 0a20 2022 2222 4372 6561 7465 7320 616e  .  """Creates an
-000139f0: 2061 7070 6c79 2066 756e 6374 696f 6e20   apply function 
-00013a00: 746f 2063 616c 6c20 6060 666e 6060 2077  to call ``fn`` w
-00013a10: 6974 6820 6120 626f 756e 6420 6d6f 6475  ith a bound modu
-00013a20: 6c65 2e0a 0a20 2055 6e6c 696b 6520 6060  le...  Unlike ``
-00013a30: 4d6f 6475 6c65 2e61 7070 6c79 6060 2074  Module.apply`` t
-00013a40: 6869 7320 6675 6e63 7469 6f6e 2072 6574  his function ret
-00013a50: 7572 6e73 2061 206e 6577 2066 756e 6374  urns a new funct
-00013a60: 696f 6e20 7769 7468 2074 6865 2073 6967  ion with the sig
-00013a70: 6e61 7475 7265 0a20 2060 6028 7661 7269  nature.  ``(vari
-00013a80: 6162 6c65 732c 202a 6172 6773 2c20 726e  ables, *args, rn
-00013a90: 6773 3d4e 6f6e 652c 202a 2a6b 7761 7267  gs=None, **kwarg
-00013aa0: 7329 202d 3e20 5460 6020 7768 6572 6520  s) -> T`` where 
-00013ab0: 6054 6020 6973 2074 6865 2072 6574 7572  `T` is the retur
-00013ac0: 6e20 7479 7065 0a20 206f 6620 6060 666e  n type.  of ``fn
-00013ad0: 6060 2e20 4966 2060 606d 7574 6162 6c65  ``. If ``mutable
-00013ae0: 6060 2069 7320 6e6f 7420 6060 4661 6c73  `` is not ``Fals
-00013af0: 6560 6020 7468 6520 7265 7475 726e 2074  e`` the return t
-00013b00: 7970 6520 6973 2061 2074 7570 6c65 2077  ype is a tuple w
-00013b10: 6865 7265 2074 6865 0a20 2073 6563 6f6e  here the.  secon
-00013b20: 6420 6974 656d 2069 7320 6120 6060 4672  d item is a ``Fr
-00013b30: 6f7a 656e 4469 6374 6060 2077 6974 6820  ozenDict`` with 
-00013b40: 7468 6520 6d75 7461 7465 6420 7661 7269  the mutated vari
-00013b50: 6162 6c65 732e 0a0a 2020 5468 6520 6170  ables...  The ap
-00013b60: 706c 7920 6675 6e63 7469 6f6e 2074 6861  ply function tha
-00013b70: 7420 6973 2072 6574 7572 6e65 6420 6361  t is returned ca
-00013b80: 6e20 6265 2064 6972 6563 746c 7920 636f  n be directly co
-00013b90: 6d70 6f73 6564 2077 6974 680a 2020 4a41  mposed with.  JA
-00013ba0: 5820 7472 616e 7366 6f72 6d61 7469 6f6e  X transformation
-00013bb0: 7320 6c69 6b65 2060 606a 6178 2e6a 6974  s like ``jax.jit
-00013bc0: 6060 3a3a 0a0a 2020 2020 6465 6620 6628  ``::..    def f(
-00013bd0: 666f 6f2c 2078 293a 0a20 2020 2020 207a  foo, x):.      z
-00013be0: 203d 2066 6f6f 2e65 6e63 6f64 6528 7829   = foo.encode(x)
-00013bf0: 0a20 2020 2020 2079 203d 2066 6f6f 2e64  .      y = foo.d
-00013c00: 6563 6f64 6528 7a29 0a20 2020 2020 2023  ecode(z).      #
-00013c10: 202e 2e2e 0a20 2020 2020 2072 6574 7572   ....      retur
-00013c20: 6e20 790a 0a20 2020 2066 6f6f 203d 2046  n y..    foo = F
-00013c30: 6f6f 2829 0a20 2020 2066 5f6a 6974 7465  oo().    f_jitte
-00013c40: 6420 3d20 6a61 782e 6a69 7428 6e6e 2e61  d = jax.jit(nn.a
-00013c50: 7070 6c79 2866 2c20 666f 6f29 290a 2020  pply(f, foo)).  
-00013c60: 2020 665f 6a69 7474 6564 2876 6172 6961    f_jitted(varia
-00013c70: 626c 6573 2c20 7829 0a0a 2020 4172 6773  bles, x)..  Args
-00013c80: 3a0a 2020 2020 666e 3a20 5468 6520 6675  :.    fn: The fu
-00013c90: 6e63 7469 6f6e 2074 6861 7420 7368 6f75  nction that shou
-00013ca0: 6c64 2062 6520 6170 706c 6965 642e 2054  ld be applied. T
-00013cb0: 6865 2066 6972 7374 2061 7267 756d 656e  he first argumen
-00013cc0: 7420 7061 7373 6564 2077 696c 6c0a 2020  t passed will.  
-00013cd0: 2020 2020 6265 2061 6e20 6d6f 6475 6c65      be an module
-00013ce0: 2069 6e73 7461 6e63 6520 6f66 2074 6865   instance of the
-00013cf0: 2060 606d 6f64 756c 6560 6020 7769 7468   ``module`` with
-00013d00: 2076 6172 6961 626c 6573 2061 6e64 2052   variables and R
-00013d10: 4e47 7320 626f 756e 640a 2020 2020 2020  NGs bound.      
-00013d20: 746f 2069 742e 0a20 2020 206d 6f64 756c  to it..    modul
-00013d30: 653a 2054 6865 2060 604d 6f64 756c 6560  e: The ``Module`
-00013d40: 6020 7468 6174 2077 696c 6c20 6265 2075  ` that will be u
-00013d50: 7365 6420 746f 2062 696e 6420 7661 7269  sed to bind vari
-00013d60: 6162 6c65 7320 616e 6420 524e 4773 2074  ables and RNGs t
-00013d70: 6f2e 0a20 2020 2020 2054 6865 2060 604d  o..      The ``M
-00013d80: 6f64 756c 6560 6020 7061 7373 6564 2061  odule`` passed a
-00013d90: 7320 7468 6520 6669 7273 7420 6172 6775  s the first argu
-00013da0: 6d65 6e74 2074 6f20 6060 666e 6060 2077  ment to ``fn`` w
-00013db0: 696c 6c20 6265 2061 2063 6c6f 6e65 0a20  ill be a clone. 
-00013dc0: 2020 2020 206f 6620 6d6f 6475 6c65 2e0a       of module..
-00013dd0: 2020 2020 6d75 7461 626c 653a 2043 616e      mutable: Can
-00013de0: 2062 6520 626f 6f6c 2c20 7374 722c 206f   be bool, str, o
-00013df0: 7220 6c69 7374 2e20 5370 6563 6966 6965  r list. Specifie
-00013e00: 7320 7768 6963 6820 636f 6c6c 6563 7469  s which collecti
-00013e10: 6f6e 7320 7368 6f75 6c64 2062 650a 2020  ons should be.  
-00013e20: 2020 2020 7472 6561 7465 6420 6173 206d      treated as m
-00013e30: 7574 6162 6c65 3a20 6060 626f 6f6c 6060  utable: ``bool``
-00013e40: 3a20 616c 6c2f 6e6f 2063 6f6c 6c65 6374  : all/no collect
-00013e50: 696f 6e73 2061 7265 206d 7574 6162 6c65  ions are mutable
-00013e60: 2e0a 2020 2020 2020 6060 7374 7260 603a  ..      ``str``:
-00013e70: 2054 6865 206e 616d 6520 6f66 2061 2073   The name of a s
-00013e80: 696e 676c 6520 6d75 7461 626c 6520 636f  ingle mutable co
-00013e90: 6c6c 6563 7469 6f6e 2e20 6060 6c69 7374  llection. ``list
-00013ea0: 6060 3a20 410a 2020 2020 2020 6c69 7374  ``: A.      list
-00013eb0: 206f 6620 6e61 6d65 7320 6f66 206d 7574   of names of mut
-00013ec0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
-00013ed0: 2e0a 2020 2020 6361 7074 7572 655f 696e  ..    capture_in
-00013ee0: 7465 726d 6564 6961 7465 733a 2049 6620  termediates: If 
-00013ef0: 6054 7275 6560 2c20 6361 7074 7572 6573  `True`, captures
-00013f00: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
-00013f10: 7475 726e 2076 616c 7565 730a 2020 2020  turn values.    
-00013f20: 2020 6f66 2061 6c6c 204d 6f64 756c 6573    of all Modules
-00013f30: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
-00013f40: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
-00013f50: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
-00013f60: 6c74 206f 6e6c 790a 2020 2020 2020 7468  lt only.      th
-00013f70: 6520 7265 7475 726e 2076 616c 7565 7320  e return values 
-00013f80: 6f66 2061 6c6c 2060 5f5f 6361 6c6c 5f5f  of all `__call__
-00013f90: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
-00013fa0: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
-00013fb0: 2063 616e 0a20 2020 2020 2062 6520 7061   can.      be pa
-00013fc0: 7373 6564 2074 6f20 6368 616e 6765 2074  ssed to change t
-00013fd0: 6865 2066 696c 7465 7220 6265 6861 7669  he filter behavi
-00013fe0: 6f72 2e20 5468 6520 6669 6c74 6572 2066  or. The filter f
-00013ff0: 756e 6374 696f 6e20 7461 6b65 730a 2020  unction takes.  
-00014000: 2020 2020 7468 6520 4d6f 6475 6c65 2069      the Module i
-00014010: 6e73 7461 6e63 6520 616e 6420 6d65 7468  nstance and meth
-00014020: 6f64 206e 616d 6520 616e 6420 7265 7475  od name and retu
-00014030: 726e 7320 6120 626f 6f6c 2069 6e64 6963  rns a bool indic
-00014040: 6174 696e 670a 2020 2020 2020 7768 6574  ating.      whet
-00014050: 6865 7220 7468 6520 6f75 7470 7574 206f  her the output o
-00014060: 6620 7468 6174 206d 6574 686f 6420 696e  f that method in
-00014070: 766f 6361 7469 6f6e 2073 686f 756c 6420  vocation should 
-00014080: 6265 2073 746f 7265 642e 0a20 2052 6574  be stored..  Ret
-00014090: 7572 6e73 3a0a 2020 2020 5468 6520 6170  urns:.    The ap
-000140a0: 706c 7920 6675 6e63 7469 6f6e 2077 7261  ply function wra
-000140b0: 7070 696e 6720 6060 666e 6060 2e0a 2020  pping ``fn``..  
-000140c0: 2222 220a 2020 4066 756e 6374 6f6f 6c73  """.  @functools
-000140d0: 2e77 7261 7073 2866 6e29 0a20 2064 6566  .wraps(fn).  def
-000140e0: 2073 636f 7065 5f66 6e28 7363 6f70 652c   scope_fn(scope,
-000140f0: 202a 6172 6773 2c20 2a2a 6b77 6172 6773   *args, **kwargs
-00014100: 293a 0a20 2020 205f 636f 6e74 6578 742e  ):.    _context.
-00014110: 6361 7074 7572 655f 7374 6163 6b2e 6170  capture_stack.ap
-00014120: 7065 6e64 2863 6170 7475 7265 5f69 6e74  pend(capture_int
-00014130: 6572 6d65 6469 6174 6573 290a 2020 2020  ermediates).    
-00014140: 7472 793a 0a20 2020 2020 2072 6574 7572  try:.      retur
-00014150: 6e20 666e 286d 6f64 756c 652e 636c 6f6e  n fn(module.clon
-00014160: 6528 7061 7265 6e74 3d73 636f 7065 2c20  e(parent=scope, 
-00014170: 5f64 6565 705f 636c 6f6e 653d 5472 7565  _deep_clone=True
-00014180: 292c 202a 6172 6773 2c20 2a2a 6b77 6172  ), *args, **kwar
-00014190: 6773 290a 2020 2020 6669 6e61 6c6c 793a  gs).    finally:
-000141a0: 0a20 2020 2020 205f 636f 6e74 6578 742e  .      _context.
-000141b0: 6361 7074 7572 655f 7374 6163 6b2e 706f  capture_stack.po
-000141c0: 7028 290a 0a20 2069 6620 6361 7074 7572  p()..  if captur
-000141d0: 655f 696e 7465 726d 6564 6961 7465 7320  e_intermediates 
-000141e0: 6973 2054 7275 653a 2020 2320 7079 6c69  is True:  # pyli
-000141f0: 6e74 3a20 6469 7361 626c 653d 672d 626f  nt: disable=g-bo
-00014200: 6f6c 2d69 642d 636f 6d70 6172 6973 6f6e  ol-id-comparison
-00014210: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
-00014220: 6572 6d65 6469 6174 6573 203d 2063 6170  ermediates = cap
-00014230: 7475 7265 5f63 616c 6c5f 696e 7465 726d  ture_call_interm
-00014240: 6564 6961 7465 730a 2020 6966 2063 6170  ediates.  if cap
-00014250: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
-00014260: 6573 3a0a 2020 2020 6d75 7461 626c 6520  es:.    mutable 
-00014270: 3d20 756e 696f 6e5f 6669 6c74 6572 7328  = union_filters(
-00014280: 6d75 7461 626c 652c 2027 696e 7465 726d  mutable, 'interm
-00014290: 6564 6961 7465 7327 290a 2020 7265 7475  ediates').  retu
-000142a0: 726e 2063 6f72 652e 6170 706c 7928 7363  rn core.apply(sc
-000142b0: 6f70 655f 666e 2c20 6d75 7461 626c 653d  ope_fn, mutable=
-000142c0: 6d75 7461 626c 6529 0a0a 0a40 7472 6163  mutable)...@trac
-000142d0: 6562 6163 6b5f 7574 696c 2e61 7069 5f62  eback_util.api_b
-000142e0: 6f75 6e64 6172 790a 6465 6620 696e 6974  oundary.def init
-000142f0: 5f77 6974 685f 6f75 7470 7574 2866 6e3a  _with_output(fn:
-00014300: 2043 616c 6c61 626c 655b 2e2e 2e2c 2041   Callable[..., A
-00014310: 6e79 5d2c 206d 6f64 756c 653a 204d 6f64  ny], module: Mod
-00014320: 756c 652c 0a20 2020 2020 2020 2020 2020  ule,.           
-00014330: 2020 2020 2020 2020 2020 6d75 7461 626c            mutabl
-00014340: 653a 2043 6f6c 6c65 6374 696f 6e46 696c  e: CollectionFil
-00014350: 7465 7220 3d20 4465 6e79 4c69 7374 2827  ter = DenyList('
-00014360: 696e 7465 726d 6564 6961 7465 7327 292c  intermediates'),
-00014370: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00014380: 2020 2020 2020 6361 7074 7572 655f 696e        capture_in
-00014390: 7465 726d 6564 6961 7465 733a 2055 6e69  termediates: Uni
-000143a0: 6f6e 5b62 6f6f 6c2c 2043 616c 6c61 626c  on[bool, Callabl
-000143b0: 655b 5b4d 6f64 756c 652c 2073 7472 5d2c  e[[Module, str],
-000143c0: 2062 6f6f 6c5d 5d20 3d20 4661 6c73 652c   bool]] = False,
-000143d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000143e0: 2020 2020 2020 2920 2d3e 2043 616c 6c61        ) -> Calla
-000143f0: 626c 655b 2e2e 2e2c 2054 7570 6c65 5b41  ble[..., Tuple[A
-00014400: 6e79 2c20 556e 696f 6e5b 4672 6f7a 656e  ny, Union[Frozen
-00014410: 5661 7269 6162 6c65 4469 6374 2c20 4469  VariableDict, Di
-00014420: 6374 5b73 7472 2c20 416e 795d 5d5d 5d3a  ct[str, Any]]]]:
-00014430: 0a20 2022 2222 4372 6561 7465 7320 616e  .  """Creates an
-00014440: 2069 6e69 7420 6675 6e63 7469 6f6e 2074   init function t
-00014450: 6f20 6361 6c6c 2060 6066 6e60 6020 7769  o call ``fn`` wi
-00014460: 7468 2061 2062 6f75 6e64 206d 6f64 756c  th a bound modul
-00014470: 6520 7468 6174 2061 6c73 6f20 7265 7475  e that also retu
-00014480: 726e 7320 7468 6520 6675 6e63 7469 6f6e  rns the function
-00014490: 206f 7574 7075 7473 2e0a 0a20 2055 6e6c   outputs...  Unl
-000144a0: 696b 6520 6060 4d6f 6475 6c65 2e69 6e69  ike ``Module.ini
-000144b0: 745f 7769 7468 5f6f 7574 7075 7460 6020  t_with_output`` 
-000144c0: 7468 6973 2066 756e 6374 696f 6e20 7265  this function re
-000144d0: 7475 726e 7320 6120 6e65 7720 6675 6e63  turns a new func
-000144e0: 7469 6f6e 2077 6974 6820 7468 6520 7369  tion with the si
-000144f0: 676e 6174 7572 650a 2020 6060 2872 6e67  gnature.  ``(rng
-00014500: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
-00014510: 6773 2920 2d3e 2028 542c 2076 6172 6961  gs) -> (T, varia
-00014520: 626c 6573 2960 6020 7768 6572 6520 6054  bles)`` where `T
-00014530: 6020 6973 2074 6865 2072 6574 7572 6e20  ` is the return 
-00014540: 7479 7065 206f 6620 6060 666e 6060 2e0a  type of ``fn``..
-00014550: 2020 5468 6520 726e 6773 2063 616e 2062    The rngs can b
-00014560: 6520 6120 6469 6374 206f 6620 5052 4e47  e a dict of PRNG
-00014570: 4b65 7973 206f 7220 6120 7369 6e67 6c65  Keys or a single
-00014580: 2060 6060 5052 4e47 4b65 7960 6020 7768   ```PRNGKey`` wh
-00014590: 6963 6820 6973 0a20 2065 7175 6976 616c  ich is.  equival
-000145a0: 656e 7420 746f 2070 6173 7369 6e67 2061  ent to passing a
-000145b0: 2064 6963 7420 7769 7468 206f 6e65 2050   dict with one P
-000145c0: 524e 474b 6579 2077 6974 6820 7468 6520  RNGKey with the 
-000145d0: 6e61 6d65 2022 7061 7261 6d73 222e 0a0a  name "params"...
-000145e0: 2020 5468 6520 696e 6974 2066 756e 6374    The init funct
-000145f0: 696f 6e20 7468 6174 2069 7320 7265 7475  ion that is retu
-00014600: 726e 6564 2063 616e 2062 6520 6469 7265  rned can be dire
-00014610: 6374 6c79 2063 6f6d 706f 7365 6420 7769  ctly composed wi
-00014620: 7468 0a20 204a 4158 2074 7261 6e73 666f  th.  JAX transfo
-00014630: 726d 6174 696f 6e73 206c 696b 6520 6060  rmations like ``
-00014640: 6a61 782e 6a69 7460 603a 3a0a 0a20 2020  jax.jit``::..   
-00014650: 2064 6566 2066 2866 6f6f 2c20 7829 3a0a   def f(foo, x):.
-00014660: 2020 2020 2020 7a20 3d20 666f 6f2e 656e        z = foo.en
-00014670: 636f 6465 2878 290a 2020 2020 2020 7920  code(x).      y 
-00014680: 3d20 666f 6f2e 6465 636f 6465 287a 290a  = foo.decode(z).
-00014690: 2020 2020 2020 2320 2e2e 2e0a 2020 2020        # ....    
-000146a0: 2020 7265 7475 726e 2079 0a0a 2020 2020    return y..    
-000146b0: 666f 6f20 3d20 466f 6f28 290a 2020 2020  foo = Foo().    
-000146c0: 665f 6a69 7474 6564 203d 206a 6178 2e6a  f_jitted = jax.j
-000146d0: 6974 286e 6e2e 696e 6974 5f77 6974 685f  it(nn.init_with_
-000146e0: 6f75 7470 7574 2866 2c20 666f 6f29 290a  output(f, foo)).
-000146f0: 2020 2020 792c 2076 6172 6961 626c 6573      y, variables
-00014700: 203d 2066 5f6a 6974 7465 6428 726e 672c   = f_jitted(rng,
-00014710: 2078 290a 0a20 2041 7267 733a 0a20 2020   x)..  Args:.   
-00014720: 2066 6e3a 2054 6865 2066 756e 6374 696f   fn: The functio
-00014730: 6e20 7468 6174 2073 686f 756c 6420 6265  n that should be
-00014740: 2061 7070 6c69 6564 2e20 5468 6520 6669   applied. The fi
-00014750: 7273 7420 6172 6775 6d65 6e74 2070 6173  rst argument pas
-00014760: 7365 6420 7769 6c6c 0a20 2020 2020 2062  sed will.      b
-00014770: 6520 616e 206d 6f64 756c 6520 696e 7374  e an module inst
-00014780: 616e 6365 206f 6620 7468 6520 6060 6d6f  ance of the ``mo
-00014790: 6475 6c65 6060 2077 6974 6820 7661 7269  dule`` with vari
-000147a0: 6162 6c65 7320 616e 6420 524e 4773 2062  ables and RNGs b
-000147b0: 6f75 6e64 0a20 2020 2020 2074 6f20 6974  ound.      to it
-000147c0: 2e0a 2020 2020 6d6f 6475 6c65 3a20 5468  ..    module: Th
-000147d0: 6520 6060 4d6f 6475 6c65 6060 2074 6861  e ``Module`` tha
-000147e0: 7420 7769 6c6c 2062 6520 7573 6564 2074  t will be used t
-000147f0: 6f20 6269 6e64 2076 6172 6961 626c 6573  o bind variables
-00014800: 2061 6e64 2052 4e47 7320 746f 2e0a 2020   and RNGs to..  
-00014810: 2020 2020 5468 6520 6060 4d6f 6475 6c65      The ``Module
-00014820: 6060 2070 6173 7365 6420 6173 2074 6865  `` passed as the
-00014830: 2066 6972 7374 2061 7267 756d 656e 7420   first argument 
-00014840: 746f 2060 6066 6e60 6020 7769 6c6c 2062  to ``fn`` will b
-00014850: 6520 6120 636c 6f6e 650a 2020 2020 2020  e a clone.      
-00014860: 6f66 206d 6f64 756c 652e 0a20 2020 206d  of module..    m
-00014870: 7574 6162 6c65 3a20 4361 6e20 6265 2062  utable: Can be b
-00014880: 6f6f 6c2c 2073 7472 2c20 6f72 206c 6973  ool, str, or lis
-00014890: 742e 2053 7065 6369 6669 6573 2077 6869  t. Specifies whi
-000148a0: 6368 2063 6f6c 6c65 6374 696f 6e73 2073  ch collections s
-000148b0: 686f 756c 6420 6265 0a20 2020 2020 2074  hould be.      t
-000148c0: 7265 6174 6564 2061 7320 6d75 7461 626c  reated as mutabl
-000148d0: 653a 2060 6062 6f6f 6c60 603a 2061 6c6c  e: ``bool``: all
-000148e0: 2f6e 6f20 636f 6c6c 6563 7469 6f6e 7320  /no collections 
-000148f0: 6172 6520 6d75 7461 626c 652e 0a20 2020  are mutable..   
-00014900: 2020 2060 6073 7472 6060 3a20 5468 6520     ``str``: The 
-00014910: 6e61 6d65 206f 6620 6120 7369 6e67 6c65  name of a single
-00014920: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
-00014930: 696f 6e2e 2060 606c 6973 7460 603a 2041  ion. ``list``: A
-00014940: 0a20 2020 2020 206c 6973 7420 6f66 206e  .      list of n
-00014950: 616d 6573 206f 6620 6d75 7461 626c 6520  ames of mutable 
-00014960: 636f 6c6c 6563 7469 6f6e 732e 2042 7920  collections. By 
-00014970: 6465 6661 756c 7420 616c 6c20 636f 6c6c  default all coll
-00014980: 6563 7469 6f6e 730a 2020 2020 2020 6578  ections.      ex
-00014990: 6365 7074 2022 696e 7465 726d 6564 6961  cept "intermedia
-000149a0: 7465 7322 2061 7265 206d 7574 6162 6c65  tes" are mutable
-000149b0: 2e0a 2020 2020 6361 7074 7572 655f 696e  ..    capture_in
-000149c0: 7465 726d 6564 6961 7465 733a 2049 6620  termediates: If 
-000149d0: 6054 7275 6560 2c20 6361 7074 7572 6573  `True`, captures
-000149e0: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
-000149f0: 7475 726e 2076 616c 7565 730a 2020 2020  turn values.    
-00014a00: 2020 6f66 2061 6c6c 204d 6f64 756c 6573    of all Modules
-00014a10: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
-00014a20: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
-00014a30: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
-00014a40: 6c74 206f 6e6c 790a 2020 2020 2020 7468  lt only.      th
-00014a50: 6520 7265 7475 726e 2076 616c 7565 7320  e return values 
-00014a60: 6f66 2061 6c6c 2060 5f5f 6361 6c6c 5f5f  of all `__call__
-00014a70: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
-00014a80: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
-00014a90: 2063 616e 0a20 2020 2020 2062 6520 7061   can.      be pa
-00014aa0: 7373 6564 2074 6f20 6368 616e 6765 2074  ssed to change t
-00014ab0: 6865 2066 696c 7465 7220 6265 6861 7669  he filter behavi
-00014ac0: 6f72 2e20 5468 6520 6669 6c74 6572 2066  or. The filter f
-00014ad0: 756e 6374 696f 6e20 7461 6b65 730a 2020  unction takes.  
-00014ae0: 2020 2020 7468 6520 4d6f 6475 6c65 2069      the Module i
-00014af0: 6e73 7461 6e63 6520 616e 6420 6d65 7468  nstance and meth
-00014b00: 6f64 206e 616d 6520 616e 6420 7265 7475  od name and retu
-00014b10: 726e 7320 6120 626f 6f6c 2069 6e64 6963  rns a bool indic
-00014b20: 6174 696e 670a 2020 2020 2020 7768 6574  ating.      whet
-00014b30: 6865 7220 7468 6520 6f75 7470 7574 206f  her the output o
-00014b40: 6620 7468 6174 206d 6574 686f 6420 696e  f that method in
-00014b50: 766f 6361 7469 6f6e 2073 686f 756c 6420  vocation should 
-00014b60: 6265 2073 746f 7265 642e 0a20 2052 6574  be stored..  Ret
-00014b70: 7572 6e73 3a0a 2020 2020 5468 6520 696e  urns:.    The in
-00014b80: 6974 2066 756e 6374 696f 6e20 7772 6170  it function wrap
-00014b90: 7069 6e67 2060 6066 6e60 602e 0a20 2022  ping ``fn``..  "
-00014ba0: 2222 0a20 2040 6675 6e63 746f 6f6c 732e  "".  @functools.
-00014bb0: 7772 6170 7328 666e 290a 2020 6465 6620  wraps(fn).  def 
-00014bc0: 7363 6f70 655f 666e 2873 636f 7065 2c20  scope_fn(scope, 
-00014bd0: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-00014be0: 3a0a 2020 2020 5f63 6f6e 7465 7874 2e63  :.    _context.c
-00014bf0: 6170 7475 7265 5f73 7461 636b 2e61 7070  apture_stack.app
-00014c00: 656e 6428 6361 7074 7572 655f 696e 7465  end(capture_inte
-00014c10: 726d 6564 6961 7465 7329 0a20 2020 2074  rmediates).    t
-00014c20: 7279 3a0a 2020 2020 2020 7265 7475 726e  ry:.      return
-00014c30: 2066 6e28 6d6f 6475 6c65 2e63 6c6f 6e65   fn(module.clone
-00014c40: 2870 6172 656e 743d 7363 6f70 652c 205f  (parent=scope, _
-00014c50: 6465 6570 5f63 6c6f 6e65 3d54 7275 6529  deep_clone=True)
-00014c60: 2c20 2a61 7267 732c 202a 2a6b 7761 7267  , *args, **kwarg
-00014c70: 7329 0a20 2020 2066 696e 616c 6c79 3a0a  s).    finally:.
-00014c80: 2020 2020 2020 5f63 6f6e 7465 7874 2e63        _context.c
-00014c90: 6170 7475 7265 5f73 7461 636b 2e70 6f70  apture_stack.pop
-00014ca0: 2829 0a0a 2020 6966 2063 6170 7475 7265  ()..  if capture
-00014cb0: 5f69 6e74 6572 6d65 6469 6174 6573 2069  _intermediates i
-00014cc0: 7320 5472 7565 3a20 2023 2070 796c 696e  s True:  # pylin
-00014cd0: 743a 2064 6973 6162 6c65 3d67 2d62 6f6f  t: disable=g-boo
-00014ce0: 6c2d 6964 2d63 6f6d 7061 7269 736f 6e0a  l-id-comparison.
-00014cf0: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
-00014d00: 726d 6564 6961 7465 7320 3d20 6361 7074  rmediates = capt
-00014d10: 7572 655f 6361 6c6c 5f69 6e74 6572 6d65  ure_call_interme
-00014d20: 6469 6174 6573 0a20 2069 6620 6361 7074  diates.  if capt
-00014d30: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
-00014d40: 733a 0a20 2020 206d 7574 6162 6c65 203d  s:.    mutable =
-00014d50: 2075 6e69 6f6e 5f66 696c 7465 7273 286d   union_filters(m
-00014d60: 7574 6162 6c65 2c20 2769 6e74 6572 6d65  utable, 'interme
-00014d70: 6469 6174 6573 2729 0a20 2072 6574 7572  diates').  retur
-00014d80: 6e20 636f 7265 2e69 6e69 7428 7363 6f70  n core.init(scop
-00014d90: 655f 666e 2c20 6d75 7461 626c 653d 6d75  e_fn, mutable=mu
-00014da0: 7461 626c 6529 0a0a 0a40 7472 6163 6562  table)...@traceb
-00014db0: 6163 6b5f 7574 696c 2e61 7069 5f62 6f75  ack_util.api_bou
-00014dc0: 6e64 6172 790a 6465 6620 696e 6974 2866  ndary.def init(f
-00014dd0: 6e3a 2043 616c 6c61 626c 655b 2e2e 2e2c  n: Callable[...,
-00014de0: 2041 6e79 5d2c 206d 6f64 756c 653a 204d   Any], module: M
-00014df0: 6f64 756c 652c 0a20 2020 2020 2020 2020  odule,.         
-00014e00: 6d75 7461 626c 653a 2043 6f6c 6c65 6374  mutable: Collect
-00014e10: 696f 6e46 696c 7465 7220 3d20 4465 6e79  ionFilter = Deny
-00014e20: 4c69 7374 2827 696e 7465 726d 6564 6961  List('intermedia
-00014e30: 7465 7327 292c 0a20 2020 2020 2020 2020  tes'),.         
-00014e40: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-00014e50: 6961 7465 733a 2055 6e69 6f6e 5b62 6f6f  iates: Union[boo
-00014e60: 6c2c 2043 616c 6c61 626c 655b 5b4d 6f64  l, Callable[[Mod
-00014e70: 756c 652c 2073 7472 5d2c 2062 6f6f 6c5d  ule, str], bool]
-00014e80: 5d20 3d20 4661 6c73 652c 0a20 2020 2020  ] = False,.     
-00014e90: 2020 2020 2920 2d3e 2043 616c 6c61 626c      ) -> Callabl
-00014ea0: 655b 2e2e 2e2c 2055 6e69 6f6e 5b46 726f  e[..., Union[Fro
-00014eb0: 7a65 6e56 6172 6961 626c 6544 6963 742c  zenVariableDict,
-00014ec0: 2044 6963 745b 7374 722c 2041 6e79 5d5d   Dict[str, Any]]
-00014ed0: 5d3a 0a20 2022 2222 4372 6561 7465 7320  ]:.  """Creates 
-00014ee0: 616e 2069 6e69 7420 6675 6e63 7469 6f6e  an init function
-00014ef0: 2074 6f20 6361 6c6c 2060 6066 6e60 6020   to call ``fn`` 
-00014f00: 7769 7468 2061 2062 6f75 6e64 206d 6f64  with a bound mod
-00014f10: 756c 652e 0a0a 2020 556e 6c69 6b65 2060  ule...  Unlike `
-00014f20: 604d 6f64 756c 652e 696e 6974 6060 2074  `Module.init`` t
-00014f30: 6869 7320 6675 6e63 7469 6f6e 2072 6574  his function ret
-00014f40: 7572 6e73 2061 206e 6577 2066 756e 6374  urns a new funct
-00014f50: 696f 6e20 7769 7468 2074 6865 2073 6967  ion with the sig
-00014f60: 6e61 7475 7265 0a20 2060 6028 726e 6773  nature.  ``(rngs
-00014f70: 2c20 2a61 7267 732c 202a 2a6b 7761 7267  , *args, **kwarg
-00014f80: 7329 202d 3e20 7661 7269 6162 6c65 7360  s) -> variables`
-00014f90: 602e 0a20 2054 6865 2072 6e67 7320 6361  `..  The rngs ca
-00014fa0: 6e20 6265 2061 2064 6963 7420 6f66 2050  n be a dict of P
-00014fb0: 524e 474b 6579 7320 6f72 2061 2073 696e  RNGKeys or a sin
-00014fc0: 676c 6520 6060 6050 524e 474b 6579 6060  gle ```PRNGKey``
-00014fd0: 2077 6869 6368 2069 730a 2020 6571 7569   which is.  equi
-00014fe0: 7661 6c65 6e74 2074 6f20 7061 7373 696e  valent to passin
-00014ff0: 6720 6120 6469 6374 2077 6974 6820 6f6e  g a dict with on
-00015000: 6520 5052 4e47 4b65 7920 7769 7468 2074  e PRNGKey with t
-00015010: 6865 206e 616d 6520 2270 6172 616d 7322  he name "params"
-00015020: 2e0a 0a20 2054 6865 2069 6e69 7420 6675  ...  The init fu
-00015030: 6e63 7469 6f6e 2074 6861 7420 6973 2072  nction that is r
-00015040: 6574 7572 6e65 6420 6361 6e20 6265 2064  eturned can be d
-00015050: 6972 6563 746c 7920 636f 6d70 6f73 6564  irectly composed
-00015060: 2077 6974 680a 2020 4a41 5820 7472 616e   with.  JAX tran
-00015070: 7366 6f72 6d61 7469 6f6e 7320 6c69 6b65  sformations like
-00015080: 2060 606a 6178 2e6a 6974 6060 3a3a 0a0a   ``jax.jit``::..
-00015090: 2020 2020 6465 6620 6628 666f 6f2c 2078      def f(foo, x
-000150a0: 293a 0a20 2020 2020 207a 203d 2066 6f6f  ):.      z = foo
-000150b0: 2e65 6e63 6f64 6528 7829 0a20 2020 2020  .encode(x).     
-000150c0: 2079 203d 2066 6f6f 2e64 6563 6f64 6528   y = foo.decode(
-000150d0: 7a29 0a20 2020 2020 2023 202e 2e2e 0a20  z).      # .... 
-000150e0: 2020 2020 2072 6574 7572 6e20 790a 0a20       return y.. 
-000150f0: 2020 2066 6f6f 203d 2046 6f6f 2829 0a20     foo = Foo(). 
-00015100: 2020 2066 5f6a 6974 7465 6420 3d20 6a61     f_jitted = ja
-00015110: 782e 6a69 7428 6e6e 2e69 6e69 7428 662c  x.jit(nn.init(f,
-00015120: 2066 6f6f 2929 0a20 2020 2076 6172 6961   foo)).    varia
-00015130: 626c 6573 203d 2066 5f6a 6974 7465 6428  bles = f_jitted(
-00015140: 726e 672c 2078 290a 0a20 2041 7267 733a  rng, x)..  Args:
-00015150: 0a20 2020 2066 6e3a 2054 6865 2066 756e  .    fn: The fun
-00015160: 6374 696f 6e20 7468 6174 2073 686f 756c  ction that shoul
-00015170: 6420 6265 2061 7070 6c69 6564 2e20 5468  d be applied. Th
-00015180: 6520 6669 7273 7420 6172 6775 6d65 6e74  e first argument
-00015190: 2070 6173 7365 6420 7769 6c6c 0a20 2020   passed will.   
-000151a0: 2020 2062 6520 616e 206d 6f64 756c 6520     be an module 
-000151b0: 696e 7374 616e 6365 206f 6620 7468 6520  instance of the 
-000151c0: 6060 6d6f 6475 6c65 6060 2077 6974 6820  ``module`` with 
-000151d0: 7661 7269 6162 6c65 7320 616e 6420 524e  variables and RN
-000151e0: 4773 2062 6f75 6e64 0a20 2020 2020 2074  Gs bound.      t
-000151f0: 6f20 6974 2e0a 2020 2020 6d6f 6475 6c65  o it..    module
-00015200: 3a20 5468 6520 6060 4d6f 6475 6c65 6060  : The ``Module``
-00015210: 2074 6861 7420 7769 6c6c 2062 6520 7573   that will be us
-00015220: 6564 2074 6f20 6269 6e64 2076 6172 6961  ed to bind varia
-00015230: 626c 6573 2061 6e64 2052 4e47 7320 746f  bles and RNGs to
-00015240: 2e0a 2020 2020 2020 5468 6520 6060 4d6f  ..      The ``Mo
-00015250: 6475 6c65 6060 2070 6173 7365 6420 6173  dule`` passed as
-00015260: 2074 6865 2066 6972 7374 2061 7267 756d   the first argum
-00015270: 656e 7420 746f 2060 6066 6e60 6020 7769  ent to ``fn`` wi
-00015280: 6c6c 2062 6520 6120 636c 6f6e 650a 2020  ll be a clone.  
-00015290: 2020 2020 6f66 206d 6f64 756c 652e 0a20      of module.. 
-000152a0: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
-000152b0: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
-000152c0: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
-000152d0: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
-000152e0: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
-000152f0: 2020 2074 7265 6174 6564 2061 7320 6d75     treated as mu
-00015300: 7461 626c 653a 2060 6062 6f6f 6c60 603a  table: ``bool``:
-00015310: 2061 6c6c 2f6e 6f20 636f 6c6c 6563 7469   all/no collecti
-00015320: 6f6e 7320 6172 6520 6d75 7461 626c 652e  ons are mutable.
-00015330: 0a20 2020 2020 2060 6073 7472 6060 3a20  .      ``str``: 
-00015340: 5468 6520 6e61 6d65 206f 6620 6120 7369  The name of a si
-00015350: 6e67 6c65 206d 7574 6162 6c65 2063 6f6c  ngle mutable col
-00015360: 6c65 6374 696f 6e2e 2060 606c 6973 7460  lection. ``list`
-00015370: 603a 2041 0a20 2020 2020 206c 6973 7420  `: A.      list 
-00015380: 6f66 206e 616d 6573 206f 6620 6d75 7461  of names of muta
-00015390: 626c 6520 636f 6c6c 6563 7469 6f6e 732e  ble collections.
-000153a0: 2042 7920 6465 6661 756c 7420 616c 6c20   By default all 
-000153b0: 636f 6c6c 6563 7469 6f6e 730a 2020 2020  collections.    
-000153c0: 2020 6578 6365 7074 2022 696e 7465 726d    except "interm
-000153d0: 6564 6961 7465 7322 2061 7265 206d 7574  ediates" are mut
-000153e0: 6162 6c65 2e0a 2020 2020 6361 7074 7572  able..    captur
-000153f0: 655f 696e 7465 726d 6564 6961 7465 733a  e_intermediates:
-00015400: 2049 6620 6054 7275 6560 2c20 6361 7074   If `True`, capt
-00015410: 7572 6573 2069 6e74 6572 6d65 6469 6174  ures intermediat
-00015420: 6520 7265 7475 726e 2076 616c 7565 730a  e return values.
-00015430: 2020 2020 2020 6f66 2061 6c6c 204d 6f64        of all Mod
-00015440: 756c 6573 2069 6e73 6964 6520 7468 6520  ules inside the 
-00015450: 2269 6e74 6572 6d65 6469 6174 6573 2220  "intermediates" 
-00015460: 636f 6c6c 6563 7469 6f6e 2e20 4279 2064  collection. By d
-00015470: 6566 6175 6c74 206f 6e6c 790a 2020 2020  efault only.    
-00015480: 2020 7468 6520 7265 7475 726e 2076 616c    the return val
-00015490: 7565 7320 6f66 2061 6c6c 2060 5f5f 6361  ues of all `__ca
-000154a0: 6c6c 5f5f 6020 6d65 7468 6f64 7320 6172  ll__` methods ar
-000154b0: 6520 7374 6f72 6564 2e20 4120 6675 6e63  e stored. A func
-000154c0: 7469 6f6e 2063 616e 0a20 2020 2020 2062  tion can.      b
-000154d0: 6520 7061 7373 6564 2074 6f20 6368 616e  e passed to chan
-000154e0: 6765 2074 6865 2066 696c 7465 7220 6265  ge the filter be
-000154f0: 6861 7669 6f72 2e20 5468 6520 6669 6c74  havior. The filt
-00015500: 6572 2066 756e 6374 696f 6e20 7461 6b65  er function take
-00015510: 730a 2020 2020 2020 7468 6520 4d6f 6475  s.      the Modu
-00015520: 6c65 2069 6e73 7461 6e63 6520 616e 6420  le instance and 
-00015530: 6d65 7468 6f64 206e 616d 6520 616e 6420  method name and 
-00015540: 7265 7475 726e 7320 6120 626f 6f6c 2069  returns a bool i
-00015550: 6e64 6963 6174 696e 670a 2020 2020 2020  ndicating.      
-00015560: 7768 6574 6865 7220 7468 6520 6f75 7470  whether the outp
-00015570: 7574 206f 6620 7468 6174 206d 6574 686f  ut of that metho
-00015580: 6420 696e 766f 6361 7469 6f6e 2073 686f  d invocation sho
-00015590: 756c 6420 6265 2073 746f 7265 642e 0a20  uld be stored.. 
-000155a0: 2052 6574 7572 6e73 3a0a 2020 2020 5468   Returns:.    Th
-000155b0: 6520 696e 6974 2066 756e 6374 696f 6e20  e init function 
-000155c0: 7772 6170 7069 6e67 2060 6066 6e60 602e  wrapping ``fn``.
-000155d0: 0a20 2022 2222 0a20 2069 6e69 745f 666e  .  """.  init_fn
-000155e0: 203d 2069 6e69 745f 7769 7468 5f6f 7574   = init_with_out
-000155f0: 7075 7428 666e 2c20 6d6f 6475 6c65 2c20  put(fn, module, 
-00015600: 6d75 7461 626c 652c 2063 6170 7475 7265  mutable, capture
-00015610: 5f69 6e74 6572 6d65 6469 6174 6573 290a  _intermediates).
-00015620: 2020 4066 756e 6374 6f6f 6c73 2e77 7261    @functools.wra
-00015630: 7073 2869 6e69 745f 666e 290a 2020 6465  ps(init_fn).  de
-00015640: 6620 696e 6974 5f77 7261 7070 6572 282a  f init_wrapper(*
-00015650: 6172 6773 2c20 2a2a 6b77 6172 6773 293a  args, **kwargs):
-00015660: 0a20 2020 2072 6574 7572 6e20 696e 6974  .    return init
-00015670: 5f66 6e28 2a61 7267 732c 202a 2a6b 7761  _fn(*args, **kwa
-00015680: 7267 7329 5b31 5d0a 2020 7265 7475 726e  rgs)[1].  return
-00015690: 2069 6e69 745f 7772 6170 7065 720a        init_wrapper.
+00012a90: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+00012aa0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00012ab0: 2020 2020 2020 2020 2020 2020 e294 820a              ....
+00012ac0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00012ad0: 2020 e294 8220 2020 2020 2020 20e2 9482    ...        ...
+00012ae0: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
+00012af0: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+00012b00: 20e2 9482 2034 3020 2831 3630 2042 2920   ... 40 (160 B) 
+00012b10: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
+00012b20: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
+00012b30: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012b40: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
+00012b50: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012b60: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012b70: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012b80: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012b90: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012ba0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012bb0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012bc0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012bd0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012be0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012bf0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012c00: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
+00012c10: 4465 6e73 655f 3120 e294 8220 4465 6e73  Dense_1 ... Dens
+00012c20: 6520 20e2 9482 2066 6c6f 6174 3332 5b31  e  ... float32[1
+00012c30: 362c 345d 20e2 9482 2066 6c6f 6174 3332  6,4] ... float32
+00012c40: 5b31 362c 325d 20e2 9482 2062 6961 733a  [16,2] ... bias:
+00012c50: 2066 6c6f 6174 3332 5b32 5d20 2020 2020   float32[2]     
+00012c60: e294 820a 2020 2020 2020 e294 8220 2020  ....      ...   
+00012c70: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00012c80: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+00012c90: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00012ca0: 2020 2020 20e2 9482 206b 6572 6e65 6c3a       ... kernel:
+00012cb0: 2066 6c6f 6174 3332 5b34 2c32 5d20 e294   float32[4,2] ..
+00012cc0: 820a 2020 2020 2020 e294 8220 2020 2020  ..      ...     
+00012cd0: 2020 2020 e294 8220 2020 2020 2020 20e2      ...        .
+00012ce0: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+00012cf0: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+00012d00: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00012d10: 2020 2020 2020 2020 2020 2020 e294 820a              ....
+00012d20: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00012d30: 2020 e294 8220 2020 2020 2020 20e2 9482    ...        ...
+00012d40: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
+00012d50: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+00012d60: 20e2 9482 2031 3020 2834 3020 4229 2020   ... 10 (40 B)  
+00012d70: 2020 2020 2020 2020 2020 e294 820a 2020            ....  
+00012d80: 2020 2020 e294 9ce2 9480 e294 80e2 9480      ............
+00012d90: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012da0: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
+00012db0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012dc0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012dd0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012de0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012df0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012e00: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012e10: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00012e20: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012e30: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012e40: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012e50: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012e60: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
+00012e70: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+00012e80: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00012e90: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00012ea0: 2054 6f74 616c 20e2 9482 2035 3020 2832   Total ... 50 (2
+00012eb0: 3030 2042 2920 2020 2020 2020 2020 2020  00 B)           
+00012ec0: e294 820a 2020 2020 2020 e294 94e2 9480  ....      ......
+00012ed0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012ee0: 9480 e294 80e2 9480 e294 b4e2 9480 e294  ................
+00012ef0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012f00: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
+00012f10: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012f20: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012f30: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
+00012f40: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012f50: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012f60: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
+00012f70: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00012f80: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00012f90: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00012fa0: 9480 e294 80e2 9480 e294 980a 0a20 2020  .............   
+00012fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012fc0: 2020 2020 2020 2020 2054 6f74 616c 2050           Total P
+00012fd0: 6172 616d 6574 6572 733a 2035 3020 2832  arameters: 50 (2
+00012fe0: 3030 2042 290a 0a20 2020 202a 2a4e 6f74  00 B)..    **Not
+00012ff0: 652a 2a3a 2072 6f77 7320 6f72 6465 7220  e**: rows order 
+00013000: 696e 2074 6865 2074 6162 6c65 2064 6f65  in the table doe
+00013010: 7320 6e6f 7420 7265 7072 6573 656e 7420  s not represent 
+00013020: 6578 6563 7574 696f 6e20 6f72 6465 722c  execution order,
+00013030: 0a20 2020 2069 6e73 7465 6164 2069 7420  .    instead it 
+00013040: 616c 6967 6e73 2077 6974 6820 7468 6520  aligns with the 
+00013050: 6f72 6465 7220 6f66 206b 6579 7320 696e  order of keys in
+00013060: 2060 7661 7269 6162 6c65 7360 2077 6869   `variables` whi
+00013070: 6368 2061 7265 2073 6f72 7465 640a 2020  ch are sorted.  
+00013080: 2020 616c 7068 6162 6574 6963 616c 6c79    alphabetically
+00013090: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   
+000130a0: 2020 2072 6e67 733a 2054 6865 2072 6e67     rngs: The rng
+000130b0: 7320 666f 7220 7468 6520 7661 7269 6162  s for the variab
+000130c0: 6c65 2063 6f6c 6c65 6374 696f 6e73 2061  le collections a
+000130d0: 7320 7061 7373 6564 2074 6f20 604d 6f64  s passed to `Mod
+000130e0: 756c 652e 696e 6974 602e 0a20 2020 2020  ule.init`..     
+000130f0: 202a 6172 6773 3a20 5468 6520 6172 6775   *args: The argu
+00013100: 6d65 6e74 7320 746f 2074 6865 2066 6f72  ments to the for
+00013110: 7761 7264 2063 6f6d 7075 7461 7469 6f6e  ward computation
+00013120: 2e0a 2020 2020 2020 6465 7074 683a 2063  ..      depth: c
+00013130: 6f6e 7472 6f6c 7320 686f 7720 6d61 6e79  ontrols how many
+00013140: 2073 7562 6d6f 6475 6c65 2064 6565 7020   submodule deep 
+00013150: 7468 6520 7375 6d6d 6172 7920 6361 6e20  the summary can 
+00013160: 676f 2e20 4279 2064 6566 6175 6c74 2069  go. By default i
+00013170: 7473 0a20 2020 2020 2020 2060 4e6f 6e65  ts.        `None
+00013180: 6020 7768 6963 6820 6d65 616e 7320 6e6f  ` which means no
+00013190: 206c 696d 6974 2e20 4966 2061 2073 7562   limit. If a sub
+000131a0: 6d6f 6475 6c65 2069 7320 6e6f 7420 7368  module is not sh
+000131b0: 6f77 6e20 6265 6361 7573 6520 6f66 2074  own because of t
+000131c0: 6865 0a20 2020 2020 2020 2064 6570 7468  he.        depth
+000131d0: 206c 696d 6974 2c20 6974 7320 7061 7261   limit, its para
+000131e0: 6d65 7465 7220 636f 756e 7420 616e 6420  meter count and 
+000131f0: 6279 7465 7320 7769 6c6c 2062 6520 6164  bytes will be ad
+00013200: 6465 6420 746f 2074 6865 2072 6f77 206f  ded to the row o
+00013210: 6620 6974 730a 2020 2020 2020 2020 6669  f its.        fi
+00013220: 7273 7420 7368 6f77 6e20 616e 6365 7374  rst shown ancest
+00013230: 6f72 2073 7563 6820 7468 6174 2074 6865  or such that the
+00013240: 2073 756d 206f 6620 616c 6c20 726f 7773   sum of all rows
+00013250: 2061 6c77 6179 7320 6164 6473 2075 7020   always adds up 
+00013260: 746f 2074 6865 0a20 2020 2020 2020 2074  to the.        t
+00013270: 6f74 616c 206e 756d 6265 7220 6f66 2070  otal number of p
+00013280: 6172 616d 6574 6572 7320 6f66 2074 6865  arameters of the
+00013290: 204d 6f64 756c 652e 0a20 2020 2020 2073   Module..      s
+000132a0: 686f 775f 7265 7065 6174 6564 3a20 4966  how_repeated: If
+000132b0: 2060 5472 7565 602c 2072 6570 6561 7465   `True`, repeate
+000132c0: 6420 6361 6c6c 7320 746f 2074 6865 2073  d calls to the s
+000132d0: 616d 6520 6d6f 6475 6c65 2077 696c 6c20  ame module will 
+000132e0: 6265 2073 686f 776e 0a20 2020 2020 2020  be shown.       
+000132f0: 2069 6e20 7468 6520 7461 626c 652c 206f   in the table, o
+00013300: 7468 6572 7769 7365 206f 6e6c 7920 7468  therwise only th
+00013310: 6520 6669 7273 7420 6361 6c6c 2077 696c  e first call wil
+00013320: 6c20 6265 2073 686f 776e 2e20 4465 6661  l be shown. Defa
+00013330: 756c 7420 6973 0a20 2020 2020 2020 2060  ult is.        `
+00013340: 4661 6c73 6560 2e0a 2020 2020 2020 6d75  False`..      mu
+00013350: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
+00013360: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
+00013370: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
+00013380: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
+00013390: 6f75 6c64 2062 650a 2020 2020 2020 2020  ould be.        
+000133a0: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
+000133b0: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
+000133c0: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
+000133d0: 2061 7265 206d 7574 6162 6c65 2e20 6060   are mutable. ``
+000133e0: 7374 7260 603a 2054 6865 0a20 2020 2020  str``: The.     
+000133f0: 2020 206e 616d 6520 6f66 2061 2073 696e     name of a sin
+00013400: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
+00013410: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
+00013420: 3a20 4120 6c69 7374 206f 6620 6e61 6d65  : A list of name
+00013430: 7320 6f66 206d 7574 6162 6c65 0a20 2020  s of mutable.   
+00013440: 2020 2020 2063 6f6c 6c65 6374 696f 6e73       collections
+00013450: 2e20 4279 2064 6566 6175 6c74 2061 6c6c  . By default all
+00013460: 2063 6f6c 6c65 6374 696f 6e73 2065 7863   collections exc
+00013470: 6570 7420 2769 6e74 6572 6d65 6469 6174  ept 'intermediat
+00013480: 6573 2720 6172 650a 2020 2020 2020 2020  es' are.        
+00013490: 6d75 7461 626c 652e 0a20 2020 2020 2063  mutable..      c
+000134a0: 6f6e 736f 6c65 5f6b 7761 7267 733a 2041  onsole_kwargs: A
+000134b0: 6e20 6f70 7469 6f6e 616c 2064 6963 7469  n optional dicti
+000134c0: 6f6e 6172 7920 7769 7468 2061 6464 6974  onary with addit
+000134d0: 696f 6e61 6c20 6b65 7977 6f72 6420 6172  ional keyword ar
+000134e0: 6775 6d65 6e74 7320 7468 6174 0a20 2020  guments that.   
+000134f0: 2020 2020 2061 7265 2070 6173 7365 6420       are passed 
+00013500: 746f 2060 7269 6368 2e63 6f6e 736f 6c65  to `rich.console
+00013510: 2e43 6f6e 736f 6c65 6020 7768 656e 2072  .Console` when r
+00013520: 656e 6465 7269 6e67 2074 6865 2074 6162  endering the tab
+00013530: 6c65 2e20 4465 6661 756c 7420 6172 6775  le. Default argu
+00013540: 6d65 6e74 730a 2020 2020 2020 2020 6172  ments.        ar
+00013550: 6520 607b 2766 6f72 6365 5f74 6572 6d69  e `{'force_termi
+00013560: 6e61 6c27 3a20 5472 7565 2c20 2766 6f72  nal': True, 'for
+00013570: 6365 5f6a 7570 7974 6572 273a 2046 616c  ce_jupyter': Fal
+00013580: 7365 7d60 2e0a 2020 2020 2020 7461 626c  se}`..      tabl
+00013590: 655f 6b77 6172 6773 3a20 416e 206f 7074  e_kwargs: An opt
+000135a0: 696f 6e61 6c20 6469 6374 696f 6e61 7279  ional dictionary
+000135b0: 2077 6974 6820 6164 6469 7469 6f6e 616c   with additional
+000135c0: 206b 6579 776f 7264 2061 7267 756d 656e   keyword argumen
+000135d0: 7473 2074 6861 740a 2020 2020 2020 2020  ts that.        
+000135e0: 6172 6520 7061 7373 6564 2074 6f20 6072  are passed to `r
+000135f0: 6963 682e 7461 626c 652e 5461 626c 6560  ich.table.Table`
+00013600: 2063 6f6e 7374 7275 6374 6f72 2e0a 2020   constructor..  
+00013610: 2020 2020 636f 6c75 6d6e 5f6b 7761 7267      column_kwarg
+00013620: 733a 2041 6e20 6f70 7469 6f6e 616c 2064  s: An optional d
+00013630: 6963 7469 6f6e 6172 7920 7769 7468 2061  ictionary with a
+00013640: 6464 6974 696f 6e61 6c20 6b65 7977 6f72  dditional keywor
+00013650: 6420 6172 6775 6d65 6e74 7320 7468 6174  d arguments that
+00013660: 0a20 2020 2020 2020 2061 7265 2070 6173  .        are pas
+00013670: 7365 6420 746f 2060 7269 6368 2e74 6162  sed to `rich.tab
+00013680: 6c65 2e54 6162 6c65 2e61 6464 5f63 6f6c  le.Table.add_col
+00013690: 756d 6e60 2077 6865 6e20 6164 6469 6e67  umn` when adding
+000136a0: 2063 6f6c 756d 6e73 2074 6f20 7468 6520   columns to the 
+000136b0: 7461 626c 652e 0a20 2020 2020 202a 2a6b  table..      **k
+000136c0: 7761 7267 733a 206b 6579 776f 7264 2061  wargs: keyword a
+000136d0: 7267 756d 656e 7473 2074 6f20 7061 7373  rguments to pass
+000136e0: 2074 6f20 7468 6520 666f 7277 6172 6420   to the forward 
+000136f0: 636f 6d70 7574 6174 696f 6e2e 0a0a 2020  computation...  
+00013700: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     
+00013710: 2041 2073 7472 696e 6720 7375 6d6d 6172   A string summar
+00013720: 697a 696e 6720 7468 6520 4d6f 6475 6c65  izing the Module
+00013730: 2e0a 2020 2020 2222 220a 2020 2020 6672  ..    """.    fr
+00013740: 6f6d 2066 6c61 782e 6c69 6e65 6e20 696d  om flax.linen im
+00013750: 706f 7274 2073 756d 6d61 7279 0a0a 2020  port summary..  
+00013760: 2020 7461 6275 6c61 7465 5f66 6e20 3d20    tabulate_fn = 
+00013770: 7375 6d6d 6172 792e 7461 6275 6c61 7465  summary.tabulate
+00013780: 280a 2020 2020 2020 2020 7365 6c66 2c0a  (.        self,.
+00013790: 2020 2020 2020 2020 726e 6773 2c0a 2020          rngs,.  
+000137a0: 2020 2020 2020 6465 7074 683d 6465 7074        depth=dept
+000137b0: 682c 0a20 2020 2020 2020 2073 686f 775f  h,.        show_
+000137c0: 7265 7065 6174 6564 3d73 686f 775f 7265  repeated=show_re
+000137d0: 7065 6174 6564 2c0a 2020 2020 2020 2020  peated,.        
+000137e0: 6d75 7461 626c 653d 6d75 7461 626c 652c  mutable=mutable,
+000137f0: 0a20 2020 2020 2020 2063 6f6e 736f 6c65  .        console
+00013800: 5f6b 7761 7267 733d 636f 6e73 6f6c 655f  _kwargs=console_
+00013810: 6b77 6172 6773 2c0a 2020 2020 2020 2020  kwargs,.        
+00013820: 7461 626c 655f 6b77 6172 6773 3d74 6162  table_kwargs=tab
+00013830: 6c65 5f6b 7761 7267 732c 0a20 2020 2020  le_kwargs,.     
+00013840: 2020 2063 6f6c 756d 6e5f 6b77 6172 6773     column_kwargs
+00013850: 3d63 6f6c 756d 6e5f 6b77 6172 6773 2c0a  =column_kwargs,.
+00013860: 2020 2020 290a 2020 2020 7265 7475 726e      ).    return
+00013870: 2074 6162 756c 6174 655f 666e 282a 6172   tabulate_fn(*ar
+00013880: 6773 2c20 2a2a 6b77 6172 6773 290a 0a0a  gs, **kwargs)...
+00013890: 5f50 6172 656e 7454 7970 6520 3d20 556e  _ParentType = Un
+000138a0: 696f 6e5b 5479 7065 5b4d 6f64 756c 655d  ion[Type[Module]
+000138b0: 2c20 5479 7065 5b53 636f 7065 5d2c 2054  , Type[Scope], T
+000138c0: 7970 655b 5f53 656e 7469 6e65 6c5d 2c20  ype[_Sentinel], 
+000138d0: 4e6f 6e65 5d0a 0a0a 6465 6620 6d65 7267  None]...def merg
+000138e0: 655f 7061 7261 6d28 6e61 6d65 3a20 7374  e_param(name: st
+000138f0: 722c 2061 3a20 4f70 7469 6f6e 616c 5b54  r, a: Optional[T
+00013900: 5d2c 2062 3a20 4f70 7469 6f6e 616c 5b54  ], b: Optional[T
+00013910: 5d29 202d 3e20 543a 0a20 2022 2222 4d65  ]) -> T:.  """Me
+00013920: 7267 6573 2063 6f6e 7374 7275 6374 696f  rges constructio
+00013930: 6e2d 2061 6e64 2063 616c 6c2d 7469 6d65  n- and call-time
+00013940: 2061 7267 756d 656e 742e 0a0a 2020 5468   argument...  Th
+00013950: 6973 2069 7320 6120 7574 696c 6974 7920  is is a utility 
+00013960: 666f 7220 7375 7070 6f72 7469 6e67 2061  for supporting a
+00013970: 2070 6174 7465 726e 2077 6865 7265 2061   pattern where a
+00013980: 204d 6f64 756c 6520 6879 7065 7270 6172   Module hyperpar
+00013990: 616d 6574 6572 0a20 2063 616e 2062 6520  ameter.  can be 
+000139a0: 7061 7373 6564 2065 6974 6865 7220 746f  passed either to
+000139b0: 2060 605f 5f69 6e69 745f 5f60 6020 6f72   ``__init__`` or
+000139c0: 2060 605f 5f63 616c 6c5f 5f60 602c 2061   ``__call__``, a
+000139d0: 6e64 2074 6865 2076 616c 7565 2074 6861  nd the value tha
+000139e0: 7420 6973 0a20 206e 6f74 2060 4e6f 6e65  t is.  not `None
+000139f0: 6020 7769 6c6c 2062 6520 7573 6564 2e0a  ` will be used..
+00013a00: 0a20 2045 7861 6d70 6c65 3a3a 0a0a 2020  .  Example::..  
+00013a10: 2020 636c 6173 7320 466f 6f28 6e6e 2e4d    class Foo(nn.M
+00013a20: 6f64 756c 6529 3a0a 2020 2020 2020 7472  odule):.      tr
+00013a30: 6169 6e3a 204f 7074 696f 6e61 6c5b 626f  ain: Optional[bo
+00013a40: 6f6c 5d20 3d20 4e6f 6e65 0a0a 2020 2020  ol] = None..    
+00013a50: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
+00013a60: 656c 662c 2074 7261 696e 3a20 4f70 7469  elf, train: Opti
+00013a70: 6f6e 616c 5b62 6f6f 6c5d 203d 204e 6f6e  onal[bool] = Non
+00013a80: 6529 3a0a 2020 2020 2020 2020 7472 6169  e):.        trai
+00013a90: 6e20 3d20 6e6e 2e6d 6572 6765 5f70 6172  n = nn.merge_par
+00013aa0: 616d 2827 7472 6169 6e27 2c20 7365 6c66  am('train', self
+00013ab0: 2e74 7261 696e 2c20 7472 6169 6e29 0a0a  .train, train)..
+00013ac0: 2020 416e 2065 7272 6f72 2069 7320 7468    An error is th
+00013ad0: 726f 776e 2077 6865 6e20 626f 7468 2061  rown when both a
+00013ae0: 7267 756d 656e 7473 2061 7265 2060 4e6f  rguments are `No
+00013af0: 6e65 6020 6f72 2062 6f74 6820 7661 6c75  ne` or both valu
+00013b00: 6573 2061 7265 206e 6f74 0a20 2060 4e6f  es are not.  `No
+00013b10: 6e65 602e 0a0a 2020 4172 6773 3a0a 2020  ne`...  Args:.  
+00013b20: 2020 6e61 6d65 3a20 7468 6520 6e61 6d65    name: the name
+00013b30: 206f 6620 7468 6520 7061 7261 6d65 7465   of the paramete
+00013b40: 722e 2055 7365 6420 666f 7220 6572 726f  r. Used for erro
+00013b50: 7220 6d65 7373 6167 6573 2e0a 2020 2020  r messages..    
+00013b60: 613a 206f 7074 696f 6e20 610a 2020 2020  a: option a.    
+00013b70: 623a 206f 7074 696f 6e20 620a 2020 5265  b: option b.  Re
+00013b80: 7475 726e 733a 0a20 2020 2061 206f 7220  turns:.    a or 
+00013b90: 6220 7768 6963 6865 7665 7220 6973 206e  b whichever is n
+00013ba0: 6f74 2060 4e6f 6e65 602e 0a0a 2020 2222  ot `None`...  ""
+00013bb0: 220a 2020 6966 2061 2069 7320 4e6f 6e65  ".  if a is None
+00013bc0: 2061 6e64 2062 2069 7320 4e6f 6e65 3a0a   and b is None:.
+00013bd0: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00013be0: 7272 6f72 280a 2020 2020 2020 2020 6627  rror(.        f'
+00013bf0: 5061 7261 6d65 7465 7220 227b 6e61 6d65  Parameter "{name
+00013c00: 7d22 206d 7573 7420 6265 2070 6173 7365  }" must be passe
+00013c10: 6420 746f 2074 6865 2063 6f6e 7374 7275  d to the constru
+00013c20: 6374 6f72 206f 7220 6174 2063 616c 6c20  ctor or at call 
+00013c30: 7469 6d65 2e27 0a20 2020 2029 0a20 2069  time.'.    ).  i
+00013c40: 6620 6120 6973 206e 6f74 204e 6f6e 6520  f a is not None 
+00013c50: 616e 6420 6220 6973 206e 6f74 204e 6f6e  and b is not Non
+00013c60: 653a 0a20 2020 2072 6169 7365 2056 616c  e:.    raise Val
+00013c70: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
+00013c80: 2066 2750 6172 616d 6574 6572 2022 7b6e   f'Parameter "{n
+00013c90: 616d 657d 2220 7761 7320 7061 7373 6564  ame}" was passed
+00013ca0: 2074 6f20 7468 6520 636f 6e73 7472 7563   to the construc
+00013cb0: 746f 7220 616e 6420 6174 2063 616c 6c20  tor and at call 
+00013cc0: 7469 6d65 2e27 0a20 2020 2020 2020 2027  time.'.        '
+00013cd0: 2053 686f 756c 6420 6265 2070 6173 7365   Should be passe
+00013ce0: 6420 6a75 7374 206f 6e63 652e 270a 2020  d just once.'.  
+00013cf0: 2020 290a 2020 6966 2061 2069 7320 4e6f    ).  if a is No
+00013d00: 6e65 3a0a 2020 2020 6173 7365 7274 2062  ne:.    assert b
+00013d10: 2069 7320 6e6f 7420 4e6f 6e65 0a20 2020   is not None.   
+00013d20: 2072 6574 7572 6e20 620a 2020 7265 7475   return b.  retu
+00013d30: 726e 2061 0a0a 0a40 7472 6163 6562 6163  rn a...@tracebac
+00013d40: 6b5f 7574 696c 2e61 7069 5f62 6f75 6e64  k_util.api_bound
+00013d50: 6172 790a 6465 6620 6170 706c 7928 0a20  ary.def apply(. 
+00013d60: 2020 2066 6e3a 2043 616c 6c61 626c 655b     fn: Callable[
+00013d70: 2e2e 2e2c 2041 6e79 5d2c 0a20 2020 206d  ..., Any],.    m
+00013d80: 6f64 756c 653a 204d 6f64 756c 652c 0a20  odule: Module,. 
+00013d90: 2020 206d 7574 6162 6c65 3a20 436f 6c6c     mutable: Coll
+00013da0: 6563 7469 6f6e 4669 6c74 6572 203d 2046  ectionFilter = F
+00013db0: 616c 7365 2c0a 2020 2020 6361 7074 7572  alse,.    captur
+00013dc0: 655f 696e 7465 726d 6564 6961 7465 733a  e_intermediates:
+00013dd0: 2055 6e69 6f6e 5b62 6f6f 6c2c 2043 616c   Union[bool, Cal
+00013de0: 6c61 626c 655b 5b4d 6f64 756c 652c 2073  lable[[Module, s
+00013df0: 7472 5d2c 2062 6f6f 6c5d 5d20 3d20 4661  tr], bool]] = Fa
+00013e00: 6c73 652c 0a29 202d 3e20 4361 6c6c 6162  lse,.) -> Callab
+00013e10: 6c65 5b2e 2e2e 2c20 416e 795d 3a0a 2020  le[..., Any]:.  
+00013e20: 2222 2243 7265 6174 6573 2061 6e20 6170  """Creates an ap
+00013e30: 706c 7920 6675 6e63 7469 6f6e 2074 6f20  ply function to 
+00013e40: 6361 6c6c 2060 6066 6e60 6020 7769 7468  call ``fn`` with
+00013e50: 2061 2062 6f75 6e64 206d 6f64 756c 652e   a bound module.
+00013e60: 0a0a 2020 556e 6c69 6b65 2060 604d 6f64  ..  Unlike ``Mod
+00013e70: 756c 652e 6170 706c 7960 6020 7468 6973  ule.apply`` this
+00013e80: 2066 756e 6374 696f 6e20 7265 7475 726e   function return
+00013e90: 7320 6120 6e65 7720 6675 6e63 7469 6f6e  s a new function
+00013ea0: 2077 6974 6820 7468 6520 7369 676e 6174   with the signat
+00013eb0: 7572 650a 2020 6060 2876 6172 6961 626c  ure.  ``(variabl
+00013ec0: 6573 2c20 2a61 7267 732c 2072 6e67 733d  es, *args, rngs=
+00013ed0: 4e6f 6e65 2c20 2a2a 6b77 6172 6773 2920  None, **kwargs) 
+00013ee0: 2d3e 2054 6060 2077 6865 7265 2060 5460  -> T`` where `T`
+00013ef0: 2069 7320 7468 6520 7265 7475 726e 2074   is the return t
+00013f00: 7970 650a 2020 6f66 2060 6066 6e60 602e  ype.  of ``fn``.
+00013f10: 2049 6620 6060 6d75 7461 626c 6560 6020   If ``mutable`` 
+00013f20: 6973 206e 6f74 2060 6046 616c 7365 6060  is not ``False``
+00013f30: 2074 6865 2072 6574 7572 6e20 7479 7065   the return type
+00013f40: 2069 7320 6120 7475 706c 6520 7768 6572   is a tuple wher
+00013f50: 6520 7468 650a 2020 7365 636f 6e64 2069  e the.  second i
+00013f60: 7465 6d20 6973 2061 2060 6046 726f 7a65  tem is a ``Froze
+00013f70: 6e44 6963 7460 6020 7769 7468 2074 6865  nDict`` with the
+00013f80: 206d 7574 6174 6564 2076 6172 6961 626c   mutated variabl
+00013f90: 6573 2e0a 0a20 2054 6865 2061 7070 6c79  es...  The apply
+00013fa0: 2066 756e 6374 696f 6e20 7468 6174 2069   function that i
+00013fb0: 7320 7265 7475 726e 6564 2063 616e 2062  s returned can b
+00013fc0: 6520 6469 7265 6374 6c79 2063 6f6d 706f  e directly compo
+00013fd0: 7365 6420 7769 7468 0a20 204a 4158 2074  sed with.  JAX t
+00013fe0: 7261 6e73 666f 726d 6174 696f 6e73 206c  ransformations l
+00013ff0: 696b 6520 6060 6a61 782e 6a69 7460 603a  ike ``jax.jit``:
+00014000: 3a0a 0a20 2020 2064 6566 2066 2866 6f6f  :..    def f(foo
+00014010: 2c20 7829 3a0a 2020 2020 2020 7a20 3d20  , x):.      z = 
+00014020: 666f 6f2e 656e 636f 6465 2878 290a 2020  foo.encode(x).  
+00014030: 2020 2020 7920 3d20 666f 6f2e 6465 636f      y = foo.deco
+00014040: 6465 287a 290a 2020 2020 2020 2320 2e2e  de(z).      # ..
+00014050: 2e0a 2020 2020 2020 7265 7475 726e 2079  ..      return y
+00014060: 0a0a 2020 2020 666f 6f20 3d20 466f 6f28  ..    foo = Foo(
+00014070: 290a 2020 2020 665f 6a69 7474 6564 203d  ).    f_jitted =
+00014080: 206a 6178 2e6a 6974 286e 6e2e 6170 706c   jax.jit(nn.appl
+00014090: 7928 662c 2066 6f6f 2929 0a20 2020 2066  y(f, foo)).    f
+000140a0: 5f6a 6974 7465 6428 7661 7269 6162 6c65  _jitted(variable
+000140b0: 732c 2078 290a 0a20 2041 7267 733a 0a20  s, x)..  Args:. 
+000140c0: 2020 2066 6e3a 2054 6865 2066 756e 6374     fn: The funct
+000140d0: 696f 6e20 7468 6174 2073 686f 756c 6420  ion that should 
+000140e0: 6265 2061 7070 6c69 6564 2e20 5468 6520  be applied. The 
+000140f0: 6669 7273 7420 6172 6775 6d65 6e74 2070  first argument p
+00014100: 6173 7365 6420 7769 6c6c 0a20 2020 2020  assed will.     
+00014110: 2062 6520 616e 206d 6f64 756c 6520 696e   be an module in
+00014120: 7374 616e 6365 206f 6620 7468 6520 6060  stance of the ``
+00014130: 6d6f 6475 6c65 6060 2077 6974 6820 7661  module`` with va
+00014140: 7269 6162 6c65 7320 616e 6420 524e 4773  riables and RNGs
+00014150: 2062 6f75 6e64 0a20 2020 2020 2074 6f20   bound.      to 
+00014160: 6974 2e0a 2020 2020 6d6f 6475 6c65 3a20  it..    module: 
+00014170: 5468 6520 6060 4d6f 6475 6c65 6060 2074  The ``Module`` t
+00014180: 6861 7420 7769 6c6c 2062 6520 7573 6564  hat will be used
+00014190: 2074 6f20 6269 6e64 2076 6172 6961 626c   to bind variabl
+000141a0: 6573 2061 6e64 2052 4e47 7320 746f 2e0a  es and RNGs to..
+000141b0: 2020 2020 2020 5468 6520 6060 4d6f 6475        The ``Modu
+000141c0: 6c65 6060 2070 6173 7365 6420 6173 2074  le`` passed as t
+000141d0: 6865 2066 6972 7374 2061 7267 756d 656e  he first argumen
+000141e0: 7420 746f 2060 6066 6e60 6020 7769 6c6c  t to ``fn`` will
+000141f0: 2062 6520 6120 636c 6f6e 650a 2020 2020   be a clone.    
+00014200: 2020 6f66 206d 6f64 756c 652e 0a20 2020    of module..   
+00014210: 206d 7574 6162 6c65 3a20 4361 6e20 6265   mutable: Can be
+00014220: 2062 6f6f 6c2c 2073 7472 2c20 6f72 206c   bool, str, or l
+00014230: 6973 742e 2053 7065 6369 6669 6573 2077  ist. Specifies w
+00014240: 6869 6368 2063 6f6c 6c65 6374 696f 6e73  hich collections
+00014250: 2073 686f 756c 6420 6265 0a20 2020 2020   should be.     
+00014260: 2074 7265 6174 6564 2061 7320 6d75 7461   treated as muta
+00014270: 626c 653a 2060 6062 6f6f 6c60 603a 2061  ble: ``bool``: a
+00014280: 6c6c 2f6e 6f20 636f 6c6c 6563 7469 6f6e  ll/no collection
+00014290: 7320 6172 6520 6d75 7461 626c 652e 0a20  s are mutable.. 
+000142a0: 2020 2020 2060 6073 7472 6060 3a20 5468       ``str``: Th
+000142b0: 6520 6e61 6d65 206f 6620 6120 7369 6e67  e name of a sing
+000142c0: 6c65 206d 7574 6162 6c65 2063 6f6c 6c65  le mutable colle
+000142d0: 6374 696f 6e2e 2060 606c 6973 7460 603a  ction. ``list``:
+000142e0: 2041 0a20 2020 2020 206c 6973 7420 6f66   A.      list of
+000142f0: 206e 616d 6573 206f 6620 6d75 7461 626c   names of mutabl
+00014300: 6520 636f 6c6c 6563 7469 6f6e 732e 0a20  e collections.. 
+00014310: 2020 2063 6170 7475 7265 5f69 6e74 6572     capture_inter
+00014320: 6d65 6469 6174 6573 3a20 4966 2060 5472  mediates: If `Tr
+00014330: 7565 602c 2063 6170 7475 7265 7320 696e  ue`, captures in
+00014340: 7465 726d 6564 6961 7465 2072 6574 7572  termediate retur
+00014350: 6e20 7661 6c75 6573 0a20 2020 2020 206f  n values.      o
+00014360: 6620 616c 6c20 4d6f 6475 6c65 7320 696e  f all Modules in
+00014370: 7369 6465 2074 6865 2022 696e 7465 726d  side the "interm
+00014380: 6564 6961 7465 7322 2063 6f6c 6c65 6374  ediates" collect
+00014390: 696f 6e2e 2042 7920 6465 6661 756c 7420  ion. By default 
+000143a0: 6f6e 6c79 0a20 2020 2020 2074 6865 2072  only.      the r
+000143b0: 6574 7572 6e20 7661 6c75 6573 206f 6620  eturn values of 
+000143c0: 616c 6c20 605f 5f63 616c 6c5f 5f60 206d  all `__call__` m
+000143d0: 6574 686f 6473 2061 7265 2073 746f 7265  ethods are store
+000143e0: 642e 2041 2066 756e 6374 696f 6e20 6361  d. A function ca
+000143f0: 6e0a 2020 2020 2020 6265 2070 6173 7365  n.      be passe
+00014400: 6420 746f 2063 6861 6e67 6520 7468 6520  d to change the 
+00014410: 6669 6c74 6572 2062 6568 6176 696f 722e  filter behavior.
+00014420: 2054 6865 2066 696c 7465 7220 6675 6e63   The filter func
+00014430: 7469 6f6e 2074 616b 6573 0a20 2020 2020  tion takes.     
+00014440: 2074 6865 204d 6f64 756c 6520 696e 7374   the Module inst
+00014450: 616e 6365 2061 6e64 206d 6574 686f 6420  ance and method 
+00014460: 6e61 6d65 2061 6e64 2072 6574 7572 6e73  name and returns
+00014470: 2061 2062 6f6f 6c20 696e 6469 6361 7469   a bool indicati
+00014480: 6e67 0a20 2020 2020 2077 6865 7468 6572  ng.      whether
+00014490: 2074 6865 206f 7574 7075 7420 6f66 2074   the output of t
+000144a0: 6861 7420 6d65 7468 6f64 2069 6e76 6f63  hat method invoc
+000144b0: 6174 696f 6e20 7368 6f75 6c64 2062 6520  ation should be 
+000144c0: 7374 6f72 6564 2e0a 2020 5265 7475 726e  stored..  Return
+000144d0: 733a 0a20 2020 2054 6865 2061 7070 6c79  s:.    The apply
+000144e0: 2066 756e 6374 696f 6e20 7772 6170 7069   function wrappi
+000144f0: 6e67 2060 6066 6e60 602e 0a20 2022 2222  ng ``fn``..  """
+00014500: 0a0a 2020 4066 756e 6374 6f6f 6c73 2e77  ..  @functools.w
+00014510: 7261 7073 2866 6e29 0a20 2064 6566 2073  raps(fn).  def s
+00014520: 636f 7065 5f66 6e28 7363 6f70 652c 202a  cope_fn(scope, *
+00014530: 6172 6773 2c20 2a2a 6b77 6172 6773 293a  args, **kwargs):
+00014540: 0a20 2020 205f 636f 6e74 6578 742e 6361  .    _context.ca
+00014550: 7074 7572 655f 7374 6163 6b2e 6170 7065  pture_stack.appe
+00014560: 6e64 2863 6170 7475 7265 5f69 6e74 6572  nd(capture_inter
+00014570: 6d65 6469 6174 6573 290a 2020 2020 7472  mediates).    tr
+00014580: 793a 0a20 2020 2020 2072 6574 7572 6e20  y:.      return 
+00014590: 666e 286d 6f64 756c 652e 636c 6f6e 6528  fn(module.clone(
+000145a0: 7061 7265 6e74 3d73 636f 7065 2c20 5f64  parent=scope, _d
+000145b0: 6565 705f 636c 6f6e 653d 5472 7565 292c  eep_clone=True),
+000145c0: 202a 6172 6773 2c20 2a2a 6b77 6172 6773   *args, **kwargs
+000145d0: 290a 2020 2020 6669 6e61 6c6c 793a 0a20  ).    finally:. 
+000145e0: 2020 2020 205f 636f 6e74 6578 742e 6361       _context.ca
+000145f0: 7074 7572 655f 7374 6163 6b2e 706f 7028  pture_stack.pop(
+00014600: 290a 0a20 2069 6620 6361 7074 7572 655f  )..  if capture_
+00014610: 696e 7465 726d 6564 6961 7465 7320 6973  intermediates is
+00014620: 2054 7275 653a 2020 2320 7079 6c69 6e74   True:  # pylint
+00014630: 3a20 6469 7361 626c 653d 672d 626f 6f6c  : disable=g-bool
+00014640: 2d69 642d 636f 6d70 6172 6973 6f6e 0a20  -id-comparison. 
+00014650: 2020 2063 6170 7475 7265 5f69 6e74 6572     capture_inter
+00014660: 6d65 6469 6174 6573 203d 2063 6170 7475  mediates = captu
+00014670: 7265 5f63 616c 6c5f 696e 7465 726d 6564  re_call_intermed
+00014680: 6961 7465 730a 2020 6966 2063 6170 7475  iates.  if captu
+00014690: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
+000146a0: 3a0a 2020 2020 6d75 7461 626c 6520 3d20  :.    mutable = 
+000146b0: 756e 696f 6e5f 6669 6c74 6572 7328 6d75  union_filters(mu
+000146c0: 7461 626c 652c 2027 696e 7465 726d 6564  table, 'intermed
+000146d0: 6961 7465 7327 290a 2020 7265 7475 726e  iates').  return
+000146e0: 2063 6f72 652e 6170 706c 7928 7363 6f70   core.apply(scop
+000146f0: 655f 666e 2c20 6d75 7461 626c 653d 6d75  e_fn, mutable=mu
+00014700: 7461 626c 6529 0a0a 0a40 7472 6163 6562  table)...@traceb
+00014710: 6163 6b5f 7574 696c 2e61 7069 5f62 6f75  ack_util.api_bou
+00014720: 6e64 6172 790a 6465 6620 696e 6974 5f77  ndary.def init_w
+00014730: 6974 685f 6f75 7470 7574 280a 2020 2020  ith_output(.    
+00014740: 666e 3a20 4361 6c6c 6162 6c65 5b2e 2e2e  fn: Callable[...
+00014750: 2c20 416e 795d 2c0a 2020 2020 6d6f 6475  , Any],.    modu
+00014760: 6c65 3a20 4d6f 6475 6c65 2c0a 2020 2020  le: Module,.    
+00014770: 6d75 7461 626c 653a 2043 6f6c 6c65 6374  mutable: Collect
+00014780: 696f 6e46 696c 7465 7220 3d20 4465 6e79  ionFilter = Deny
+00014790: 4c69 7374 2827 696e 7465 726d 6564 6961  List('intermedia
+000147a0: 7465 7327 292c 0a20 2020 2063 6170 7475  tes'),.    captu
+000147b0: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
+000147c0: 3a20 556e 696f 6e5b 626f 6f6c 2c20 4361  : Union[bool, Ca
+000147d0: 6c6c 6162 6c65 5b5b 4d6f 6475 6c65 2c20  llable[[Module, 
+000147e0: 7374 725d 2c20 626f 6f6c 5d5d 203d 2046  str], bool]] = F
+000147f0: 616c 7365 2c0a 2920 2d3e 2043 616c 6c61  alse,.) -> Calla
+00014800: 626c 655b 2e2e 2e2c 2054 7570 6c65 5b41  ble[..., Tuple[A
+00014810: 6e79 2c20 556e 696f 6e5b 4672 6f7a 656e  ny, Union[Frozen
+00014820: 5661 7269 6162 6c65 4469 6374 2c20 4469  VariableDict, Di
+00014830: 6374 5b73 7472 2c20 416e 795d 5d5d 5d3a  ct[str, Any]]]]:
+00014840: 0a20 2022 2222 4372 6561 7465 7320 616e  .  """Creates an
+00014850: 2069 6e69 7420 6675 6e63 7469 6f6e 2074   init function t
+00014860: 6f20 6361 6c6c 2060 6066 6e60 6020 7769  o call ``fn`` wi
+00014870: 7468 2061 2062 6f75 6e64 206d 6f64 756c  th a bound modul
+00014880: 6520 7468 6174 2061 6c73 6f20 7265 7475  e that also retu
+00014890: 726e 7320 7468 6520 6675 6e63 7469 6f6e  rns the function
+000148a0: 206f 7574 7075 7473 2e0a 0a20 2055 6e6c   outputs...  Unl
+000148b0: 696b 6520 6060 4d6f 6475 6c65 2e69 6e69  ike ``Module.ini
+000148c0: 745f 7769 7468 5f6f 7574 7075 7460 6020  t_with_output`` 
+000148d0: 7468 6973 2066 756e 6374 696f 6e20 7265  this function re
+000148e0: 7475 726e 7320 6120 6e65 7720 6675 6e63  turns a new func
+000148f0: 7469 6f6e 2077 6974 6820 7468 6520 7369  tion with the si
+00014900: 676e 6174 7572 650a 2020 6060 2872 6e67  gnature.  ``(rng
+00014910: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
+00014920: 6773 2920 2d3e 2028 542c 2076 6172 6961  gs) -> (T, varia
+00014930: 626c 6573 2960 6020 7768 6572 6520 6054  bles)`` where `T
+00014940: 6020 6973 2074 6865 2072 6574 7572 6e20  ` is the return 
+00014950: 7479 7065 206f 6620 6060 666e 6060 2e0a  type of ``fn``..
+00014960: 2020 5468 6520 726e 6773 2063 616e 2062    The rngs can b
+00014970: 6520 6120 6469 6374 206f 6620 5052 4e47  e a dict of PRNG
+00014980: 4b65 7973 206f 7220 6120 7369 6e67 6c65  Keys or a single
+00014990: 2060 6060 5052 4e47 4b65 7960 6020 7768   ```PRNGKey`` wh
+000149a0: 6963 6820 6973 0a20 2065 7175 6976 616c  ich is.  equival
+000149b0: 656e 7420 746f 2070 6173 7369 6e67 2061  ent to passing a
+000149c0: 2064 6963 7420 7769 7468 206f 6e65 2050   dict with one P
+000149d0: 524e 474b 6579 2077 6974 6820 7468 6520  RNGKey with the 
+000149e0: 6e61 6d65 2022 7061 7261 6d73 222e 0a0a  name "params"...
+000149f0: 2020 5468 6520 696e 6974 2066 756e 6374    The init funct
+00014a00: 696f 6e20 7468 6174 2069 7320 7265 7475  ion that is retu
+00014a10: 726e 6564 2063 616e 2062 6520 6469 7265  rned can be dire
+00014a20: 6374 6c79 2063 6f6d 706f 7365 6420 7769  ctly composed wi
+00014a30: 7468 0a20 204a 4158 2074 7261 6e73 666f  th.  JAX transfo
+00014a40: 726d 6174 696f 6e73 206c 696b 6520 6060  rmations like ``
+00014a50: 6a61 782e 6a69 7460 603a 3a0a 0a20 2020  jax.jit``::..   
+00014a60: 2064 6566 2066 2866 6f6f 2c20 7829 3a0a   def f(foo, x):.
+00014a70: 2020 2020 2020 7a20 3d20 666f 6f2e 656e        z = foo.en
+00014a80: 636f 6465 2878 290a 2020 2020 2020 7920  code(x).      y 
+00014a90: 3d20 666f 6f2e 6465 636f 6465 287a 290a  = foo.decode(z).
+00014aa0: 2020 2020 2020 2320 2e2e 2e0a 2020 2020        # ....    
+00014ab0: 2020 7265 7475 726e 2079 0a0a 2020 2020    return y..    
+00014ac0: 666f 6f20 3d20 466f 6f28 290a 2020 2020  foo = Foo().    
+00014ad0: 665f 6a69 7474 6564 203d 206a 6178 2e6a  f_jitted = jax.j
+00014ae0: 6974 286e 6e2e 696e 6974 5f77 6974 685f  it(nn.init_with_
+00014af0: 6f75 7470 7574 2866 2c20 666f 6f29 290a  output(f, foo)).
+00014b00: 2020 2020 792c 2076 6172 6961 626c 6573      y, variables
+00014b10: 203d 2066 5f6a 6974 7465 6428 726e 672c   = f_jitted(rng,
+00014b20: 2078 290a 0a20 2041 7267 733a 0a20 2020   x)..  Args:.   
+00014b30: 2066 6e3a 2054 6865 2066 756e 6374 696f   fn: The functio
+00014b40: 6e20 7468 6174 2073 686f 756c 6420 6265  n that should be
+00014b50: 2061 7070 6c69 6564 2e20 5468 6520 6669   applied. The fi
+00014b60: 7273 7420 6172 6775 6d65 6e74 2070 6173  rst argument pas
+00014b70: 7365 6420 7769 6c6c 0a20 2020 2020 2062  sed will.      b
+00014b80: 6520 616e 206d 6f64 756c 6520 696e 7374  e an module inst
+00014b90: 616e 6365 206f 6620 7468 6520 6060 6d6f  ance of the ``mo
+00014ba0: 6475 6c65 6060 2077 6974 6820 7661 7269  dule`` with vari
+00014bb0: 6162 6c65 7320 616e 6420 524e 4773 2062  ables and RNGs b
+00014bc0: 6f75 6e64 0a20 2020 2020 2074 6f20 6974  ound.      to it
+00014bd0: 2e0a 2020 2020 6d6f 6475 6c65 3a20 5468  ..    module: Th
+00014be0: 6520 6060 4d6f 6475 6c65 6060 2074 6861  e ``Module`` tha
+00014bf0: 7420 7769 6c6c 2062 6520 7573 6564 2074  t will be used t
+00014c00: 6f20 6269 6e64 2076 6172 6961 626c 6573  o bind variables
+00014c10: 2061 6e64 2052 4e47 7320 746f 2e0a 2020   and RNGs to..  
+00014c20: 2020 2020 5468 6520 6060 4d6f 6475 6c65      The ``Module
+00014c30: 6060 2070 6173 7365 6420 6173 2074 6865  `` passed as the
+00014c40: 2066 6972 7374 2061 7267 756d 656e 7420   first argument 
+00014c50: 746f 2060 6066 6e60 6020 7769 6c6c 2062  to ``fn`` will b
+00014c60: 6520 6120 636c 6f6e 650a 2020 2020 2020  e a clone.      
+00014c70: 6f66 206d 6f64 756c 652e 0a20 2020 206d  of module..    m
+00014c80: 7574 6162 6c65 3a20 4361 6e20 6265 2062  utable: Can be b
+00014c90: 6f6f 6c2c 2073 7472 2c20 6f72 206c 6973  ool, str, or lis
+00014ca0: 742e 2053 7065 6369 6669 6573 2077 6869  t. Specifies whi
+00014cb0: 6368 2063 6f6c 6c65 6374 696f 6e73 2073  ch collections s
+00014cc0: 686f 756c 6420 6265 0a20 2020 2020 2074  hould be.      t
+00014cd0: 7265 6174 6564 2061 7320 6d75 7461 626c  reated as mutabl
+00014ce0: 653a 2060 6062 6f6f 6c60 603a 2061 6c6c  e: ``bool``: all
+00014cf0: 2f6e 6f20 636f 6c6c 6563 7469 6f6e 7320  /no collections 
+00014d00: 6172 6520 6d75 7461 626c 652e 0a20 2020  are mutable..   
+00014d10: 2020 2060 6073 7472 6060 3a20 5468 6520     ``str``: The 
+00014d20: 6e61 6d65 206f 6620 6120 7369 6e67 6c65  name of a single
+00014d30: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
+00014d40: 696f 6e2e 2060 606c 6973 7460 603a 2041  ion. ``list``: A
+00014d50: 0a20 2020 2020 206c 6973 7420 6f66 206e  .      list of n
+00014d60: 616d 6573 206f 6620 6d75 7461 626c 6520  ames of mutable 
+00014d70: 636f 6c6c 6563 7469 6f6e 732e 2042 7920  collections. By 
+00014d80: 6465 6661 756c 7420 616c 6c20 636f 6c6c  default all coll
+00014d90: 6563 7469 6f6e 730a 2020 2020 2020 6578  ections.      ex
+00014da0: 6365 7074 2022 696e 7465 726d 6564 6961  cept "intermedia
+00014db0: 7465 7322 2061 7265 206d 7574 6162 6c65  tes" are mutable
+00014dc0: 2e0a 2020 2020 6361 7074 7572 655f 696e  ..    capture_in
+00014dd0: 7465 726d 6564 6961 7465 733a 2049 6620  termediates: If 
+00014de0: 6054 7275 6560 2c20 6361 7074 7572 6573  `True`, captures
+00014df0: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
+00014e00: 7475 726e 2076 616c 7565 730a 2020 2020  turn values.    
+00014e10: 2020 6f66 2061 6c6c 204d 6f64 756c 6573    of all Modules
+00014e20: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
+00014e30: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
+00014e40: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
+00014e50: 6c74 206f 6e6c 790a 2020 2020 2020 7468  lt only.      th
+00014e60: 6520 7265 7475 726e 2076 616c 7565 7320  e return values 
+00014e70: 6f66 2061 6c6c 2060 5f5f 6361 6c6c 5f5f  of all `__call__
+00014e80: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
+00014e90: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
+00014ea0: 2063 616e 0a20 2020 2020 2062 6520 7061   can.      be pa
+00014eb0: 7373 6564 2074 6f20 6368 616e 6765 2074  ssed to change t
+00014ec0: 6865 2066 696c 7465 7220 6265 6861 7669  he filter behavi
+00014ed0: 6f72 2e20 5468 6520 6669 6c74 6572 2066  or. The filter f
+00014ee0: 756e 6374 696f 6e20 7461 6b65 730a 2020  unction takes.  
+00014ef0: 2020 2020 7468 6520 4d6f 6475 6c65 2069      the Module i
+00014f00: 6e73 7461 6e63 6520 616e 6420 6d65 7468  nstance and meth
+00014f10: 6f64 206e 616d 6520 616e 6420 7265 7475  od name and retu
+00014f20: 726e 7320 6120 626f 6f6c 2069 6e64 6963  rns a bool indic
+00014f30: 6174 696e 670a 2020 2020 2020 7768 6574  ating.      whet
+00014f40: 6865 7220 7468 6520 6f75 7470 7574 206f  her the output o
+00014f50: 6620 7468 6174 206d 6574 686f 6420 696e  f that method in
+00014f60: 766f 6361 7469 6f6e 2073 686f 756c 6420  vocation should 
+00014f70: 6265 2073 746f 7265 642e 0a20 2052 6574  be stored..  Ret
+00014f80: 7572 6e73 3a0a 2020 2020 5468 6520 696e  urns:.    The in
+00014f90: 6974 2066 756e 6374 696f 6e20 7772 6170  it function wrap
+00014fa0: 7069 6e67 2060 6066 6e60 602e 0a20 2022  ping ``fn``..  "
+00014fb0: 2222 0a0a 2020 4066 756e 6374 6f6f 6c73  ""..  @functools
+00014fc0: 2e77 7261 7073 2866 6e29 0a20 2064 6566  .wraps(fn).  def
+00014fd0: 2073 636f 7065 5f66 6e28 7363 6f70 652c   scope_fn(scope,
+00014fe0: 202a 6172 6773 2c20 2a2a 6b77 6172 6773   *args, **kwargs
+00014ff0: 293a 0a20 2020 205f 636f 6e74 6578 742e  ):.    _context.
+00015000: 6361 7074 7572 655f 7374 6163 6b2e 6170  capture_stack.ap
+00015010: 7065 6e64 2863 6170 7475 7265 5f69 6e74  pend(capture_int
+00015020: 6572 6d65 6469 6174 6573 290a 2020 2020  ermediates).    
+00015030: 7472 793a 0a20 2020 2020 2072 6574 7572  try:.      retur
+00015040: 6e20 666e 286d 6f64 756c 652e 636c 6f6e  n fn(module.clon
+00015050: 6528 7061 7265 6e74 3d73 636f 7065 2c20  e(parent=scope, 
+00015060: 5f64 6565 705f 636c 6f6e 653d 5472 7565  _deep_clone=True
+00015070: 292c 202a 6172 6773 2c20 2a2a 6b77 6172  ), *args, **kwar
+00015080: 6773 290a 2020 2020 6669 6e61 6c6c 793a  gs).    finally:
+00015090: 0a20 2020 2020 205f 636f 6e74 6578 742e  .      _context.
+000150a0: 6361 7074 7572 655f 7374 6163 6b2e 706f  capture_stack.po
+000150b0: 7028 290a 0a20 2069 6620 6361 7074 7572  p()..  if captur
+000150c0: 655f 696e 7465 726d 6564 6961 7465 7320  e_intermediates 
+000150d0: 6973 2054 7275 653a 2020 2320 7079 6c69  is True:  # pyli
+000150e0: 6e74 3a20 6469 7361 626c 653d 672d 626f  nt: disable=g-bo
+000150f0: 6f6c 2d69 642d 636f 6d70 6172 6973 6f6e  ol-id-comparison
+00015100: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
+00015110: 6572 6d65 6469 6174 6573 203d 2063 6170  ermediates = cap
+00015120: 7475 7265 5f63 616c 6c5f 696e 7465 726d  ture_call_interm
+00015130: 6564 6961 7465 730a 2020 6966 2063 6170  ediates.  if cap
+00015140: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+00015150: 6573 3a0a 2020 2020 6d75 7461 626c 6520  es:.    mutable 
+00015160: 3d20 756e 696f 6e5f 6669 6c74 6572 7328  = union_filters(
+00015170: 6d75 7461 626c 652c 2027 696e 7465 726d  mutable, 'interm
+00015180: 6564 6961 7465 7327 290a 2020 7265 7475  ediates').  retu
+00015190: 726e 2063 6f72 652e 696e 6974 2873 636f  rn core.init(sco
+000151a0: 7065 5f66 6e2c 206d 7574 6162 6c65 3d6d  pe_fn, mutable=m
+000151b0: 7574 6162 6c65 290a 0a0a 4074 7261 6365  utable)...@trace
+000151c0: 6261 636b 5f75 7469 6c2e 6170 695f 626f  back_util.api_bo
+000151d0: 756e 6461 7279 0a64 6566 2069 6e69 7428  undary.def init(
+000151e0: 0a20 2020 2066 6e3a 2043 616c 6c61 626c  .    fn: Callabl
+000151f0: 655b 2e2e 2e2c 2041 6e79 5d2c 0a20 2020  e[..., Any],.   
+00015200: 206d 6f64 756c 653a 204d 6f64 756c 652c   module: Module,
+00015210: 0a20 2020 206d 7574 6162 6c65 3a20 436f  .    mutable: Co
+00015220: 6c6c 6563 7469 6f6e 4669 6c74 6572 203d  llectionFilter =
+00015230: 2044 656e 794c 6973 7428 2769 6e74 6572   DenyList('inter
+00015240: 6d65 6469 6174 6573 2729 2c0a 2020 2020  mediates'),.    
+00015250: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+00015260: 6961 7465 733a 2055 6e69 6f6e 5b62 6f6f  iates: Union[boo
+00015270: 6c2c 2043 616c 6c61 626c 655b 5b4d 6f64  l, Callable[[Mod
+00015280: 756c 652c 2073 7472 5d2c 2062 6f6f 6c5d  ule, str], bool]
+00015290: 5d20 3d20 4661 6c73 652c 0a29 202d 3e20  ] = False,.) -> 
+000152a0: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 556e  Callable[..., Un
+000152b0: 696f 6e5b 4672 6f7a 656e 5661 7269 6162  ion[FrozenVariab
+000152c0: 6c65 4469 6374 2c20 4469 6374 5b73 7472  leDict, Dict[str
+000152d0: 2c20 416e 795d 5d5d 3a0a 2020 2222 2243  , Any]]]:.  """C
+000152e0: 7265 6174 6573 2061 6e20 696e 6974 2066  reates an init f
+000152f0: 756e 6374 696f 6e20 746f 2063 616c 6c20  unction to call 
+00015300: 6060 666e 6060 2077 6974 6820 6120 626f  ``fn`` with a bo
+00015310: 756e 6420 6d6f 6475 6c65 2e0a 0a20 2055  und module...  U
+00015320: 6e6c 696b 6520 6060 4d6f 6475 6c65 2e69  nlike ``Module.i
+00015330: 6e69 7460 6020 7468 6973 2066 756e 6374  nit`` this funct
+00015340: 696f 6e20 7265 7475 726e 7320 6120 6e65  ion returns a ne
+00015350: 7720 6675 6e63 7469 6f6e 2077 6974 6820  w function with 
+00015360: 7468 6520 7369 676e 6174 7572 650a 2020  the signature.  
+00015370: 6060 2872 6e67 732c 202a 6172 6773 2c20  ``(rngs, *args, 
+00015380: 2a2a 6b77 6172 6773 2920 2d3e 2076 6172  **kwargs) -> var
+00015390: 6961 626c 6573 6060 2e0a 2020 5468 6520  iables``..  The 
+000153a0: 726e 6773 2063 616e 2062 6520 6120 6469  rngs can be a di
+000153b0: 6374 206f 6620 5052 4e47 4b65 7973 206f  ct of PRNGKeys o
+000153c0: 7220 6120 7369 6e67 6c65 2060 6060 5052  r a single ```PR
+000153d0: 4e47 4b65 7960 6020 7768 6963 6820 6973  NGKey`` which is
+000153e0: 0a20 2065 7175 6976 616c 656e 7420 746f  .  equivalent to
+000153f0: 2070 6173 7369 6e67 2061 2064 6963 7420   passing a dict 
+00015400: 7769 7468 206f 6e65 2050 524e 474b 6579  with one PRNGKey
+00015410: 2077 6974 6820 7468 6520 6e61 6d65 2022   with the name "
+00015420: 7061 7261 6d73 222e 0a0a 2020 5468 6520  params"...  The 
+00015430: 696e 6974 2066 756e 6374 696f 6e20 7468  init function th
+00015440: 6174 2069 7320 7265 7475 726e 6564 2063  at is returned c
+00015450: 616e 2062 6520 6469 7265 6374 6c79 2063  an be directly c
+00015460: 6f6d 706f 7365 6420 7769 7468 0a20 204a  omposed with.  J
+00015470: 4158 2074 7261 6e73 666f 726d 6174 696f  AX transformatio
+00015480: 6e73 206c 696b 6520 6060 6a61 782e 6a69  ns like ``jax.ji
+00015490: 7460 603a 3a0a 0a20 2020 2064 6566 2066  t``::..    def f
+000154a0: 2866 6f6f 2c20 7829 3a0a 2020 2020 2020  (foo, x):.      
+000154b0: 7a20 3d20 666f 6f2e 656e 636f 6465 2878  z = foo.encode(x
+000154c0: 290a 2020 2020 2020 7920 3d20 666f 6f2e  ).      y = foo.
+000154d0: 6465 636f 6465 287a 290a 2020 2020 2020  decode(z).      
+000154e0: 2320 2e2e 2e0a 2020 2020 2020 7265 7475  # ....      retu
+000154f0: 726e 2079 0a0a 2020 2020 666f 6f20 3d20  rn y..    foo = 
+00015500: 466f 6f28 290a 2020 2020 665f 6a69 7474  Foo().    f_jitt
+00015510: 6564 203d 206a 6178 2e6a 6974 286e 6e2e  ed = jax.jit(nn.
+00015520: 696e 6974 2866 2c20 666f 6f29 290a 2020  init(f, foo)).  
+00015530: 2020 7661 7269 6162 6c65 7320 3d20 665f    variables = f_
+00015540: 6a69 7474 6564 2872 6e67 2c20 7829 0a0a  jitted(rng, x)..
+00015550: 2020 4172 6773 3a0a 2020 2020 666e 3a20    Args:.    fn: 
+00015560: 5468 6520 6675 6e63 7469 6f6e 2074 6861  The function tha
+00015570: 7420 7368 6f75 6c64 2062 6520 6170 706c  t should be appl
+00015580: 6965 642e 2054 6865 2066 6972 7374 2061  ied. The first a
+00015590: 7267 756d 656e 7420 7061 7373 6564 2077  rgument passed w
+000155a0: 696c 6c0a 2020 2020 2020 6265 2061 6e20  ill.      be an 
+000155b0: 6d6f 6475 6c65 2069 6e73 7461 6e63 6520  module instance 
+000155c0: 6f66 2074 6865 2060 606d 6f64 756c 6560  of the ``module`
+000155d0: 6020 7769 7468 2076 6172 6961 626c 6573  ` with variables
+000155e0: 2061 6e64 2052 4e47 7320 626f 756e 640a   and RNGs bound.
+000155f0: 2020 2020 2020 746f 2069 742e 0a20 2020        to it..   
+00015600: 206d 6f64 756c 653a 2054 6865 2060 604d   module: The ``M
+00015610: 6f64 756c 6560 6020 7468 6174 2077 696c  odule`` that wil
+00015620: 6c20 6265 2075 7365 6420 746f 2062 696e  l be used to bin
+00015630: 6420 7661 7269 6162 6c65 7320 616e 6420  d variables and 
+00015640: 524e 4773 2074 6f2e 0a20 2020 2020 2054  RNGs to..      T
+00015650: 6865 2060 604d 6f64 756c 6560 6020 7061  he ``Module`` pa
+00015660: 7373 6564 2061 7320 7468 6520 6669 7273  ssed as the firs
+00015670: 7420 6172 6775 6d65 6e74 2074 6f20 6060  t argument to ``
+00015680: 666e 6060 2077 696c 6c20 6265 2061 2063  fn`` will be a c
+00015690: 6c6f 6e65 0a20 2020 2020 206f 6620 6d6f  lone.      of mo
+000156a0: 6475 6c65 2e0a 2020 2020 6d75 7461 626c  dule..    mutabl
+000156b0: 653a 2043 616e 2062 6520 626f 6f6c 2c20  e: Can be bool, 
+000156c0: 7374 722c 206f 7220 6c69 7374 2e20 5370  str, or list. Sp
+000156d0: 6563 6966 6965 7320 7768 6963 6820 636f  ecifies which co
+000156e0: 6c6c 6563 7469 6f6e 7320 7368 6f75 6c64  llections should
+000156f0: 2062 650a 2020 2020 2020 7472 6561 7465   be.      treate
+00015700: 6420 6173 206d 7574 6162 6c65 3a20 6060  d as mutable: ``
+00015710: 626f 6f6c 6060 3a20 616c 6c2f 6e6f 2063  bool``: all/no c
+00015720: 6f6c 6c65 6374 696f 6e73 2061 7265 206d  ollections are m
+00015730: 7574 6162 6c65 2e0a 2020 2020 2020 6060  utable..      ``
+00015740: 7374 7260 603a 2054 6865 206e 616d 6520  str``: The name 
+00015750: 6f66 2061 2073 696e 676c 6520 6d75 7461  of a single muta
+00015760: 626c 6520 636f 6c6c 6563 7469 6f6e 2e20  ble collection. 
+00015770: 6060 6c69 7374 6060 3a20 410a 2020 2020  ``list``: A.    
+00015780: 2020 6c69 7374 206f 6620 6e61 6d65 7320    list of names 
+00015790: 6f66 206d 7574 6162 6c65 2063 6f6c 6c65  of mutable colle
+000157a0: 6374 696f 6e73 2e20 4279 2064 6566 6175  ctions. By defau
+000157b0: 6c74 2061 6c6c 2063 6f6c 6c65 6374 696f  lt all collectio
+000157c0: 6e73 0a20 2020 2020 2065 7863 6570 7420  ns.      except 
+000157d0: 2269 6e74 6572 6d65 6469 6174 6573 2220  "intermediates" 
+000157e0: 6172 6520 6d75 7461 626c 652e 0a20 2020  are mutable..   
+000157f0: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
+00015800: 6469 6174 6573 3a20 4966 2060 5472 7565  diates: If `True
+00015810: 602c 2063 6170 7475 7265 7320 696e 7465  `, captures inte
+00015820: 726d 6564 6961 7465 2072 6574 7572 6e20  rmediate return 
+00015830: 7661 6c75 6573 0a20 2020 2020 206f 6620  values.      of 
+00015840: 616c 6c20 4d6f 6475 6c65 7320 696e 7369  all Modules insi
+00015850: 6465 2074 6865 2022 696e 7465 726d 6564  de the "intermed
+00015860: 6961 7465 7322 2063 6f6c 6c65 6374 696f  iates" collectio
+00015870: 6e2e 2042 7920 6465 6661 756c 7420 6f6e  n. By default on
+00015880: 6c79 0a20 2020 2020 2074 6865 2072 6574  ly.      the ret
+00015890: 7572 6e20 7661 6c75 6573 206f 6620 616c  urn values of al
+000158a0: 6c20 605f 5f63 616c 6c5f 5f60 206d 6574  l `__call__` met
+000158b0: 686f 6473 2061 7265 2073 746f 7265 642e  hods are stored.
+000158c0: 2041 2066 756e 6374 696f 6e20 6361 6e0a   A function can.
+000158d0: 2020 2020 2020 6265 2070 6173 7365 6420        be passed 
+000158e0: 746f 2063 6861 6e67 6520 7468 6520 6669  to change the fi
+000158f0: 6c74 6572 2062 6568 6176 696f 722e 2054  lter behavior. T
+00015900: 6865 2066 696c 7465 7220 6675 6e63 7469  he filter functi
+00015910: 6f6e 2074 616b 6573 0a20 2020 2020 2074  on takes.      t
+00015920: 6865 204d 6f64 756c 6520 696e 7374 616e  he Module instan
+00015930: 6365 2061 6e64 206d 6574 686f 6420 6e61  ce and method na
+00015940: 6d65 2061 6e64 2072 6574 7572 6e73 2061  me and returns a
+00015950: 2062 6f6f 6c20 696e 6469 6361 7469 6e67   bool indicating
+00015960: 0a20 2020 2020 2077 6865 7468 6572 2074  .      whether t
+00015970: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
+00015980: 7420 6d65 7468 6f64 2069 6e76 6f63 6174  t method invocat
+00015990: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
+000159a0: 6f72 6564 2e0a 2020 5265 7475 726e 733a  ored..  Returns:
+000159b0: 0a20 2020 2054 6865 2069 6e69 7420 6675  .    The init fu
+000159c0: 6e63 7469 6f6e 2077 7261 7070 696e 6720  nction wrapping 
+000159d0: 6060 666e 6060 2e0a 2020 2222 220a 2020  ``fn``..  """.  
+000159e0: 696e 6974 5f66 6e20 3d20 696e 6974 5f77  init_fn = init_w
+000159f0: 6974 685f 6f75 7470 7574 2866 6e2c 206d  ith_output(fn, m
+00015a00: 6f64 756c 652c 206d 7574 6162 6c65 2c20  odule, mutable, 
+00015a10: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+00015a20: 6961 7465 7329 0a0a 2020 4066 756e 6374  iates)..  @funct
+00015a30: 6f6f 6c73 2e77 7261 7073 2869 6e69 745f  ools.wraps(init_
+00015a40: 666e 290a 2020 6465 6620 696e 6974 5f77  fn).  def init_w
+00015a50: 7261 7070 6572 282a 6172 6773 2c20 2a2a  rapper(*args, **
+00015a60: 6b77 6172 6773 293a 0a20 2020 2072 6574  kwargs):.    ret
+00015a70: 7572 6e20 696e 6974 5f66 6e28 2a61 7267  urn init_fn(*arg
+00015a80: 732c 202a 2a6b 7761 7267 7329 5b31 5d0a  s, **kwargs)[1].
+00015a90: 0a20 2072 6574 7572 6e20 696e 6974 5f77  .  return init_w
+00015aa0: 7261 7070 6572 0a                        rapper.
```

### Comparing `flax-0.7.0/flax/linen/normalization.py` & `flax-0.7.1/flax/linen/normalization.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,29 +10,27 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Normalization modules for Flax."""
 
-import functools
-from typing import (Any, Callable, Iterable, Optional, Tuple, Union)
+from typing import Any, Callable, Iterable, Optional, Sequence, Tuple, Union
 from flax.linen.dtypes import canonicalize_dtype
 from flax.linen.module import Module, compact, merge_param  # pylint: disable=g-multiple-import
 from jax import lax
 from jax.nn import initializers
 import jax.numpy as jnp
 
 
 PRNGKey = Any
 Array = Any
 Shape = Tuple[int, ...]
 Dtype = Any  # this could be a real type?
-
-Axes = Union[int, Any]
+Axes = Union[int, Sequence[int]]
 
 
 def _canonicalize_axes(rank: int, axes: Axes) -> Tuple[int, ...]:
   """Returns a tuple of deduplicated, sorted, and positive axes."""
   if not isinstance(axes, Iterable):
     axes = (axes,)
   return tuple(set([rank + axis if axis < 0 else axis for axis in axes]))
@@ -42,79 +40,96 @@
   """Computes the elementwise square of the absolute value |x|^2."""
   if jnp.iscomplexobj(x):
     return lax.square(lax.real(x)) + lax.square(lax.imag(x))
   else:
     return lax.square(x)
 
 
-def _compute_stats(x: Array, axes: Optional[Axes],
-                   dtype: Optional[Dtype],
-                   axis_name: Optional[str] = None,
-                   axis_index_groups: Any = None,
-                   use_mean: bool = True):
+def _compute_stats(
+    x: Array,
+    axes: Axes,
+    dtype: Optional[Dtype],
+    axis_name: Optional[str] = None,
+    axis_index_groups: Any = None,
+    use_mean: bool = True,
+    use_fast_variance: bool = True,
+):
   """Computes mean and variance statistics.
 
   This implementation takes care of a few important details:
   - Computes in float32 precision for stability in half precision training.
-  - mean and variance are computable in a single XLA fusion,
-    by using Var = E[|x|^2] - |E[x]|^2 instead of Var = E[|x - E[x]|^2]).
+  - If `use_fast_variance` is `True`, mean and variance are computed using
+    Var = E[|x|^2] - |E[x]|^2, instead of Var = E[|x - E[x]|^2]), in a single
+    XLA fusion.
   - Clips negative variances to zero which can happen due to
     roundoff errors. This avoids downstream NaNs.
   - Supports averaging across a parallel axis and subgroups of a parallel axis
     with a single `lax.pmean` call to avoid latency.
 
   Arguments:
     x: Input array.
     axes: The axes in ``x`` to compute mean and variance statistics for.
-    dtype: Optional dtype specifying the minimal precision. Statistics
-      are always at least float32 for stability (default: dtype of x).
+    dtype: Optional dtype specifying the minimal precision. Statistics are
+      always at least float32 for stability (default: dtype of x).
     axis_name: Optional name for the pmapped axis to compute mean over.
     axis_index_groups: Optional axis indices.
     use_mean: If true, calculate the mean from the input and use it when
-      computing the variance. If false, set the mean to zero and compute
-      the variance without subtracting the mean.
+      computing the variance. If false, set the mean to zero and compute the
+      variance without subtracting the mean.
+    use_fast_variance: If true, use a faster, but less numerically stable,
+      calculation for the variance.
 
   Returns:
     A pair ``(mean, var)``.
   """
   if dtype is None:
     dtype = jnp.result_type(x)
   # promote x to at least float32, this avoids half precision computation
   # but preserves double or complex floating points
   dtype = jnp.promote_types(dtype, jnp.float32)
   x = jnp.asarray(x, dtype)
 
-  mean2 = jnp.mean(_abs_sq(x), axes)
-  if use_mean:
-    mean = jnp.mean(x, axes)
-  else:
-    mean = jnp.zeros(mean2.shape, dtype=dtype)
+  def mean(x, axes=axes):
+    mu = x.mean(axes)
+    if axis_name is None:
+      return mu
+    return lax.pmean(mu, axis_name, axis_index_groups=axis_index_groups)
 
-  if axis_name is not None:
-    pmean = functools.partial(
-        lax.pmean, axis_name=axis_name, axis_index_groups=axis_index_groups
-    )
-    if use_mean:
-      mean, mean2 = jnp.split(pmean(jnp.concatenate([mean, mean2])), 2)
+  if use_mean:
+    if use_fast_variance:
+      axes = _canonicalize_axes(x.ndim, axes)
+      mu, mu2 = mean(jnp.stack([x, _abs_sq(x)]), axes=[a + 1 for a in axes])
+      # mean2 - _abs_sq(mean) is not guaranteed to be non-negative due
+      # to floating point round-off errors.
+      var = jnp.maximum(0.0, mu2 - _abs_sq(mu))
     else:
-      mean2 = pmean(mean2)
-  # mean2 - _abs_sq(mean) is not guaranteed to be non-negative due
-  # to floating point round-off errors.
-  var = jnp.maximum(0., mean2 - _abs_sq(mean))
-  return mean, var
-
-
-def _normalize(mdl: Module, x: Array, mean: Array, var: Array,
-               reduction_axes: Axes, feature_axes: Axes,
-               dtype: Dtype, param_dtype: Dtype,
-               epsilon: float,
-               use_bias: bool, use_scale: bool,
-               bias_init: Callable[[PRNGKey, Shape, Dtype], Array],
-               scale_init: Callable[[PRNGKey, Shape, Dtype], Array]):
-  """"Normalizes the input of a normalization layer and optionally applies a learned scale and bias.
+      mu = mean(x)
+      var = mean(_abs_sq(x - jnp.expand_dims(mu, axes)))
+  else:
+    var = mean(_abs_sq(x))
+    mu = jnp.zeros_like(var)
+  return mu, var
+
+
+def _normalize(
+    mdl: Module,
+    x: Array,
+    mean: Array,
+    var: Array,
+    reduction_axes: Axes,
+    feature_axes: Axes,
+    dtype: Dtype,
+    param_dtype: Dtype,
+    epsilon: float,
+    use_bias: bool,
+    use_scale: bool,
+    bias_init: Callable[[PRNGKey, Shape, Dtype], Array],
+    scale_init: Callable[[PRNGKey, Shape, Dtype], Array],
+):
+  """ "Normalizes the input of a normalization layer and optionally applies a learned scale and bias.
 
   Arguments:
     mdl: Module to apply the normalization in (normalization params will reside
       in this module).
     x: The input.
     mean: Mean to use for normalization.
     var: Variance to use for normalization.
@@ -142,22 +157,24 @@
 
   mean = jnp.expand_dims(mean, reduction_axes)
   var = jnp.expand_dims(var, reduction_axes)
   y = x - mean
   mul = lax.rsqrt(var + epsilon)
   args = [x]
   if use_scale:
-    scale = mdl.param('scale', scale_init, reduced_feature_shape,
-                      param_dtype).reshape(feature_shape)
+    scale = mdl.param(
+        'scale', scale_init, reduced_feature_shape, param_dtype
+    ).reshape(feature_shape)
     mul *= scale
     args.append(scale)
   y *= mul
   if use_bias:
-    bias = mdl.param('bias', bias_init, reduced_feature_shape,
-                     param_dtype).reshape(feature_shape)
+    bias = mdl.param(
+        'bias', bias_init, reduced_feature_shape, param_dtype
+    ).reshape(feature_shape)
     y += bias
     args.append(bias)
   dtype = canonicalize_dtype(*args, dtype=dtype)
   return jnp.asarray(y, dtype)
 
 
 class BatchNorm(Module):
@@ -209,14 +226,15 @@
       devices. See `jax.pmap` for a description of axis names (default: None).
     axis_index_groups: groups of axis indices within that named axis
       representing subsets of devices to reduce over (default: None). For
       example, `[[0, 1], [2, 3]]` would independently batch-normalize over
       the examples on the first two and last two devices. See `jax.lax.psum`
       for more details.
   """
+
   use_running_average: Optional[bool] = None
   axis: int = -1
   momentum: float = 0.99
   epsilon: float = 1e-5
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   use_bias: bool = True
@@ -243,45 +261,62 @@
         will be used instead of computing the batch statistics on the input.
 
     Returns:
       Normalized inputs (the same shape as inputs).
     """
 
     use_running_average = merge_param(
-        'use_running_average', self.use_running_average, use_running_average)
+        'use_running_average', self.use_running_average, use_running_average
+    )
     feature_axes = _canonicalize_axes(x.ndim, self.axis)
     reduction_axes = tuple(i for i in range(x.ndim) if i not in feature_axes)
     feature_shape = [x.shape[ax] for ax in feature_axes]
 
-    ra_mean = self.variable('batch_stats', 'mean',
-                            lambda s: jnp.zeros(s, jnp.float32),
-                            feature_shape)
-    ra_var = self.variable('batch_stats', 'var',
-                           lambda s: jnp.ones(s, jnp.float32),
-                           feature_shape)
+    ra_mean = self.variable(
+        'batch_stats',
+        'mean',
+        lambda s: jnp.zeros(s, jnp.float32),
+        feature_shape,
+    )
+    ra_var = self.variable(
+        'batch_stats', 'var', lambda s: jnp.ones(s, jnp.float32), feature_shape
+    )
 
     if use_running_average:
       mean, var = ra_mean.value, ra_var.value
     else:
       mean, var = _compute_stats(
-          x, reduction_axes,
+          x,
+          reduction_axes,
           dtype=self.dtype,
           axis_name=self.axis_name if not self.is_initializing() else None,
-          axis_index_groups=self.axis_index_groups)
+          axis_index_groups=self.axis_index_groups,
+      )
 
       if not self.is_initializing():
-        ra_mean.value = self.momentum * ra_mean.value + (1 -
-                                                         self.momentum) * mean
+        ra_mean.value = (
+            self.momentum * ra_mean.value + (1 - self.momentum) * mean
+        )
         ra_var.value = self.momentum * ra_var.value + (1 - self.momentum) * var
 
     return _normalize(
-        self, x, mean, var, reduction_axes, feature_axes,
-        self.dtype, self.param_dtype, self.epsilon,
-        self.use_bias, self.use_scale,
-        self.bias_init, self.scale_init)
+        self,
+        x,
+        mean,
+        var,
+        reduction_axes,
+        feature_axes,
+        self.dtype,
+        self.param_dtype,
+        self.epsilon,
+        self.use_bias,
+        self.use_scale,
+        self.bias_init,
+        self.scale_init,
+    )
 
 
 class LayerNorm(Module):
   """Layer normalization (https://arxiv.org/abs/1607.06450).
 
   LayerNorm normalizes the activations of the layer for each given example in a
   batch independently, rather than across a batch like Batch Normalization.
@@ -302,48 +337,68 @@
     feature_axes: Feature axes for learned bias and scaling.
     axis_name: the axis name used to combine batch statistics from multiple
       devices. See `jax.pmap` for a description of axis names (default: None).
       This is only needed if the model is subdivided across devices, i.e. the
       array being normalized is sharded across devices within a pmap.
     axis_index_groups: groups of axis indices within that named axis
       representing subsets of devices to reduce over (default: None). For
-      example, `[[0, 1], [2, 3]]` would independently batch-normalize over
-      the examples on the first two and last two devices. See `jax.lax.psum`
-      for more details.
+      example, `[[0, 1], [2, 3]]` would independently batch-normalize over the
+      examples on the first two and last two devices. See `jax.lax.psum` for
+      more details.
+    use_fast_variance: If true, use a faster, but less numerically stable,
+      calculation for the variance.
   """
+
   epsilon: float = 1e-6
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   use_bias: bool = True
   use_scale: bool = True
   bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros
   scale_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.ones
   reduction_axes: Axes = -1
   feature_axes: Axes = -1
   axis_name: Optional[str] = None
   axis_index_groups: Any = None
+  use_fast_variance: bool = True
 
   @compact
   def __call__(self, x):
     """Applies layer normalization on the input.
 
     Args:
       x: the inputs
 
     Returns:
       Normalized inputs (the same shape as inputs).
     """
-    mean, var = _compute_stats(x, self.reduction_axes, self.dtype,
-                               self.axis_name, self.axis_index_groups)
+    mean, var = _compute_stats(
+        x,
+        self.reduction_axes,
+        self.dtype,
+        self.axis_name,
+        self.axis_index_groups,
+        use_fast_variance=self.use_fast_variance,
+    )
 
     return _normalize(
-        self, x, mean, var, self.reduction_axes, self.feature_axes,
-        self.dtype, self.param_dtype, self.epsilon,
-        self.use_bias, self.use_scale,
-        self.bias_init, self.scale_init)
+        self,
+        x,
+        mean,
+        var,
+        self.reduction_axes,
+        self.feature_axes,
+        self.dtype,
+        self.param_dtype,
+        self.epsilon,
+        self.use_bias,
+        self.use_scale,
+        self.bias_init,
+        self.scale_init,
+    )
 
 
 class RMSNorm(Module):
   """RMS Layer normalization (https://arxiv.org/abs/1910.07467).
 
   RMSNorm normalizes the activations of the layer for each given example in a
   batch independently, rather than across a batch like Batch Normalization.
@@ -377,14 +432,15 @@
       array being normalized is sharded across devices within a pmap.
     axis_index_groups: groups of axis indices within that named axis
       representing subsets of devices to reduce over (default: None). For
       example, `[[0, 1], [2, 3]]` would independently batch-normalize over
       the examples on the first two and last two devices. See `jax.lax.psum`
       for more details.
   """
+
   epsilon: float = 1e-6
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   use_scale: bool = True
   scale_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.ones
   reduction_axes: Axes = -1
   feature_axes: Axes = -1
@@ -397,58 +453,74 @@
 
     Args:
       x: the inputs
 
     Returns:
       Normalized inputs (the same shape as inputs).
     """
-    mean, var = _compute_stats(x, self.reduction_axes, self.dtype,
-                               self.axis_name, self.axis_index_groups,
-                               use_mean=False)
+    mean, var = _compute_stats(
+        x,
+        self.reduction_axes,
+        self.dtype,
+        self.axis_name,
+        self.axis_index_groups,
+        use_mean=False,
+    )
 
     return _normalize(
-        self, x, mean, var, self.reduction_axes, self.feature_axes,
-        self.dtype, self.param_dtype, self.epsilon,
-        False, self.use_scale,
-        initializers.zeros, self.scale_init)
+        self,
+        x,
+        mean,
+        var,
+        self.reduction_axes,
+        self.feature_axes,
+        self.dtype,
+        self.param_dtype,
+        self.epsilon,
+        False,
+        self.use_scale,
+        initializers.zeros,
+        self.scale_init,
+    )
 
 
 class GroupNorm(Module):
   """Group normalization (arxiv.org/abs/1803.08494).
 
-    This op is similar to batch normalization, but statistics are shared across
-    equally-sized groups of channels and not shared across batch dimension.
-    Thus, group normalization does not depend on the batch composition and does
-    not require maintaining internal state for storing statistics.
-    The user should either specify the total number of channel groups or the
-    number of channels per group.
-
-    Attributes:
-      num_groups: the total number of channel groups. The default value of 32 is
-        proposed by the original group normalization paper.
-      group_size: the number of channels in a group.
-      epsilon: A small float added to variance to avoid dividing by zero.
-      dtype: the dtype of the result (default: infer from input and params).
-      param_dtype: the dtype passed to parameter initializers (default: float32).
-      use_bias:  If True, bias (beta) is added.
-      use_scale: If True, multiply by scale (gamma). When the next layer is
-        linear (also e.g. nn.relu), this can be disabled since the scaling will
-        be done by the next layer.
-      bias_init: Initializer for bias, by default, zero.
-      scale_init: Initializer for scale, by default, one.
-      axis_name: the axis name used to combine batch statistics from multiple
-        devices. See `jax.pmap` for a description of axis names (default: None).
-        This is only needed if the model is subdivided across devices, i.e. the
-        array being normalized is sharded across devices within a pmap.
-      axis_index_groups: groups of axis indices within that named axis
-        representing subsets of devices to reduce over (default: None). For
-        example, `[[0, 1], [2, 3]]` would independently batch-normalize over
-        the examples on the first two and last two devices. See `jax.lax.psum`
-        for more details.
+  This op is similar to batch normalization, but statistics are shared across
+  equally-sized groups of channels and not shared across batch dimension.
+  Thus, group normalization does not depend on the batch composition and does
+  not require maintaining internal state for storing statistics.
+  The user should either specify the total number of channel groups or the
+  number of channels per group.
+
+  Attributes:
+    num_groups: the total number of channel groups. The default value of 32 is
+      proposed by the original group normalization paper.
+    group_size: the number of channels in a group.
+    epsilon: A small float added to variance to avoid dividing by zero.
+    dtype: the dtype of the result (default: infer from input and params).
+    param_dtype: the dtype passed to parameter initializers (default: float32).
+    use_bias:  If True, bias (beta) is added.
+    use_scale: If True, multiply by scale (gamma). When the next layer is linear
+      (also e.g. nn.relu), this can be disabled since the scaling will be done
+      by the next layer.
+    bias_init: Initializer for bias, by default, zero.
+    scale_init: Initializer for scale, by default, one.
+    axis_name: the axis name used to combine batch statistics from multiple
+      devices. See `jax.pmap` for a description of axis names (default: None).
+      This is only needed if the model is subdivided across devices, i.e. the
+      array being normalized is sharded across devices within a pmap.
+    axis_index_groups: groups of axis indices within that named axis
+      representing subsets of devices to reduce over (default: None). For
+      example, `[[0, 1], [2, 3]]` would independently batch-normalize over the
+      examples on the first two and last two devices. See `jax.lax.psum` for
+      more details.
   """
+
   num_groups: Optional[int] = 32
   group_size: Optional[int] = None
   epsilon: float = 1e-6
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   use_bias: bool = True
   use_scale: bool = True
@@ -468,42 +540,63 @@
 
     Returns:
       Normalized inputs (the same shape as inputs).
     """
     reduction_axes = list(range(1, x.ndim - 1)) + [-1]
     feature_axes = (-1,)
 
-    if ((self.num_groups is None and self.group_size is None) or
-        (self.num_groups is not None and self.group_size is not None)):
-      raise ValueError('Either `num_groups` or `group_size` should be '
-                       'specified. If `group_size` is to be specified, '
-                       'pass `num_groups=None` as argument to override '
-                       'the default `num_groups` value of 32.')
+    if (self.num_groups is None and self.group_size is None) or (
+        self.num_groups is not None and self.group_size is not None
+    ):
+      raise ValueError(
+          'Either `num_groups` or `group_size` should be '
+          'specified. If `group_size` is to be specified, '
+          'pass `num_groups=None` as argument to override '
+          'the default `num_groups` value of 32.'
+      )
 
     channels = x.shape[-1]
     if self.group_size is not None:
       if channels % self.group_size != 0:
-        raise ValueError('Number of channels ({}) is not multiple of the '
-                         'group size ({}).'.format(channels, self.group_size))
+        raise ValueError(
+            'Number of channels ({}) is not multiple of the '
+            'group size ({}).'.format(channels, self.group_size)
+        )
       num_groups = channels // self.group_size
     else:
       num_groups = self.num_groups
       assert isinstance(num_groups, int)
 
     if num_groups <= 0 or channels % num_groups != 0:
-      raise ValueError('Number of groups ({}) does not divide the number'
-                       ' of channels ({}).'.format(num_groups, channels))
+      raise ValueError(
+          'Number of groups ({}) does not divide the number'
+          ' of channels ({}).'.format(num_groups, channels)
+      )
 
     group_size = x.shape[-1] // num_groups
     group_shape = x.shape[:-1] + (num_groups, group_size)
 
     mean, var = _compute_stats(
-        x.reshape(group_shape), reduction_axes, self.dtype, self.axis_name,
-        self.axis_index_groups)
+        x.reshape(group_shape),
+        reduction_axes,
+        self.dtype,
+        self.axis_name,
+        self.axis_index_groups,
+    )
     mean = jnp.repeat(mean, group_size, axis=-1)
     var = jnp.repeat(var, group_size, axis=-1)
 
     return _normalize(
-        self, x, mean, var, reduction_axes[:-1], feature_axes,
-        self.dtype, self.param_dtype, self.epsilon,
-        self.use_bias, self.use_scale,
-        self.bias_init, self.scale_init)
+        self,
+        x,
+        mean,
+        var,
+        reduction_axes[:-1],
+        feature_axes,
+        self.dtype,
+        self.param_dtype,
+        self.epsilon,
+        self.use_bias,
+        self.use_scale,
+        self.bias_init,
+        self.scale_init,
+    )
```

### Comparing `flax-0.7.0/flax/linen/partitioning.py` & `flax-0.7.1/flax/linen/partitioning.py`

 * *Files 2% similar despite different names*

```diff
@@ -73,14 +73,15 @@
 # Annotated parameters and Module axis metadata handling.
 # ------------------------------------------------------------------------------
 
 
 @struct.dataclass
 class AxisMetadata:
   """Contains a tuple of axis names, which is passed through FLAX."""
+
   names: LogicalPartitionSpecPytree = struct.field(pytree_node=False)
 
 
 def _param_with_axes_sow_reduce_fn(x, y):
   """Reduction function for sow() calls.
 
   Args:
@@ -98,28 +99,31 @@
       AxisMetadata.
   """
   if not isinstance(y, AxisMetadata):
     raise TypeError('Expected newly sown value to be an AxisMetadata')
 
   if isinstance(x, AxisMetadata):
     if x != y:
-      raise ValueError('If axis names are sown twice, expected them to match. '
-                       f'Got {x} and {y}.')
+      raise ValueError(
+          'If axis names are sown twice, expected them to match. '
+          f'Got {x} and {y}.'
+      )
   elif x:
     # Shouldn't happen, so raise a fairly internal error.
     raise AssertionError(f'Non-initial-or-AxisMetadata value encountered: {x}')
   return y
 
 
 def param_with_axes(
     name: str,
     init_fn,
     *init_args,
     axes: Optional[Tuple[str, ...]] = None,
-    module: Optional['nn.Module'] = None):
+    module: Optional['nn.Module'] = None,
+):
   """Declares and returns a parameter with logical axes in the current Module.
 
   See :mod:`flax.linen.module.param` for original docstring.
 
   Args:
     name: The parameter name.
     init_fn: The function that will be called to compute the initial value
@@ -141,39 +145,45 @@
   if module is None:
     module = nn.module._context.module_stack[-1]  # pylint: disable=protected-access
     assert module is not None
   # define/fetch parameter on that module
   module_param = module.param(name, init_fn, *init_args)
   if axes is not None:
     # apply logical axis constraint immediately
-    module_param = with_sharding_constraint(module_param,
-                                            jax.sharding.PartitionSpec(*axes))
+    module_param = with_sharding_constraint(
+        module_param, jax.sharding.PartitionSpec(*axes)
+    )
     # record logical axis constraint for global axis metadata
     module.sow(
-        'params_axes', f'{name}_axes', AxisMetadata(axes),  # type: ignore
-        reduce_fn=_param_with_axes_sow_reduce_fn)
+        'params_axes',
+        f'{name}_axes',
+        AxisMetadata(axes),  # type: ignore
+        reduce_fn=_param_with_axes_sow_reduce_fn,
+    )
   return module_param
 
 
 class PartitionedVariable(flax.core.scope.Variable):
   """A PartitionedVariable object allows mutable access to a variable.
 
   PartitionedVariables are identified by a collection (e.g., "batch_stats") and
   a name (e.g., "moving_mean"). The value property gives access to the
   variable's content and can be assigned to for mutation.  Additionally,
   PartitionedVariables enforce logical sharding constraints on both retrieval
   and assignment.
   """
 
-  def __init__(self,
-               scope,
-               collection: str,
-               name: str,
-               axes: Optional[Tuple[str, ...]] = None,
-               fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED):
+  def __init__(
+      self,
+      scope,
+      collection: str,
+      name: str,
+      axes: Optional[Tuple[str, ...]] = None,
+      fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED,
+  ):
     """Initializes a partitioned variable.
 
     Args:
       scope: The scope in which the variable is stored.
       collection: The collection of the variable (e.g., "params").
       name: The name of the variable (e.g., "dense").
       axes: logical axes name of variable.
@@ -204,15 +214,16 @@
 def _core_variable_with_axes(
     scope,
     col: str,
     name: str,
     init_fn: Callable[..., Any],
     *init_args,
     axes: Optional[Tuple[str, ...]] = None,
-    fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED):
+    fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED,
+):
   """Variant of flax core variable scope call with sharding constraints."""
   scope.reserve(name)
   if not scope.has_variable(col, name):
     if not scope.is_mutable_collection(col):
       raise flax.errors.ScopeVariableNotFoundError(name, col, scope.path_text)
     init_value = init_fn(*init_args)
     if axes is not None:
@@ -224,15 +235,16 @@
 def variable_with_axes(
     collection: str,
     name: str,
     init_fn,
     *init_args,
     axes: Optional[Tuple[str, ...]] = None,
     module: Optional['nn.Module'] = None,
-    fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED):
+    fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED,
+):
   """Declares and returns a variable with logical axes in the current Module.
 
   See :mod:`flax.linen.module.variable` for original docstring.
 
   Args:
     collection: The name of the variable collection.
     name: The variable name.
@@ -260,20 +272,24 @@
   module_var = _core_variable_with_axes(
       module.scope,
       collection,
       name,
       init_fn,
       *init_args,
       axes=axes,
-      fallback=fallback)
+      fallback=fallback,
+  )
   if axes is not None:
     # record logical axis constraint for global axis metadata
     module.sow(
-        f'{collection}_axes', f'{name}_axes', AxisMetadata(axes),  # type: ignore
-        reduce_fn=_param_with_axes_sow_reduce_fn)
+        f'{collection}_axes',
+        f'{name}_axes',
+        AxisMetadata(axes),  # type: ignore
+        reduce_fn=_param_with_axes_sow_reduce_fn,
+    )
   return module_var
 
 
 def get_axis_names(axes_metadata):
   """Gets axis names for variables as logical PartitionSpecs.
 
   Args:
@@ -281,36 +297,41 @@
       set of collections.
 
   Returns:
     Collection of Partitionspecs with logical axis names, with the "_axes"
     suffix on variable names removed to match original variable collection for
     annotations.
   """
+
   def leaf_rewrite(x):
     return None if x is None else jax.sharding.PartitionSpec(*x)
+
   def rewrite(tree):
     return jax.tree_util.tree_map(leaf_rewrite, tree, is_leaf=_is_logical_spec)
+
   axes_metadata = unfreeze(axes_metadata)  # pytype: disable=wrong-arg-types
   flat_dict = {
       re.sub(r'_axes$', '', '/'.join(k)): rewrite(v.names)
       for k, v in flatten_dict(axes_metadata).items()
   }
-  return freeze(unflatten_dict(
-      {tuple(k.split('/')): v for k, v in flat_dict.items()}))
+  return freeze(
+      unflatten_dict({tuple(k.split('/')): v for k, v in flat_dict.items()})
+  )
 
 
 # Metadata Aware Scan
 # -----------------------------------------------------------------------------
 
 
 def _tree_map_axes(fn, tree):
   """Only map over AxisMetadata leaves in pytree - identity for other leaves."""
   safe_fn = lambda x: fn(x) if isinstance(x, AxisMetadata) else x
   return jax.tree_util.tree_map(
-      safe_fn, tree, is_leaf=lambda x: isinstance(x, AxisMetadata))
+      safe_fn, tree, is_leaf=lambda x: isinstance(x, AxisMetadata)
+  )
 
 
 def _is_mutable(axis_col: str) -> bool:
   """Determines whether a collection is mutable.
 
   For example, when a module is called with `module.apply(..., mutable=['z'])`,
   this function will return True for `axis_col='z'` and False otherwise.
@@ -342,65 +363,72 @@
     if names is None:
       return names
     names = list(names)
     names.insert(axis_pos, axis_name)
     return tuple(names)
 
   def insert_fn(x):
-    new_names = jax.tree_util.tree_map(insert_fn_leaf, x.names,
-                                       is_leaf=_is_logical_spec)
+    new_names = jax.tree_util.tree_map(
+        insert_fn_leaf, x.names, is_leaf=_is_logical_spec
+    )
     return x.replace(names=new_names)
 
   def remove_fn_leaf(names):
     if names is None:
       return names
     names = list(names)
     if names[axis_pos] != axis_name:
-      raise ValueError(f'Expected axis {axis_name} at position {axis_pos} in '
-                       f'axis metadata {names}.')
+      raise ValueError(
+          f'Expected axis {axis_name} at position {axis_pos} in '
+          f'axis metadata {names}.'
+      )
     names.pop(axis_pos)
     return tuple(names)
 
   def remove_fn(x):
-    new_names = jax.tree_util.tree_map(remove_fn_leaf, x.names,
-                                       is_leaf=_is_logical_spec)
+    new_names = jax.tree_util.tree_map(
+        remove_fn_leaf, x.names, is_leaf=_is_logical_spec
+    )
     return x.replace(names=new_names)
 
   return nn.transforms.map_variables(
       fn,
       axis_col,
       mutable=_is_mutable(axis_col),
       trans_in_fn=lambda tree: _tree_map_axes(remove_fn, tree),
-      trans_out_fn=lambda tree: _tree_map_axes(insert_fn, tree)
-      )
+      trans_out_fn=lambda tree: _tree_map_axes(insert_fn, tree),
+  )
 
 
 # pylint: disable=dangerous-default-value
 def scan_with_axes(
     target: 'flax.linen.transforms.Target',
-    variable_axes: Mapping[flax.core.lift.CollectionFilter,
-                           flax.core.lift.InOutScanAxis] = {},
+    variable_axes: Mapping[
+        flax.core.lift.CollectionFilter, flax.core.lift.InOutScanAxis
+    ] = {},
     variable_broadcast: flax.core.lift.CollectionFilter = False,
     variable_carry: flax.core.lift.CollectionFilter = False,
     split_rngs: Mapping[flax.core.lift.PRNGSequenceFilter, bool] = {},
     in_axes=0,
     out_axes=0,
     length: Optional[int] = None,
     reverse: bool = False,
     unroll: int = 1,
     axis_name: str = 'layers',
     axes_collections: Tuple[str, ...] = ('params',),
     data_transform: Optional[Callable[..., Any]] = None,
-    methods=None) -> 'flax.linen.transforms.Target':
+    methods=None,
+) -> 'flax.linen.transforms.Target':
   """Wrapped version of nn.scan that handles logical axis metadata."""
 
   # we broadcast the static metadata collections.
   axes_filters = tuple(f'{col}_axes' for col in axes_collections)
   variable_broadcast = flax.core.scope.union_filters(
-      variable_broadcast, axes_filters)
+      variable_broadcast, axes_filters
+  )
 
   # perform usual lifted scan
   scanned = flax.linen.transforms.lift_transform(
       flax.core.lift.scan,
       target,
       variable_axes=variable_axes,
       variable_broadcast=variable_broadcast,
@@ -408,39 +436,44 @@
       split_rngs=split_rngs,
       in_axes=in_axes,
       out_axes=out_axes,
       length=length,
       reverse=reverse,
       unroll=unroll,
       data_transform=data_transform,
-      methods=methods)
+      methods=methods,
+  )
 
   # add scan axis to logical axes metadata
   for col in axes_collections:
     if col in variable_axes:
-      scanned = _add_axis_to_metadata(scanned,
-                                      axis_pos=variable_axes[col],
-                                      axis_name=axis_name,
-                                      axis_col=f'{col}_axes')
+      scanned = _add_axis_to_metadata(
+          scanned,
+          axis_pos=variable_axes[col],
+          axis_name=axis_name,
+          axis_col=f'{col}_axes',
+      )
   return scanned
 
 
 # pylint: disable=dangerous-default-value
-def vmap_with_axes(target: 'flax.linen.transforms.Target',
-                   variable_axes: Mapping[flax.core.lift.CollectionFilter,
-                                          flax.core.lift.InOutAxis],
-                   split_rngs: Mapping[flax.core.lift.PRNGSequenceFilter,
-                                       bool] = {},
-                   in_axes=0,
-                   out_axes=0,
-                   axis_size: Optional[int] = None,
-                   axis_name: Optional[str] = None,
-                   partitioning_axis_names: Mapping[Any, str] = {},
-                   spmd_axis_name: Optional[str] = None,
-                   methods=None) -> 'flax.linen.transforms.Target':
+def vmap_with_axes(
+    target: 'flax.linen.transforms.Target',
+    variable_axes: Mapping[
+        flax.core.lift.CollectionFilter, flax.core.lift.InOutAxis
+    ],
+    split_rngs: Mapping[flax.core.lift.PRNGSequenceFilter, bool] = {},
+    in_axes=0,
+    out_axes=0,
+    axis_size: Optional[int] = None,
+    axis_name: Optional[str] = None,
+    partitioning_axis_names: Mapping[Any, str] = {},
+    spmd_axis_name: Optional[str] = None,
+    methods=None,
+) -> 'flax.linen.transforms.Target':
   """Wrapped version of nn.vmap that handles logical axis metadata."""
 
   # tell normal vmap to broadcast axis metadata.
   variable_axes = dict(variable_axes)  # shallow copy
   for name in partitioning_axis_names:
     variable_axes[f'{name}_axes'] = None
 
@@ -451,41 +484,45 @@
       variable_axes=variable_axes,
       split_rngs=split_rngs,
       in_axes=in_axes,
       out_axes=out_axes,
       axis_size=axis_size,
       axis_name=axis_name,
       spmd_axis_name=spmd_axis_name,
-      methods=methods)
+      methods=methods,
+  )
 
   for collection_name, axis in variable_axes.items():
     if collection_name in partitioning_axis_names:
       vmapped = _add_axis_to_metadata(  # pylint: disable=protected-access
           vmapped,
           axis_pos=axis,
           axis_name=partitioning_axis_names[collection_name],
-          axis_col=f'{collection_name}_axes')
+          axis_col=f'{collection_name}_axes',
+      )
 
   return vmapped
 
 
 # Remat abstraction bug hotfix
 # ------------------------------------------------------------------------------
 # TODO(levskaya): upstream this fix into main flax.core.lift.remat.
 # Workaround a scan(remat(...)) abstraction bug by manually implementing a
 # static_argnums behavior for flax remat via closure before applying jax remat.
 
 
-def core_remat_static(fn,
-                      variables=True,
-                      rngs=True,
-                      concrete=False,
-                      prevent_cse=True,
-                      static_argnums=(),
-                      policy=None):
+def core_remat_static(
+    fn,
+    variables=True,
+    rngs=True,
+    concrete=False,
+    prevent_cse=True,
+    static_argnums=(),
+    policy=None,
+):
   """Flax functional core remat version with static_argnums."""
 
   static_argnums = tuple(sorted(static_argnums))
 
   def _repack_remat_args(dyn_args, static_args):
     """Remake arg list from static and dynamic args given static_argnums."""
     args = []
@@ -500,40 +537,45 @@
     return tuple(args)
 
   def inner(scope_fn, repack_fn, variable_groups, rng_groups, *args):
     static_args = tuple(x for i, x in enumerate(args) if i in static_argnums)
     dyn_args = tuple(x for i, x in enumerate(args) if i not in static_argnums)
 
     @functools.partial(
-        jax.remat, concrete=concrete, prevent_cse=prevent_cse, policy=policy)
+        jax.remat, concrete=concrete, prevent_cse=prevent_cse, policy=policy
+    )
     @functools.wraps(fn)
     def rematted(variable_groups, rng_groups, *dyn_args):
       args = _repack_remat_args(dyn_args, static_args)
       scope = scope_fn(variable_groups, rng_groups)
       y = fn(scope, *args)
       return y, repack_fn(scope)
 
     return rematted(variable_groups, rng_groups, *dyn_args)
 
   return flax.core.lift.pack(
-      inner, (variables,), (variables,), (rngs,), name='remat')
+      inner, (variables,), (variables,), (rngs,), name='remat'
+  )
 
 
-def remat(target,
-          variables=True,
-          rngs=True,
-          concrete=False,
-          prevent_cse=True,
-          static_argnums=(),
-          policy=None,
-          methods=None):
+def remat(
+    target,
+    variables=True,
+    rngs=True,
+    concrete=False,
+    prevent_cse=True,
+    static_argnums=(),
+    policy=None,
+    methods=None,
+):
   """Flax lifted remat that supports static_argnums."""
   return flax.linen.transforms.lift_transform(
       core_remat_static,
       target,
       variables=variables,
       rngs=rngs,
       concrete=concrete,
       prevent_cse=prevent_cse,
       static_argnums=static_argnums,
       policy=policy,
-      methods=methods)
+      methods=methods,
+  )
```

### Comparing `flax-0.7.0/flax/linen/pooling.py` & `flax-0.7.1/flax/linen/pooling.py`

 * *Files 3% similar despite different names*

```diff
@@ -39,16 +39,17 @@
       of `n` `(low, high)` integer pairs that give the padding to apply before
       and after each spatial dimension.
   Returns:
     The output of the reduction for each window slice.
   """
   num_batch_dims = inputs.ndim - (len(window_shape) + 1)
   strides = strides or (1,) * len(window_shape)
-  assert len(window_shape) == len(strides), (
-      f"len({window_shape}) must equal len({strides})")
+  assert len(window_shape) == len(
+      strides
+  ), f"len({window_shape}) must equal len({strides})"
   strides = (1,) * num_batch_dims + strides + (1,)
   dims = (1,) * num_batch_dims + window_shape + (1,)
 
   is_single_input = False
   if num_batch_dims == 0:
     # add singleton batch dimension because lax.reduce_window always
     # needs a batch dimension.
@@ -58,25 +59,29 @@
     is_single_input = True
 
   assert inputs.ndim == len(dims), f"len({inputs.shape}) != len({dims})"
   if not isinstance(padding, str):
     padding = tuple(map(tuple, padding))
     assert len(padding) == len(window_shape), (
         f"padding {padding} must specify pads for same number of dims as "
-        f"window_shape {window_shape}")
-    assert all([len(x) == 2 for x in padding]), (
-        f"each entry in padding {padding} must be length 2")
+        f"window_shape {window_shape}"
+    )
+    assert all(
+        [len(x) == 2 for x in padding]
+    ), f"each entry in padding {padding} must be length 2"
     padding = ((0, 0),) + padding + ((0, 0),)
   y = lax.reduce_window(inputs, init, reduce_fn, dims, strides, padding)
   if is_single_input:
     y = jnp.squeeze(y, axis=0)
   return y
 
 
-def avg_pool(inputs, window_shape, strides=None, padding="VALID", count_include_pad=True):
+def avg_pool(
+    inputs, window_shape, strides=None, padding="VALID", count_include_pad=True
+):
   """Pools the input by taking the average over a window.
 
   Args:
     inputs: input data with dimensions (batch, window dims..., features).
     window_shape: a shape tuple defining the window to reduce over.
     strides: a sequence of `n` integers, representing the inter-window
       strides (default: `(1, ..., 1)`).
@@ -84,22 +89,24 @@
       of `n` `(low, high)` integer pairs that give the padding to apply before
       and after each spatial dimension (default: `'VALID'`).
     count_include_pad: a boolean whether to include padded tokens
       in the average calculation (default: `True`).
   Returns:
     The average for each window slice.
   """
-  y = pool(inputs, 0., lax.add, window_shape, strides, padding)
+  y = pool(inputs, 0.0, lax.add, window_shape, strides, padding)
   if count_include_pad:
     y = y / np.prod(window_shape)
   else:
     div_shape = inputs.shape[:-1] + (1,)
     if len(div_shape) - 2 == len(window_shape):
-        div_shape = (1,) + div_shape[1:]
-    y = y / pool(jnp.ones(div_shape), 0., lax.add, window_shape, strides, padding)
+      div_shape = (1,) + div_shape[1:]
+    y = y / pool(
+        jnp.ones(div_shape), 0.0, lax.add, window_shape, strides, padding
+    )
   return y
 
 
 def max_pool(inputs, window_shape, strides=None, padding="VALID"):
   """Pools the input by taking the maximum of a window slice.
 
   Args:
```

### Comparing `flax-0.7.0/flax/linen/recurrent.py` & `flax-0.7.1/flax/linen/recurrent.py`

 * *Files 5% similar despite different names*

```diff
@@ -15,93 +15,99 @@
 """Recurrent neural network modules.
 
 THe RNNCell modules can be scanned using lifted transforms. For more information
 see: https://flax.readthedocs.io/en/latest/developer_notes/lift.html.
 """
 
 from abc import ABCMeta
-from functools import partial   # pylint: disable=g-importing-member
-from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Tuple, Union, TypeVar, cast
-from typing_extensions import Protocol
+from functools import partial  # pylint: disable=g-importing-member
+from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Tuple, TypeVar, Union, cast
 from absl import logging
-
+from flax.core import lift
+from flax.core.frozen_dict import FrozenDict
+from flax.linen import initializers
+from flax.linen import transforms
 from flax.linen.activation import sigmoid
 from flax.linen.activation import tanh
 from flax.linen.dtypes import promote_dtype
-from flax.linen import initializers
 from flax.linen.linear import Conv
 from flax.linen.linear import default_kernel_init
 from flax.linen.linear import Dense
 from flax.linen.linear import PrecisionLike
 from flax.linen.module import compact, nowrap
 from flax.linen.module import Module
+import jax
 from jax import numpy as jnp
 from jax import random
 import numpy as np
-from flax.core import lift
-from flax.core.frozen_dict import FrozenDict
-from flax.linen import transforms
-import jax
+from typing_extensions import Protocol
 
 A = TypeVar('A')
 PRNGKey = jax.random.KeyArray
 Shape = Tuple[int, ...]
 Dtype = Any  # this could be a real type?
 Array = jax.Array
 Carry = Any
 CarryHistory = Any
 Output = Any
 
+
 class _Never:
   pass
 
+
 NEVER = _Never()
 
 LEGACY_UPDATE_MESSAGE = (
-  "The RNNCellBase API has changed, "
-  "the error you are experiencing might be caused by this change. Please "
-  "update your code to the new API, for more information on how to do this "
-  "please check out the RNNCellBase migration guide: "
-  "https://flax.readthedocs.io/en/latest/guides/rnncell_upgrade_guide.html"
+    'The RNNCellBase API has changed, '
+    'the error you are experiencing might be caused by this change. Please '
+    'update your code to the new API, for more information on how to do this '
+    'please check out the RNNCellBase migration guide: '
+    'https://flax.readthedocs.io/en/latest/guides/rnncell_upgrade_guide.html'
 )
 
+
 class RNNCellCompatibilityMeta(ABCMeta):
   """Metaclass for RNNCell compatibility."""
 
   def __call__(self, *args: Any, **kwds: Any) -> Any:
     try:
       return super().__call__(*args, **kwds)
     except TypeError as e:
       msg = e.args[0]
       raise TypeError(f'{msg} \n\n {LEGACY_UPDATE_MESSAGE}') from e
 
+
 def deprecation_method_decorator(f):
   def wrapper(*args, **kwargs):
     if len(args) < 1 or not isinstance(args[0], RNNCellBase):
       raise TypeError(LEGACY_UPDATE_MESSAGE)
     return f(*args, **kwargs)
+
   return wrapper
 
+
 class RNNCellBase(Module):
   """RNN cell base class."""
 
   @nowrap
-  def initialize_carry(self, rng: PRNGKey, input_shape: Tuple[int, ...]) -> Carry:
+  def initialize_carry(
+      self, rng: PRNGKey, input_shape: Tuple[int, ...]
+  ) -> Carry:
     """Initialize the RNN cell carry.
 
     Args:
       rng: random number generator passed to the init_fn.
       input_shape: a tuple providing the shape of the input to the cell.
 
     Returns:
       An initialized carry for the given RNN cell.
     """
     raise NotImplementedError
 
-
   @property
   def num_feature_axes(self) -> int:
     """Returns the number of feature axes of the RNN cell."""
     raise NotImplementedError
 
 
 class LSTMCell(RNNCellBase, metaclass=RNNCellCompatibilityMeta):
@@ -157,39 +163,44 @@
 
     Returns:
       A tuple with the new carry and the output.
     """
     c, h = carry
     hidden_features = h.shape[-1]
     # input and recurrent layers are summed so only one needs a bias.
-    dense_h = partial(Dense,
-                      features=hidden_features,
-                      use_bias=True,
-                      kernel_init=self.recurrent_kernel_init,
-                      bias_init=self.bias_init,
-                      dtype=self.dtype,
-                      param_dtype=self.param_dtype)
-    dense_i = partial(Dense,
-                      features=hidden_features,
-                      use_bias=False,
-                      kernel_init=self.kernel_init,
-                      dtype=self.dtype,
-                      param_dtype=self.param_dtype)
+    dense_h = partial(
+        Dense,
+        features=hidden_features,
+        use_bias=True,
+        kernel_init=self.recurrent_kernel_init,
+        bias_init=self.bias_init,
+        dtype=self.dtype,
+        param_dtype=self.param_dtype,
+    )
+    dense_i = partial(
+        Dense,
+        features=hidden_features,
+        use_bias=False,
+        kernel_init=self.kernel_init,
+        dtype=self.dtype,
+        param_dtype=self.param_dtype,
+    )
     i = self.gate_fn(dense_i(name='ii')(inputs) + dense_h(name='hi')(h))
     f = self.gate_fn(dense_i(name='if')(inputs) + dense_h(name='hf')(h))
     g = self.activation_fn(dense_i(name='ig')(inputs) + dense_h(name='hg')(h))
     o = self.gate_fn(dense_i(name='io')(inputs) + dense_h(name='ho')(h))
     new_c = f * c + i * g
     new_h = o * self.activation_fn(new_c)
     return (new_c, new_h), new_h
 
   @nowrap
   @deprecation_method_decorator
   def initialize_carry(
-      self, rng: PRNGKey, input_shape: Tuple[int, ...]) -> Tuple[Array, Array]:
+      self, rng: PRNGKey, input_shape: Tuple[int, ...]
+  ) -> Tuple[Array, Array]:
     """Initialize the RNN cell carry.
 
     Args:
       rng: random number generator passed to the init_fn.
       input_shape: a tuple providing the shape of the input to the cell.
     Returns:
       An initialized carry for the given RNN cell.
@@ -210,21 +221,26 @@
   """Dummy module for creating parameters matching `flax.linen.Dense`."""
 
   features: int
   use_bias: bool = True
   param_dtype: Dtype = jnp.float32
   precision: PrecisionLike = None
   kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
-  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = initializers.zeros_init()
+  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
+      initializers.zeros_init()
+  )
 
   @compact
   def __call__(self, inputs: Array) -> Tuple[Array, Optional[Array]]:
     k = self.param(
-        'kernel', self.kernel_init, (inputs.shape[-1], self.features),
-        self.param_dtype)
+        'kernel',
+        self.kernel_init,
+        (inputs.shape[-1], self.features),
+        self.param_dtype,
+    )
     if self.use_bias:
       b = self.param('bias', self.bias_init, (self.features,), self.param_dtype)
     else:
       b = None
     return k, b
 
 
@@ -270,48 +286,53 @@
   recurrent_kernel_init: initializers.Initializer = initializers.orthogonal()
   bias_init: initializers.Initializer = initializers.zeros_init()
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   carry_init: initializers.Initializer = initializers.zeros_init()
 
   @compact
-  def __call__(self, carry: Tuple[Array, Array],
-               inputs: Array) -> Tuple[Tuple[Array, Array], Array]:
+  def __call__(
+      self, carry: Tuple[Array, Array], inputs: Array
+  ) -> Tuple[Tuple[Array, Array], Array]:
     r"""An optimized long short-term memory (LSTM) cell.
 
     Args:
       carry: the hidden state of the LSTM cell, initialized using
         `LSTMCell.initialize_carry`.
       inputs: an ndarray with the input for the current time step. All
         dimensions except the final are considered batch dimensions.
 
     Returns:
       A tuple with the new carry and the output.
     """
     c, h = carry
     hidden_features = h.shape[-1]
 
-    def _concat_dense(inputs: Array,
-                      params: Mapping[str, Tuple[Array, Optional[Array]]],
-                      use_bias: bool = True) -> Dict[str, Array]:
+    def _concat_dense(
+        inputs: Array,
+        params: Mapping[str, Tuple[Array, Optional[Array]]],
+        use_bias: bool = True,
+    ) -> Dict[str, Array]:
       # Concatenates the individual kernels and biases, given in params, into a
       # single kernel and single bias for efficiency before applying them using
       # dot_general.
       kernels = [kernel for kernel, _ in params.values()]
       kernel = jnp.concatenate(kernels, axis=-1)
       if use_bias:
         biases = []
         for _, bias in params.values():
           if bias is None:
             raise ValueError('bias is None but use_bias is True.')
           biases.append(bias)
         bias = jnp.concatenate(biases, axis=-1)
       else:
         bias = None
-      inputs, kernel, bias = promote_dtype(inputs, kernel, bias, dtype=self.dtype)
+      inputs, kernel, bias = promote_dtype(
+          inputs, kernel, bias, dtype=self.dtype
+      )
       y = jnp.dot(inputs, kernel)
       if use_bias:
         # This assert is here since mypy can't infer that bias cannot be None
         assert bias is not None
         y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))
 
       # Split the result back into individual (i, f, g, o) outputs.
@@ -320,23 +341,29 @@
       return dict(zip(params.keys(), ys))
 
     # Create params with the same names/shapes as `LSTMCell` for compatibility.
     dense_params_h = {}
     dense_params_i = {}
     for component in ['i', 'f', 'g', 'o']:
       dense_params_i[component] = DenseParams(
-          features=hidden_features, use_bias=False,
+          features=hidden_features,
+          use_bias=False,
           param_dtype=self.param_dtype,
-          kernel_init=self.kernel_init, bias_init=self.bias_init,
-          name=f'i{component}')(inputs) # type: ignore[call-arg]
+          kernel_init=self.kernel_init,
+          bias_init=self.bias_init,
+          name=f'i{component}',  # type: ignore[call-arg]
+      )(inputs)
       dense_params_h[component] = DenseParams(
-          features=hidden_features, use_bias=True,
+          features=hidden_features,
+          use_bias=True,
           param_dtype=self.param_dtype,
-          kernel_init=self.recurrent_kernel_init, bias_init=self.bias_init,
-          name=f'h{component}')(h) # type: ignore[call-arg]
+          kernel_init=self.recurrent_kernel_init,
+          bias_init=self.bias_init,
+          name=f'h{component}',  # type: ignore[call-arg]
+      )(h)
     dense_h = _concat_dense(h, dense_params_h, use_bias=True)
     dense_i = _concat_dense(inputs, dense_params_i, use_bias=False)
 
     i = self.gate_fn(dense_h['i'] + dense_i['i'])
     f = self.gate_fn(dense_h['f'] + dense_i['f'])
     g = self.activation_fn(dense_h['g'] + dense_i['g'])
     o = self.gate_fn(dense_h['o'] + dense_i['o'])
@@ -344,15 +371,16 @@
     new_c = f * c + i * g
     new_h = o * self.activation_fn(new_c)
     return (new_c, new_h), new_h
 
   @nowrap
   @deprecation_method_decorator
   def initialize_carry(
-      self, rng: PRNGKey, input_shape: Tuple[int, ...]) -> Tuple[Array, Array]:
+      self, rng: PRNGKey, input_shape: Tuple[int, ...]
+  ) -> Tuple[Array, Array]:
     """Initialize the RNN cell carry.
 
     Args:
       rng: random number generator passed to the init_fn.
       input_shape: a tuple providing the shape of the input to the cell.
 
     Returns:
@@ -365,14 +393,15 @@
     h = self.carry_init(key2, mem_shape, self.param_dtype)
     return c, h
 
   @property
   def num_feature_axes(self) -> int:
     return 1
 
+
 class GRUCell(RNNCellBase, metaclass=RNNCellCompatibilityMeta):
   r"""GRU cell.
 
   The mathematical definition of the cell is as follows
 
   .. math::
 
@@ -419,34 +448,39 @@
 
     Returns:
       A tuple with the new carry and the output.
     """
     h = carry
     hidden_features = h.shape[-1]
     # input and recurrent layers are summed so only one needs a bias.
-    dense_h = partial(Dense,
-                      features=hidden_features,
-                      use_bias=False,
-                      dtype=self.dtype,
-                      param_dtype=self.param_dtype,
-                      kernel_init=self.recurrent_kernel_init,
-                      bias_init=self.bias_init)
-    dense_i = partial(Dense,
-                      features=hidden_features,
-                      use_bias=True,
-                      dtype=self.dtype,
-                      param_dtype=self.param_dtype,
-                      kernel_init=self.kernel_init,
-                      bias_init=self.bias_init)
+    dense_h = partial(
+        Dense,
+        features=hidden_features,
+        use_bias=False,
+        dtype=self.dtype,
+        param_dtype=self.param_dtype,
+        kernel_init=self.recurrent_kernel_init,
+        bias_init=self.bias_init,
+    )
+    dense_i = partial(
+        Dense,
+        features=hidden_features,
+        use_bias=True,
+        dtype=self.dtype,
+        param_dtype=self.param_dtype,
+        kernel_init=self.kernel_init,
+        bias_init=self.bias_init,
+    )
     r = self.gate_fn(dense_i(name='ir')(inputs) + dense_h(name='hr')(h))
     z = self.gate_fn(dense_i(name='iz')(inputs) + dense_h(name='hz')(h))
     # add bias because the linear transformations aren't directly summed.
-    n = self.activation_fn(dense_i(name='in')(inputs) +
-                           r * dense_h(name='hn', use_bias=True)(h))
-    new_h = (1. - z) * n + z * h
+    n = self.activation_fn(
+        dense_i(name='in')(inputs) + r * dense_h(name='hn', use_bias=True)(h)
+    )
+    new_h = (1.0 - z) * n + z * h
     return new_h, new_h
 
   @nowrap
   @deprecation_method_decorator
   def initialize_carry(self, rng: PRNGKey, input_shape: Tuple[int, ...]):
     """Initialize the RNN cell carry.
 
@@ -524,33 +558,37 @@
       carry: the hidden state of the Conv2DLSTM cell,
         initialized using `Conv2DLSTM.initialize_carry`.
       inputs: input data with dimensions (batch, spatial_dims..., features).
     Returns:
       A tuple with the new carry and the output.
     """
     c, h = carry
-    input_to_hidden = partial(Conv,
-                              features=4*self.features,
-                              kernel_size=self.kernel_size,
-                              strides=self.strides,
-                              padding=self.padding,
-                              use_bias=self.use_bias,
-                              dtype=self.dtype,
-                              param_dtype=self.param_dtype,
-                              name='ih')
-
-    hidden_to_hidden = partial(Conv,
-                               features=4*self.features,
-                               kernel_size=self.kernel_size,
-                               strides=self.strides,
-                               padding=self.padding,
-                               use_bias=self.use_bias,
-                               dtype=self.dtype,
-                               param_dtype=self.param_dtype,
-                               name='hh')
+    input_to_hidden = partial(
+        Conv,
+        features=4 * self.features,
+        kernel_size=self.kernel_size,
+        strides=self.strides,
+        padding=self.padding,
+        use_bias=self.use_bias,
+        dtype=self.dtype,
+        param_dtype=self.param_dtype,
+        name='ih',
+    )
+
+    hidden_to_hidden = partial(
+        Conv,
+        features=4 * self.features,
+        kernel_size=self.kernel_size,
+        strides=self.strides,
+        padding=self.padding,
+        use_bias=self.use_bias,
+        dtype=self.dtype,
+        param_dtype=self.param_dtype,
+        name='hh',
+    )
 
     gates = input_to_hidden()(inputs) + hidden_to_hidden()(h)
     i, g, f, o = jnp.split(gates, indices_or_sections=4, axis=-1)
 
     f = sigmoid(f + 1)
     new_c = f * c + sigmoid(i) * jnp.tanh(g)
     new_h = sigmoid(o) * jnp.tanh(new_c)
@@ -565,26 +603,27 @@
       rng: random number generator passed to the init_fn.
       input_shape: a tuple providing the shape of the input to the cell.
 
     Returns:
       An initialized carry for the given RNN cell.
     """
     # (*batch_dims, *signal_dims, features)
-    signal_dims = input_shape[-self.num_feature_axes:-1]
-    batch_dims = input_shape[:-self.num_feature_axes]
+    signal_dims = input_shape[-self.num_feature_axes : -1]
+    batch_dims = input_shape[: -self.num_feature_axes]
     key1, key2 = random.split(rng)
     mem_shape = batch_dims + signal_dims + (self.features,)
     c = self.carry_init(key1, mem_shape, self.param_dtype)
     h = self.carry_init(key2, mem_shape, self.param_dtype)
     return c, h
 
   @property
   def num_feature_axes(self) -> int:
     return len(self.kernel_size) + 1
 
+
 class RNN(Module):
   """The ``RNN`` module takes any :class:`RNNCellBase` instance and applies it over a sequence
   using :func:`flax.linen.scan`.
 
   Example::
 
     >>> import jax.numpy as jnp
@@ -677,44 +716,50 @@
       the loop. Mutations to these variables are carried to the next iteration
       and will be preserved when the scan finishes. This argument is forwarded to
       `nn.scan`.
     split_rngs: a mapping from PRNGSequenceFilter to bool specifying whether a collection's
       PRNG key should be split such that its values are different at each step, or replicated
       such that its values remain the same at each step. This argument is forwarded to `nn.scan`.
   """
+
   cell: RNNCellBase
   cell_size: Any = NEVER
   time_major: bool = False
   return_carry: bool = False
   reverse: bool = False
   keep_order: bool = False
   unroll: int = 1
-  variable_axes: Mapping[lift.CollectionFilter,lift.InOutScanAxis] = FrozenDict()
+  variable_axes: Mapping[lift.CollectionFilter, lift.InOutScanAxis] = (
+      FrozenDict()
+  )
   variable_broadcast: lift.CollectionFilter = 'params'
   variable_carry: lift.CollectionFilter = False
-  split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict({'params': False})
+  split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict(
+      {'params': False}
+  )
 
   def __post_init__(self) -> None:
     if self.cell_size is not NEVER:
       raise TypeError(
-        f'The `cell_size` argument is no longer available`. ' + LEGACY_UPDATE_MESSAGE
+          f'The `cell_size` argument is no longer available`. '
+          + LEGACY_UPDATE_MESSAGE
       )
     return super().__post_init__()
 
   def __call__(
-    self,
-    inputs: jax.Array,
-    *,
-    initial_carry: Optional[Carry] = None,
-    init_key: Optional[random.KeyArray] = None,
-    seq_lengths: Optional[Array] = None,
-    return_carry: Optional[bool] = None,
-    time_major: Optional[bool] = None,
-    reverse: Optional[bool] = None,
-    keep_order: Optional[bool] = None,
+      self,
+      inputs: jax.Array,
+      *,
+      initial_carry: Optional[Carry] = None,
+      init_key: Optional[random.KeyArray] = None,
+      seq_lengths: Optional[Array] = None,
+      return_carry: Optional[bool] = None,
+      time_major: Optional[bool] = None,
+      reverse: Optional[bool] = None,
+      keep_order: Optional[bool] = None,
   ) -> Union[Output, Tuple[Carry, Output]]:
     """
     Applies the RNN to the inputs.
 
     ``__call__`` allows you to optionally override some attributes like ``return_carry``
     and ``time_major`` defined in the constructor.
 
@@ -753,105 +798,120 @@
     if reverse is None:
       reverse = self.reverse
     if keep_order is None:
       keep_order = self.keep_order
 
     # Infer the number of batch dimensions from the input shape.
     # Cells like ConvLSTM have additional spatial dimensions.
-    time_axis = 0 if time_major else inputs.ndim - (self.cell.num_feature_axes + 1)
+    time_axis = (
+        0 if time_major else inputs.ndim - (self.cell.num_feature_axes + 1)
+    )
 
     # make time_axis positive
     if time_axis < 0:
       time_axis += inputs.ndim
 
     if time_major:
       # we add +1 because we moved the time axis to the front
-      batch_dims = inputs.shape[1:-self.cell.num_feature_axes]
+      batch_dims = inputs.shape[1 : -self.cell.num_feature_axes]
     else:
       batch_dims = inputs.shape[:time_axis]
 
     # maybe reverse the sequence
     if reverse:
       inputs = jax.tree_map(
-        lambda x: flip_sequences(
-          x, seq_lengths, num_batch_dims=len(batch_dims), time_major=time_major), # type: ignore
-        inputs)
+          lambda x: flip_sequences(
+              x, seq_lengths, num_batch_dims=len(batch_dims), time_major=time_major  # type: ignore
+          ),
+          inputs,
+      )
 
     carry: Carry
     if initial_carry is None:
       if init_key is None:
         init_key = random.PRNGKey(0)
 
-      input_shape = inputs.shape[:time_axis] + inputs.shape[time_axis + 1:]
+      input_shape = inputs.shape[:time_axis] + inputs.shape[time_axis + 1 :]
       carry = self.cell.initialize_carry(init_key, input_shape)
     else:
       carry = initial_carry
 
+    slice_carry = seq_lengths is not None and return_carry
+
     def scan_fn(
-      cell: RNNCellBase, carry: Carry, x: Array
+        cell: RNNCellBase, carry: Carry, x: Array
     ) -> Union[Tuple[Carry, Array], Tuple[Carry, Tuple[Carry, Array]]]:
       carry, y = cell(carry, x)
       # When we have a segmentation mask we return the carry as an output
       # so that we can select the last carry for each sequence later.
       # This uses more memory but is faster than using jnp.where at each
       # iteration. As a small optimization do this when we really need it.
-      if seq_lengths is not None and return_carry:
+      if slice_carry:
         return carry, (carry, y)
       else:
         return carry, y
 
     scan = transforms.scan(
-      scan_fn,
-      in_axes=time_axis,
-      out_axes=time_axis if seq_lengths is None else (0, time_axis),
-      unroll=self.unroll,
-      variable_axes=self.variable_axes,
-      variable_broadcast=self.variable_broadcast,
-      variable_carry=self.variable_carry,
-      split_rngs=self.split_rngs,
+        scan_fn,
+        in_axes=time_axis,
+        out_axes=(0, time_axis) if slice_carry else time_axis,
+        unroll=self.unroll,
+        variable_axes=self.variable_axes,
+        variable_broadcast=self.variable_broadcast,
+        variable_carry=self.variable_carry,
+        split_rngs=self.split_rngs,
     )
 
     scan_output = scan(self.cell, carry, inputs)
 
     # Next we select the final carry. If a segmentation mask was provided and
     # return_carry is True we slice the carry history and select the last valid
     # carry for each sequence. Otherwise we just use the last carry.
-    if seq_lengths is not None and return_carry:
+    if slice_carry:
+      assert seq_lengths is not None
       _, (carries, outputs) = scan_output
       # seq_lengths[None] expands the shape of the mask to match the
       # number of dimensions of the carry.
       carry = _select_last_carry(carries, seq_lengths)
     else:
       carry, outputs = scan_output
 
     if reverse and keep_order:
       outputs = jax.tree_map(
-        lambda x: flip_sequences(
-          x, seq_lengths, num_batch_dims=len(batch_dims), time_major=time_major), # type: ignore
-        outputs)
+          lambda x: flip_sequences(
+              x, seq_lengths, num_batch_dims=len(batch_dims), time_major=time_major  # type: ignore
+          ),
+          outputs,
+      )
 
     if return_carry:
       return carry, outputs
     else:
       return outputs
 
+
 def _select_last_carry(sequence: A, seq_lengths: jnp.ndarray) -> A:
   last_idx = seq_lengths - 1
 
   def _slice_array(x: jnp.ndarray):
     return x[last_idx, jnp.arange(x.shape[1])]
 
   return jax.tree_map(_slice_array, sequence)
 
+
 def _expand_dims_like(x, target):
   """Expands the shape of `x` to match `target`'s shape by adding singleton dimensions."""
   return x.reshape(list(x.shape) + [1] * (target.ndim - x.ndim))
 
+
 def flip_sequences(
-  inputs: Array, seq_lengths: Optional[Array], num_batch_dims: int, time_major: bool
+    inputs: Array,
+    seq_lengths: Optional[Array],
+    num_batch_dims: int,
+    time_major: bool,
 ) -> Array:
   """Flips a sequence of inputs along the time axis.
 
   This function can be used to prepare inputs for the reverse direction of a
   bidirectional LSTM. It solves the issue that, when naively flipping multiple
   padded sequences stored in a matrix, the first elements would be padding
   values for those sequences that were padded. This function keeps the padding
@@ -883,65 +943,73 @@
     # reverse inputs and return
     inputs = jnp.flip(inputs, axis=time_axis)
     return inputs
 
   seq_lengths = jnp.expand_dims(seq_lengths, axis=time_axis)
 
   # create indexes
-  idxs = jnp.arange(max_steps - 1, -1, -1) # [max_steps]
+  idxs = jnp.arange(max_steps - 1, -1, -1)  # [max_steps]
   if time_major:
     idxs = jnp.reshape(idxs, [max_steps] + [1] * num_batch_dims)
   else:
-    idxs = jnp.reshape(idxs, [1] * num_batch_dims + [max_steps]) # [1, ..., max_steps]
-  idxs = (idxs + seq_lengths) % max_steps # [*batch, max_steps]
-  idxs = _expand_dims_like(idxs, target=inputs) # [*batch, max_steps, *features]
+    idxs = jnp.reshape(
+        idxs, [1] * num_batch_dims + [max_steps]
+    )  # [1, ..., max_steps]
+  idxs = (idxs + seq_lengths) % max_steps  # [*batch, max_steps]
+  idxs = _expand_dims_like(
+      idxs, target=inputs
+  )  # [*batch, max_steps, *features]
   # Select the inputs in flipped order.
   outputs = jnp.take_along_axis(inputs, idxs, axis=time_axis)
 
   return outputs
 
 
 def _concatenate(a: Array, b: Array) -> Array:
   """Concatenates two arrays along the last dimension."""
   return jnp.concatenate([a, b], axis=-1)
 
+
 class RNNBase(Protocol):
+
   def __call__(
-    self,
-    inputs: jax.Array,
-    *,
-    initial_carry: Optional[Carry] = None,
-    init_key: Optional[random.KeyArray] = None,
-    seq_lengths: Optional[Array] = None,
-    return_carry: Optional[bool] = None,
-    time_major: Optional[bool] = None,
-    reverse: Optional[bool] = None,
-    keep_order: Optional[bool] = None,
+      self,
+      inputs: jax.Array,
+      *,
+      initial_carry: Optional[Carry] = None,
+      init_key: Optional[random.KeyArray] = None,
+      seq_lengths: Optional[Array] = None,
+      return_carry: Optional[bool] = None,
+      time_major: Optional[bool] = None,
+      reverse: Optional[bool] = None,
+      keep_order: Optional[bool] = None,
   ) -> Union[Output, Tuple[Carry, Output]]:
     ...
 
+
 class Bidirectional(Module):
   """Processes the input in both directions and merges the results."""
+
   forward_rnn: RNNBase
   backward_rnn: RNNBase
   merge_fn: Callable[[Array, Array], Array] = _concatenate
   time_major: bool = False
   return_carry: bool = False
 
   def __call__(
-    self,
-    inputs: jax.Array,
-    *,
-    initial_carry: Optional[Carry] = None,
-    init_key: Optional[random.KeyArray] = None,
-    seq_lengths: Optional[Array] = None,
-    return_carry: Optional[bool] = None,
-    time_major: Optional[bool] = None,
-    reverse: Optional[bool] = None,
-    keep_order: Optional[bool] = None,
+      self,
+      inputs: jax.Array,
+      *,
+      initial_carry: Optional[Carry] = None,
+      init_key: Optional[random.KeyArray] = None,
+      seq_lengths: Optional[Array] = None,
+      return_carry: Optional[bool] = None,
+      time_major: Optional[bool] = None,
+      reverse: Optional[bool] = None,
+      keep_order: Optional[bool] = None,
   ) -> Union[Output, Tuple[Carry, Output]]:
     if time_major is None:
       time_major = self.time_major
     if return_carry is None:
       return_carry = self.return_carry
     if init_key is not None:
       key_forward, key_backward = random.split(init_key)
@@ -950,28 +1018,43 @@
     if initial_carry is not None:
       initial_carry_forward, initial_carry_backward = initial_carry
     else:
       initial_carry_forward = initial_carry_backward = None
     # Throw a warning in case the user accidentally re-uses the forward RNN
     # for the backward pass and does not intend for them to share parameters.
     if self.forward_rnn is self.backward_rnn:
-      logging.warning(("forward_rnn and backward_rnn is the same object, so "
-      "they will share parameters."))
+      logging.warning(
+          (
+              'forward_rnn and backward_rnn is the same object, so '
+              'they will share parameters.'
+          )
+      )
 
     # Encode in the forward direction.
     carry_forward, outputs_forward = self.forward_rnn(
-      inputs, initial_carry=initial_carry_forward, init_key=key_forward,
-      seq_lengths=seq_lengths, return_carry=True,
-      time_major=time_major, reverse=False)
+        inputs,
+        initial_carry=initial_carry_forward,
+        init_key=key_forward,
+        seq_lengths=seq_lengths,
+        return_carry=True,
+        time_major=time_major,
+        reverse=False,
+    )
 
     carry_backward, outputs_backward = self.backward_rnn(
-      inputs, initial_carry=initial_carry_backward, init_key=key_backward,
-      seq_lengths=seq_lengths, return_carry=True,
-      time_major=time_major, reverse=True, keep_order=True)
+        inputs,
+        initial_carry=initial_carry_backward,
+        init_key=key_backward,
+        seq_lengths=seq_lengths,
+        return_carry=True,
+        time_major=time_major,
+        reverse=True,
+        keep_order=True,
+    )
 
     carry = (carry_forward, carry_backward)
     outputs = jax.tree_map(self.merge_fn, outputs_forward, outputs_backward)
 
     if return_carry:
       return carry, outputs
     else:
-      return outputs
+      return outputs
```

### Comparing `flax-0.7.0/flax/linen/spmd.py` & `flax-0.7.1/flax/linen/spmd.py`

 * *Files 1% similar despite different names*

```diff
@@ -48,19 +48,22 @@
 LogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name
 PartitionSpecPytree = Any  # pylint: disable=invalid-name
 
 
 # Dynamic Axis Mapping Context
 # ------------------------------------------------------------------------------
 
+
 @dataclasses.dataclass
 class _AxisRules(threading.local):
   """Dynamic logical axis to mesh axis binding context."""
+
   rules: LogicalRules = ()
 
+
 # Global axis binding context.
 _axis_rules = _AxisRules()
 
 
 def set_logical_axis_rules(rules: LogicalRules):
   """Sets the global logical axis to mesh axis binding."""
   _axis_rules.rules = rules
@@ -111,28 +114,32 @@
   """Same as logical_to_mesh_axes, but doesn't fill in _unassigned_axis."""
   if array_dim_names is None:
     return None
   if rules is None:
     rules = _axis_rules.rules
   axis_name_counts = collections.Counter(array_dim_names)
   dups = tuple(
-      k for k, v in axis_name_counts.items() if v > 1 and k is not None)
+      k for k, v in axis_name_counts.items() if v > 1 and k is not None
+  )
   if dups:
     raise ValueError(
-        f'Unsupported: Dimensions {dups} occur more than once in array names.')
+        f'Unsupported: Dimensions {dups} occur more than once in array names.'
+    )
   if not isinstance(rules, (tuple, list)):
     raise ValueError('Unknown axis rule specification type.')
   # We assign mesh axes using a priority based ruleset over logical axis names.
   result: List[Union[_UnassignedAxis, None, str, Tuple[str]]]
   result = [_unassigned_axis] * len(array_dim_names)
   for rule_model_name, rule_mesh_names in rules:
     if rule_model_name in array_dim_names:
       pos = array_dim_names.index(rule_model_name)
-      if (_mesh_assignment_free(rule_mesh_names, result) and
-          result[pos] == _unassigned_axis):
+      if (
+          _mesh_assignment_free(rule_mesh_names, result)
+          and result[pos] == _unassigned_axis
+      ):
         result[pos] = rule_mesh_names
   return result
 
 
 def logical_to_mesh_axes(
     array_dim_names: Optional[Sequence[Optional[str]]],
     rules: Optional[LogicalRules] = None,
@@ -173,18 +180,15 @@
   if result is None:
     return None
   # We default to None - ie unsharded along the dimension.
   result = [None if x is _unassigned_axis else x for x in result]
   return jax.sharding.PartitionSpec(*result)
 
 
-def logical_to_mesh(
-    tree: Any,
-    rules: Optional[LogicalRules] = None
-) -> Any:
+def logical_to_mesh(tree: Any, rules: Optional[LogicalRules] = None) -> Any:
   """Applies logical_to_mesh_axes to pytrees of logical PartitionSpecs."""
   return jax.tree_map(
       lambda x: logical_to_mesh_axes(x, rules),
       tree,
       is_leaf=lambda x: isinstance(x, jax.sharding.PartitionSpec),
   )
 
@@ -206,104 +210,121 @@
   """Checks if global xmap/pjit mesh resource environment is defined."""
   maps_env = maps.thread_resources.env
   return maps_env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison
 
 
 class RulesFallback(enum.Enum):
   """How a sharding constraint should behave when no matching rule is found."""
+
   AXIS_IS_UNSHARDED = 'axis_is_unsharded'
   RAISE_ERROR = 'raise_error'
   NO_CONSTRAINT = 'no_constraint'
 
 
 def _with_sharding_constraint(
     x: Array,
     axis_resources: Optional[jax.sharding.PartitionSpec],
-    mesh: Optional[jax.sharding.Mesh] = None):
+    mesh: Optional[jax.sharding.Mesh] = None,
+):
   """Wrapper for pjit with_sharding_constraint, no-op on cpu or outside pjit."""
-  if jax.devices()[0].platform == 'cpu' or (not _global_mesh_defined() and mesh is None):
+  if jax.devices()[0].platform == 'cpu' or (
+      not _global_mesh_defined() and mesh is None
+  ):
     return x
   else:
     if mesh is not None and axis_resources is not None:
       sharding = jax.sharding.NamedSharding(mesh, axis_resources)
       return pjit.with_sharding_constraint(x, sharding)
     return pjit.with_sharding_constraint(x, axis_resources)
 
 
 def _with_sharding_constraint_one_fallback(
     axis_resources: LogicalPartitionSpec,
     x: Array,
     fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED,
     rules: Optional[LogicalRules] = None,
-    mesh: Optional[jax.sharding.Mesh] = None):
+    mesh: Optional[jax.sharding.Mesh] = None,
+):
   """Either imposes a sharding constraint or applies fallback."""
   mesh_axes = _logical_to_mesh_axes(axis_resources, rules)
   if mesh_axes is None:
     return _with_sharding_constraint(x, None, mesh=mesh)
 
   if fallback == RulesFallback.AXIS_IS_UNSHARDED:
     mesh_axes = [None if x is _unassigned_axis else x for x in mesh_axes]
   else:
     if any(x is _unassigned_axis for x in mesh_axes):
       if fallback == RulesFallback.RAISE_ERROR:
         raise ValueError(f'Axis names {axis_resources} did not match a rule')
       else:
         return x
-  return _with_sharding_constraint(x, jax.sharding.PartitionSpec(*mesh_axes), mesh=mesh)
+  return _with_sharding_constraint(
+      x, jax.sharding.PartitionSpec(*mesh_axes), mesh=mesh
+  )
 
 
 def _is_logical_spec(x):
   return x is None or (
-      isinstance(x, tuple) and all(isinstance(e, str) or e is None for e in x))
+      isinstance(x, tuple) and all(isinstance(e, str) or e is None for e in x)
+  )
 
 
 def with_logical_constraint(
     x: ArrayPytree,
     logical_axis_resources: LogicalPartitionSpecPytree,
     rules: Optional[LogicalRules] = None,
     mesh: Optional[jax.sharding.Mesh] = None,
-    fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED):
+    fallback: RulesFallback = RulesFallback.AXIS_IS_UNSHARDED,
+):
   """Version of pjit's with_sharding_constraint that uses logical axis names."""
   # If no axis binding is set, this is a no-op.
   if rules is None:
     rules = _axis_rules.rules
   if not rules or logical_axis_resources is None:
     return x
   # Translate logical names to mesh assignments.
   return jax.tree_util.tree_map(
       functools.partial(
-          _with_sharding_constraint_one_fallback, fallback=fallback,
-          rules=rules, mesh=mesh),
+          _with_sharding_constraint_one_fallback,
+          fallback=fallback,
+          rules=rules,
+          mesh=mesh,
+      ),
       logical_axis_resources,
       x,
-      is_leaf=_is_logical_spec)
+      is_leaf=_is_logical_spec,
+  )
 
 
 # Logical Partitioning Axis Metadata
 # ------------------------------------------------------------------------------
 
 
 class LogicallyPartitioned(meta.Partitioned):
   rules: Optional[LogicalRules] = struct.field(default=None, pytree_node=False)
+
   def unbox(self, apply_constraint=True) -> Any:
     """Returns the wrapped value with the partitioning constraint applied."""
     if apply_constraint and (_global_mesh_defined() or self.mesh is not None):
       return with_logical_constraint(
-          self.value, self.get_partition_spec(),
-          rules=self.rules, mesh=self.mesh)
+          self.value,
+          self.get_partition_spec(),
+          rules=self.rules,
+          mesh=self.mesh,
+      )
     else:
       return self.value
 
 
 def with_logical_partitioning(
     fn: Callable[..., Any],
     names: meta.LogicalNames,
     mesh: Optional[jax.sharding.Mesh] = None,
     rules: Optional[LogicalRules] = None,
-  ) ->  Callable[..., LogicallyPartitioned]:
+) -> Callable[..., LogicallyPartitioned]:
   """Wraps a function's return value with LogicallyPartitioned.
 
   Example::
 
     kernel_init = with_logical_partitioning(
         nn.initializers.lecun_normal, (None, "data"))
     partitioned_dense = nn.Dense(features, kernel_init=kernel_init)
@@ -315,12 +336,15 @@
       resource is used if available.
     rules: Optional logical to mesh rules use. If None, the global rules
       are used if available.
   Returns:
     A function wrapping ``fn`` that will return an instance of
     ``LogicallyPartitioned``.
   """
+
   @functools.wraps(fn)
   def wrapper(*args, **kwargs):
-    return LogicallyPartitioned(fn(*args, **kwargs), names,
-                                rules=rules, mesh=mesh)
+    return LogicallyPartitioned(
+        fn(*args, **kwargs), names, rules=rules, mesh=mesh
+    )
+
   return wrapper
```

### Comparing `flax-0.7.0/flax/linen/stochastic.py` & `flax-0.7.1/flax/linen/stochastic.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,35 +27,40 @@
 
 KeyArray = Union[jax.Array, jax.random.KeyArray]
 
 
 class Dropout(Module):
   """Create a dropout layer.
 
-    Note: When using :meth:`Module.apply() <flax.linen.Module.apply>`, make sure
-    to include an RNG seed named `'dropout'`. For example::
+  Note: When using :meth:`Module.apply() <flax.linen.Module.apply>`, make sure
+  to include an RNG seed named `'dropout'`. For example::
 
-      model.apply({'params': params}, inputs=inputs, train=True, rngs={'dropout': dropout_rng})`
+    model.apply({'params': params}, inputs=inputs, train=True, rngs={'dropout':
+    dropout_rng})`
 
-    Attributes:
-      rate: the dropout probability.  (_not_ the keep rate!)
-      broadcast_dims: dimensions that will share the same dropout mask
-      deterministic: if false the inputs are scaled by `1 / (1 - rate)` and
-        masked, whereas if true, no mask is applied and the inputs are returned
-        as is.
-      rng_collection: the rng collection name to use when requesting an rng key.
+  Attributes:
+    rate: the dropout probability.  (_not_ the keep rate!)
+    broadcast_dims: dimensions that will share the same dropout mask
+    deterministic: if false the inputs are scaled by `1 / (1 - rate)` and
+      masked, whereas if true, no mask is applied and the inputs are returned as
+      is.
+    rng_collection: the rng collection name to use when requesting an rng key.
   """
+
   rate: float
   broadcast_dims: Sequence[int] = ()
   deterministic: Optional[bool] = None
   rng_collection: str = 'dropout'
 
   @compact
   def __call__(
-    self, inputs, deterministic: Optional[bool] = None, rng: Optional[KeyArray] = None
+      self,
+      inputs,
+      deterministic: Optional[bool] = None,
+      rng: Optional[KeyArray] = None,
   ):
     """Applies a random dropout mask to the input.
 
     Args:
       inputs: the inputs that should be randomly masked.
       deterministic: if false the inputs are scaled by `1 / (1 - rate)` and
         masked, whereas if true, no mask is applied and the inputs are returned
@@ -63,24 +68,25 @@
       rng: an optional PRNGKey used as the random key, if not specified, one
         will be generated using ``make_rng`` with the ``rng_collection`` name.
 
     Returns:
       The masked inputs reweighted to preserve mean.
     """
     deterministic = merge_param(
-        'deterministic', self.deterministic, deterministic)
+        'deterministic', self.deterministic, deterministic
+    )
 
-    if (self.rate == 0.) or deterministic:
+    if (self.rate == 0.0) or deterministic:
       return inputs
 
     # Prevent gradient NaNs in 1.0 edge-case.
     if self.rate == 1.0:
       return jnp.zeros_like(inputs)
 
-    keep_prob = 1. - self.rate
+    keep_prob = 1.0 - self.rate
     if rng is None:
       rng = self.make_rng(self.rng_collection)
     broadcast_shape = list(inputs.shape)
     for dim in self.broadcast_dims:
       broadcast_shape[dim] = 1
     mask = random.bernoulli(rng, p=keep_prob, shape=broadcast_shape)
     mask = jnp.broadcast_to(mask, inputs.shape)
```

### Comparing `flax-0.7.0/flax/linen/summary.py` & `flax-0.7.1/flax/linen/summary.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Flax Module summary library."""
 from abc import ABC, abstractmethod
 import dataclasses
 import io
+from types import MappingProxyType
 from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple, Type, Union
 from flax.core import unfreeze
 
 import flax.linen.module as module_lib
 from flax.core import meta
 from flax.core.scope import CollectionFilter, FrozenVariableDict, MutableVariableDict
 import jax
@@ -28,23 +29,25 @@
 import rich.table
 import rich.text
 import yaml
 import numpy as np
 
 PRNGKey = Any  # pylint: disable=invalid-name
 RNGSequences = Dict[str, PRNGKey]
-Array = Any    # pylint: disable=invalid-name
+Array = Any  # pylint: disable=invalid-name
+
 
 class _ValueRepresentation(ABC):
   """A class that represents a value in the summary table."""
 
   @abstractmethod
   def render(self) -> str:
     ...
 
+
 @dataclasses.dataclass
 class _ArrayRepresentation(_ValueRepresentation):
   shape: Tuple[int, ...]
   dtype: Any
 
   @classmethod
   def from_array(cls, x: Array) -> '_ArrayRepresentation':
@@ -54,33 +57,40 @@
   def render_array(cls, x) -> str:
     return cls.from_array(x).render()
 
   def render(self):
     shape_repr = ','.join(str(x) for x in self.shape)
     return f'[dim]{self.dtype}[/dim][{shape_repr}]'
 
+
 @dataclasses.dataclass
 class _PartitionedArrayRepresentation(_ValueRepresentation):
   array_representation: _ArrayRepresentation
   names: meta.LogicalNames
 
   @classmethod
-  def from_partitioned(cls, partitioned: meta.Partitioned) -> '_PartitionedArrayRepresentation':
-    return cls(_ArrayRepresentation.from_array(partitioned.value), partitioned.names)
+  def from_partitioned(
+      cls, partitioned: meta.Partitioned
+  ) -> '_PartitionedArrayRepresentation':
+    return cls(
+        _ArrayRepresentation.from_array(partitioned.value), partitioned.names
+    )
 
   def render(self):
     return self.array_representation.render() + f' [dim]P[/dim]{self.names}'
 
+
 @dataclasses.dataclass
 class _ObjectRepresentation(_ValueRepresentation):
   obj: Any
 
   def render(self):
     return repr(self.obj)
 
+
 @dataclasses.dataclass
 class Row:
   """Contains the information about a single row in the summary table.
 
   Attributes:
     path: A tuple of strings that represents the path to the module.
     outputs: Output of the Module as reported by `capture_intermediates`.
@@ -88,60 +98,73 @@
       included).
     counted_variables: Dictionary of variables that should be counted for this
       row, if no summarization is done (e.g. `depth=None` in `module_summary`)
       then this field is the same as `module_variables`, however if a
       summarization is done then this dictionary potentially contains parameters
       from submodules depending on the depth of the Module in question.
   """
+
   path: Tuple[str, ...]
   module_type: Type[module_lib.Module]
   method: str
   inputs: Any
   outputs: Any
   module_variables: Dict[str, Dict[str, Any]]
   counted_variables: Dict[str, Dict[str, Any]]
 
   def __post_init__(self):
     self.inputs = self.inputs
     self.outputs = self.outputs
     self.module_variables = self.module_variables
     self.counted_variables = self.counted_variables
 
-  def size_and_bytes(self, collections: Iterable[str]) -> Dict[str, Tuple[int, int]]:
+  def size_and_bytes(
+      self, collections: Iterable[str]
+  ) -> Dict[str, Tuple[int, int]]:
     return {
-        col: _size_and_bytes(self.counted_variables[col])
-        if col in self.counted_variables else (0, 0) for col in collections
+        col: (
+            _size_and_bytes(self.counted_variables[col])
+            if col in self.counted_variables
+            else (0, 0)
+        )
+        for col in collections
     }
 
 
 class Table(List[Row]):
   """A list of Row objects.
 
   Table inherits from `List[Row]` so it has all the methods of a list, however
   it also contains some additional fields:
 
   * `module`: the module that this table is summarizing
   * `collections`: a list containing the parameter collections (e.g. 'params', 'batch_stats', etc)
   """
 
-  def __init__(self, module: module_lib.Module, collections: Sequence[str],
-               rows: Iterable[Row]):
+  def __init__(
+      self,
+      module: module_lib.Module,
+      collections: Sequence[str],
+      rows: Iterable[Row],
+  ):
     super().__init__(rows)
     self.module = module
     self.collections = collections
 
 
 def tabulate(
-  module: module_lib.Module,
-  rngs: Union[PRNGKey, RNGSequences],
-  depth: Optional[int] = None,
-  show_repeated: bool = False,
-  mutable: CollectionFilter = True,
-  console_kwargs: Optional[Mapping[str, Any]] = None,
-  **kwargs,
+    module: module_lib.Module,
+    rngs: Union[PRNGKey, RNGSequences],
+    depth: Optional[int] = None,
+    show_repeated: bool = False,
+    mutable: CollectionFilter = True,
+    console_kwargs: Optional[Mapping[str, Any]] = None,
+    table_kwargs: Mapping[str, Any] = MappingProxyType({}),
+    column_kwargs: Mapping[str, Any] = MappingProxyType({}),
+    **kwargs,
 ) -> Callable[..., str]:
   """Returns a function that creates a summary of the Module represented as a table.
 
   This function accepts most of the same arguments and internally calls `Module.init`,
   except that it returns a function of the form `(*args, **kwargs) -> str` where `*args`
   and `**kwargs` are passed to `method` (e.g. `__call__`) during the forward pass.
 
@@ -213,39 +236,45 @@
       mutable.
     show_repeated: If `True`, repeated calls to the same module will be shown
       in the table, otherwise only the first call will be shown. Default is
       `False`.
     console_kwargs: An optional dictionary with additional keyword arguments that
       are passed to `rich.console.Console` when rendering the table. Default arguments
       are `{'force_terminal': True, 'force_jupyter': False}`.
+    table_kwargs: An optional dictionary with additional keyword arguments that
+      are passed to `rich.table.Table` constructor.
+    column_kwargs: An optional dictionary with additional keyword arguments that
+      are passed to `rich.table.Table.add_column` when adding columns to the table.
     **kwargs: Additional arguments passed to `Module.init`.
 
   Returns:
     A function that accepts the same `*args` and `**kwargs` of the forward pass
     (`method`) and returns a string with a tabular representation of the
     Modules.
   """
 
   def _tabulate_fn(*fn_args, **fn_kwargs):
-    table_fn = _get_module_table(module, depth=depth, show_repeated=show_repeated)
+    table_fn = _get_module_table(
+        module, depth=depth, show_repeated=show_repeated
+    )
     table = table_fn(rngs, *fn_args, mutable=mutable, **fn_kwargs, **kwargs)
-    return _render_table(table, console_kwargs)
+    return _render_table(table, console_kwargs, table_kwargs, column_kwargs)
 
   return _tabulate_fn
 
+
 def _get_module_table(
     module: module_lib.Module,
     depth: Optional[int],
     show_repeated: bool,
 ) -> Callable[..., Table]:
   """A function that takes a Module and returns function with the same signature as `init`
   but returns the Table representation of the Module."""
 
   def _get_table_fn(*args, **kwargs):
-
     with module_lib._tabulate_context():
 
       def _get_variables():
         return module.init(*args, **kwargs)
 
       variables = jax.eval_shape(_get_variables)
       calls = module_lib._context.call_info_stack[-1].calls
@@ -275,41 +304,63 @@
           counted_vars = module_vars
       else:
         module_vars, _ = _get_module_variables(c.path, variables, all_paths)
         counted_vars = module_vars
 
       visited_paths.add(c.path)
       rows.append(
-        Row(c.path, c.module_type, c.method, inputs, c.outputs, module_vars, counted_vars))
+          Row(
+              c.path,
+              c.module_type,
+              c.method,
+              inputs,
+              c.outputs,
+              module_vars,
+              counted_vars,
+          )
+      )
 
     return Table(module, tuple(collections), rows)
 
   return _get_table_fn
 
+
 def _get_module_variables(
-  path: Tuple[str, ...], variables: FrozenVariableDict, all_paths: Set[Tuple[str, ...]]
+    path: Tuple[str, ...],
+    variables: FrozenVariableDict,
+    all_paths: Set[Tuple[str, ...]],
 ) -> Tuple[MutableVariableDict, Any]:
   """A function that takes a path and variables structure and returns a
-  (module_variables, submodule_variables) tuple for that path. _get_module_variables
-  uses the `all_paths` set to determine if a variable belongs to a submodule or not."""
+
+  (module_variables, submodule_variables) tuple for that path.
+  _get_module_variables
+  uses the `all_paths` set to determine if a variable belongs to a submodule or
+  not.
+  """
   module_variables = _get_path_variables(path, variables)
   submodule_variables: Any = {collection: {} for collection in module_variables}
-  all_keys = set(key for collection in module_variables.values() for key in collection)
+  all_keys = set(
+      key for collection in module_variables.values() for key in collection
+  )
 
   for key in all_keys:
     submodule_path = path + (key,)
     if submodule_path in all_paths:
-
       for collection in module_variables:
         if key in module_variables[collection]:
-          submodule_variables[collection][key] = module_variables[collection].pop(key)
+          submodule_variables[collection][key] = module_variables[
+              collection
+          ].pop(key)
 
   return module_variables, submodule_variables
 
-def _get_path_variables(path: Tuple[str, ...], variables: FrozenVariableDict) -> MutableVariableDict:
+
+def _get_path_variables(
+    path: Tuple[str, ...], variables: FrozenVariableDict
+) -> MutableVariableDict:
   """A function that takes a path and a variables structure and returns the variable structure at
   that path."""
   path_variables = {}
 
   for collection in variables:
     collection_variables = variables[collection]
     for name in path:
@@ -319,122 +370,148 @@
       collection_variables = collection_variables[name]
 
     if collection_variables is not None:
       path_variables[collection] = unfreeze(collection_variables)
 
   return path_variables
 
+
 def _process_inputs(args, kwargs) -> Any:
   """A function that normalizes the representation of the ``args`` and ``kwargs``
   for the ``inputs`` column."""
   if args and kwargs:
     input_values = (*args, kwargs)
   elif args and not kwargs:
     input_values = args[0] if len(args) == 1 else args
   elif kwargs and not args:
     input_values = kwargs
   else:
     input_values = ()
 
   return input_values
 
-def _render_table(table: Table, console_extras: Optional[Mapping[str, Any]]) -> str:
+
+def _render_table(
+    table: Table,
+    console_extras: Optional[Mapping[str, Any]],
+    table_kwargs: Mapping[str, Any],
+    column_kwargs: Mapping[str, Any],
+) -> str:
   """A function that renders a Table to a string representation using rich."""
   console_kwargs = {'force_terminal': True, 'force_jupyter': False}
   if console_extras is not None:
     console_kwargs.update(console_extras)
 
   non_params_cols = 4
   rich_table = rich.table.Table(
       show_header=True,
       show_lines=True,
       show_footer=True,
       title=f'{table.module.__class__.__name__} Summary',
+      **table_kwargs,
   )
 
-  rich_table.add_column('path')
-  rich_table.add_column('module')
-  rich_table.add_column('inputs')
-  rich_table.add_column('outputs')
+  rich_table.add_column('path', **column_kwargs)
+  rich_table.add_column('module', **column_kwargs)
+  rich_table.add_column('inputs', **column_kwargs)
+  rich_table.add_column('outputs', **column_kwargs)
 
   for col in table.collections:
-    rich_table.add_column(col)
+    rich_table.add_column(col, **column_kwargs)
 
   for row in table:
     collections_size_repr = []
 
     for collection, size_bytes in row.size_and_bytes(table.collections).items():
       col_repr = ''
 
       if collection in row.module_variables:
         module_variables = _represent_tree(row.module_variables[collection])
         module_variables = _normalize_structure(module_variables)
         col_repr += _as_yaml_str(
-          _summary_tree_map(_maybe_render, module_variables))
+            _summary_tree_map(_maybe_render, module_variables)
+        )
         if col_repr:
           col_repr += '\n\n'
 
       col_repr += f'[bold]{_size_and_bytes_repr(*size_bytes)}[/bold]'
       collections_size_repr.append(col_repr)
 
     no_show_methods = {'__call__', '<lambda>'}
     path_repr = '/'.join(row.path)
-    method_repr = f' [dim]({row.method})[/dim]' if row.method not in no_show_methods else ''
+    method_repr = (
+        f' [dim]({row.method})[/dim]'
+        if row.method not in no_show_methods
+        else ''
+    )
     rich_table.add_row(
         path_repr,
         row.module_type.__name__ + method_repr,
-        _as_yaml_str(_summary_tree_map(_maybe_render, _normalize_structure(row.inputs))),
-        _as_yaml_str(_summary_tree_map(_maybe_render, _normalize_structure(row.outputs))),
-        *collections_size_repr)
+        _as_yaml_str(
+            _summary_tree_map(_maybe_render, _normalize_structure(row.inputs))
+        ),
+        _as_yaml_str(
+            _summary_tree_map(_maybe_render, _normalize_structure(row.outputs))
+        ),
+        *collections_size_repr,
+    )
 
   # add footer with totals
   rich_table.columns[non_params_cols - 1].footer = rich.text.Text.from_markup(
-      'Total', justify='right')
+      'Total', justify='right'
+  )
 
   # get collection totals
   collection_total = {col: (0, 0) for col in table.collections}
   for row in table:
     for col, size_bytes in row.size_and_bytes(table.collections).items():
       collection_total[col] = (
           collection_total[col][0] + size_bytes[0],
           collection_total[col][1] + size_bytes[1],
       )
 
   # add totals to footer
   for i, col in enumerate(table.collections):
-    rich_table.columns[non_params_cols + i].footer = \
-      _size_and_bytes_repr(*collection_total[col])
+    rich_table.columns[non_params_cols + i].footer = _size_and_bytes_repr(
+        *collection_total[col]
+    )
 
   # add final totals to caption
   caption_totals = (0, 0)
-  for (size, num_bytes) in collection_total.values():
+  for size, num_bytes in collection_total.values():
     caption_totals = (
         caption_totals[0] + size,
         caption_totals[1] + num_bytes,
     )
 
   rich_table.caption_style = 'bold'
-  rich_table.caption = f'\nTotal Parameters: {_size_and_bytes_repr(*caption_totals)}'
+  rich_table.caption = (
+      f'\nTotal Parameters: {_size_and_bytes_repr(*caption_totals)}'
+  )
 
   return '\n' + _get_rich_repr(rich_table, console_kwargs) + '\n'
 
+
 def _summary_tree_map(f, tree, *rest):
   return jax.tree_util.tree_map(f, tree, *rest, is_leaf=lambda x: x is None)
 
+
 def _size_and_bytes_repr(size: int, num_bytes: int) -> str:
   if not size:
     return ''
   bytes_repr = _bytes_repr(num_bytes)
   return f'{size:,} [dim]({bytes_repr})[/dim]'
 
 
 def _size_and_bytes(pytree: Any) -> Tuple[int, int]:
   leaves = jax.tree_util.tree_leaves(pytree)
   size = sum(x.size for x in leaves if hasattr(x, 'size'))
-  num_bytes = sum(x.size * x.dtype.itemsize for x in leaves if hasattr(x, 'size'))
+  num_bytes = sum(
+      x.size * x.dtype.itemsize for x in leaves if hasattr(x, 'size')
+  )
   return size, num_bytes
 
 
 def _get_rich_repr(obj, console_kwargs):
   f = io.StringIO()
   console = rich.console.Console(file=f, **console_kwargs)
   console.print(obj)
@@ -450,51 +527,65 @@
       value,
       file,
       default_flow_style=False,
       indent=2,
       sort_keys=False,
       explicit_end=False,
   )
-  return file.getvalue().replace('\n...', '').replace('\'', '').strip()
+  return file.getvalue().replace('\n...', '').replace("'", '').strip()
 
 
 def _normalize_structure(obj):
   if isinstance(obj, _ValueRepresentation):
     return obj
   if isinstance(obj, (tuple, list)):
     return tuple(map(_normalize_structure, obj))
   elif isinstance(obj, Mapping):
     return {k: _normalize_structure(v) for k, v in obj.items()}
   elif dataclasses.is_dataclass(obj):
-    return {f.name: _normalize_structure(getattr(obj, f.name)) for f in dataclasses.fields(obj)}
+    return {
+        f.name: _normalize_structure(getattr(obj, f.name))
+        for f in dataclasses.fields(obj)
+    }
   else:
     return obj
 
+
 def _bytes_repr(num_bytes):
-  count, units = ((f'{num_bytes / 1e9 :,.1f}', 'GB') if num_bytes > 1e9 else
-                  (f'{num_bytes / 1e6 :,.1f}', 'MB') if num_bytes > 1e6 else
-                  (f'{num_bytes / 1e3 :,.1f}', 'KB') if num_bytes > 1e3 else
-                  (f'{num_bytes:,}', 'B'))
+  count, units = (
+      (f'{num_bytes / 1e9 :,.1f}', 'GB')
+      if num_bytes > 1e9
+      else (f'{num_bytes / 1e6 :,.1f}', 'MB')
+      if num_bytes > 1e6
+      else (f'{num_bytes / 1e3 :,.1f}', 'KB')
+      if num_bytes > 1e3
+      else (f'{num_bytes:,}', 'B')
+  )
 
   return f'{count} {units}'
 
 
 def _get_value_representation(x: Any) -> _ValueRepresentation:
   if isinstance(x, (int, float, bool, type(None))) or (
-    isinstance(x, np.ndarray) and np.isscalar(x)):
+      isinstance(x, np.ndarray) and np.isscalar(x)
+  ):
     return _ObjectRepresentation(x)
   elif isinstance(x, meta.Partitioned):
     return _PartitionedArrayRepresentation.from_partitioned(x)
   try:
     return _ArrayRepresentation.from_array(x)
   except:
     return _ObjectRepresentation(x)
 
+
 def _represent_tree(x):
   """Returns a tree with the same structure as `x` but with each leaf replaced
   by a `_ValueRepresentation` object."""
   return jax.tree_util.tree_map(
-    _get_value_representation, x,
-    is_leaf=lambda x: x is None or isinstance(x, meta.Partitioned))
+      _get_value_representation,
+      x,
+      is_leaf=lambda x: x is None or isinstance(x, meta.Partitioned),
+  )
+
 
 def _maybe_render(x):
-  return x.render() if hasattr(x, 'render') else repr(x)
+  return x.render() if hasattr(x, 'render') else repr(x)
```

### Comparing `flax-0.7.0/flax/linen/transforms.py` & `flax-0.7.1/flax/linen/transforms.py`

 * *Files 1% similar despite different names*

```diff
@@ -21,80 +21,95 @@
 
 A lifted transformation can be applied to a ``Module`` class or a
 function that takes a ``Module`` instance as its first argument.
 """
 import dataclasses
 import functools
 import inspect
-from typing import (Any, Callable, Dict, Iterable, Mapping, Optional, Sequence,
-                    Tuple, Type, TypeVar, Union)
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Iterable,
+    Mapping,
+    Optional,
+    Sequence,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
 
 from flax import errors
 from flax import struct
 from flax import traceback_util
 from flax.core import lift
 from flax.core import meta
 from flax.core import Scope
 from flax.core.frozen_dict import FrozenDict
 from flax.linen import module as linen_module
+from flax.linen.module import _derive_profiling_name
+from flax.linen.module import _get_unbound_fn
 from flax.linen.module import Module
 from flax.linen.module import Variable
 from flax.linen.module import wrap_method_once
-from flax.linen.module import _get_unbound_fn
-from flax.linen.module import _derive_profiling_name
 import jax
 
 traceback_util.register_exclusion(__file__)
 
 # pylint: disable=protected-access,dangerous-default-value
 
 
 # Utils
 # -----------------------------------------------------------------------------
 def clean_clone(x):
   """Remove scopes and tracers from children."""
   if isinstance(x, Module):
     object.__setattr__(
-        x, 'children',
-        {k: clean_clone(v) for k, v in x.children.items()})
+        x, 'children', {k: clean_clone(v) for k, v in x.children.items()}
+    )
     object.__setattr__(x, 'scope', None)
   return x
 
 
 @struct.dataclass
 class VariablePlaceholder:
   """Used to mark Variables in a JAX-compatible way when lifting arguments."""
+
   collection: str = struct.field(pytree_node=False)
   name: str = struct.field(pytree_node=False)
   unbox: bool = struct.field(pytree_node=False)
   id: int = struct.field(pytree_node=False)
 
 
 @struct.dataclass
 class InstancePlaceholder:
   """Marks module instances in a JAX-compatible way when lifting arguments."""
+
   cls: Type[Any] = struct.field(pytree_node=False)
   attrs: Dict[Any, Any] = struct.field(pytree_node=False)
   id: int = struct.field(pytree_node=False)
 
 
 def _memoize_by_id(fn, refs):
   """Memoization by module/variable id to handle aliasing in traversal."""
+
   @functools.wraps(fn)
   def wrapped_fn(x):
     nonlocal refs
     if isinstance(x, (VariablePlaceholder, InstancePlaceholder)):
       x_id = x.id
     elif isinstance(x, (Variable, Module)):
       x_id = x._id
     else:
       return fn(x)
     if x_id not in refs:
       refs[x_id] = fn(x)
     return refs[x_id]
+
   return wrapped_fn
 
 
 def get_module_scopes(module, args=None, kwargs=None):
   """Get all scopes on module, including constructor Module arguments.
 
   To properly functionalize a Module that has other bound Modules passed in
@@ -118,14 +133,15 @@
     fields as well as any Scopes passed via argument Variables, an updated args
     list, and an updated kwargs dict that have both had Variables replaced with
     VariablePlaceholders and Module instances replaced with InstancePlaceholders
     that are compatible with jax functions.
   """
   scopes = []
   refs = {}
+
   # Gather scopes associated with Variables and Module instances passed as
   # positional and keyword arguments.
   @functools.partial(_memoize_by_id, refs=refs)
   def get_arg_scope(x):
     nonlocal scopes
     if isinstance(x, Variable) and isinstance(x.scope, Scope):
       scopes.append(x.scope)
@@ -137,35 +153,38 @@
           f.name: getattr(x, f.name)
           for f in dataclasses.fields(x)
           if f.name != 'parent' and f.init
       }
       attrs = jax.tree_util.tree_map(get_arg_scope, attrs)
       return InstancePlaceholder(x.__class__, attrs, x._id)
     return x
+
   new_args, new_kwargs = jax.tree_util.tree_map(get_arg_scope, (args, kwargs))
 
   # Gather scopes in Variables and Submodules passed as Module attributes.
   @functools.partial(_memoize_by_id, refs=refs)
   def get_scopes(module):
     nonlocal scopes
     module._try_setup(shallow=True)
+
     def get_scopes_inner(x):
       nonlocal scopes
       if isinstance(x, Module) and isinstance(x.scope, Scope):
         get_scopes(x)
       elif isinstance(x, Variable) and isinstance(x.scope, Scope):
         scopes.append(x.scope)
 
     attrs = {
         f.name: getattr(module, f.name)
         for f in dataclasses.fields(module)
         if f.name != 'parent' and f.init
     }
     jax.tree_util.tree_map(get_scopes_inner, attrs)
     scopes.append(module.scope)
+
   get_scopes(module)
   return scopes, new_args, new_kwargs
 
 
 def set_module_scopes(module, args, kwargs, scopes):
   """Set all scopes on module, including those on Modules in dataclass fields.
 
@@ -190,87 +209,93 @@
   Returns:
     A copy of the module with it and its attributes bound to the scopes passed
     to this function, an updated args list, and an updated kwargs dict with
     updated Variable and Module instance references.
   """
   idx = 0
   refs = {}
+
   # Set scopes associated with Variables and Module instances passed as
   # positional and keyword arguments.
   @functools.partial(_memoize_by_id, refs=refs)
   def set_arg_scope(x):
     nonlocal idx
     if isinstance(x, VariablePlaceholder):
-      new_x = Variable(scope=scopes[idx],
-                       collection=x.collection,
-                       name=x.name,
-                       unbox=x.unbox)
+      new_x = Variable(
+          scope=scopes[idx], collection=x.collection, name=x.name, unbox=x.unbox
+      )
       idx += 1
       return new_x
     elif isinstance(x, InstancePlaceholder):
       instance_scope = scopes[idx]
       idx += 1
       instance_attrs = jax.tree_util.tree_map(set_arg_scope, x.attrs)
       return x.cls(parent=instance_scope, **instance_attrs)
     return x
 
   def is_placeholder(x):
     return isinstance(x, (VariablePlaceholder, InstancePlaceholder))
 
   new_args, new_kwargs = jax.tree_util.tree_map(
-      set_arg_scope, (args, kwargs), is_leaf=is_placeholder)
+      set_arg_scope, (args, kwargs), is_leaf=is_placeholder
+  )
 
   # set scopes in Variables and Submodules passed as Module attributes
   @functools.partial(_memoize_by_id, refs=refs)
   def set_scopes(module):
     nonlocal idx
+
     def set_scopes_inner(x):
       nonlocal idx
       if isinstance(x, Module) and isinstance(x.scope, Scope):
         return set_scopes(x)
       elif isinstance(x, Variable) and isinstance(x.scope, Scope):
-        new_x = Variable(scope=scopes[idx],
-                         collection=x.collection,
-                         name=x.name,
-                         unbox=x.unbox)
+        new_x = Variable(
+            scope=scopes[idx],
+            collection=x.collection,
+            name=x.name,
+            unbox=x.unbox,
+        )
         idx += 1
         return new_x
       else:
         return x
 
     attrs = {
         f.name: getattr(module, f.name)
         for f in dataclasses.fields(module)
         if f.name != 'parent' and f.init
     }
     new_attrs = jax.tree_util.tree_map(set_scopes_inner, attrs)
     new_module = module.clone(parent=scopes[idx], **new_attrs)
     idx += 1
     return new_module
+
   new_module = set_scopes(module)
   assert len(scopes) == idx, f'scope list mismatch {len(scopes)} != {idx}'
   return new_module, new_args, new_kwargs
 
 
 def _test_transformed_return_values(tree, method_name):
   """Tests whether the return value contains any Modules or Variables."""
-  impure = any(map(lambda x: isinstance(x, (Module, Variable)),
-                   jax.tree_util.tree_leaves(tree)))
+  impure = any(
+      map(
+          lambda x: isinstance(x, (Module, Variable)),
+          jax.tree_util.tree_leaves(tree),
+      )
+  )
   if impure:
     raise errors.TransformedMethodReturnValueError(method_name)
 
 
 # Class lifting
 # -----------------------------------------------------------------------------
 def module_class_lift_transform(
-    transform,
-    module_class,
-    *trafo_args,
-    methods=None,
-    **trafo_kwargs):
+    transform, module_class, *trafo_args, methods=None, **trafo_kwargs
+):
   """Module class lift transform."""
   # TODO(marcvanzee): Improve docstrings (#1977).
   # TODO(levskaya): find nicer argument convention for multi-method case?
 
   # Prepare per-method transform args, kwargs.
   if methods is None:
     # Default case, just transform __call__
@@ -279,32 +304,36 @@
     # Transform every method in methods with given args, kwargs.
     class_trafo_args = {m: (trafo_args, trafo_kwargs) for m in methods}
   elif isinstance(methods, dict):
     # Pass different trafo args per each method.
     class_trafo_args = {k: ((), v) for k, v in methods.items()}
   else:
     raise ValueError(
-        'transform methods argument must be None, tuple, list, or dict.')
+        'transform methods argument must be None, tuple, list, or dict.'
+    )
 
   # Handle partially initialized module class constructors.
-  if (isinstance(module_class, functools.partial) and
-      issubclass(module_class.func, Module)):
+  if isinstance(module_class, functools.partial) and issubclass(
+      module_class.func, Module
+  ):
     partial_object = module_class
     module_class = module_class.func
   else:
     partial_object = None
 
   def create_trans_fn(fn_name, fn_trafo_args):
     # get existing unbound method from class
     fn = getattr(module_class, fn_name)
     trafo_args, trafo_kwargs = fn_trafo_args
+
     # we need to create a scope-function from our class for the given method
     @functools.wraps(fn)
     def wrapped_fn(self, *args, **kwargs):
       state = self._state.export()
+
       # make a scope-function to transform
       def core_fn(scopes, *args, **kwargs):
         # make a clone of self using its arguments
         attrs = {
             f.name: getattr(self, f.name)
             for f in dataclasses.fields(self)
             if f.name != 'parent' and f.init
@@ -313,144 +342,171 @@
         cloned = module_class(parent=None, **attrs)
         cloned, args, kwargs = set_module_scopes(cloned, args, kwargs, scopes)
         object.__setattr__(cloned, '_state', state.export())
         res = fn(cloned, *args, **kwargs)
         self._state.reimport(cloned._state)
         _test_transformed_return_values(res, fn_name)
         return res
+
       # here we apply the given lifting transform to the scope-ingesting fn
       trafo_fn = transform(core_fn, *trafo_args, **trafo_kwargs)
       module_scopes, args, kwargs = get_module_scopes(self, args, kwargs)
       ret = trafo_fn(module_scopes, *args, **kwargs)
       return ret
+
     return wrapped_fn
-  transformed_fns = {fn_name: create_trans_fn(fn_name, fn_trafo_args)
-                     for fn_name, fn_trafo_args in class_trafo_args.items()}
+
+  transformed_fns = {
+      fn_name: create_trans_fn(fn_name, fn_trafo_args)
+      for fn_name, fn_trafo_args in class_trafo_args.items()
+  }
   # construct new dynamic class w. transformed methods
   transformed_cls = type(
       transform.__name__.capitalize() + module_class.__name__,
       (module_class,),
-      transformed_fns)
+      transformed_fns,
+  )
   # Handle partially initialized module class constructors.
   if partial_object is not None:
-    transformed_cls = functools.partial(transformed_cls,
-                                        *partial_object.args,
-                                        **partial_object.keywords)
+    transformed_cls = functools.partial(
+        transformed_cls, *partial_object.args, **partial_object.keywords
+    )
   return transformed_cls
 
 
 # Function lifting as decorator on methods __inside__ class definition.
 # -----------------------------------------------------------------------------
-def decorator_lift_transform(transform, class_fn, *trafo_args,
-                             multi_scope=True, **trafo_kwargs):
+def decorator_lift_transform(
+    transform, class_fn, *trafo_args, multi_scope=True, **trafo_kwargs
+):
   """Decorator for lifted transform."""
   # TODO(marcvanzee): Improve docstrings (#1977).
   # Due to the ordering of method decorators, we must wrap the class_fn
   # with the module state management wrapper first to maintain Module state
   # correctly.
   if isinstance(class_fn, tuple):
     class_fns = class_fn
   else:
     class_fns = (class_fn,)
   prewrapped_fns = [wrap_method_once(class_fn) for class_fn in class_fns]
+
   @functools.wraps(prewrapped_fns[0])
   def wrapped_fn(self, *args, **kwargs):
     state = self._state.export()
+
     # make a scope-function to transform
     def core_fn(prewrapped_fn, class_fn, scopes, *args, **kwargs):
       if not multi_scope:
         scopes = [scopes]
       cloned, args, kwargs = set_module_scopes(self, args, kwargs, scopes)
       object.__setattr__(cloned, '_state', state.export())
       res = prewrapped_fn(cloned, *args, **kwargs)
       self._state.reimport(cloned._state)
       _test_transformed_return_values(res, getattr(class_fn, '__name__', None))
       return res
-    core_fns = [functools.partial(core_fn, prewrapped_fn, class_fn)
-                for prewrapped_fn, class_fn in zip(prewrapped_fns, class_fns)]
+
+    core_fns = [
+        functools.partial(core_fn, prewrapped_fn, class_fn)
+        for prewrapped_fn, class_fn in zip(prewrapped_fns, class_fns)
+    ]
     # here we apply the given lifting transform to the scope-ingesting fn
     trafo_fn = transform(*core_fns, *trafo_args, **trafo_kwargs)
     module_scopes, args, kwargs = get_module_scopes(self, args, kwargs)
     if not multi_scope:
       if len(module_scopes) != 1:
         # TODO(levskaya): transforms like jvp & vjp have args that follow the
         # pytree structure of scopes. The user doesn't explicitly control shared
         # modules passed as arguments to methods or as attributes to Module
         # constructors. Therefore, there is no obvious API for specifying
         # arguments per lifted Module.
         raise NotImplementedError(
             'This transform does not yet support'
-            ' Modules that include other Modules passed as arguments.')
+            ' Modules that include other Modules passed as arguments.'
+        )
       module_scopes = module_scopes[0]
     return trafo_fn(module_scopes, *args, **kwargs)
+
   return wrapped_fn
 
 
 # Utility to wrap a class or to use as decorator in def of class method.
 # -----------------------------------------------------------------------------
 
 TransformTarget = Union[Type[Module], Callable[..., Any]]
 Target = TypeVar('Target', bound=TransformTarget)
 
 
 def _is_module_class(target: TransformTarget) -> bool:
-  return (inspect.isclass(target) and issubclass(target, Module) or
-          (isinstance(target, functools.partial)) and
-          _is_module_class(target.func))
+  return (
+      inspect.isclass(target)
+      and issubclass(target, Module)
+      or (isinstance(target, functools.partial))
+      and _is_module_class(target.func)
+  )
 
 
-def lift_transform(transform,
-                   target,
-                   *trafo_args,
-                   methods=None,
-                   **trafo_kwargs):
+def lift_transform(
+    transform, target, *trafo_args, methods=None, **trafo_kwargs
+):
   """Applies to class or as a decorator on class fns."""
   # TODO(marcvanzee): Improve docstrings (#1977).
   if _is_module_class(target):
     return module_class_lift_transform(
-        transform, target, *trafo_args, methods=methods, **trafo_kwargs)
+        transform, target, *trafo_args, methods=methods, **trafo_kwargs
+    )
   # we presume this is being used as a function decorator in class definition
   elif callable(target) and not isinstance(target, Module):
     return decorator_lift_transform(
-        transform, target, *trafo_args, **trafo_kwargs)
+        transform, target, *trafo_args, **trafo_kwargs
+    )
   else:
     raise errors.TransformTargetError(target)
 
 
-def lift_direct_transform(transform: Callable[..., Any],
-                          targets: Tuple[Callable[..., Any], ...],
-                          mdl: Module,
-                          *args, multi_scope=True, **kwargs):
+def lift_direct_transform(
+    transform: Callable[..., Any],
+    targets: Tuple[Callable[..., Any], ...],
+    mdl: Module,
+    *args,
+    multi_scope=True,
+    **kwargs,
+):
   """Lift direct transform."""
   # TODO(marcvanzee): Improve docstrings (#1977).
   for target in targets:
     if _is_module_class(target):
       raise ValueError(
-          f'The {transform.__name__} transform can only be applied on a Module method.'
-          ' That is function that takes a Module instance as its first arg.')
+          f'The {transform.__name__} transform can only be applied on a Module'
+          ' method. That is function that takes a Module instance as its first'
+          ' arg.'
+      )
     elif not callable(target):
       raise ValueError('transform target must be callable')
   # normalize self.foo bound methods to class.foo unbound methods.
   targets = tuple(_get_unbound_fn(target) for target in targets)
   aug_transform = lambda *fns: functools.partial(transform, *fns)
   return decorator_lift_transform(
-      aug_transform, targets, multi_scope=multi_scope)(mdl, *args, **kwargs)
+      aug_transform, targets, multi_scope=multi_scope
+  )(mdl, *args, **kwargs)
 
 
-def vmap(target: Target,
-         variable_axes: Mapping[lift.CollectionFilter,
-                                lift.InOutAxis] = FrozenDict(),
-         split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict(),
-         in_axes=0,
-         out_axes=0,
-         axis_size: Optional[int] = None,
-         axis_name: Optional[str] = None,
-         spmd_axis_name: Optional[str] = None,
-         metadata_params: Mapping[Any, Any] = {},
-         methods=None) -> Target:
+def vmap(
+    target: Target,
+    variable_axes: Mapping[
+        lift.CollectionFilter, lift.InOutAxis
+    ] = FrozenDict(),
+    split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict(),
+    in_axes=0,
+    out_axes=0,
+    axis_size: Optional[int] = None,
+    axis_name: Optional[str] = None,
+    spmd_axis_name: Optional[str] = None,
+    metadata_params: Mapping[Any, Any] = {},
+    methods=None,
+) -> Target:
   """A lifted version of ``jax.vmap``.
 
   See ``jax.vmap`` for the unlifted batch transform in Jax.
 
   ``vmap`` can be used to add a batch axis to a ``Module``.
   For example we could create a version of ``Dense`` with
   a batch axis that does not share parameters::
@@ -514,25 +570,28 @@
       split_rngs,
       methods=methods,
       in_axes=in_axes,
       out_axes=out_axes,
       axis_size=axis_size,
       axis_name=axis_name,
       metadata_params=metadata_params,
-      spmd_axis_name=spmd_axis_name)
+      spmd_axis_name=spmd_axis_name,
+  )
 
 
-def jit(target: Target,
-        variables: lift.CollectionFilter = True,
-        rngs: lift.PRNGSequenceFilter = True,
-        static_argnums: Union[int, Iterable[int]] = (),
-        donate_argnums: Union[int, Iterable[int]] = (),
-        device=None,
-        backend: Union[str, None] = None,
-        methods=None) -> Target:
+def jit(
+    target: Target,
+    variables: lift.CollectionFilter = True,
+    rngs: lift.PRNGSequenceFilter = True,
+    static_argnums: Union[int, Iterable[int]] = (),
+    donate_argnums: Union[int, Iterable[int]] = (),
+    device=None,
+    backend: Union[str, None] = None,
+    methods=None,
+) -> Target:
   """Lifted version of ``jax.jit``.
 
   Args:
     target: a ``Module`` or a function taking a ``Module``
       as its first argument.
     variables: The variable collections that are lifted. By default all
       collections are lifted.
@@ -565,31 +624,36 @@
       ``'tpu'``.
     methods: If `target` is a `Module`, the methods of `Module` to jit.
 
   Returns:
     A wrapped version of target, set up for just-in-time compilation.
   """
   return lift_transform(
-      lift.jit, target,
-      variables=variables, rngs=rngs,
+      lift.jit,
+      target,
+      variables=variables,
+      rngs=rngs,
       static_argnums=static_argnums,
       donate_argnums=donate_argnums,
       device=device,
       backend=backend,
-      methods=methods)
+      methods=methods,
+  )
 
 
-def checkpoint(target: Target,
-               variables: lift.CollectionFilter = True,
-               rngs: lift.PRNGSequenceFilter = True,
-               concrete: bool = False,
-               prevent_cse: bool = True,
-               static_argnums: Union[int, Tuple[int, ...]] = (),
-               policy: Optional[Callable[..., bool]] = None,
-               methods=None) -> Target:
+def checkpoint(
+    target: Target,
+    variables: lift.CollectionFilter = True,
+    rngs: lift.PRNGSequenceFilter = True,
+    concrete: bool = False,
+    prevent_cse: bool = True,
+    static_argnums: Union[int, Tuple[int, ...]] = (),
+    policy: Optional[Callable[..., bool]] = None,
+    methods=None,
+) -> Target:
   """Lifted version of ``jax.checkpoint``.
 
   Checkpointing is a technique for reducing memory usage by recomputing
   activations during backpropagation. When training large models, it can be
   helpful to checkpoint parts of the model to trade off memory usage for
   additional computation.
 
@@ -647,33 +711,41 @@
     A wrapped version of ``target``. When computing gradients intermediate
     computations will be re-computed on the backward pass.
   """
   # subtract 1 from each static_argnums because 'self' is not passed to the
   # lifted function
   static_argnums = jax.tree_util.tree_map(lambda x: x - 1, static_argnums)
   return lift_transform(
-      lift.checkpoint, target,
-      variables=variables, rngs=rngs, concrete=concrete,
+      lift.checkpoint,
+      target,
+      variables=variables,
+      rngs=rngs,
+      concrete=concrete,
       static_argnums=static_argnums,
-      prevent_cse=prevent_cse, policy=policy,
-      methods=methods)
+      prevent_cse=prevent_cse,
+      policy=policy,
+      methods=methods,
+  )
 
 
 remat = checkpoint
 
 
 def remat_scan(
     target: Target,
     lengths: Optional[Sequence[int]] = (),
     policy: Optional[Callable[..., bool]] = None,
     variable_broadcast: lift.CollectionFilter = False,
     variable_carry: lift.CollectionFilter = False,
-    variable_axes: Mapping[lift.CollectionFilter,
-                           lift.InOutScanAxis] = FrozenDict({True: 0}),
-    split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict({True: True})
+    variable_axes: Mapping[
+        lift.CollectionFilter, lift.InOutScanAxis
+    ] = FrozenDict({True: 0}),
+    split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict(
+        {True: True}
+    ),
 ) -> Target:
   """Combines remat and scan for memory efficiency and constant time compilation.
 
   ``remat_scan`` allows for constant compile times and sublinear
   memory usage with respect to model depth. At a small constant
   penalty. This is typically beneficial for very deep models.
 
@@ -709,37 +781,42 @@
       If split is False the PRNGs will be the same across iterations. Defaults
       to ``{True: True}``.
 
   Returns:
     A wrapped version of ``target`` that repeats itself prod(lengths) times.
   """
   return lift_transform(
-      lift.remat_scan, target,
+      lift.remat_scan,
+      target,
       lengths=lengths,
       variable_broadcast=variable_broadcast,
       variable_carry=variable_carry,
       variable_axes=variable_axes,
       split_rngs=split_rngs,
       policy=policy,
   )
 
 
-def scan(target: Target,
-         variable_axes: Mapping[lift.CollectionFilter,
-                                lift.InOutScanAxis] = FrozenDict(),
-         variable_broadcast: lift.CollectionFilter = False,
-         variable_carry: lift.CollectionFilter = False,
-         split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict(),
-         in_axes=0, out_axes=0,
-         length: Optional[int] = None,
-         reverse: bool = False,
-         unroll: int = 1,
-         data_transform: Optional[Callable[..., Any]] = None,
-         metadata_params: Mapping[Any, Any] = {},
-         methods=None) -> Target:
+def scan(
+    target: Target,
+    variable_axes: Mapping[
+        lift.CollectionFilter, lift.InOutScanAxis
+    ] = FrozenDict(),
+    variable_broadcast: lift.CollectionFilter = False,
+    variable_carry: lift.CollectionFilter = False,
+    split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict(),
+    in_axes=0,
+    out_axes=0,
+    length: Optional[int] = None,
+    reverse: bool = False,
+    unroll: int = 1,
+    data_transform: Optional[Callable[..., Any]] = None,
+    metadata_params: Mapping[Any, Any] = {},
+    methods=None,
+) -> Target:
   """A lifted version of ``jax.lax.scan``.
 
   See ``jax.lax.scan`` for the unlifted scan in Jax.
 
   To improve consistency with ``vmap``, this version of scan
   uses ``in_axes`` and ``out_axes`` to determine which arguments
   are scanned over and along which axis.
@@ -753,16 +830,16 @@
   #. **carry**: A carried value is updated at each loop iteration. It must
      have the same shape and dtype throughout the loop.
 
   #. **broadcast**: a value that is closed over by the loop. When a variable
      is broadcasted they are typically initialized inside the loop body but
      independent of the loop variables.
 
-  The loop body should have the signature
-  ``(scope, body, carry, *xs) -> (carry, ys)``, where ``xs`` and ``ys``
+  The ``target`` should have the signature
+  ``(module, carry, *xs) -> (carry, ys)``, where ``xs`` and ``ys``
   are the scan values that go in and out of the loop.
 
   Example::
 
     >>> import flax.linen as nn
     >>> import jax
     >>> import jax.numpy as jnp
@@ -840,16 +917,16 @@
     >>> model = ResidualMLP(n_layers=4)
     >>> variables = model.init(jax.random.PRNGKey(42), jnp.ones((1, 2)))
 
   To reduce both compilation and memory usage, you can use :func:`remat_scan`
   which will in addition checkpoint each layer in the scan loop.
 
   Args:
-    target: a ``Module`` or a function taking a ``Module``
-      as its first argument.
+    target: a ``Module`` or a function taking a ``Module`` as its first
+      argument.
     variable_axes: the variable collections that are scanned over.
     variable_broadcast: Specifies the broadcasted variable collections. A
       broadcasted variable should not depend on any computation that cannot be
       lifted out of the loop. This is typically used to define shared parameters
       inside the fn.
     variable_carry: Specifies the variable collections that are carried through
       the loop. Mutations to these variables are carried to the next iteration
@@ -860,52 +937,57 @@
       prefix tree of the arguments. Use `flax.core.broadcast` to feed an entire
       input to each iteration of the scan body.
     out_axes: Specifies the axis to scan over for the return value. Should be a
       prefix tree of the return value.
     length: Specifies the number of loop iterations. This only needs to be
       specified if it cannot be derived from the scan arguments.
     reverse: If true, scan from end to start in reverse order.
-    unroll: how many scan iterations to unroll within a single
-      iteration of a loop (default: 1).
+    unroll: how many scan iterations to unroll within a single iteration of a
+      loop (default: 1).
     data_transform: optional function to transform raw functional-core variable
       and rng groups inside lifted scan body_fn, intended for inline SPMD
       annotations.
     metadata_params: arguments dict passed to AxisMetadata instances in the
       variable tree.
     methods: If `target` is a `Module`, the methods of `Module` to scan over.
 
   Returns:
-    The scan function with the signature ``(scope, carry, *xxs) -> (carry,
-    yys)``, where ``xxs`` and ``yys`` are the scan values that go in and out of
+    The scan function with the signature ``(module, carry, *xs) -> (carry,
+    ys)``, where ``xs`` and ``ys`` are the scan values that go in and out of
     the loop.
   """
   return lift_transform(
-      lift.scan, target,
+      lift.scan,
+      target,
       variable_axes=variable_axes,
       variable_broadcast=variable_broadcast,
       variable_carry=variable_carry,
       split_rngs=split_rngs,
-      in_axes=in_axes, out_axes=out_axes,
+      in_axes=in_axes,
+      out_axes=out_axes,
       length=length,
       reverse=reverse,
       unroll=unroll,
       data_transform=data_transform,
       metadata_params=metadata_params,
-      methods=methods)
+      methods=methods,
+  )
 
 
 def map_variables(
     target: Target,
     mapped_collections: lift.CollectionFilter = True,
     trans_in_fn: Callable[..., Any] = lift.id_fn,
     trans_out_fn: Callable[..., Any] = lift.id_fn,
-    init: bool = False, mutable: bool = False,
+    init: bool = False,
+    mutable: bool = False,
     rngs: lift.PRNGSequenceFilter = True,
     variables: lift.CollectionFilter = True,
-    methods=None) -> Target:
+    methods=None,
+) -> Target:
   """Map Variables inside a module.
 
   ``map_variables`` can be used to transform the variables inside a module
   both before and after the module is applied. This is useful among other
   things for masking the weights of a module without having to modify the
   module itself.
 
@@ -950,33 +1032,37 @@
     methods: If `target` is a `Module`, the methods of `Module` to map variables
       for.
   Returns:
     a wrapped version of ``target`` that will map the specified collections.
   """
 
   return lift_transform(
-      lift.map_variables, target,
+      lift.map_variables,
+      target,
       mapped_collections,
-      trans_in_fn, trans_out_fn,
-      init, mutable,
-      rngs, variables,
+      trans_in_fn,
+      trans_out_fn,
+      init,
+      mutable,
+      rngs,
+      variables,
       methods=methods,
   )
 
 
 def vjp(
     fn: Callable[..., Any],
     mdl: Module,
     *primals,
     has_aux: bool = False,
     reduce_axes=(),
     vjp_variables: lift.CollectionFilter = 'params',
     variables: lift.CollectionFilter = True,
     rngs: lift.PRNGSequenceFilter = True,
-    ) -> Tuple[Any, Any]:
+) -> Tuple[Any, Any]:
   """A lifted version of ``jax.vjp``.
 
   See ``jax.vjp`` for the unlifted vector-Jacobiam product (backward gradient).
 
   Note that a gradient is returned for all variables in the collections
   specified by `vjp_variables`. However, the backward funtion only expects
   a cotangent for the return value of `fn`. If variables require a co-tangent
@@ -1030,20 +1116,25 @@
     ``primals_out`` to a tuple of cotangent vectors with the same shape as
     ``primals``, representing the vector-Jacobian product of ``fn`` evaluated at
     ``primals``. If ``has_aux`` is ``True``, returns a
     ``(primals_out, vjpfun, aux)`` tuple where ``aux`` is the auxiliary data
     returned by ``fn``.
   """
   return lift_direct_transform(
-      lift.vjp, (fn,), mdl, *primals,
+      lift.vjp,
+      (fn,),
+      mdl,
+      *primals,
       multi_scope=False,
-      has_aux=has_aux, reduce_axes=reduce_axes,
+      has_aux=has_aux,
+      reduce_axes=reduce_axes,
       vjp_variables=vjp_variables,
       variables=variables,
-      rngs=rngs)
+      rngs=rngs,
+  )
 
 
 def jvp(
     fn: Callable[..., Any],
     mdl: Module,
     primals,
     tangents,
@@ -1114,32 +1205,39 @@
     A ``(primals_out, tangents_out)`` pair, where ``primals_out`` is
     ``fun(*primals)``, and ``tangents_out`` is the Jacobian-vector product of
     ``function`` evaluated at ``primals`` with ``tangents``. The
     ``tangents_out`` value has the same Python tree structure and shapes as
     ``primals_out``.
   """
   return lift_direct_transform(
-      lift.jvp, (fn,), mdl, primals, tangents, variable_tangents,
+      lift.jvp,
+      (fn,),
+      mdl,
+      primals,
+      tangents,
+      variable_tangents,
       multi_scope=False,
       variables=variables,
-      rngs=rngs)
+      rngs=rngs,
+  )
 
 
 ModuleT = TypeVar('ModuleT', bound=Module)
 C = TypeVar('C')
 
 
 def while_loop(
     cond_fn: Callable[[ModuleT, C], bool],
     body_fn: Callable[[ModuleT, C], C],
     mdl: ModuleT,
     init: C,
     carry_variables: lift.CollectionFilter = False,
     broadcast_variables: lift.CollectionFilter = True,
-    split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict()) -> C:
+    split_rngs: Mapping[lift.PRNGSequenceFilter, bool] = FrozenDict(),
+) -> C:
   """Lifted version of jax.lax.while_loop.
 
   The lifted scope is passed to `cond_fn` and `body_fn`.
   Broadcasted variables are immutable. The carry variable are
   mutable but cannot change shape and dtype.
   This also means you cannot initialize variables inside
   the body. Consider calling `body_fn` once manually before
@@ -1180,30 +1278,39 @@
       therefore read-only (default: all collections)
     split_rngs: Split PRNG sequences will be different for each loop iterations.
       If split is False the PRNGs will be the same across iterations.
   Returns:
     The final state after executing the while loop.
   """
   return lift_direct_transform(
-      lift.while_loop, (cond_fn, body_fn), mdl,
+      lift.while_loop,
+      (cond_fn, body_fn),
+      mdl,
       init,
-      carry_variables, broadcast_variables,
-      split_rngs)
+      carry_variables,
+      broadcast_variables,
+      split_rngs,
+  )
 
 
 def _cond_wrapper(t_fn, f_fn, scope, pred, *ops, variables, rngs):
-  return lift.cond(pred, t_fn, f_fn, scope, *ops, variables=variables, rngs=rngs)
+  return lift.cond(
+      pred, t_fn, f_fn, scope, *ops, variables=variables, rngs=rngs
+  )
 
 
 def cond(
     pred: Any,
-    true_fun: Callable[..., C], false_fun: Callable[..., C],
-    mdl: Module, *operands,
+    true_fun: Callable[..., C],
+    false_fun: Callable[..., C],
+    mdl: Module,
+    *operands,
     variables: lift.CollectionFilter = True,
-    rngs: lift.PRNGSequenceFilter = True) -> C:
+    rngs: lift.PRNGSequenceFilter = True,
+) -> C:
   """Lifted version of ``jax.lax.cond``.
 
   The returned values from ``true_fun`` and ``false_fun``
   must have the same Pytree structure, shapes, and dtypes.
   The variables created or updated inside the
   branches must also have the same structure.
   Note that this constraint is violated when
@@ -1238,31 +1345,42 @@
     variables: The variable collections passed to the conditional
       branches (default: all)
     rngs: The PRNG sequences passed to the conditionals (default: all)
   Returns:
     The result of the evaluated branch (``true_fun`` or ``false_fun``).
   """
   return lift_direct_transform(
-      _cond_wrapper, (true_fun, false_fun), mdl,
-      pred, *operands,
-      variables=variables, rngs=rngs)
+      _cond_wrapper,
+      (true_fun, false_fun),
+      mdl,
+      pred,
+      *operands,
+      variables=variables,
+      rngs=rngs,
+  )
+
 
 def _switch_wrapper(*args, variables, rngs, n_branches):
   # first n_branches arguments are branches.
   # then scope, index, and the rest are *operands
   branches = args[:n_branches]
   scope, index, *operands = args[n_branches:]
-  return lift.switch(index, branches, scope, *operands, variables=variables, rngs=rngs)
+  return lift.switch(
+      index, branches, scope, *operands, variables=variables, rngs=rngs
+  )
+
 
 def switch(
     index: Any,
     branches: Sequence[Callable[..., C]],
-    mdl: Module, *operands,
+    mdl: Module,
+    *operands,
     variables: lift.CollectionFilter = True,
-    rngs: lift.PRNGSequenceFilter = True) -> C:
+    rngs: lift.PRNGSequenceFilter = True,
+) -> C:
   """Lifted version of ``jax.lax.switch``.
 
   The returned values from ``branches``
   must have the same Pytree structure, shapes, and dtypes.
   The variables created or updated inside the
   branches must also have the same structure.
   Note that this constraint is violated when
@@ -1322,37 +1440,48 @@
     variables: The variable collections passed to the conditional
       branches (default: all)
     rngs: The PRNG sequences passed to the conditionals (default: all)
   Returns:
     The result of the evaluated branch.
   """
   return lift_direct_transform(
-      _switch_wrapper, tuple(branches), mdl,
-      index, *operands,
-      variables=variables, rngs=rngs, n_branches=len(branches))
+      _switch_wrapper,
+      tuple(branches),
+      mdl,
+      index,
+      *operands,
+      variables=variables,
+      rngs=rngs,
+      n_branches=len(branches),
+  )
+
 
 # a version of lift.custom_vjp with a single scope function
 # this avoids having to lift multiple functions in
 # lift_transform.
 def _custom_vjp_single_scope_fn(
     fn: Callable[..., Any],
     backward_fn: Callable[..., Any],
     grad_vars: lift.CollectionFilter = 'params',
-    nondiff_argnums=()):
+    nondiff_argnums=(),
+):
   nodiff_fn = functools.partial(fn, needs_residual=False)
   forward_fn = functools.partial(fn, needs_residual=True)
-  return lift.custom_vjp(nodiff_fn, forward_fn, backward_fn, grad_vars,
-                         nondiff_argnums)
+  return lift.custom_vjp(
+      nodiff_fn, forward_fn, backward_fn, grad_vars, nondiff_argnums
+  )
 
 
-def custom_vjp(fn: Callable[..., Any],
-               forward_fn: Callable[..., Any],
-               backward_fn: Callable[..., Any],
-               grad_vars: lift.CollectionFilter = 'params',
-               nondiff_argnums=()):
+def custom_vjp(
+    fn: Callable[..., Any],
+    forward_fn: Callable[..., Any],
+    backward_fn: Callable[..., Any],
+    grad_vars: lift.CollectionFilter = 'params',
+    nondiff_argnums=(),
+):
   """Lifted version of `jax.custom_vjp`.
 
   `forward_fn` and `backward_fn` together define a custom vjp for `fn`.
   The original `fn` will run in case a vjp (backward gradient) is not computed.
 
   The `forward_fn` receives the same arguments as `fn` but is expected to return
   a tuple containing the output of `fn(mdl, *args)` and the residuals that are
@@ -1402,24 +1531,29 @@
       nondiff args).
     grad_vars: The collections for which a vjp will be computed
       (default: "params").
     nondiff_argnums: arguments for which no vjp is computed.
   Returns:
     A function with the same signature as `fn` with the custom vjp.
   """
+
   def shared_forward_fn(*args, needs_residual, **kwargs):
     if needs_residual:
       return forward_fn(*args, **kwargs)
     else:
-      return fn(*args, ** kwargs)
+      return fn(*args, **kwargs)
+
   return decorator_lift_transform(
-      _custom_vjp_single_scope_fn, shared_forward_fn,
-      backward_fn=backward_fn, grad_vars=grad_vars,
+      _custom_vjp_single_scope_fn,
+      shared_forward_fn,
+      backward_fn=backward_fn,
+      grad_vars=grad_vars,
       nondiff_argnums=nondiff_argnums,
-      multi_scope=False)
+      multi_scope=False,
+  )
 
 
 def named_call(class_fn, force=True):
   """Labels a method for labelled traces in profiles.
 
   Note that it is better to use the `jax.named_scope` context manager directly
   to add names to JAX's metadata name stack.
@@ -1427,30 +1561,33 @@
   Args:
     class_fn: The class method to label.
     force: If True, the named_call transform is applied even if it is globally
       disabled. (e.g.: by calling `flax.linen.disable_named_call()`)
   Returns:
     A wrapped version of ``class_fn`` that is labeled.
   """
+
   # We use JAX's dynamic name-stack named_call. No transform boundary needed!
   @functools.wraps(class_fn)
   def wrapped_fn(self, *args, **kwargs):
-    if ((not force and not linen_module._use_named_call)  # pylint: disable=protected-access
-        or self._state.in_setup):  # pylint: disable=protected-access
+    if (not force and not linen_module._use_named_call) or self._state.in_setup:  # pylint: disable=protected-access  # pylint: disable=protected-access
       return class_fn(self, *args, **kwargs)
     full_name = _derive_profiling_name(self, class_fn)
     return jax.named_call(class_fn, name=full_name)(self, *args, **kwargs)
+
   return wrapped_fn
 
 
 def add_metadata_axis(
     target: Target,
-    variable_axes: Mapping[lift.CollectionFilter,
-                          lift.InOutAxis] = FrozenDict(),
-    metadata_params: Dict[Any, Any] = {}) -> Target:
+    variable_axes: Mapping[
+        lift.CollectionFilter, lift.InOutAxis
+    ] = FrozenDict(),
+    metadata_params: Dict[Any, Any] = {},
+) -> Target:
   """A helper to manipulate boxed axis metadata.
 
   This is a helper to manipulate the *metadata* in boxed variables, similar
   to how lifted ``vmap`` and ``scan`` will handle the introduction and stripping
   of the new metadata axis across a transform boundary.
 
   Args:
@@ -1462,20 +1599,23 @@
     methods: If `target` is a `Module`, the methods of `Module` to vmap over.
     metadata_params: arguments dict passed to AxisMetadata instances in the
       variable tree.
   Returns:
     A transformed version of ``target`` that performs a transform of the
     axis metadata on its variables.
   """
+
   def add_fn(axis):
     return lambda x: meta.add_axis(x, axis, metadata_params)
+
   def remove_fn(axis):
     return lambda x: meta.remove_axis(x, axis, metadata_params)
+
   for col_name, axis in variable_axes.items():
     target = map_variables(
         target,
         col_name,
         trans_in_fn=remove_fn(axis),
         trans_out_fn=add_fn(axis),
         mutable=True,
     )
-  return target
+  return target
```

### Comparing `flax-0.7.0/flax/metrics/__init__.py` & `flax-0.7.1/flax/training/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,7 +8,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+"""Flax training utilities."""
```

### Comparing `flax-0.7.0/flax/metrics/tensorboard.py` & `flax-0.7.1/flax/metrics/tensorboard.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,16 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Write Summaries from JAX for use with Tensorboard.
-"""
+"""Write Summaries from JAX for use with Tensorboard."""
 
 import contextlib
 import functools
 import os
 
 # pylint: disable=g-import-not-at-top
 from flax import io
@@ -39,15 +38,24 @@
    Flattened dict.
   """
   items = []
   for k, v in input_dict.items():
     new_key = parent_key + sep + k if parent_key else k
 
     # Valid types according to https://github.com/tensorflow/tensorboard/blob/1204566da5437af55109f7a4af18f9f8b7c4f864/tensorboard/plugins/hparams/summary_v2.py
-    valid_types = (bool, int, float, str, np.bool_, np.integer, np.floating, np.character)
+    valid_types = (
+        bool,
+        int,
+        float,
+        str,
+        np.bool_,
+        np.integer,
+        np.floating,
+        np.character,
+    )
 
     if isinstance(v, dict):
       # Recursively flatten the dict.
       items.extend(_flatten_dict(v, new_key, sep=sep).items())
       continue
     elif not isinstance(v, valid_types):
       # Cast any incompatible values as strings such that they can be handled by hparams
@@ -163,16 +171,21 @@
     # [-1.0, 1.0].
     audiodata = np.clip(np.array(audiodata), -1, 1)
 
     # Convert to tensor value as tf.summary.audio expects data to be a tensor.
     audio = tf.convert_to_tensor(audiodata, dtype=tf.float32)
     with self._as_default(self._event_writer):
       tf.summary.audio(
-          name=tag, data=audio, sample_rate=sample_rate, step=step,
-          max_outputs=max_outputs, encoding='wav')
+          name=tag,
+          data=audio,
+          sample_rate=sample_rate,
+          step=step,
+          max_outputs=max_outputs,
+          encoding='wav',
+      )
 
   def histogram(self, tag, values, step, bins=None):
     """Saves histogram of values.
 
     Args:
       tag: str: label for this data
       values: ndarray: will be flattened by this routine
@@ -207,19 +220,15 @@
       tag: str: label for this data
       tensor: ndarray: tensor data to save.
       step: int: training step
       metadata: Optional SummaryMetadata, as a proto or serialized bytes.
     Note: markdown formatting is rendered by tensorboard.
     """
     with self._as_default(self._event_writer):
-      tf.summary.write(
-          tag=tag,
-          tensor=tensor,
-          step=step,
-          metadata=metadata)
+      tf.summary.write(tag=tag, tensor=tensor, step=step, metadata=metadata)
 
   def hparams(self, hparams):
     """Saves hyper parameters.
 
     Args:
       hparams: Flat mapping from hyper parameter name to value.
     """
```

### Comparing `flax-0.7.0/flax/serialization.py` & `flax-0.7.1/flax/serialization.py`

 * *Files 6% similar despite different names*

```diff
@@ -32,14 +32,15 @@
 
 class _ErrorContext(threading.local):
   """Context for deserialization error messages."""
 
   def __init__(self):
     self.path = []
 
+
 _error_context = _ErrorContext()
 
 
 @contextmanager
 def _record_path(name):
   try:
     _error_context.path.append(name)
@@ -51,14 +52,15 @@
 def current_path():
   """Current state_dict path during deserialization for error messages."""
   return '/'.join(_error_context.path)
 
 
 class _NamedTuple:
   """Fake type marker for namedtuple for registry."""
+
   pass
 
 
 def _is_namedtuple(x):
   """Duck typing test for namedtuple factory-generated objects."""
   return isinstance(x, tuple) and hasattr(x, '_fields')
 
@@ -105,42 +107,46 @@
   state_dict = ty_to_state_dict(target)
   assert isinstance(state_dict, dict), 'A state dict must be a Python dict.'
   for key in state_dict.keys():
     assert isinstance(key, str), 'A state dict must only have string keys.'
   return state_dict
 
 
-def register_serialization_state(ty, ty_to_state_dict, ty_from_state_dict,
-                                 override=False):
+def register_serialization_state(
+    ty, ty_to_state_dict, ty_from_state_dict, override=False
+):
   """Register a type for serialization.
 
   Args:
     ty: the type to be registered
     ty_to_state_dict: a function that takes an instance of ty and
       returns its state as a dictionary.
     ty_from_state_dict: a function that takes an instance of ty and
       a state dict, and returns a copy of the instance with the restored state.
     override: override a previously registered serialization handler
       (default: False).
   """
   if ty in _STATE_DICT_REGISTRY and not override:
-    raise ValueError(f'a serialization handler for "{ty.__name__}"'
-                     ' is already registered')
+    raise ValueError(
+        f'a serialization handler for "{ty.__name__}" is already registered'
+    )
   _STATE_DICT_REGISTRY[ty] = (ty_to_state_dict, ty_from_state_dict)
 
 
 def _list_state_dict(xs: List[Any]) -> Dict[str, Any]:
   return {str(i): to_state_dict(x) for i, x in enumerate(xs)}
 
 
 def _restore_list(xs, state_dict: Dict[str, Any]) -> List[Any]:
   if len(state_dict) != len(xs):
-    raise ValueError('The size of the list and the state dict do not match,'
-                     f' got {len(xs)} and {len(state_dict)} '
-                     f'at path {current_path()}')
+    raise ValueError(
+        'The size of the list and the state dict do not match,'
+        f' got {len(xs)} and {len(state_dict)} '
+        f'at path {current_path()}'
+    )
   ys = []
   for i in range(len(state_dict)):
     y = from_state_dict(xs[i], state_dict[str(i)], name=str(i))
     ys.append(y)
   return ys
 
 
@@ -153,68 +159,75 @@
     )
   return {str(key): to_state_dict(value) for key, value in xs.items()}
 
 
 def _restore_dict(xs, states: Dict[str, Any]) -> Dict[str, Any]:
   diff = set(map(str, xs.keys())).difference(states.keys())
   if diff:
-    raise ValueError('The target dict keys and state dict keys do not match,'
-                   f' target dict contains keys {diff} which are not present in state dict '
-                   f'at path {current_path()}')
+    raise ValueError(
+        'The target dict keys and state dict keys do not match, target dict'
+        f' contains keys {diff} which are not present in state dict at path'
+        f' {current_path()}'
+    )
 
-  return {key: from_state_dict(value, states[str(key)], name=str(key))
-          for key, value in xs.items()}
+  return {
+      key: from_state_dict(value, states[str(key)], name=str(key))
+      for key, value in xs.items()
+  }
 
 
 def _namedtuple_state_dict(nt) -> Dict[str, Any]:
   return {key: to_state_dict(getattr(nt, key)) for key in nt._fields}
 
 
 def _restore_namedtuple(xs, state_dict: Dict[str, Any]):
   """Rebuild namedtuple from serialized dict."""
   if set(state_dict.keys()) == {'name', 'fields', 'values'}:
     # TODO(jheek): remove backward compatible named tuple restoration early 2022
-    state_dict = {state_dict['fields'][str(i)]: state_dict['values'][str(i)]
-                  for i in range(len(state_dict['fields']))}
+    state_dict = {
+        state_dict['fields'][str(i)]: state_dict['values'][str(i)]
+        for i in range(len(state_dict['fields']))
+    }
 
   sd_keys = set(state_dict.keys())
   nt_keys = set(xs._fields)
 
   if sd_keys != nt_keys:
     raise ValueError(
         'The field names of the state dict and the named tuple do not match,'
-        f' got {sd_keys} and {nt_keys} at path {current_path()}')
+        f' got {sd_keys} and {nt_keys} at path {current_path()}'
+    )
   fields = {
       k: from_state_dict(getattr(xs, k), v, name=k)
       for k, v in state_dict.items()
   }
   return type(xs)(**fields)
 
 
 register_serialization_state(dict, _dict_state_dict, _restore_dict)
 register_serialization_state(list, _list_state_dict, _restore_list)
 register_serialization_state(
-    tuple, _list_state_dict,
-    lambda xs, state_dict: tuple(_restore_list(list(xs), state_dict)))
-register_serialization_state(_NamedTuple,
-                             _namedtuple_state_dict,
-                             _restore_namedtuple)
+    tuple,
+    _list_state_dict,
+    lambda xs, state_dict: tuple(_restore_list(list(xs), state_dict)),
+)
+register_serialization_state(
+    _NamedTuple, _namedtuple_state_dict, _restore_namedtuple
+)
 
 register_serialization_state(
     jax.tree_util.Partial,
-    lambda x: (
-        {
-            "args": to_state_dict(x.args),
-            "keywords": to_state_dict(x.keywords),
-        }
-    ),
+    lambda x: ({
+        'args': to_state_dict(x.args),
+        'keywords': to_state_dict(x.keywords),
+    }),
     lambda x, sd: jax.tree_util.Partial(
         x.func,
-        *from_state_dict(x.args, sd["args"]),
-        **from_state_dict(x.keywords, sd["keywords"]),
+        *from_state_dict(x.args, sd['args']),
+        **from_state_dict(x.keywords, sd['keywords']),
     ),
 )
 
 # On-the-wire / disk serialization format
 
 # We encode state-dicts via msgpack, using its custom type extension.
 # https://github.com/msgpack/msgpack/blob/master/spec.md
@@ -228,16 +241,18 @@
 
 
 def _ndarray_to_bytes(arr) -> bytes:
   """Save ndarray to simple msgpack encoding."""
   if isinstance(arr, jax.Array):
     arr = np.array(arr)
   if arr.dtype.hasobject or arr.dtype.isalignedstruct:
-    raise ValueError('Object and structured dtypes not supported '
-                     'for serialization of ndarrays.')
+    raise ValueError(
+        'Object and structured dtypes not supported '
+        'for serialization of ndarrays.'
+    )
   tpl = (arr.shape, arr.dtype.name, arr.tobytes('C'))
   return msgpack.packb(tpl, use_bin_type=True)
 
 
 def _dtype_from_name(name: str):
   """Handle JAX bfloat16 dtype correctly."""
   if name == b'bfloat16':
@@ -245,40 +260,42 @@
   else:
     return np.dtype(name)
 
 
 def _ndarray_from_bytes(data: bytes) -> np.ndarray:
   """Load ndarray from simple msgpack encoding."""
   shape, dtype_name, buffer = msgpack.unpackb(data, raw=True)
-  return np.frombuffer(buffer,
-                       dtype=_dtype_from_name(dtype_name),
-                       count=-1,
-                       offset=0).reshape(shape, order='C')
+  return np.frombuffer(
+      buffer, dtype=_dtype_from_name(dtype_name), count=-1, offset=0
+  ).reshape(shape, order='C')
 
 
 class _MsgpackExtType(enum.IntEnum):
   """Messagepack custom type ids."""
+
   ndarray = 1
   native_complex = 2
   npscalar = 3
 
 
 def _msgpack_ext_pack(x):
   """Messagepack encoders for custom types."""
   # TODO(flax-dev): Array here only work when they are fully addressable.
   # If they are not fully addressable, use the GDA path for checkpointing.
   if isinstance(x, (np.ndarray, jax.Array)):
     return msgpack.ExtType(_MsgpackExtType.ndarray, _ndarray_to_bytes(x))
   if np.issctype(type(x)):
     # pack scalar as ndarray
-    return msgpack.ExtType(_MsgpackExtType.npscalar,
-                           _ndarray_to_bytes(np.asarray(x)))
+    return msgpack.ExtType(
+        _MsgpackExtType.npscalar, _ndarray_to_bytes(np.asarray(x))
+    )
   elif isinstance(x, complex):
-    return msgpack.ExtType(_MsgpackExtType.native_complex,
-                           msgpack.packb((x.real, x.imag)))
+    return msgpack.ExtType(
+        _MsgpackExtType.native_complex, msgpack.packb((x.real, x.imag))
+    )
   return x
 
 
 def _msgpack_ext_unpack(code, data):
   """Messagepack decoders for custom types."""
   if code == _MsgpackExtType.ndarray:
     return _ndarray_from_bytes(data)
@@ -317,18 +334,19 @@
 _tuple_to_dict = lambda tpl: {str(x): y for x, y in enumerate(tpl)}
 _dict_to_tuple = lambda dct: tuple(dct[str(i)] for i in range(len(dct)))
 
 
 def _chunk(arr) -> Dict[str, Any]:
   """Convert array to a canonical dictionary of chunked arrays."""
   chunksize = max(1, int(MAX_CHUNK_SIZE / arr.dtype.itemsize))
-  data = {'__msgpack_chunked_array__': True,
-          'shape': _tuple_to_dict(arr.shape)}
+  data = {'__msgpack_chunked_array__': True, 'shape': _tuple_to_dict(arr.shape)}
   flatarr = arr.reshape(-1)
-  chunks = [flatarr[i:i + chunksize] for i in range(0, flatarr.size, chunksize)]
+  chunks = [
+      flatarr[i : i + chunksize] for i in range(0, flatarr.size, chunksize)
+  ]
   data['chunks'] = _tuple_to_dict(chunks)
   return data
 
 
 def _unchunk(data: Dict[str, Any]):
   """Convert canonical dictionary of chunked arrays back into array."""
   assert '__msgpack_chunked_array__' in data
@@ -401,15 +419,16 @@
     encoded_pytree: msgpack-encoded bytes of python tree.
 
   Returns:
     Python tree of dict, list, tuple with python primitive
     and array leaves.
   """
   state_dict = msgpack.unpackb(
-      encoded_pytree, ext_hook=_msgpack_ext_unpack, raw=False)
+      encoded_pytree, ext_hook=_msgpack_ext_unpack, raw=False
+  )
   return _unchunk_array_leaves_in_place(state_dict)
 
 
 def from_bytes(target, encoded_bytes: bytes):
   """Restore optimizer or other object from msgpack-serialized state-dict.
 
   Args:
```

### Comparing `flax-0.7.0/flax/struct.py` & `flax-0.7.1/flax/struct.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,34 +8,33 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Utilities for defining custom classes that can be used with jax transformations.
-"""
+"""Utilities for defining custom classes that can be used with jax transformations."""
 
 import dataclasses
 from typing import TypeVar, Callable, Tuple, Union, Any
 
 from . import serialization
 
 import jax
 from typing_extensions import dataclass_transform  # pytype: disable=not-supported-yet
 
 
-_T = TypeVar("_T")
+_T = TypeVar('_T')
 
 
 def field(pytree_node=True, **kwargs):
   return dataclasses.field(metadata={'pytree_node': pytree_node}, **kwargs)
 
 
-@dataclass_transform(field_descriptors=(field,)) # type: ignore[literal-required]
+@dataclass_transform(field_descriptors=(field,))  # type: ignore[literal-required]
 def dataclass(clz: _T) -> _T:
   """Create a class which can be passed to functional transformations.
 
   NOTE: Inherit from ``PyTreeNode`` instead to avoid type checking issues when
   using PyType.
 
   Jax transformations such as `jax.jit` and `jax.grad` require objects that are
@@ -82,38 +81,38 @@
     class DirectionAndScaleKernel:
       direction: Array
       scale: Array
 
       @classmethod
       def create(cls, kernel):
         scale = jax.numpy.linalg.norm(kernel, axis=0, keepdims=True)
-        directin = direction / scale
+        direction = direction / scale
         return cls(direction, scale)
 
   Args:
     clz: the class that will be transformed by the decorator.
   Returns:
     The new class.
   """
   # check if already a flax dataclass
   if '_flax_dataclass' in clz.__dict__:
     return clz
 
-  data_clz = dataclasses.dataclass(frozen=True)(clz) # type: ignore
+  data_clz = dataclasses.dataclass(frozen=True)(clz)  # type: ignore
   meta_fields = []
   data_fields = []
   for field_info in dataclasses.fields(data_clz):
     is_pytree_node = field_info.metadata.get('pytree_node', True)
     if is_pytree_node:
       data_fields.append(field_info.name)
     else:
       meta_fields.append(field_info.name)
 
   def replace(self, **updates):
-    """"Returns a new object replacing the specified fields with new values."""
+    """ "Returns a new object replacing the specified fields with new values."""
     return dataclasses.replace(self, **updates)
 
   data_clz.replace = replace
 
   def iterate_clz(x):
     meta = tuple(getattr(x, name) for name in meta_fields)
     data = tuple(getattr(x, name) for name in data_fields)
@@ -134,50 +133,59 @@
     return data_clz(**kwargs)
 
   jax.tree_util.register_pytree_with_keys(
       data_clz, iterate_clz_with_keys, clz_from_iterable
   )
 
   def to_state_dict(x):
-    state_dict = {name: serialization.to_state_dict(getattr(x, name))
-                  for name in data_fields}
+    state_dict = {
+        name: serialization.to_state_dict(getattr(x, name))
+        for name in data_fields
+    }
     return state_dict
 
   def from_state_dict(x, state):
     """Restore the state of a data class."""
     state = state.copy()  # copy the state so we can pop the restored fields.
     updates = {}
     for name in data_fields:
       if name not in state:
-        raise ValueError(f'Missing field {name} in state dict while restoring'
-                         f' an instance of {clz.__name__},'
-                         f' at path {serialization.current_path()}')
+        raise ValueError(
+            f'Missing field {name} in state dict while restoring'
+            f' an instance of {clz.__name__},'
+            f' at path {serialization.current_path()}'
+        )
       value = getattr(x, name)
       value_state = state.pop(name)
-      updates[name] = serialization.from_state_dict(value, value_state, name=name)
+      updates[name] = serialization.from_state_dict(
+          value, value_state, name=name
+      )
     if state:
       names = ','.join(state.keys())
-      raise ValueError(f'Unknown field(s) "{names}" in state dict while'
-                       f' restoring an instance of {clz.__name__}'
-                       f' at path {serialization.current_path()}')
+      raise ValueError(
+          f'Unknown field(s) "{names}" in state dict while'
+          f' restoring an instance of {clz.__name__}'
+          f' at path {serialization.current_path()}'
+      )
     return x.replace(**updates)
 
   serialization.register_serialization_state(
-      data_clz, to_state_dict, from_state_dict)
+      data_clz, to_state_dict, from_state_dict
+  )
 
   # add a _flax_dataclass flag to distinguish from regular dataclasses
-  data_clz._flax_dataclass = True # type: ignore[attr-defined]
+  data_clz._flax_dataclass = True  # type: ignore[attr-defined]
 
-  return data_clz # type: ignore
+  return data_clz  # type: ignore
 
 
 TNode = TypeVar('TNode', bound='PyTreeNode')
 
 
-@dataclass_transform(field_descriptors=(field,)) # type: ignore[literal-required]
+@dataclass_transform(field_descriptors=(field,))  # type: ignore[literal-required]
 class PyTreeNode:
   """Base class for dataclasses that should act like a JAX pytree node.
 
   See ``flax.struct.dataclass`` for the ``jax.tree_util`` behavior.
   This base class additionally avoids type checking errors when using PyType.
 
   Example::
```

### Comparing `flax-0.7.0/flax/testing/__init__.py` & `flax-0.7.1/flax/testing/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/flax/testing/benchmark.py` & `flax-0.7.1/flax/testing/benchmark.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 """Benchmark class for Flax regression and integration testing.
 
 This file defines utility functions for collecting model training results
 from TensorBoard summaries, and for reporting benchmarks in a JSON format
 for pickup by continuous integration / monitoring frameworks.
 
 When the `benchmark_output_dir` is provided, the benchmark results are
@@ -41,28 +40,30 @@
 from tensorboard.backend.event_processing import event_file_loader
 from tensorboard.backend.event_processing import io_wrapper
 from tensorboard.summary import v1 as summary_lib
 from tensorboard.util import tensor_util
 
 
 flags.DEFINE_string(
-    'benchmark_output_dir', default=None, help='Benchmark output directory.')
+    'benchmark_output_dir', default=None, help='Benchmark output directory.'
+)
 
 
 FLAGS = flags.FLAGS
 
-_SCALAR_PLUGIN_NAME = summary_lib.scalar_pb(
-    '', 0).value[0].metadata.plugin_data.plugin_name
+_SCALAR_PLUGIN_NAME = (
+    summary_lib.scalar_pb('', 0).value[0].metadata.plugin_data.plugin_name
+)
 
 
 def _make_events_generator(path):
   """Makes a generator yielding TensorBoard events from files in `path`."""
   return directory_watcher.DirectoryWatcher(
-      path, event_file_loader.EventFileLoader,
-      io_wrapper.IsSummaryEventsFile).Load()
+      path, event_file_loader.EventFileLoader, io_wrapper.IsSummaryEventsFile
+  ).Load()
 
 
 def _is_scalar_value(value):
   if value.HasField('metadata') and value.metadata.HasField('plugin_data'):
     plugin_data = value.metadata.plugin_data
     return plugin_data.plugin_name == _SCALAR_PLUGIN_NAME
 
@@ -72,16 +73,20 @@
 def _process_event(event):
   """Parse TensorBoard scalars into a (tag, wall_time, step, scalar) tuple."""
   for value in event.summary.value:
     if not _is_scalar_value(value):
       continue
 
     if value.HasField('tensor'):
-      yield (value.tag, event.wall_time,
-             event.step, tensor_util.make_ndarray(value.tensor).item())
+      yield (
+          value.tag,
+          event.wall_time,
+          event.step,
+          tensor_util.make_ndarray(value.tensor).item(),
+      )
 
 
 def _get_tensorboard_scalars(path):
   """Read and parse scalar TensorBoard summaries.
 
   Args:
     path: str. Path containing TensorBoard event files.
@@ -114,21 +119,19 @@
 
   def __init__(self, *args, **kwargs):
     """Wrap test methods in a try-except decorator to delay exceptions."""
     super().__init__(*args, **kwargs)
     for func_name in dir(self):
       if func_name.startswith('assert'):
         func = getattr(self, func_name)
-        patched_func = functools.partial(
-            self._collect_assert_wrapper, fn=func)
+        patched_func = functools.partial(self._collect_assert_wrapper, fn=func)
         setattr(self, func_name, patched_func)
 
     # Create target directory if defined.
-    if FLAGS.benchmark_output_dir and not io.exists(
-        FLAGS.benchmark_output_dir):
+    if FLAGS.benchmark_output_dir and not io.exists(FLAGS.benchmark_output_dir):
       io.makedirs(FLAGS.benchmark_output_dir)
 
   # pylint: disable=invalid-name
   def _collect_assert_wrapper(self, *args, fn=None, **kwargs):
     """Wrapper around assert methods that caputres and collects failures."""
     try:
       return fn(*args, **kwargs)
@@ -158,16 +161,17 @@
     if defined else uses a temporary directory. This helps to export summary
     files to tensorboard as multiple separate runs for each test method.
     """
     if FLAGS.benchmark_output_dir:
       model_dir = FLAGS.benchmark_output_dir
     else:
       model_dir = tempfile.mkdtemp()
-    model_dir_path = os.path.join(model_dir, self._reported_name or
-                                  self._get_test_name())
+    model_dir_path = os.path.join(
+        model_dir, self._reported_name or self._get_test_name()
+    )
     # Create directories if they don't exist.
     if not io.exists(model_dir_path):
       io.makedirs(model_dir_path)
     return model_dir_path
 
   def has_outstanding_fails(self):
     """Determine whether the benchmark failed, but the error is deferred."""
@@ -251,23 +255,27 @@
         "string" -> "string" map containing anything else of interest
       }
     }
     ```
     """
     name = self._reported_name
     if not name:
-      raise ValueError('Unable to determine test name for reporting '
-                       'benchmark results. Make sure you are using '
-                       '`self.report_*` methods.')
+      raise ValueError(
+          'Unable to determine test name for reporting '
+          'benchmark results. Make sure you are using '
+          '`self.report_*` methods.'
+      )
 
     succeeded = not self.has_outstanding_fails()
-    results = {'name': name,
-               'succeeded': succeeded,
-               'metrics': self._reported_metrics,
-               'extras': self._reported_extras}
+    results = {
+        'name': name,
+        'succeeded': succeeded,
+        'metrics': self._reported_metrics,
+        'extras': self._reported_extras,
+    }
     if self._reported_wall_time is not None:
       results['wall_time'] = self._reported_wall_time
     if not succeeded:
       msg = '\n'.join([str(fail) for fail in self._outstanding_fails])
       results['extras']['failed_assertions'] = msg
 
     results_str = json.dumps(results)
```

### Comparing `flax-0.7.0/flax/traceback_util.py` & `flax-0.7.1/flax/traceback_util.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/flax/training/__init__.py` & `flax-0.7.1/flax/metrics/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,8 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Flax training utilities."""
+# Copyright 2023 The Flax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
```

### Comparing `flax-0.7.0/flax/training/checkpoints.py` & `flax-0.7.1/flax/training/checkpoints.py`

 * *Files 4% similar despite different names*

```diff
@@ -38,36 +38,39 @@
 from flax.training import orbax_utils
 import jax
 from jax import monitoring
 from jax import process_index
 from jax import tree_util as jtu
 from jax.experimental.multihost_utils import sync_global_devices
 import numpy as np
-import orbax.checkpoint as orbax
+import orbax.checkpoint as ocp
 
 _READ_CHECKPOINT_EVENT: str = '/jax/checkpoint/read/durations_sec'
 _WRITE_CHECKPOINT_EVENT: str = '/jax/checkpoint/write/durations_sec'
 _IMPORT_GDAM_SUCCESSFUL = False
 try:
   from jax.experimental.array_serialization.serialization import get_tensorstore_spec
   from jax.experimental.array_serialization.serialization import GlobalAsyncCheckpointManager
+
   _IMPORT_GDAM_SUCCESSFUL = True
 except ImportError:
-  logging.warning('GlobalAsyncCheckpointManager is not imported correctly. '
-                  'Checkpointing of GlobalDeviceArrays will not be available.'
-                  'To use the feature, install tensorstore.')
+  logging.warning(
+      'GlobalAsyncCheckpointManager is not imported correctly. '
+      'Checkpointing of GlobalDeviceArrays will not be available.'
+      'To use the feature, install tensorstore.'
+  )
 
 
 # Single-group reg-exps for int or float numerical substrings.
 # captures sign:
-SIGNED_FLOAT_RE = re.compile(
-    r'([-+]?(?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?)')
+SIGNED_FLOAT_RE = re.compile(r'([-+]?(?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?)')
 # does not capture sign:
 UNSIGNED_FLOAT_RE = re.compile(
-    r'[-+]?((?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?)')
+    r'[-+]?((?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?)'
+)
 # Module name followed by number.
 MODULE_NUM_RE = re.compile(r'(.*)_\d+$')
 # Alternative schemes handled by `gfile`, e.g. on Google Cloud Storage (GCS).
 SCHEME_RE = re.compile('^(?P<scheme>[a-z][a-z0-9.+-]+://)?(?P<path>.*)', re.I)
 
 # Multiprocess arrays (GlobalDeviceArray, or JAX array with multiprocess
 # sharding) is across processes and will be stored in directories with this
@@ -95,41 +98,44 @@
 def _is_multiprocess_array(value: Any) -> bool:
   """Use GlobalAsyncCheckpointManager to save the array if it's only partially available on this host."""
   if isinstance(value, jax.Array):
     return not value.is_fully_addressable
   return False
 
 
-def _checkpoint_path(ckpt_dir: str,
-                     step: Union[int, float, str],
-                     prefix: str = 'checkpoint_') -> str:
+def _checkpoint_path(
+    ckpt_dir: str, step: Union[int, float, str], prefix: str = 'checkpoint_'
+) -> str:
   return os.path.join(ckpt_dir, f'{prefix}{step}')
 
 
 def _checkpoint_path_step(path: str) -> Optional[float]:
   """Returns the step number of a checkpoint path."""
   for s in SIGNED_FLOAT_RE.split(path)[::-1]:
     if SIGNED_FLOAT_RE.match(s):
       return float(s)
   return None
 
+
 def _allowempty_listdir(path: str):
   try:
     return io.listdir(path)
   except io.NotFoundError:
     return []
 
+
 def _safe_remove(path: str):
   """Identify whether a path is a dir or list and choose the correct remove method."""
   if io.isdir(path):
     io.rmtree(path)
   else:
     io.remove(path)
 
-class AsyncManager():
+
+class AsyncManager:
   """A simple object to track async checkpointing.
 
   How to use: create an instance and pass to save_checkpoint() calls:
     am = AsyncManager()
     save_checkpoint(..., async_manager=am)
   """
 
@@ -151,15 +157,15 @@
 
     The future will be tracked as self.save_future.
 
     Args:
       task: The callable to be executed asynchronously.
     """
     self.wait_previous_save()
-    self.save_future = self.executor.submit(task) # type: ignore
+    self.save_future = self.executor.submit(task)  # type: ignore
 
 
 def _split_mp_arrays(
     target: Dict[str, Any]
 ) -> Tuple[Dict[str, Any], List[Tuple[MultiprocessArrayType, str]]]:
   """Split out the multiprocess arrays from the target pytree to save."""
   # When target is a single leaf instead of a pytree dict.
@@ -175,38 +181,49 @@
       subpath = '/'.join(key)
       mpa_targets.append((value, subpath))
       flattened[key] = MP_ARRAY_PH + subpath
   target = traverse_util.unflatten_dict(flattened)
   return target, mpa_targets
 
 
-def _make_mpa_dirs(mpa_targets: List[Tuple[MultiprocessArrayType, str]],
-                   tmp_path: str):
+def _make_mpa_dirs(
+    mpa_targets: List[Tuple[MultiprocessArrayType, str]], tmp_path: str
+):
   # Temporary array path is not used in GCS.
   if tmp_path.startswith('gs://'):
     return
   mpa_tmp_path = tmp_path + MP_ARRAY_POSTFIX
   # Clean up the previous MPA dir, in case some leftover from last preemption
   # lingers.
   if io.exists(mpa_tmp_path):
     logging.info('Removing outdated MPA temporary files at %s', mpa_tmp_path)
     io.rmtree(mpa_tmp_path)
   _, mpa_subpaths = zip(*mpa_targets)
   for subpath in mpa_subpaths:
     io.makedirs(os.path.join(mpa_tmp_path, subpath))
 
 
-def _save_mpas(gda_manager, mpa_targets: List[Tuple[MultiprocessArrayType, str]],
-               tmp_path: str, final_path: str, base_path: str, keep: int,
-               overwrite: bool, keep_every_n_steps: Optional[int],
-               ckpt_start_time: float,
-               async_manager: Optional[AsyncManager] = None):
+def _save_mpas(
+    gda_manager,
+    mpa_targets: List[Tuple[MultiprocessArrayType, str]],
+    tmp_path: str,
+    final_path: str,
+    base_path: str,
+    keep: int,
+    overwrite: bool,
+    keep_every_n_steps: Optional[int],
+    ckpt_start_time: float,
+    async_manager: Optional[AsyncManager] = None,
+):
   """Save the multiprocess arrays given the paths."""
   mpa_list, mpa_subpaths = zip(*mpa_targets)
-  mpa_tmp_path, mpa_final_path = tmp_path + MP_ARRAY_POSTFIX, final_path + MP_ARRAY_POSTFIX
+  mpa_tmp_path, mpa_final_path = (
+      tmp_path + MP_ARRAY_POSTFIX,
+      final_path + MP_ARRAY_POSTFIX,
+  )
   write_commit_success = False
   # If the checkpoint directory is a GCS directory, then keep the final
   # checkpoint directory as the temporary checkpoint directory. This is because
   # renames are not atomic on GCS. When restoring check for the existence of a
   # success file.
   # TODO: figure out a way to unit-test the behavior.
   if tmp_path.startswith('gs://'):
@@ -224,87 +241,102 @@
           base_path,
           keep,
           overwrite,
           keep_every_n_steps,
           ckpt_start_time,
           has_mpa=True,
           write_commit_success=write_commit_success,
-          async_manager=async_manager))
+          async_manager=async_manager,
+      ),
+  )
 
 
-def _restore_mpas(state_dict,
-                  target: Optional[Any],
-                  ckpt_path: str,
-                  step: Optional[Union[int, float]],
-                  gda_manager: Optional[Any],
-                  allow_partial: bool = False):
+def _restore_mpas(
+    state_dict,
+    target: Optional[Any],
+    ckpt_path: str,
+    step: Optional[Union[int, float]],
+    gda_manager: Optional[Any],
+    allow_partial: bool = False,
+):
   """Restore the multiprocess arrays given the target structure and type."""
 
   def _check_mpa_errors():
     if not gda_manager:
       raise errors.MPACheckpointingRequiredError(ckpt_path, step)
     if not target and not allow_partial:
       raise errors.MPARestoreTargetRequiredError(ckpt_path, step)
 
   def _safe_deserialize(
       target_mpas: List[Tuple[Tuple[Any, ...], MultiprocessArrayType, str]],
-      gda_manager: Any) -> List[MultiprocessArrayType]:
+      gda_manager: Any,
+  ) -> List[MultiprocessArrayType]:
     gda_manager.wait_until_finished()
 
     # Check if reading from GCS and the array dir is potentially corrupted.
     if ckpt_path.startswith('gs://') and not io.exists(
-        os.path.join(ckpt_path + MP_ARRAY_POSTFIX, COMMIT_SUCCESS_FILE)):
+        os.path.join(ckpt_path + MP_ARRAY_POSTFIX, COMMIT_SUCCESS_FILE)
+    ):
       raise errors.MPARestoreDataCorruptedError(step, ckpt_path)
 
     # Check if the given target array types are valid.
     shardings = []
     for _, arr, path in target_mpas:
       if isinstance(arr, jax.Array):
         shardings.append(arr.sharding)
 
     # Restore the arrays.
     ts_specs = [get_tensorstore_spec(path) for _, _, path in target_mpas]
     return gda_manager.deserialize(shardings, ts_specs)
 
   # When target is a single leaf instead of a pytree dict.
   if not isinstance(state_dict, (core.FrozenDict, dict)):
-    if _is_multiprocess_array(target) and isinstance(
-        state_dict, str) and state_dict.startswith(MP_ARRAY_PH):
+    if (
+        _is_multiprocess_array(target)
+        and isinstance(state_dict, str)
+        and state_dict.startswith(MP_ARRAY_PH)
+    ):
       _check_mpa_errors()
-      return _safe_deserialize([((), target, ckpt_path + MP_ARRAY_POSTFIX)],
-                               gda_manager)[0]
+      return _safe_deserialize(
+          [((), target, ckpt_path + MP_ARRAY_POSTFIX)], gda_manager
+      )[0]
     return state_dict
 
   # Go through the restored checkpoint pytree for all MPAs
   flattened = traverse_util.flatten_dict(state_dict, keep_empty_nodes=True)
   target_flattened = {}
   if target:
     target_flattened = traverse_util.flatten_dict(
-        serialization.to_state_dict(target), keep_empty_nodes=True)
+        serialization.to_state_dict(target), keep_empty_nodes=True
+    )
   # A list of (state_dict_key, target_array, array_file_path) for every array
   # to be restored
   target_mpas = []
   for key, value in flattened.items():
     if isinstance(value, str) and value.startswith(MP_ARRAY_PH):
       _check_mpa_errors()
-      if not target or (key not in target_flattened) or (
-          not _is_multiprocess_array(target_flattened[key])):
+      if (
+          not target
+          or (key not in target_flattened)
+          or (not _is_multiprocess_array(target_flattened[key]))
+      ):
         if allow_partial:
           logging.warning(
               'Multiprocess array %s could not be restored because a valid'
               ' array is not found in target at the corresponding location.'
               ' Proceed to restore other arrays because'
               ' allow_partial_restoration=True',
               key,
           )
         else:
           raise errors.MPARestoreTargetRequiredError(ckpt_path, step, key)
       else:
-        mpa_path = os.path.join(ckpt_path + MP_ARRAY_POSTFIX,
-                                value[len(MP_ARRAY_PH):])
+        mpa_path = os.path.join(
+            ckpt_path + MP_ARRAY_POSTFIX, value[len(MP_ARRAY_PH) :]
+        )
         target_mpas.append((key, target_flattened[key], mpa_path))
 
   # If any MPA needs to be restored, call deserialize
   if target_mpas:
     mpa_list = _safe_deserialize(target_mpas, gda_manager)
     for mpa, (key, _, _) in zip(mpa_list, target_mpas):
       flattened[key] = mpa
@@ -323,37 +355,44 @@
   Returns:
     List of filenames sorted 'naturally', not lexicographically: any
     integer substrings are used to subsort numerically. e.g.
     file_1, file_10, file_2  -->  file_1, file_2, file_10
     file_0.1, file_-0.2, file_2.0  -->  file_-0.2, file_0.1, file_2.0
   """
   float_re = SIGNED_FLOAT_RE if signed else UNSIGNED_FLOAT_RE
+
   def maybe_num(s):
     if float_re.match(s):
       return float(s)
     else:
       return s
+
   def split_keys(s):
     return [maybe_num(c) for c in float_re.split(s)]
+
   return sorted(file_list, key=split_keys)
 
 
 def safe_normpath(path: str) -> str:
   """Normalizes path safely to get around `io.glob()` limitations."""
   match = SCHEME_RE.match(path)
   assert match is not None
   d = match.groupdict()
   return (d['scheme'] or '') + os.path.normpath(d['path'])
 
 
-def _remove_invalid_ckpts(ckpt_path: str, base_path: str, keep: int,
-                          overwrite: bool, keep_every_n_steps: Optional[int],
-                          has_mpa: bool) -> None:
-  """Clean up the checkpoint space according to `overwrite`, `keep`, and `keep_every_n_steps` parameters.
-  """
+def _remove_invalid_ckpts(
+    ckpt_path: str,
+    base_path: str,
+    keep: int,
+    overwrite: bool,
+    keep_every_n_steps: Optional[int],
+    has_mpa: bool,
+) -> None:
+  """Clean up the checkpoint space according to `overwrite`, `keep`, and `keep_every_n_steps` parameters."""
   dir_path, prefix = os.path.split(base_path)
   checkpoint_files: List[Any] = [
       pathlib.PurePath(c) for c in _allowempty_listdir(dir_path)
   ]
   checkpoint_files = [
       os.path.join(dir_path, c)
       for c in checkpoint_files
@@ -381,43 +420,57 @@
   if len(checkpoint_files) > keep:
     old_ckpts = checkpoint_files[:-keep]
     # Note: old_ckpts is sorted from oldest to newest.
     for path in old_ckpts:
       if keep_every_n_steps:
         step_number = _checkpoint_path_step(path)
         if step_number and (step_number - last_kept) >= keep_every_n_steps:
-          logging.debug('Not deleting %s, because last_kept=%f and keeping '
-                        'every %d steps.',
-                        path, last_kept, keep_every_n_steps)
+          logging.debug(
+              'Not deleting %s, because last_kept=%f and keeping '
+              'every %d steps.',
+              path,
+              last_kept,
+              keep_every_n_steps,
+          )
           last_kept = step_number
           continue
       logging.info('Removing checkpoint at %s', path)
       if has_mpa:
         # MPA might be removed already but the main ckpt is still there.
         if io.exists(path + MP_ARRAY_POSTFIX):
           io.rmtree(path + MP_ARRAY_POSTFIX)
       _safe_remove(path)
 
 
-def _save_commit(ckpt_tmp_path: str, ckpt_path: str, base_path: str, keep: int,
-                 overwrite: bool, keep_every_n_steps: Optional[int],
-                 ckpt_start_time: float, has_mpa: bool,
-                 write_commit_success: bool,
-                 async_manager: Optional[AsyncManager] = None) -> None:
+def _save_commit(
+    ckpt_tmp_path: str,
+    ckpt_path: str,
+    base_path: str,
+    keep: int,
+    overwrite: bool,
+    keep_every_n_steps: Optional[int],
+    ckpt_start_time: float,
+    has_mpa: bool,
+    write_commit_success: bool,
+    async_manager: Optional[AsyncManager] = None,
+) -> None:
   """Commit changes after saving checkpoints to disk.
 
   This function does the following, sequentially:
     1. Make sure all ckpt writing finishes, and rename them from temp path to
     the final path.
     2. Remove newer checkpoints (files that ordered larger than this save) if
     `overwrite=True`.
     3. Remove old checkpoint files based on `keep` and `keep_every_n_steps`.
     4. Record program duration saved by this checkpoint.
   """
-  mpa_ckpt_tmp_path, mpa_ckpt_path = ckpt_tmp_path + MP_ARRAY_POSTFIX, ckpt_path + MP_ARRAY_POSTFIX
+  mpa_ckpt_tmp_path, mpa_ckpt_path = (
+      ckpt_tmp_path + MP_ARRAY_POSTFIX,
+      ckpt_path + MP_ARRAY_POSTFIX,
+  )
   # Rename the multiprocess array path once serialization and writing finished.
   if has_mpa:
     if write_commit_success:
       commit_success_path = os.path.join(mpa_ckpt_path, COMMIT_SUCCESS_FILE)
       with io.GFile(commit_success_path, 'w') as f:
         f.write(f'Checkpoint commit was successful to {mpa_ckpt_path}')
     else:
@@ -433,29 +486,34 @@
   # Commit the main checkpoint file after arrays (if any) are committed
   if async_manager:
     async_manager.wait_previous_save()
   io.rename(ckpt_tmp_path, ckpt_path, overwrite=overwrite)
   logging.info('Saved checkpoint at %s', ckpt_path)
 
   # Remove newer and older invalid checkpoints.
-  _remove_invalid_ckpts(ckpt_path, base_path, keep, overwrite,
-                        keep_every_n_steps, has_mpa)
+  _remove_invalid_ckpts(
+      ckpt_path, base_path, keep, overwrite, keep_every_n_steps, has_mpa
+  )
   # Record checkpoint-related metrics.
-  orbax.utils.record_saved_duration(ckpt_start_time)
+  ocp.utils.record_saved_duration(ckpt_start_time)
   if async_manager:
     jax.monitoring.record_event_duration_secs(
         '/jax/checkpoint/write/async/total_duration_secs',
-        time.time() - ckpt_start_time)
+        time.time() - ckpt_start_time,
+    )
 
 
-def _check_overwrite_error(ckpt_tmp_path: str, ckpt_path: str, base_path: str,
-                           step: int):
+def _check_overwrite_error(
+    ckpt_tmp_path: str, ckpt_path: str, base_path: str, step: int
+):
   """Throw error if a ckpt file of this step or higher already exists."""
   dir_path, prefix = os.path.split(base_path)
-  checkpoint_files: List[Any] = [pathlib.PurePath(c) for c in _allowempty_listdir(dir_path)]
+  checkpoint_files: List[Any] = [
+      pathlib.PurePath(c) for c in _allowempty_listdir(dir_path)
+  ]
   checkpoint_files = [
       os.path.join(dir_path, c)
       for c in checkpoint_files
       if c.match(f'{prefix}*') and not c.match(f'*{MP_ARRAY_POSTFIX}')
   ]
   if ckpt_path in checkpoint_files:
     raise errors.InvalidCheckpointError(ckpt_path, step)
@@ -466,19 +524,25 @@
   # was written, but before it was renamed to the final checkpoint name
   if checkpoint_files[-1] == ckpt_tmp_path:
     checkpoint_files.pop()
   if ckpt_path != checkpoint_files[-1]:
     raise errors.InvalidCheckpointError(ckpt_path, step)
 
 
-def _save_main_ckpt_file(target: bytes, has_mpa: bool, paths: Tuple[str, str],
-                         base_path: str, step: int,
-                         keep: int, overwrite: bool,
-                         keep_every_n_steps: Optional[int],
-                         ckpt_start_time: float):
+def _save_main_ckpt_file(
+    target: bytes,
+    has_mpa: bool,
+    paths: Tuple[str, str],
+    base_path: str,
+    step: int,
+    keep: int,
+    overwrite: bool,
+    keep_every_n_steps: Optional[int],
+    ckpt_start_time: float,
+):
   """Save the main checkpoint file via file system."""
   ckpt_tmp_path, ckpt_path = paths
   io.makedirs(os.path.dirname(ckpt_path))
 
   with io.GFile(ckpt_tmp_path, 'wb') as fp:
     fp.write(target)
 
@@ -489,41 +553,45 @@
         ckpt_path,
         base_path,
         keep,
         overwrite,
         keep_every_n_steps,
         ckpt_start_time,
         has_mpa=False,
-        write_commit_success=False)
+        write_commit_success=False,
+    )
 
 
 def _get_checkpoint_paths(
     ckpt_dir: Union[str, os.PathLike],
     step: Union[int, float],
-    prefix: str = 'checkpoint_') -> Tuple[str, str, str]:
+    prefix: str = 'checkpoint_',
+) -> Tuple[str, str, str]:
   """Generate the checkpoint paths used in this save operation."""
   ckpt_dir = os.fspath(ckpt_dir)  # Pathlib -> str
   logging.info('Saving checkpoint at step: %s', step)
   # normalize path because io.glob() can modify path './', '//' ...
   ckpt_dir = safe_normpath(ckpt_dir)
   ckpt_tmp_path = _checkpoint_path(ckpt_dir, 'tmp', prefix)
   ckpt_path = _checkpoint_path(ckpt_dir, step, prefix)
   base_path = os.path.join(ckpt_dir, prefix)
   return ckpt_path, ckpt_tmp_path, base_path
 
 
-def save_checkpoint(ckpt_dir: Union[str, os.PathLike],
-                    target: PyTree,
-                    step: Union[int, float],
-                    prefix: str = 'checkpoint_',
-                    keep: int = 1,
-                    overwrite: bool = False,
-                    keep_every_n_steps: Optional[int] = None,
-                    async_manager: Optional[AsyncManager] = None,
-                    orbax_checkpointer: Optional[orbax.Checkpointer] = None) -> str:
+def save_checkpoint(
+    ckpt_dir: Union[str, os.PathLike],
+    target: PyTree,
+    step: Union[int, float],
+    prefix: str = 'checkpoint_',
+    keep: int = 1,
+    overwrite: bool = False,
+    keep_every_n_steps: Optional[int] = None,
+    async_manager: Optional[AsyncManager] = None,
+    orbax_checkpointer: Optional[ocp.Checkpointer] = None,
+) -> str:
   """Save a checkpoint of the model. Suitable for single-host.
 
   In this method, every JAX process saves the checkpoint on its own. Do not
   use it if you have multiple processes and you intend for them to save data
   to a common directory (e.g., a GCloud bucket). To save multi-process
   checkpoints to a shared storage or to save `GlobalDeviceArray`s, use
   `save_checkpoint_multiprocess()` instead.
@@ -542,29 +610,33 @@
     overwrite: overwrite existing checkpoint files if a checkpoint at the
       current or a later step already exits (default: False).
     keep_every_n_steps: if defined, keep every checkpoints every n steps (in
       addition to keeping the last 'keep' checkpoints).
     async_manager: if defined, the save will run without blocking the main
       thread. Only works for single host. Note that an ongoing save will still
       block subsequent saves, to make sure overwrite/keep logic works correctly.
-    orbax_checkpointer: if defined, the save will be done by Orbax. In the
-      future, all Flax checkpointing features will be migrated to Orbax,
-      and starting to use an `orbax_checkpointer` is recommended. Please
-      check out the checkpointing guide (https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#save-checkpoints) for how to use Orbax checkpointers.
+    orbax_checkpointer: if defined, the save will be done by ocp. In the future,
+      all Flax checkpointing features will be migrated to Orbax, and starting to
+      use an `orbax_checkpointer` is recommended. Please check out the
+      checkpointing guide
+      (https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#save-checkpoints)
+      for how to use Orbax checkpointers.
+
   Returns:
     Filename of saved checkpoint.
   """
   start_time = time.time()
   # Make sure all saves are finished before the logic of checking and removing
   # outdated checkpoints happens.
   if async_manager:
     async_manager.wait_previous_save()
 
   ckpt_path, ckpt_tmp_path, base_path = _get_checkpoint_paths(
-      ckpt_dir, step, prefix)
+      ckpt_dir, step, prefix
+  )
 
   if config.flax_use_orbax_checkpointing or orbax_checkpointer:
     logging.info(
         'Using Orbax as backend to save Flax checkpoints. For potential'
         ' troubleshooting see:'
         ' https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#orbax-as-backend-troubleshooting'
     )
@@ -573,85 +645,101 @@
           'Multiple JAX processes detected when calling single-process'
           ' `save_checkpoint`. Your devices will HANG if this function is only'
           ' called on process 0! Troubleshoot at:'
           ' https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#if-your-devices-hang-when-writing-checkpoints'
       )
 
     # Make sure any previous work is done before making file changes.
-    if orbax_checkpointer and isinstance(orbax_checkpointer,
-                                         orbax.AsyncCheckpointer):
+    if orbax_checkpointer and isinstance(
+        orbax_checkpointer, ocp.AsyncCheckpointer
+    ):
       orbax_checkpointer.wait_until_finished()
     # If no checkpointer provided, save synchronously with default setting.
     if not orbax_checkpointer:
-      orbax_checkpointer = orbax.Checkpointer(
-          orbax.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+      orbax_checkpointer = ocp.Checkpointer(
+          ocp.PyTreeCheckpointHandler(restore_with_serialized_types=False)
       )
     # Check singular target.
     if jtu.treedef_is_leaf(jtu.tree_structure(target)) and not isinstance(
-        orbax_checkpointer._handler, orbax.ArrayCheckpointHandler  # pylint: disable=protected-access
+        orbax_checkpointer._handler, ocp.ArrayCheckpointHandler  # pylint: disable=protected-access
     ):
       raise ValueError(
           'Orbax backend only accept pytree as save target. To save singular'
           ' objects like numbers or Numpy arrays, checkout'
           ' https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#if-you-don-t-save-pytrees'
       )
 
     save_args = orbax_utils.save_args_from_target(target)
     orbax_checkpointer.save(
-        ckpt_path, target, save_args=save_args, force=overwrite)
+        ckpt_path, target, save_args=save_args, force=overwrite
+    )
     # Do a process check here in case people call this for multihost.
     if process_index() == 0:
-      _remove_invalid_ckpts(ckpt_path, base_path, keep, overwrite,
-                            keep_every_n_steps, True)
+      _remove_invalid_ckpts(
+          ckpt_path, base_path, keep, overwrite, keep_every_n_steps, True
+      )
     end_time = time.time()
-    monitoring.record_event_duration_secs(_WRITE_CHECKPOINT_EVENT,
-                                          end_time - start_time)
+    monitoring.record_event_duration_secs(
+        _WRITE_CHECKPOINT_EVENT, end_time - start_time
+    )
     return ckpt_path
 
   warnings.warn(
       (
           'Flax Checkpointing will soon be deprecated in favor of Orbax'
           ' (https://github.com/google/orbax). Please refer to the Checkpoint'
           ' Upgrade Guide'
           ' (https://flax.readthedocs.io/en/latest/guides/orbax_upgrade_guide.html)'
-          ' to self-migrate your code to Orbax.'
+          ' to self-migrate your code to ocp.'
       ),
       DeprecationWarning,
   )
   if not overwrite:
-    _check_overwrite_error(ckpt_tmp_path, ckpt_path, base_path, step) # type: ignore
+    _check_overwrite_error(ckpt_tmp_path, ckpt_path, base_path, step)  # type: ignore
 
   target = serialization.to_bytes(target)
+
   # Save the files via I/O sync or async.
   def save_main_ckpt_task():
     jax.monitoring.record_event('/jax/flax/checkpoint/save_main_ckpt_task')
-    return _save_main_ckpt_file(target, False, (ckpt_tmp_path, ckpt_path),
-                                base_path, step, keep, overwrite,
-                                keep_every_n_steps, start_time)
+    return _save_main_ckpt_file(
+        target,
+        False,
+        (ckpt_tmp_path, ckpt_path),
+        base_path,
+        step,
+        keep,
+        overwrite,
+        keep_every_n_steps,
+        start_time,
+    )
+
   if async_manager:
     async_manager.save_async(save_main_ckpt_task)
   else:
     save_main_ckpt_task()
   end_time = time.time()
-  monitoring.record_event_duration_secs(_WRITE_CHECKPOINT_EVENT,
-                                        end_time - start_time)
+  monitoring.record_event_duration_secs(
+      _WRITE_CHECKPOINT_EVENT, end_time - start_time
+  )
   return ckpt_path
 
 
 def save_checkpoint_multiprocess(
     ckpt_dir: Union[str, os.PathLike],
     target: PyTree,
     step: Union[int, float],
     prefix: str = 'checkpoint_',
     keep: int = 1,
     overwrite: bool = False,
     keep_every_n_steps: Optional[int] = None,
     async_manager: Optional[AsyncManager] = None,
     gda_manager: Optional[Any] = None,
-    orbax_checkpointer: Optional[orbax.Checkpointer] = None) -> str:
+    orbax_checkpointer: Optional[ocp.Checkpointer] = None,
+) -> str:
   """Save a checkpoint of the model in multi-process environment.
 
   Use this method to save `GlobalDeviceArray`s, or to save data to a
   common directory. Only process 0 will save the main checkpoint file and
   remove old checkpoint files.
 
   Pre-emption safe by writing to temporary before a final rename and cleanup
@@ -674,15 +762,15 @@
       thread. Only works for single host. Note that an ongoing save will still
       block subsequent saves, to make sure overwrite/keep logic works correctly.
     gda_manager: required if target contains a JAX GlobalDeviceArray. Type
       should be GlobalAsyncCheckpointManager (needs Tensorstore to be imported
       correctly). Will save the GDAs to a separate subdirectory with postfix
       "_gda" asynchronously. Same as async_manager, this will block subsequent
       saves.
-    orbax_checkpointer: if defined, the save will be done by Orbax. In the
+    orbax_checkpointer: if defined, the save will be done by Orbax In the
       future, all Flax checkpointing features will be migrated to Orbax,
       and starting to use an `orbax_checkpointer` is recommended. Please
       check out the checkpointing guide (https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#save-checkpoints) for how to use Orbax checkpointers.
 
   Returns:
     Filename of saved checkpoint.
   """
@@ -693,78 +781,93 @@
   if async_manager:
     async_manager.wait_previous_save()
   if gda_manager:
     gda_manager.wait_until_finished()
     sync_global_devices('Flax:Checkpoint:WaitLastSaveDone')
 
   ckpt_path, ckpt_tmp_path, base_path = _get_checkpoint_paths(
-      ckpt_dir, step, prefix)
+      ckpt_dir, step, prefix
+  )
 
   if config.flax_use_orbax_checkpointing or orbax_checkpointer:
     logging.info(
         'Using Orbax as backend to save Flax checkpoints. For potential'
         ' troubleshooting see:'
         ' https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#orbax-as-backend-troubleshooting'
     )
     # Make sure any previous work is done before making file changes.
-    if orbax_checkpointer and isinstance(orbax_checkpointer,
-                                         orbax.AsyncCheckpointer):
+    if orbax_checkpointer and isinstance(
+        orbax_checkpointer, ocp.AsyncCheckpointer
+    ):
       orbax_checkpointer.wait_until_finished()
 
     # If no checkpointer provided, save synchronously with default setting.
     if not orbax_checkpointer:
-      orbax_checkpointer = orbax.Checkpointer(
-          orbax.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+      orbax_checkpointer = ocp.Checkpointer(
+          ocp.PyTreeCheckpointHandler(restore_with_serialized_types=False)
       )
     # Check singular target.
     if jtu.treedef_is_leaf(jtu.tree_structure(target)) and not isinstance(
-        orbax_checkpointer._handler, orbax.ArrayCheckpointHandler  # pylint: disable=protected-access
+        orbax_checkpointer._handler, ocp.ArrayCheckpointHandler  # pylint: disable=protected-access
     ):
       raise ValueError(
           'Orbax backend only accept pytree as save target. To save singular'
           ' objects like numbers or Numpy arrays, checkout'
           ' https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html#if-you-don-t-save-pytrees'
       )
 
     if process_index() == 0:
-      _remove_invalid_ckpts(ckpt_path, base_path, keep, overwrite,
-                            keep_every_n_steps, True)
+      _remove_invalid_ckpts(
+          ckpt_path, base_path, keep, overwrite, keep_every_n_steps, True
+      )
     save_args = orbax_utils.save_args_from_target(target)
     orbax_checkpointer.save(
-        ckpt_path, target, save_args=save_args, force=overwrite)
+        ckpt_path, target, save_args=save_args, force=overwrite
+    )
     end_time = time.time()
-    monitoring.record_event_duration_secs(_WRITE_CHECKPOINT_EVENT,
-                                          end_time - start_time)
+    monitoring.record_event_duration_secs(
+        _WRITE_CHECKPOINT_EVENT, end_time - start_time
+    )
     return ckpt_path
 
   warnings.warn(
       (
           'Flax Checkpointing will soon be deprecated in favor of Orbax'
           ' (https://github.com/google/orbax). Please refer to the Checkpoint'
           ' Upgrade Guide'
           ' (https://flax.readthedocs.io/en/latest/guides/orbax_upgrade_guide.html)'
-          ' to self-migrate your code to Orbax.'
+          ' to self-migrate your code to ocp.'
       ),
       DeprecationWarning,
   )
 
   target = serialization.to_state_dict(target)
   target, mpa_targets = _split_mp_arrays(target)
   target = serialization.msgpack_serialize(target)
   has_mpa = mpa_targets and _IMPORT_GDAM_SUCCESSFUL
 
   if not overwrite:
-    _check_overwrite_error(ckpt_tmp_path, ckpt_path, base_path, step) # type: ignore
+    _check_overwrite_error(ckpt_tmp_path, ckpt_path, base_path, step)  # type: ignore
     sync_global_devices('Flax:Checkpoint:CheckOverwriteBeforeSave')
+
   # Save the files via I/O sync or async.
   def save_main_ckpt_task():
     jax.monitoring.record_event('/jax/flax/checkpoint/save_main_ckpt_task')
-    return _save_main_ckpt_file(target, has_mpa, (ckpt_tmp_path, ckpt_path),
-                                base_path, step, keep, overwrite,
-                                keep_every_n_steps, start_time)
+    return _save_main_ckpt_file(
+        target,
+        has_mpa,
+        (ckpt_tmp_path, ckpt_path),
+        base_path,
+        step,
+        keep,
+        overwrite,
+        keep_every_n_steps,
+        start_time,
+    )
+
   # Write the main checkpoint file only via process 0, to avoid race condition.
   if process_index() == 0:
     if async_manager:
       async_manager.save_async(save_main_ckpt_task)
     else:
       save_main_ckpt_task()
 
@@ -772,25 +875,37 @@
     if not gda_manager:
       raise errors.MPACheckpointingRequiredError(ckpt_path, step)
     # Creating the directory containing GDAs explicitly. This should happen only
     # on process 0 and before any worker starts to write GDA data.
     if process_index() == 0:
       _make_mpa_dirs(mpa_targets, ckpt_tmp_path)
     sync_global_devices('Flax:Checkpoint:AfterCreateMPADir')
-    _save_mpas(gda_manager, mpa_targets, ckpt_tmp_path, ckpt_path, base_path,
-               keep, overwrite, keep_every_n_steps, start_time, async_manager)
+    _save_mpas(
+        gda_manager,
+        mpa_targets,
+        ckpt_tmp_path,
+        ckpt_path,
+        base_path,
+        keep,
+        overwrite,
+        keep_every_n_steps,
+        start_time,
+        async_manager,
+    )
 
   end_time = time.time()
-  monitoring.record_event_duration_secs(_WRITE_CHECKPOINT_EVENT,
-                                        end_time - start_time)
+  monitoring.record_event_duration_secs(
+      _WRITE_CHECKPOINT_EVENT, end_time - start_time
+  )
   return ckpt_path
 
 
-def _all_checkpoints(ckpt_dir: Union[str, os.PathLike],
-                     prefix: str = 'checkpoint_') -> List[str]:
+def _all_checkpoints(
+    ckpt_dir: Union[str, os.PathLike], prefix: str = 'checkpoint_'
+) -> List[str]:
   """Retrieve all checkpoint paths in directory.
 
   Args:
     ckpt_dir: str: directory of checkpoints to restore from.
     prefix: str: name prefix of checkpoint files.
 
   Returns:
@@ -799,27 +914,29 @@
   ckpt_dir = os.fspath(ckpt_dir)  # Pathlib -> str
   checkpoint_files: List[Any] = [
       pathlib.PurePath(c) for c in _allowempty_listdir(ckpt_dir)
   ]
   checkpoint_files = [
       os.path.join(ckpt_dir, c)
       for c in checkpoint_files
-      if c.match(f'{prefix}*') and not c.match(f'{prefix}tmp') and
-      not c.match(f'*{MP_ARRAY_POSTFIX}') and
-      not c.match(f'*{orbax.utils.TMP_DIR_SUFFIX}*')
+      if c.match(f'{prefix}*')
+      and not c.match(f'{prefix}tmp')
+      and not c.match(f'*{MP_ARRAY_POSTFIX}')
+      and not c.match(f'*{ocp.utils.TMP_DIR_SUFFIX}*')
   ]
   checkpoint_files = natural_sort(checkpoint_files)
   if checkpoint_files:
     return checkpoint_files
   else:
     return []
 
 
-def latest_checkpoint(ckpt_dir: Union[str, os.PathLike],
-                      prefix: str = 'checkpoint_') -> Optional[str]:
+def latest_checkpoint(
+    ckpt_dir: Union[str, os.PathLike], prefix: str = 'checkpoint_'
+) -> Optional[str]:
   """Retrieve the path of the latest checkpoint in a directory.
 
   Args:
     ckpt_dir: str: directory of checkpoints to restore from.
     prefix: str: name prefix of checkpoint files.
 
   Returns:
@@ -828,17 +945,19 @@
   checkpoint_files = _all_checkpoints(ckpt_dir, prefix)
   if checkpoint_files:
     return checkpoint_files[-1]
   else:
     return None
 
 
-def available_steps(ckpt_dir: Union[str, os.PathLike],
-                    prefix: str = 'checkpoint_',
-                    step_type: Type = int) -> List[Union[int, float]]:
+def available_steps(
+    ckpt_dir: Union[str, os.PathLike],
+    prefix: str = 'checkpoint_',
+    step_type: Type = int,
+) -> List[Union[int, float]]:
   """Return step numbers of available checkpoints in a directory.
 
 
   Args:
     ckpt_dir: str: directory of checkpoints to restore from.
     prefix: str: name prefix of checkpoint files.
     step_type: type: type for steps, int (default) or float.
@@ -861,16 +980,17 @@
     ckpt_dir: Union[str, os.PathLike],
     target: Optional[Any],
     step: Optional[Union[int, float]] = None,
     prefix: str = 'checkpoint_',
     parallel: bool = True,
     gda_manager: Optional[Any] = None,
     allow_partial_mpa_restoration: bool = False,
-    orbax_checkpointer: Optional[orbax.Checkpointer] = None,
-    orbax_transforms: Optional[Dict] = None) -> PyTree:
+    orbax_checkpointer: Optional[ocp.Checkpointer] = None,
+    orbax_transforms: Optional[Dict] = None,
+) -> PyTree:
   """Restore last/best checkpoint from checkpoints in path.
 
   Sorts the checkpoint files naturally, returning the highest-valued
   file, e.g.:
 
   *  ``ckpt_1, ckpt_2, ckpt_3 --> ckpt_3``
 
@@ -884,38 +1004,38 @@
       deserialized state-dict is returned as-is.
     step: int or float: step number to load or None to load latest. If
       specified, ckpt_dir must be a directory.
     prefix: str: name prefix of checkpoint files.
     parallel: bool: whether to load seekable checkpoints in parallel, for speed.
     gda_manager: required if checkpoint contains a multiprocess array
       (GlobalDeviceArray or jax Array from pjit). Type should be
-      GlobalAsyncCheckpointManager (needs Tensorstore to be imported
-      correctly). Will read the arrays from the separate subdirectory with
-      postfix "_gda".
+      GlobalAsyncCheckpointManager (needs Tensorstore to be imported correctly).
+      Will read the arrays from the separate subdirectory with postfix "_gda".
     allow_partial_mpa_restoration: If true, the given `target` doesn't have to
       contain all valid multiprocess arrays. As a result, the restored Pytree
       may have some MPAs not restored correctly. Use this if you cannot provide
-      a fully valid ``target`` and don't need all the MPAs in the checkpoint
-      to be restored.
-    orbax_checkpointer: the `Orbax.Checkpointer` that handles the underlying
-      restore, if the given checkpoint is saved with Orbax.
+      a fully valid ``target`` and don't need all the MPAs in the checkpoint to
+      be restored.
+    orbax_checkpointer: the `ocp.Checkpointer` that handles the underlying
+      restore, if the given checkpoint is saved with ocp.
     orbax_transforms: the Orbax transformations that will be passed into
       `orbax_checkpointer.restore()` call.
 
   Returns:
     Restored `target` updated from checkpoint file, or if no step specified and
     no checkpoint files present, returns the passed-in `target` unchanged.
     If a file path is specified and is not found, the passed-in `target` will be
     returned. This is to match the behavior of the case where a directory path
     is specified but the directory has not yet been created.
   """
   start_time = time.time()
   # Make sure any previous work is done before checking files.
-  if orbax_checkpointer and isinstance(orbax_checkpointer,
-                                       orbax.AsyncCheckpointer):
+  if orbax_checkpointer and isinstance(
+      orbax_checkpointer, ocp.AsyncCheckpointer
+  ):
     orbax_checkpointer.wait_until_finished()
 
   ckpt_dir = os.fspath(ckpt_dir)  # Pathlib -> str
   ckpt_dir = safe_normpath(ckpt_dir)
   if step is not None:
     ckpt_path = _checkpoint_path(ckpt_dir, step, prefix)
     if not io.exists(ckpt_path):
@@ -925,47 +1045,54 @@
       logging.info('Found no checkpoint directory at %s', ckpt_dir)
       return target
     if io.isdir(ckpt_dir):
       # This means the given dir is an orbax checkpoint.
       if io.exists(os.path.join(ckpt_dir, ORBAX_CKPT_FILENAME)):
         ckpt_path = ckpt_dir
       else:
-        ckpt_path = latest_checkpoint(ckpt_dir, prefix) # type: ignore
+        ckpt_path = latest_checkpoint(ckpt_dir, prefix)  # type: ignore
         if not ckpt_path:
-          logging.info('Found no checkpoint files in %s with prefix %s',
-                       ckpt_dir, prefix)
+          logging.info(
+              'Found no checkpoint files in %s with prefix %s', ckpt_dir, prefix
+          )
           return target
     else:
       ckpt_path = ckpt_dir
 
   # Restore the checkpoint with Orbax if needed.
   is_orbax = io.exists(os.path.join(ckpt_path, ORBAX_CKPT_FILENAME))
   ckpt_type = 'orbax' if is_orbax else 'legacy Flax'
   logging.info(f'Restoring {ckpt_type} checkpoint from {ckpt_path}')
   if is_orbax:
     if not orbax_checkpointer:
-      orbax_checkpointer = orbax.Checkpointer(
-          orbax.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+      orbax_checkpointer = ocp.Checkpointer(
+          ocp.PyTreeCheckpointHandler(restore_with_serialized_types=False)
       )
 
     restore_kwargs = {}
     if target is not None:
       restore_kwargs['restore_args'] = orbax_utils.restore_args_from_target(
           target
       )
-    if orbax_transforms is not None:
-      restore_kwargs['transforms'] = orbax_transforms
+      if isinstance(orbax_checkpointer._handler, ocp.PyTreeCheckpointHandler):  # pylint: disable=protected-access
+        restore_kwargs['transforms'] = (
+            orbax_utils.maybe_construct_transformations(
+                target, orbax_transforms
+            )
+        )
     restored = orbax_checkpointer.restore(
-        ckpt_path, item=target, **restore_kwargs)
+        ckpt_path, item=target, **restore_kwargs
+    )
     restored = serialization.to_state_dict(restored)
     if target is not None:
       restored = serialization.from_state_dict(target, restored)
     end_time = time.time()
-    monitoring.record_event_duration_secs(_READ_CHECKPOINT_EVENT,
-                                          end_time - start_time)
+    monitoring.record_event_duration_secs(
+        _READ_CHECKPOINT_EVENT, end_time - start_time
+    )
     return restored
 
   ckpt_size = io.getsize(ckpt_path)
   with io.GFile(ckpt_path, 'rb') as fp:
     if parallel and fp.seekable():
       buf_size = 128 << 20  # 128M buffer.
       num_bufs = ckpt_size / buf_size
@@ -976,38 +1103,45 @@
         # NOTE: We have to re-open the file to read each chunk, otherwise the
         # parallelism has no effect. But we could reuse the file pointers
         # within each thread.
         with io.GFile(ckpt_path, 'rb') as f:
           f.seek(i * buf_size)
           buf = f.read(buf_size)
           if buf:
-            checkpoint_contents[i * buf_size:i * buf_size + len(buf)] = buf
+            checkpoint_contents[i * buf_size : i * buf_size + len(buf)] = buf
           return len(buf) / buf_size
 
       pool_size = 32
       pool = thread.ThreadPoolExecutor(pool_size)
       results = pool.map(read_chunk, range(int(num_bufs) + 1))
       pool.shutdown(wait=False)
       logging.debug(f'results: {list(results)}')
     else:
       checkpoint_contents = fp.read()
 
   state_dict = serialization.msgpack_restore(checkpoint_contents)
   if _IMPORT_GDAM_SUCCESSFUL:
-    state_dict = _restore_mpas(state_dict, target, ckpt_path, step, gda_manager,
-                               allow_partial_mpa_restoration)
+    state_dict = _restore_mpas(
+        state_dict,
+        target,
+        ckpt_path,
+        step,
+        gda_manager,
+        allow_partial_mpa_restoration,
+    )
 
   if target is None:
     restored_checkpoint = state_dict
   else:
     restored_checkpoint = serialization.from_state_dict(target, state_dict)
 
   end_time = time.time()
-  monitoring.record_event_duration_secs(_READ_CHECKPOINT_EVENT,
-                                        end_time - start_time)
+  monitoring.record_event_duration_secs(
+      _READ_CHECKPOINT_EVENT, end_time - start_time
+  )
 
   return restored_checkpoint
 
 
 def convert_pre_linen(params: PyTree) -> PyTree:
   """Converts a pre-Linen parameter pytree.
 
@@ -1070,9 +1204,9 @@
       module = match.group(1)
       num = counts.get(module, 0)
       name = f'{module}_{num}'
       counts[module] = num + 1
     params_renamed[name] = convert_pre_linen(value)
 
   if isinstance(params, core.FrozenDict):
-    params_renamed = core.freeze(params_renamed) # type: ignore
+    params_renamed = core.freeze(params_renamed)  # type: ignore
   return params_renamed
```

### Comparing `flax-0.7.0/flax/training/common_utils.py` & `flax-0.7.1/flax/training/common_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -31,15 +31,16 @@
     xs: a pytree of arrays.
   Returns:
     A matching pytree with arrays' leading dimensions sharded by the
     local device count.
   """
   local_device_count = jax.local_device_count()
   return jax.tree_util.tree_map(
-      lambda x: x.reshape((local_device_count, -1) + x.shape[1:]), xs)
+      lambda x: x.reshape((local_device_count, -1) + x.shape[1:]), xs
+  )
 
 
 def shard_prng_key(prng_key):
   """Helper to shard (aka split) a PRNGKey for use with pmap'd functions.
 
   PRNG keys can be used at train time to drive stochastic modules
   e.g. Dropout. We would like a different PRNG key for each local
@@ -95,10 +96,12 @@
     num_classes: the maximum possible index.
     on_value: the "on" value for the one-hot array, defaults to 1.0.
     off_value: the "off" value for the one-hot array, defaults to 0.0.
   Returns:
     A (n+1)-dim array whose last dimension contains one-hot vectors of length
     num_classes.
   """
-  x = (labels[..., None] == jnp.arange(num_classes).reshape((1,) * labels.ndim + (-1,)))
+  x = labels[..., None] == jnp.arange(num_classes).reshape(
+      (1,) * labels.ndim + (-1,)
+  )
   x = lax.select(x, jnp.full(x.shape, on_value), jnp.full(x.shape, off_value))
   return x.astype(jnp.float32)
```

### Comparing `flax-0.7.0/flax/training/dynamic_scale.py` & `flax-0.7.1/flax/training/dynamic_scale.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,28 +8,26 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Dynamic loss scaling for mixed precision gradients.
-"""
+"""Dynamic loss scaling for mixed precision gradients."""
 
 import functools
 from typing import Any, Callable, NamedTuple, Optional, Sequence, Union
 
 from flax import struct
 
 import jax
 from jax import lax
 import jax.numpy as jnp
 
 
-
 Array = Any
 
 
 class DynamicScaleResult(NamedTuple):
   dynamic_scale: 'DynamicScale'
   finite: Array
   aux: Any
@@ -77,28 +75,31 @@
     growth_interval: after how many steps of finite gradients the scale should
       be increased (default: 2000).
     fin_steps: indicates how many gradient steps in a row have been finite.
     scale: the current scale by which the loss is multiplied.
     minimum_scale: the minimum value that the scale can take (default: the
       smallest positive number representable in floating point).
   """
+
   growth_factor: float = struct.field(pytree_node=False, default=2.0)
   backoff_factor: float = struct.field(pytree_node=False, default=0.5)
   growth_interval: int = struct.field(pytree_node=False, default=2000)
   fin_steps: Array = 0
   scale: Array = 65536.0
   minimum_scale: Optional[float] = struct.field(
       pytree_node=False, default=jnp.finfo(jnp.float32).tiny
   )
 
-  def value_and_grad(self, fun: Callable[..., Any],
-                     argnums: Union[int, Sequence[int]] = 0,
-                     has_aux: bool = False,
-                     axis_name: Optional[str] = None,
-                     ) -> Callable[..., DynamicScaleResult]:
+  def value_and_grad(
+      self,
+      fun: Callable[..., Any],
+      argnums: Union[int, Sequence[int]] = 0,
+      has_aux: bool = False,
+      axis_name: Optional[str] = None,
+  ) -> Callable[..., DynamicScaleResult]:
     """Wrapper around `jax.value_and_grad`.
 
     Args:
       fun: Function to be differentiated. Its arguments at positions specified
         by ``argnums`` should be arrays, scalars, or standard Python containers.
         It should return a scalar (which includes arrays with shape ``()``
         but not arrays with shape ``(1,)`` etc.)
@@ -110,43 +111,50 @@
         Default False.
       axis_name: If an axis is given the gradients will be averaged across
         replicas (default: None).
     Returns:
       A function that takes the same arguments as `fun` and
       returns a DynamicScaleResult
     """
+
     @functools.wraps(fun)
     def loss_wrapper(*args):
       aux = fun(*args)
       if has_aux:
         return (self.scale * aux[0], aux[1])
       else:
         return self.scale * aux
 
     grad_fn = jax.value_and_grad(loss_wrapper, argnums, has_aux)
+
     def grad_fn_wrapper(*args):
       aux, grad = grad_fn(*args)
       aux = (aux[0] / self.scale, aux[1]) if has_aux else aux / self.scale
 
       grad = jax.tree_util.tree_map(
-          lambda g: jnp.asarray(g, jnp.float32) / self.scale, grad)
+          lambda g: jnp.asarray(g, jnp.float32) / self.scale, grad
+      )
       if axis_name is not None:
         grad = lax.pmean(grad, axis_name)
 
       finite = jnp.array(True)
       for g in jax.tree_util.tree_leaves(grad):
         finite &= jnp.all(lax.is_finite(g))
 
       grow = self.fin_steps == self.growth_interval
       fin_scale = jnp.where(
           grow & finite,
-          jnp.minimum(self.scale * self.growth_factor, jnp.finfo(jnp.float32).max),
-          self.scale)
+          jnp.minimum(
+              self.scale * self.growth_factor, jnp.finfo(jnp.float32).max
+          ),
+          self.scale,
+      )
       inf_scale = self.scale * self.backoff_factor
       if self.minimum_scale is not None:
         inf_scale = jnp.maximum(inf_scale, self.minimum_scale)
       new_scale = jnp.where(finite, fin_scale, inf_scale)
       new_fin_steps = jnp.where(grow | (~finite), 0, self.fin_steps + 1)
 
       new_self = self.replace(fin_steps=new_fin_steps, scale=new_scale)
       return DynamicScaleResult(new_self, finite, aux, grad)
+
     return grad_fn_wrapper
```

### Comparing `flax-0.7.0/flax/training/early_stopping.py` & `flax-0.7.1/flax/training/early_stopping.py`

 * *Files 4% similar despite different names*

```diff
@@ -40,34 +40,38 @@
         improvement.
     patience: Number of steps of no improvement before stopping.
     best_metric: Current best metric value.
     patience_count: Number of steps since last improving update.
     should_stop: Whether the training loop should stop to avoid
         overfitting.
   """
+
   min_delta: float = 0
   patience: int = 0
   best_metric: float = float('inf')
   patience_count: int = 0
   should_stop: bool = False
 
   def reset(self):
-    return self.replace(best_metric=float('inf'),
-                        patience_count=0,
-                        should_stop=False)
+    return self.replace(
+        best_metric=float('inf'), patience_count=0, should_stop=False
+    )
 
   def update(self, metric):
     """Update the state based on metric.
 
     Returns:
       A pair (has_improved, early_stop), where `has_improved` is True when there
       was an improvement greater than `min_delta` from the previous
       `best_metric` and `early_stop` is the updated `EarlyStop` object.
     """
 
-    if math.isinf(self.best_metric) or self.best_metric - metric > self.min_delta:
-      return True, self.replace(best_metric=metric,
-                                patience_count=0)
+    if (
+        math.isinf(self.best_metric)
+        or self.best_metric - metric > self.min_delta
+    ):
+      return True, self.replace(best_metric=metric, patience_count=0)
     else:
       should_stop = self.patience_count >= self.patience or self.should_stop
-      return False, self.replace(patience_count=self.patience_count + 1,
-                                 should_stop=should_stop)
+      return False, self.replace(
+          patience_count=self.patience_count + 1, should_stop=should_stop
+      )
```

### Comparing `flax-0.7.0/flax/training/lr_schedule.py` & `flax-0.7.1/flax/training/lr_schedule.py`

 * *Files 6% similar despite different names*

```diff
@@ -29,16 +29,17 @@
 
 
 def _piecewise_constant(boundaries, values, t):
   index = jnp.sum(boundaries < t)
   return jnp.take(values, index)
 
 
-def create_constant_learning_rate_schedule(base_learning_rate, steps_per_epoch,
-                                           warmup_length=0.0):
+def create_constant_learning_rate_schedule(
+    base_learning_rate, steps_per_epoch, warmup_length=0.0
+):
   """Create a constant learning rate schedule with optional warmup.
 
   Note that with `FLIP #1009`_ learning rate schedules in ``flax.training`` are
   **effectively deprecated** in favor of Optax_ schedules. Please refer to
   `Optimizer Schedules`_ for more information.
 
   .. _FLIP #1009: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md
@@ -56,28 +57,32 @@
       factor that will linearly ramp-up from 0 to 1 over the first
       `warmup_length` epochs
 
   Returns:
     Function `f(step) -> lr` that computes the learning rate for a given step.
   """
   logging.warning(
-    'Learning rate schedules in ``flax.training`` are effectively deprecated '
-    'in favor of Optax schedules. Please refer to '
-    'https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules'
-    ' for alternatives.')
+      'Learning rate schedules in ``flax.training`` are effectively deprecated '
+      'in favor of Optax schedules. Please refer to '
+      'https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules'
+      ' for alternatives.'
+  )
+
   def learning_rate_fn(step):
     lr = base_learning_rate
     if warmup_length > 0.0:
-      lr = lr * jnp.minimum(1., step / float(warmup_length) / steps_per_epoch)
+      lr = lr * jnp.minimum(1.0, step / float(warmup_length) / steps_per_epoch)
     return lr
+
   return learning_rate_fn
 
 
-def create_stepped_learning_rate_schedule(base_learning_rate, steps_per_epoch,
-                                          lr_sched_steps, warmup_length=0.0):
+def create_stepped_learning_rate_schedule(
+    base_learning_rate, steps_per_epoch, lr_sched_steps, warmup_length=0.0
+):
   """Create a stepped learning rate schedule with optional warmup.
 
   Note that with `FLIP #1009`_ learning rate schedules in ``flax.training`` are
   **effectively deprecated** in favor of Optax_ schedules. Please refer to
   `Optimizer Schedules`_ for more information.
 
   .. _FLIP #1009: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md
@@ -110,34 +115,37 @@
       factor that will linearly ramp-up from 0 to 1 over the first
       `warmup_length` epochs
 
   Returns:
     Function `f(step) -> lr` that computes the learning rate for a given step.
   """
   logging.warning(
-    'Learning rate schedules in ``flax.training`` are effectively deprecated '
-    'in favor of Optax schedules. Please refer to '
-    'https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules'
-    ' for alternatives.')
+      'Learning rate schedules in ``flax.training`` are effectively deprecated '
+      'in favor of Optax schedules. Please refer to '
+      'https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules'
+      ' for alternatives.'
+  )
   boundaries = [step[0] for step in lr_sched_steps]
   decays = [step[1] for step in lr_sched_steps]
   boundaries = np.array(boundaries) * steps_per_epoch
   boundaries = np.round(boundaries).astype(int)
   values = np.array([1.0] + decays) * base_learning_rate
 
   def learning_rate_fn(step):
     lr = _piecewise_constant(boundaries, values, step)
     if warmup_length > 0.0:
-      lr = lr * jnp.minimum(1., step / float(warmup_length) / steps_per_epoch)
+      lr = lr * jnp.minimum(1.0, step / float(warmup_length) / steps_per_epoch)
     return lr
+
   return learning_rate_fn
 
 
-def create_cosine_learning_rate_schedule(base_learning_rate, steps_per_epoch,
-                                         halfcos_epochs, warmup_length=0.0):
+def create_cosine_learning_rate_schedule(
+    base_learning_rate, steps_per_epoch, halfcos_epochs, warmup_length=0.0
+):
   """Create a cosine learning rate schedule with optional warmup.
 
   Note that with `FLIP #1009`_ learning rate schedules in ``flax.training`` are
   **effectively deprecated** in favor of Optax_ schedules. Please refer to
   `Optimizer Schedules`_ for more information.
 
   .. _FLIP #1009: https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md
@@ -160,22 +168,22 @@
       factor that will linearly ramp-up from 0 to 1 over the first
       `warmup_length` epochs
 
   Returns:
     Function `f(step) -> lr` that computes the learning rate for a given step.
   """
   logging.warning(
-    'Learning rate schedules in ``flax.training`` are effectively deprecated '
-    'in favor of Optax schedules. Please refer to '
-    'https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules'
-    ' for alternatives.')
+      'Learning rate schedules in ``flax.training`` are effectively deprecated '
+      'in favor of Optax schedules. Please refer to '
+      'https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules'
+      ' for alternatives.'
+  )
   halfwavelength_steps = halfcos_epochs * steps_per_epoch
 
   def learning_rate_fn(step):
     scale_factor = jnp.cos(step * jnp.pi / halfwavelength_steps) * 0.5 + 0.5
     lr = base_learning_rate * scale_factor
     if warmup_length > 0.0:
-      lr = lr * jnp.minimum(1., step / float(warmup_length) / steps_per_epoch)
+      lr = lr * jnp.minimum(1.0, step / float(warmup_length) / steps_per_epoch)
     return lr
-  return learning_rate_fn
-
 
+  return learning_rate_fn
```

### Comparing `flax-0.7.0/flax/training/orbax_utils.py` & `flax-0.7.1/flax/training/orbax_utils.py`

 * *Files 23% similar despite different names*

```diff
@@ -10,73 +10,113 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Utils for Orbax Checkpointing, available even after Flax Checkpointing is deprecated."""
 
+import dataclasses
+import inspect
 from typing import Any, Optional
 import warnings
 
 import jax
 from jax.sharding import Mesh
 import numpy as np
-from orbax import checkpoint as orbax
+import orbax.checkpoint as ocp
 
 
 PyTree = Any
 
 
 def is_multi_device_array(value: Any) -> bool:
   """Instruct Orbax to save this array with Tensorstore instead of msgpack."""
   if isinstance(value, jax.Array):
     return not value.is_fully_replicated
   return False
 
 
 def save_args_from_target(target: Any) -> Any:
   return jax.tree_util.tree_map(
-      lambda x: orbax.SaveArgs(aggregate=not is_multi_device_array(x)), target
+      lambda x: ocp.SaveArgs(aggregate=not is_multi_device_array(x)),
+      target,
   )
 
 
+def maybe_construct_transformations(
+    target: Any, transforms: Optional[Any]
+) -> Any:
+  if transforms is not None:
+    return transforms
+  flat_transforms = {}
+  flat_target = ocp.utils.to_flat_dict(target, sep='/', keep_empty_nodes=True)
+  for k, v in flat_target.items():
+    if v is None:
+      flat_transforms[k] = ocp.Transform(use_fallback=True)
+  return flat_transforms
+
+
 def restore_args_from_target(target: Any, mesh: Optional[Mesh] = None) -> Any:
   """Creates Orbax `restore_args` given a target Pytree.
 
   Args:
     target: The Pytree that has the same structure as the checkpoint. The arrays
       restored from checkpoint will have the same `sharding` as the target
       Pytree's corresponding arrays.
     mesh: DEPRECATED ARG. Please simply use your mesh to create the arrays
       in your `target`, no need to pass it here.
 
   Returns:
     A Pytree of Orbax `RestoreArgs` or `ArrayRestoreArgs`
   """
+
   def find_sharding(x):
-    if is_multi_device_array(x):
+    if isinstance(x, jax.Array):
       return x.sharding
     return None
 
-  # Simpler case: no multihost arrays
+  # Simpler case: no JAX arrays
   if not any(
-      jax.tree_util.tree_flatten(jax.tree_map(is_multi_device_array, target))[0]
+      jax.tree_util.tree_flatten(jax.tree_map(find_sharding, target))[0]
   ):
     return jax.tree_util.tree_map(
-        lambda x: orbax.RestoreArgs(restore_type=np.ndarray), target
+        lambda x: ocp.RestoreArgs(restore_type=np.ndarray), target
     )
 
-  # Multihost arrays: find sharding from the given target
+  # JAX arrays: find sharding from the given target and create RestoreArgs
+
+  # TODO(ivyzheng): remove after Orbax new release.
+  ocp_kwargs = {}
+  if (
+      'set_global_shape'
+      in inspect.signature(
+          ocp.checkpoint_utils.construct_restore_args
+      ).parameters
+  ):
+    ocp_kwargs['set_global_shape'] = False
+
   sharding_tree = jax.tree_util.tree_map(find_sharding, target)
   if mesh is not None:
     warnings.warn(
         (
             'restore_args_from_target(): `mesh` arg is deprecated. Simply'
             ' calling the function with target pytree should suffice.'
         ),
         DeprecationWarning,
     )
-    axes_tree = jax.tree_util.tree_map(lambda s: s.spec, sharding_tree)
-    return orbax.checkpoint_utils.restore_args_from_target(
-        mesh, target, axes_tree
+
+    def substitute_embedding(s):
+      return jax.sharding.NamedSharding(mesh, s.spec)
+
+    sharding_tree = jax.tree_util.tree_map(substitute_embedding, sharding_tree)
+  restore_args = ocp.checkpoint_utils.construct_restore_args(
+      target, sharding_tree, **ocp_kwargs
+  )
+  # TODO(ivyzheng): remove after Orbax new release.
+  if not ocp_kwargs:
+    restore_args = jax.tree_util.tree_map(
+        lambda ra: dataclasses.replace(ra, global_shape=None)
+        if isinstance(ra, ocp.ArrayRestoreArgs)
+        else ra,
+        restore_args,
     )
-  return orbax.checkpoint_utils.construct_restore_args(target, sharding_tree)
+  return restore_args
```

### Comparing `flax-0.7.0/flax/training/prefetch_iterator.py` & `flax-0.7.1/flax/training/prefetch_iterator.py`

 * *Files 5% similar despite different names*

```diff
@@ -8,16 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Utility for constructing an iterator which prefetches data asynchronously.
-"""
+"""Utility for constructing an iterator which prefetches data asynchronously."""
 
 import threading
 import warnings
 
 
 class PrefetchIterator:
   """Wraps an iterator to provide async prefetching.
@@ -39,16 +38,19 @@
   def __init__(self, data_iter, buffer_size=1):
     """Construct a PrefetchIterator.
 
     Args:
       data_iter: the Iterator that should be prefetched.
       buffer_size: how many items to prefetch (default: 1).
     """
-    warnings.warn('PrefetchIterator is deprecated. Use the standard `tf.data`'
-                  ' prefetch method instead', DeprecationWarning)
+    warnings.warn(
+        'PrefetchIterator is deprecated. Use the standard `tf.data`'
+        ' prefetch method instead',
+        DeprecationWarning,
+    )
 
     self._data_iter = data_iter
     self.buffer_size = buffer_size
     self._cond = threading.Condition()
     self._buffer = []
     self._active = True
     self._thread = threading.Thread(target=self._prefetch_loop, daemon=True)
@@ -73,14 +75,15 @@
   def close(self):
     with self._cond:
       self._active = False
       self._cond.notify_all()
 
   def _prefetch_loop(self):
     """Prefetch loop that prefetches a tf dataset."""
+
     def _predicate():
       return len(self._buffer) < self.buffer_size or not self._active
 
     while True:
       try:
         item = next(self._data_iter)
         with self._cond:
```

### Comparing `flax-0.7.0/flax/training/train_state.py` & `flax-0.7.1/flax/training/train_state.py`

 * *Files 2% similar despite different names*

```diff
@@ -45,14 +45,15 @@
     apply_fn: Usually set to `model.apply()`. Kept in this dataclass for
       convenience to have a shorter params list for the `train_step()` function
       in your training loop.
     params: The parameters to be updated by `tx` and used by `apply_fn`.
     tx: An Optax gradient transformation.
     opt_state: The state for `tx`.
   """
+
   step: int
   apply_fn: Callable = struct.field(pytree_node=False)
   params: core.FrozenDict[str, Any] = struct.field(pytree_node=True)
   tx: optax.GradientTransformation = struct.field(pytree_node=False)
   opt_state: optax.OptState = struct.field(pytree_node=True)
 
   def apply_gradients(self, *, grads, **kwargs):
@@ -66,16 +67,15 @@
       **kwargs: Additional dataclass attributes that should be `.replace()`-ed.
 
     Returns:
       An updated instance of `self` with `step` incremented by one, `params`
       and `opt_state` updated by applying `grads`, and additional attributes
       replaced as specified by `kwargs`.
     """
-    updates, new_opt_state = self.tx.update(
-        grads, self.opt_state, self.params)
+    updates, new_opt_state = self.tx.update(grads, self.opt_state, self.params)
     new_params = optax.apply_updates(self.params, updates)
     return self.replace(
         step=self.step + 1,
         params=new_params,
         opt_state=new_opt_state,
         **kwargs,
     )
```

### Comparing `flax-0.7.0/flax/traverse_util.py` & `flax-0.7.1/flax/traverse_util.py`

 * *Files 1% similar despite different names*

```diff
@@ -50,19 +50,21 @@
 import flax
 from flax.core.scope import VariableDict
 
 from . import struct
 
 Path = Tuple[str, ...]
 
+
 # the empty node is a struct.dataclass to be compatible with JAX.
 @struct.dataclass
 class _EmptyNode:
   pass
 
+
 empty_node = _EmptyNode()
 
 
 def flatten_dict(xs, keep_empty_nodes=False, is_leaf=None, sep=None):
   """Flatten a nested dictionary.
 
   The nested keys are flattened to a tuple.
@@ -93,37 +95,39 @@
     sep: if specified, then the keys of the returned
       dictionary will be `sep`-joined strings (if
       `None`, then keys will be tuples).
   Returns:
     The flattened dictionary.
   """
   assert isinstance(
-      xs,
-      (flax.core.FrozenDict, dict)), f'expected (frozen)dict; got {type(xs)}'
+      xs, (flax.core.FrozenDict, dict)
+  ), f'expected (frozen)dict; got {type(xs)}'
 
   def _key(path):
     if sep is None:
       return path
     return sep.join(path)
 
   def _flatten(xs, prefix):
     if not isinstance(xs, (flax.core.FrozenDict, dict)) or (
-        is_leaf and is_leaf(prefix, xs)):
+        is_leaf and is_leaf(prefix, xs)
+    ):
       return {_key(prefix): xs}
     result = {}
     is_empty = True
     for key, value in xs.items():
       is_empty = False
       path = prefix + (key,)
       result.update(_flatten(value, path))
     if keep_empty_nodes and is_empty:
       if prefix == ():  # when the whole input is empty
         return {}
       return {_key(prefix): empty_node}
     return result
+
   return _flatten(xs, ())
 
 
 def unflatten_dict(xs, sep=None):
   """Unflatten a dictionary.
 
   See `flatten_dict`
@@ -160,15 +164,16 @@
         cursor[key] = {}
       cursor = cursor[key]
     cursor[path[-1]] = value
   return result
 
 
 def path_aware_map(
-  f: Callable[[Path, Any], Any], nested_dict: VariableDict) -> VariableDict:
+    f: Callable[[Path, Any], Any], nested_dict: VariableDict
+) -> VariableDict:
   """A map function that operates over nested dictionary structures while taking
   the path to each leaf into account.
 
   Example::
 
     >>> import jax.numpy as jnp
     >>> from flax import traverse_util
@@ -183,27 +188,31 @@
       to a new value. Here ``path`` is a tuple of strings.
     nested_dict: A nested dictionary structure.
 
   Returns:
     A new nested dictionary structure with the mapped values.
   """
   flat = flatten_dict(nested_dict, keep_empty_nodes=True)
-  return unflatten_dict({
-    k: f(k, v) if v is not empty_node else v for k, v in flat.items()})
+  return unflatten_dict(
+      {k: f(k, v) if v is not empty_node else v for k, v in flat.items()}
+  )
+
 
 class Traversal(abc.ABC):
   """Base class for all traversals."""
 
   def __new__(cls, *args, **kwargs):
     # Must override __new__ instead of __init__ since this is an ABC
     warnings.warn(
         '`flax.traverse_util.Traversal` will be deprecated. If you are using '
         'it for `flax.optim`, use `optax` instead. Refer to the update guide '
         'https://flax.readthedocs.io/en/latest/guides/optax_update_guide.html '
-        'for detailed instructions.', DeprecationWarning)
+        'for detailed instructions.',
+        DeprecationWarning,
+    )
     return super().__new__(cls)
 
   @abc.abstractmethod
   def update(self, fn, inputs):
     """Update the focused items.
 
     Args:
@@ -231,14 +240,15 @@
 
     Args:
       values: a list containing the new values.
       inputs: the object that should be traversed.
     Returns:
       A new object with the updated values.
     """
+
     def update_fn(_):
       if not values:
         raise ValueError('Not enough values provided')
       return values.pop(0)
 
     y = self.update(update_fn, inputs)
     if values:
@@ -374,16 +384,18 @@
       ty = type(inputs)
       if isinstance(self._key, slice):
         sl = self._key
       else:
         sl = slice(self._key, self._key + 1)
       indices = set(range(*sl.indices(len(inputs))))
 
-      args = [fn(inputs[i]) if i in indices else inputs[i]
-              for i in range(len(inputs))]
+      args = [
+          fn(inputs[i]) if i in indices else inputs[i]
+          for i in range(len(inputs))
+      ]
       if _is_namedtuple(ty):
         return ty(*args)
       else:
         return ty(args)
     else:
       xs = copy.copy(inputs)
       xs[self._key] = fn(xs[self._key])
@@ -411,31 +423,31 @@
     if isinstance(inputs, dict):
       yield from inputs.values()
     else:
       yield from inputs
 
 
 class TraverseTree(Traversal):
-  """Traverse every item in a pytree.
-  """
+  """Traverse every item in a pytree."""
 
   def update(self, fn, inputs):
     return jax.tree_util.tree_map(fn, inputs)
 
   def iterate(self, inputs):
     yield from jax.tree_util.tree_leaves(inputs)
 
 
 def _get_params_dict(inputs):
   if isinstance(inputs, (dict, flax.core.FrozenDict)):
     return flax.core.unfreeze(inputs)
   else:
     raise ValueError(
         'Can only traverse a flax Model instance or a nested dict, not '
-        f'{type(inputs)}')
+        f'{type(inputs)}'
+    )
 
 
 def _sorted_items(x):
   """Returns items of a dict ordered by keys."""
   return sorted(x.items(), key=lambda x: x[0])
```

### Comparing `flax-0.7.0/flax/version.py` & `flax-0.7.1/flax/version.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,9 +9,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Current Flax version at head on Github."""
-__version__ = "0.7.0"
-
+__version__ = "0.7.1"
```

### Comparing `flax-0.7.0/flax.egg-info/PKG-INFO` & `flax-0.7.1/flax.egg-info/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flax
-Version: 0.7.0
+Version: 0.7.1
 Summary: Flax: A neural network library for JAX designed for flexibility
 Author-email: Flax team <flax-dev@google.com>
 Project-URL: homepage, https://github.com/google/flax
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
@@ -211,15 +211,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.7.0},
+  version = {0.7.1},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.7.0/flax.egg-info/SOURCES.txt` & `flax-0.7.1/flax.egg-info/SOURCES.txt`

 * *Files 0% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+.git-blame-ignore-revs
 .gitignore
 .pre-commit-config.yaml
 .readthedocs.yml
 AUTHORS
 CHANGELOG.md
 LICENSE
 README.md
```

### Comparing `flax-0.7.0/images/flax_logo.png` & `flax-0.7.1/images/flax_logo.png`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/images/flax_logo.svg` & `flax-0.7.1/images/flax_logo.svg`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/images/flax_logo_250px.png` & `flax-0.7.1/images/flax_logo_250px.png`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/images/flax_logo_500px.png` & `flax-0.7.1/images/flax_logo_500px.png`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/pylintrc` & `flax-0.7.1/pylintrc`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/pyproject.toml` & `flax-0.7.1/pyproject.toml`

 * *Files 14% similar despite different names*

```diff
@@ -51,14 +51,16 @@
     "pytype",
     "sentencepiece",  # WMT/LM1B examples
     "tensorflow_text>=2.11.0",  # WMT/LM1B examples
     "tensorflow_datasets",
     "tensorflow",
     "torch",
     "nbstripout",
+    "black[jupyter]==23.7.0",
+    "pyink==23.5.0",
 ]
 
 [project.urls]
 homepage = "https://github.com/google/flax"
 
 [tool.setuptools.dynamic]
 readme = {file = ["README.md"], content-type = "text/markdown"}
@@ -139,8 +141,14 @@
     "ignore:.*module 'sre_constants' is deprecated.*:DeprecationWarning",
 ]
 
 [tool.coverage.report]
 exclude_lines = [
     "@abc.abstractmethod",
     "raise NotImplementedError",
-]
+]
+
+[tool.pyink]
+pyink-indentation = 2
+pyink-use-majority-quotes = true
+line-length = 80
+preview = true
```

### Comparing `flax-0.7.0/tests/checkpoints_test.py` & `flax-0.7.1/tests/checkpoints_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -39,15 +39,16 @@
 
 
 PyTree = Any
 
 
 def check_eq(xs, ys):
   return jax.tree_util.tree_all(
-      jax.tree_util.tree_map(np.testing.assert_allclose, xs, ys))
+      jax.tree_util.tree_map(np.testing.assert_allclose, xs, ys)
+  )
 
 
 def shuffle(l):
   """Functional shuffle."""
   l = copy.copy(l)
   np.random.shuffle(l)
   return l
@@ -107,60 +108,79 @@
     for test, expect in zip(tests, expected):
       self.assertEqual(expect, checkpoints.safe_normpath(test))
 
   @parameterized.parameters({'use_orbax': True}, {'use_orbax': False})
   def test_save_restore_checkpoints(self, use_orbax):
     config.update('flax_use_orbax_checkpointing', use_orbax)
     tmp_dir = pathlib.Path(self.create_tempdir().full_path)
-    test_object0 = {'a': np.array([0, 0, 0], np.int32),
-                    'b': np.array([0, 0, 0], np.int32)}
-    test_object1 = {'a': np.array([1, 2, 3], np.int32),
-                    'b': np.array([1, 1, 1], np.int32)}
-    test_object2 = {'a': np.array([4, 5, 6], np.int32),
-                    'b': np.array([2, 2, 2], np.int32)}
+    test_object0 = {
+        'a': np.array([0, 0, 0], np.int32),
+        'b': np.array([0, 0, 0], np.int32),
+    }
+    test_object1 = {
+        'a': np.array([1, 2, 3], np.int32),
+        'b': np.array([1, 1, 1], np.int32),
+    }
+    test_object2 = {
+        'a': np.array([4, 5, 6], np.int32),
+        'b': np.array([2, 2, 2], np.int32),
+    }
     new_object = checkpoints.restore_checkpoint(
-        tmp_dir, test_object0, prefix='test_')
+        tmp_dir, test_object0, prefix='test_'
+    )
     check_eq(new_object, test_object0)
     checkpoints.save_checkpoint(
-        tmp_dir, test_object1, 0, prefix='test_', keep=1)
+        tmp_dir, test_object1, 0, prefix='test_', keep=1
+    )
     self.assertIn('test_0', os.listdir(tmp_dir))
     new_object = checkpoints.restore_checkpoint(
-        tmp_dir, test_object0, prefix='test_')
+        tmp_dir, test_object0, prefix='test_'
+    )
     check_eq(new_object, test_object1)
     checkpoints.save_checkpoint(
-        tmp_dir, test_object1, 1, prefix='test_', keep=1)
+        tmp_dir, test_object1, 1, prefix='test_', keep=1
+    )
     checkpoints.save_checkpoint(
-        tmp_dir, test_object2, 2, prefix='test_', keep=1)
+        tmp_dir, test_object2, 2, prefix='test_', keep=1
+    )
     new_object = checkpoints.restore_checkpoint(
-        tmp_dir, test_object0, prefix='test_')
+        tmp_dir, test_object0, prefix='test_'
+    )
     check_eq(new_object, test_object2)
     checkpoints.save_checkpoint(
-        tmp_dir, test_object2, 3, prefix='test_', keep=2)
+        tmp_dir, test_object2, 3, prefix='test_', keep=2
+    )
     checkpoints.save_checkpoint(
-        tmp_dir, test_object1, 4, prefix='test_', keep=2)
+        tmp_dir, test_object1, 4, prefix='test_', keep=2
+    )
     new_object = checkpoints.restore_checkpoint(
-        tmp_dir, test_object0, prefix='test_')
+        tmp_dir, test_object0, prefix='test_'
+    )
     check_eq(new_object, test_object1)
     new_object = checkpoints.restore_checkpoint(
-        tmp_dir, test_object0, step=3, prefix='test_')
+        tmp_dir, test_object0, step=3, prefix='test_'
+    )
     check_eq(new_object, test_object2)
 
     # Restore with a specific checkpoint path, not the directory path.
     new_object = checkpoints.restore_checkpoint(
-        os.path.join(tmp_dir, 'test_3'), test_object0)
+        os.path.join(tmp_dir, 'test_3'), test_object0
+    )
     check_eq(new_object, test_object2)
     # If a specific path is specified, but it does not exist, the same behavior
     # as when a directory is empty should apply: the target is returned
     # unchanged.
     new_object = checkpoints.restore_checkpoint(
-        os.path.join(tmp_dir, 'test_not_there'), test_object0)
+        os.path.join(tmp_dir, 'test_not_there'), test_object0
+    )
     check_eq(new_object, test_object0)
     with self.assertRaises(ValueError):
       checkpoints.restore_checkpoint(
-          tmp_dir, test_object0, step=5, prefix='test_')
+          tmp_dir, test_object0, step=5, prefix='test_'
+      )
 
   @parameterized.parameters({'use_orbax': True}, {'use_orbax': False})
   def test_overwrite_checkpoints(self, use_orbax):
     config.update('flax_use_orbax_checkpointing', use_orbax)
     overwrite_error = ValueError if use_orbax else errors.InvalidCheckpointError
     tmp_dir = self.create_tempdir().full_path
     test_object0 = {'a': np.array([0, 0, 0], np.int32)}
@@ -179,81 +199,101 @@
     new_object = checkpoints.restore_checkpoint(rel_tmp_dir, test_object0)
     check_eq(new_object, test_object)
     non_norm_dir_path = tmp_dir + '//'
     checkpoints.save_checkpoint(non_norm_dir_path, test_object, 4, keep=1)
     new_object = checkpoints.restore_checkpoint(non_norm_dir_path, test_object0)
     check_eq(new_object, test_object)
 
-  @parameterized.parameters({'use_orbax': True, 'keep_every_n_steps': None},
-                            {'use_orbax': False, 'keep_every_n_steps': 7})
+  @parameterized.parameters(
+      {'use_orbax': True, 'keep_every_n_steps': None},
+      {'use_orbax': False, 'keep_every_n_steps': 7},
+  )
   def test_keep(self, use_orbax, keep_every_n_steps):
     config.update('flax_use_orbax_checkpointing', use_orbax)
     tmp_dir = self.create_tempdir().full_path
     test_object = {'a': np.array([1, 2, 3], np.int32)}
     steps_start = 17
     steps_end = 37
     keep = 3
     increment = 5
 
     for step in range(steps_start, steps_end, increment):
-      checkpoints.save_checkpoint(tmp_dir,
-                                  test_object,
-                                  step=step,
-                                  keep=keep,
-                                  keep_every_n_steps=keep_every_n_steps)
+      checkpoints.save_checkpoint(
+          tmp_dir,
+          test_object,
+          step=step,
+          keep=keep,
+          keep_every_n_steps=keep_every_n_steps,
+      )
 
     last_checkpoint = -float('inf')
     for step in range(steps_start, steps_end, increment):
-      if ((steps_end - step) / increment <= keep) or (keep_every_n_steps and (
-          step - last_checkpoint) >= keep_every_n_steps):
+      if ((steps_end - step) / increment <= keep) or (
+          keep_every_n_steps and (step - last_checkpoint) >= keep_every_n_steps
+      ):
         restored = checkpoints.restore_checkpoint(
-            tmp_dir, target=None, step=step)
+            tmp_dir, target=None, step=step
+        )
         check_eq(restored, test_object)
         last_checkpoint = step
       else:
         with self.assertRaises(ValueError):
           checkpoints.restore_checkpoint(tmp_dir, target=None, step=step)
 
   @parameterized.parameters({'use_orbax': True}, {'use_orbax': False})
   def test_save_restore_checkpoints_w_float_steps(self, use_orbax):
     config.update('flax_use_orbax_checkpointing', use_orbax)
     tmp_dir = self.create_tempdir().full_path
-    test_object0 = {'a': np.array([0, 0, 0], np.int32),
-                    'b': np.array([0, 0, 0], np.int32)}
-    test_object1 = {'a': np.array([1, 2, 3], np.int32),
-                    'b': np.array([1, 1, 1], np.int32)}
-    test_object2 = {'a': np.array([4, 5, 6], np.int32),
-                    'b': np.array([2, 2, 2], np.int32)}
+    test_object0 = {
+        'a': np.array([0, 0, 0], np.int32),
+        'b': np.array([0, 0, 0], np.int32),
+    }
+    test_object1 = {
+        'a': np.array([1, 2, 3], np.int32),
+        'b': np.array([1, 1, 1], np.int32),
+    }
+    test_object2 = {
+        'a': np.array([4, 5, 6], np.int32),
+        'b': np.array([2, 2, 2], np.int32),
+    }
     checkpoints.save_checkpoint(
-        tmp_dir, test_object1, 0.0, prefix='test_', keep=1)
+        tmp_dir, test_object1, 0.0, prefix='test_', keep=1
+    )
     self.assertIn('test_0.0', os.listdir(tmp_dir))
     new_object = checkpoints.restore_checkpoint(
-        tmp_dir, test_object0, prefix='test_')
+        tmp_dir, test_object0, prefix='test_'
+    )
     check_eq(new_object, test_object1)
     checkpoints.save_checkpoint(
-        tmp_dir, test_object1, 2.0, prefix='test_', keep=1)
+        tmp_dir, test_object1, 2.0, prefix='test_', keep=1
+    )
     checkpoints.save_checkpoint(
-        tmp_dir, test_object2, 3.0, prefix='test_', keep=2)
+        tmp_dir, test_object2, 3.0, prefix='test_', keep=2
+    )
     self.assertIn('test_3.0', os.listdir(tmp_dir))
     self.assertIn('test_2.0', os.listdir(tmp_dir))
     check_eq(new_object, test_object1)
 
   @parameterized.parameters({'use_orbax': True}, {'use_orbax': False})
   def test_save_restore_checkpoints_target_none(self, use_orbax):
     config.update('flax_use_orbax_checkpointing', use_orbax)
     tmp_dir = self.create_tempdir().full_path
-    test_object0 = {'a': np.array([0, 0, 0], np.int32),
-                    'b': np.array([0, 0, 0], np.int32)}
+    test_object0 = {
+        'a': np.array([0, 0, 0], np.int32),
+        'b': np.array([0, 0, 0], np.int32),
+    }
     # Target pytree is a dictionary, so it's equal to a restored state_dict.
     checkpoints.save_checkpoint(tmp_dir, test_object0, 0)
     new_object = checkpoints.restore_checkpoint(tmp_dir, target=None)
     check_eq(new_object, test_object0)
     # Target pytree it's a tuple, check the expected state_dict is recovered.
-    test_object1 = (np.array([0, 0, 0], np.int32),
-                    np.array([1, 1, 1], np.int32))
+    test_object1 = (
+        np.array([0, 0, 0], np.int32),
+        np.array([1, 1, 1], np.int32),
+    )
     checkpoints.save_checkpoint(tmp_dir, test_object1, 1)
     new_object = checkpoints.restore_checkpoint(tmp_dir, target=None)
     expected_new_object = {str(k): v for k, v in enumerate(test_object1)}
     check_eq(new_object, expected_new_object)
 
   @parameterized.parameters({'use_orbax': True}, {'use_orbax': False})
   def test_save_restore_checkpoints_target_singular(self, use_orbax):
@@ -290,40 +330,53 @@
       check_eq(new_object, test_object0)
       checkpoints.save_checkpoint(tmp_dir, test_object0, 1)
       new_object = checkpoints.restore_checkpoint(tmp_dir, target=test_object1)
       check_eq(new_object, test_object1)
 
   def test_async_save_checkpoints(self):
     tmp_dir = pathlib.Path(self.create_tempdir().full_path)
-    test_object0 = {'a': np.array([0, 0, 0], np.int32),
-                    'b': np.array([0, 0, 0], np.int32)}
-    test_object1 = {'a': np.random.normal(size=(1000, 1000)),
-                    'b': np.random.normal(size=(1000, 1000))}
-    test_object2 = {'a': np.random.normal(size=(1000, 1000)),
-                    'b': np.random.normal(size=(1000, 1000))}
-    test_object3 = {'a': np.random.normal(size=(1000, 1000)),
-                    'b': np.random.normal(size=(1000, 1000))}
+    test_object0 = {
+        'a': np.array([0, 0, 0], np.int32),
+        'b': np.array([0, 0, 0], np.int32),
+    }
+    test_object1 = {
+        'a': np.random.normal(size=(1000, 1000)),
+        'b': np.random.normal(size=(1000, 1000)),
+    }
+    test_object2 = {
+        'a': np.random.normal(size=(1000, 1000)),
+        'b': np.random.normal(size=(1000, 1000)),
+    }
+    test_object3 = {
+        'a': np.random.normal(size=(1000, 1000)),
+        'b': np.random.normal(size=(1000, 1000)),
+    }
     am = checkpoints.AsyncManager()
     checkpoints.save_checkpoint(
-        tmp_dir, test_object1, 0, prefix='test_', keep=1, async_manager=am)
+        tmp_dir, test_object1, 0, prefix='test_', keep=1, async_manager=am
+    )
     # Hard-wait the write to be done, then check its content.
     am.save_future.result()
     self.assertIn('test_0', os.listdir(tmp_dir))
     new_object = checkpoints.restore_checkpoint(
-        tmp_dir, test_object1, prefix='test_')
+        tmp_dir, test_object1, prefix='test_'
+    )
     check_eq(new_object, test_object1)
     # Check two consecutive saves happen in the right order.
     checkpoints.save_checkpoint(
-        tmp_dir, test_object2, 1, prefix='test_', keep=1, async_manager=am)
+        tmp_dir, test_object2, 1, prefix='test_', keep=1, async_manager=am
+    )
     checkpoints.save_checkpoint(
-        tmp_dir, test_object3, 2, prefix='test_', keep=1, async_manager=am)
+        tmp_dir, test_object3, 2, prefix='test_', keep=1, async_manager=am
+    )
     am.save_future.result()
     self.assertIn('test_2', os.listdir(tmp_dir))
     new_object = checkpoints.restore_checkpoint(
-        tmp_dir, test_object1, prefix='test_')
+        tmp_dir, test_object1, prefix='test_'
+    )
     check_eq(new_object, test_object3)
 
   def test_last_checkpoint(self):
     tmp_dir = pathlib.Path(self.create_tempdir().full_path)
     with io.GFile(os.path.join(tmp_dir, 'test_tmp'), 'w') as f:
       f.write('test_tmp')
     io.makedirs(os.path.join(tmp_dir, 'test_tmp_gda'))
@@ -396,51 +449,54 @@
     checkpoints.save_checkpoint(tmp_dir, to_save, 0, prefix='test_')
     # And a legacy ckpt
     config.update('flax_use_orbax_checkpointing', False)
     checkpoints.save_checkpoint(tmp_dir, to_save, 1, prefix='test_', keep=2)
 
     # Both gets restored with same API.
     restored = checkpoints.restore_checkpoint(
-        os.path.join(tmp_dir, 'test_0'), target=target)
+        os.path.join(tmp_dir, 'test_0'), target=target
+    )
     check_eq(restored, to_save)
     restored = checkpoints.restore_checkpoint(
-        os.path.join(tmp_dir, 'test_1'), target=target)
+        os.path.join(tmp_dir, 'test_1'), target=target
+    )
     check_eq(restored, to_save)
 
+  @parameterized.parameters({'use_orbax': True}, {'use_orbax': False})
+  def test_smaller_target(self, use_orbax):
+    config.update('flax_use_orbax_checkpointing', use_orbax)
+    tmp_dir = self.create_tempdir().full_path
+    to_save = {'a': jnp.ones((16, 256, 1024))}
+    target = {'a': jnp.zeros((2, 3))}
+
+    checkpoints.save_checkpoint(tmp_dir, to_save, 0, keep=1)
+    new_object = checkpoints.restore_checkpoint(tmp_dir, target)
+    check_eq(new_object, to_save)
+
   def test_convert_pre_linen(self):
     params = checkpoints.convert_pre_linen({
         'mod_0': {
             'submod1_0': {},
             'submod2_1': {},
             'submod1_2': {},
         },
-        'mod2_2': {
-            'submod2_2_0': {}
-        },
-        'mod2_11': {
-            'submod2_11_0': {}
-        },
-        'mod2_1': {
-            'submod2_1_0': {}
-        },
+        'mod2_2': {'submod2_2_0': {}},
+        'mod2_11': {'submod2_11_0': {}},
+        'mod2_1': {'submod2_1_0': {}},
     })
     self.assertDictEqual(
-        core.unfreeze(params), {
+        core.unfreeze(params),
+        {
             'mod_0': {
                 'submod1_0': {},
                 'submod1_1': {},
                 'submod2_0': {},
             },
-            'mod2_0': {
-                'submod2_1_0': {}
-            },
-            'mod2_1': {
-                'submod2_2_0': {}
-            },
-            'mod2_2': {
-                'submod2_11_0': {}
-            },
-        })
+            'mod2_0': {'submod2_1_0': {}},
+            'mod2_1': {'submod2_2_0': {}},
+            'mod2_2': {'submod2_11_0': {}},
+        },
+    )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/colab_tpu_jax_version.ipynb` & `flax-0.7.1/tests/colab_tpu_jax_version.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/tests/core/core_frozen_dict_test.py` & `flax-0.7.1/tests/core/core_frozen_dict_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,16 +36,16 @@
     xs = {'a': 1, 'b': {'c': 2}}
     b, a = FrozenDict(xs).pop('a')
     self.assertEqual(a, 1)
     self.assertEqual(unfreeze(b), {'b': {'c': 2}})
 
   def test_frozen_dict_partially_maps(self):
     x = jax.tree_util.tree_map(
-        lambda a, b: (a, b),
-        freeze({'a': 2}), freeze({'a': {'b': 1}}))
+        lambda a, b: (a, b), freeze({'a': 2}), freeze({'a': {'b': 1}})
+    )
     self.assertEqual(unfreeze(x), {'a': (2, {'b': 1})})
 
   def test_frozen_dict_hash(self):
     xs = {'a': 1, 'b': {'c': 2}}
     ys = {'a': 1, 'b': {'c': 3}}
     self.assertNotEqual(hash(freeze(xs)), hash(freeze(ys)))
 
@@ -124,15 +124,17 @@
   @parameterized.parameters(
       {
           'x': {'a': 1, 'b': {'c': 2}},
           'pretty_str': '{\n    a: 1,\n    b: {\n        c: 2,\n    },\n}',
       },
       {
           'x': FrozenDict({'a': 1, 'b': {'c': 2}}),
-          'pretty_str': 'FrozenDict({\n    a: 1,\n    b: {\n        c: 2,\n    },\n})',
+          'pretty_str': (
+              'FrozenDict({\n    a: 1,\n    b: {\n        c: 2,\n    },\n})'
+          ),
       },
       {
           'x': 345,
           'pretty_str': '345',
       },
   )
   def test_utility_pretty_repr(self, x, pretty_str):
@@ -145,17 +147,20 @@
     self.assertEqual(
         jax.tree_util.tree_unflatten(tdef, flat_leaves),
         frozen,
     )
     flat_path_leaves, tdef = jax.tree_util.tree_flatten_with_path(frozen)
     self.assertEqual(
         flat_path_leaves,
-        [((jax.tree_util.DictKey('b'), jax.tree_util.DictKey('a')), 2),
-         ((jax.tree_util.DictKey('c'),), 1)],
+        [
+            ((jax.tree_util.DictKey('b'), jax.tree_util.DictKey('a')), 2),
+            ((jax.tree_util.DictKey('c'),), 1),
+        ],
     )
     self.assertEqual(
         jax.tree_util.tree_unflatten(tdef, [l for _, l in flat_path_leaves]),
         frozen,
     )
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/core_lift_test.py` & `flax-0.7.1/tests/core/core_lift_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -39,31 +39,37 @@
 
       lift.vmap(g, variable_axes={}, split_rngs={})((scope, a), jnp.ones((1,)))
 
     init(f)(random.PRNGKey(0))
 
   def test_undefined_param(self):
     def f(scope):
-      dense = lift.vmap(nn.dense,
-                        in_axes=(0, None), out_axes=0,
-                        variable_axes={'params': 0},
-                        split_rngs={'params': True})
+      dense = lift.vmap(
+          nn.dense,
+          in_axes=(0, None),
+          out_axes=0,
+          variable_axes={'params': 0},
+          split_rngs={'params': True},
+      )
       dense(scope.push('dense'), np.ones((3, 2)), 2)
 
     msg = r'Could not find parameter named "kernel" in scope "/vmap\(dense\)".'
     with self.assertRaisesRegex(errors.ScopeParamNotFoundError, msg):
       apply(f)({'params': {'dense': {'abc': np.ones((3, 3))}}})
 
   def test_jit_cache(self):
     compiles = 0
+
     @lift.jit
     def f(scope, x):
       nonlocal compiles
       compiles += 1
-      if scope.is_mutable_collection('intermediates') and not scope.is_mutable_collection('params'):
+      if scope.is_mutable_collection(
+          'intermediates'
+      ) and not scope.is_mutable_collection('params'):
         scope.put_variable('intermediates', 'x', x + 1)
       return nn.dense(scope, x, 1)
 
     x = np.ones((3, 2))
     _, params = init(f)(random.PRNGKey(0), x)
     init(f)(random.PRNGKey(0), x)
     self.assertEqual(compiles, 1)
@@ -73,44 +79,50 @@
     self.assertEqual(compiles, 2)  # applying again should not
     # edge case where only the implicit return of the jitted functions changes.
     # this should not use the previously cached apply.
     _, state = apply(f, mutable='intermediates')(params, x)
     self.assertEqual(compiles, 3)  # applying again should not
     self.assertEqual(state['intermediates']['x'].sum(), 3 * 2 * 2)
 
-
   def test_vjp(self):
     def g(scope, x, y):
       p = scope.param('test', nn.initializers.constant(0.5), ())
       scope.variable('state', 'counter', lambda: 0)
       return p * x * y
 
     def f(scope, x, y):
       z, bwd = lift.vjp(g, scope, x, y)
       return bwd(jnp.ones(y.shape))
 
-    x = jnp.array([1., 2., 3.])
-    y = jnp.array([4., 5., 6.])
+    x = jnp.array([1.0, 2.0, 3.0])
+    y = jnp.array([4.0, 5.0, 6.0])
     _, params = init(f)(random.PRNGKey(0), x, y)
     params_grad, x_grad, y_grad = apply(f)(params, x, y)
-    self.assertEqual(params_grad, {
-      'params': FrozenDict({'test': 32.}),
-    })
-    np.testing.assert_allclose(x_grad, [2., 2.5, 3.])
-    np.testing.assert_allclose(y_grad, [0.5, 1., 1.5])
+    self.assertEqual(
+        params_grad,
+        {
+            'params': FrozenDict({'test': 32.0}),
+        },
+    )
+    np.testing.assert_allclose(x_grad, [2.0, 2.5, 3.0])
+    np.testing.assert_allclose(y_grad, [0.5, 1.0, 1.5])
 
   def test_jvp(self):
     def g(scope, x):
       p = scope.param('test', nn.initializers.zeros_init(), ())
       scope.variable('state', 'counter', lambda: 0)
       return p * x
 
     def f(scope, x):
-      vars_t = jax.tree_util.tree_map(jnp.ones_like, scope.variables().get('params', {}))
-      _, out_t = lift.jvp(g, scope, (x,), (jnp.zeros_like(x),), {'params': vars_t})
+      vars_t = jax.tree_util.tree_map(
+          jnp.ones_like, scope.variables().get('params', {})
+      )
+      _, out_t = lift.jvp(
+          g, scope, (x,), (jnp.zeros_like(x),), {'params': vars_t}
+      )
       return out_t
 
     x = jnp.ones((3,))
     _, params = init(f)(random.PRNGKey(0), x)
     y_t = apply(f)(params, x)
     np.testing.assert_allclose(y_t, jnp.ones_like(x))
 
@@ -122,53 +134,79 @@
       scope.put_variable('state', 'acc', 0)
       scope.put_variable('state', 'rng_params', key_zero)
       scope.put_variable('state', 'rng_loop', key_zero)
 
       def cond_fn(scope, c):
         acc = scope.get_variable('state', 'acc')
         return acc < x
+
       def body_fn(scope, c):
         i = scope.get_variable('state', 'acc')
         p_rng = scope.make_rng('params')
         l_rng = scope.make_rng('loop')
-        scope.put_variable('state', 'rng_params', scope.get_variable('state', 'rng_params').at[i].set(p_rng))
-        scope.put_variable('state', 'rng_loop', scope.get_variable('state', 'rng_loop').at[i].set(l_rng))
+        scope.put_variable(
+            'state',
+            'rng_params',
+            scope.get_variable('state', 'rng_params').at[i].set(p_rng),
+        )
+        scope.put_variable(
+            'state',
+            'rng_loop',
+            scope.get_variable('state', 'rng_loop').at[i].set(l_rng),
+        )
         inc = scope.get_variable('params', 'inc')
         scope.put_variable('state', 'acc', i + inc)
         return c + 2
-      return lift.while_loop(cond_fn, body_fn, scope, 0, carry_variables='state', split_rngs={'params': False, 'loop': True})
+
+      return lift.while_loop(
+          cond_fn,
+          body_fn,
+          scope,
+          0,
+          carry_variables='state',
+          split_rngs={'params': False, 'loop': True},
+      )
+
     x = 2
-    c, vars = apply(f, mutable=True)({}, x, rngs={'params': random.PRNGKey(1), 'loop': random.PRNGKey(2)})
+    c, vars = apply(f, mutable=True)(
+        {}, x, rngs={'params': random.PRNGKey(1), 'loop': random.PRNGKey(2)}
+    )
     self.assertEqual(vars['state']['acc'], x)
     self.assertEqual(c, 2 * x)
-    np.testing.assert_array_equal(vars['state']['rng_params'][0], vars['state']['rng_params'][1])
-    np.testing.assert_array_compare(operator.__ne__, vars['state']['rng_loop'][0], vars['state']['rng_loop'][1])
+    np.testing.assert_array_equal(
+        vars['state']['rng_params'][0], vars['state']['rng_params'][1]
+    )
+    np.testing.assert_array_compare(
+        operator.__ne__,
+        vars['state']['rng_loop'][0],
+        vars['state']['rng_loop'][1],
+    )
 
   def test_cond(self):
     def f(scope, x, pred):
       scope.variable('state', 'true_count', lambda: 0)
       scope.variable('state', 'false_count', lambda: 0)
+
       def true_fn(scope, x):
         scope.variable('state', 'true_count').value += 1
         return scope.child(nn.dense)(x, 2)
 
       def false_fn(scope, x):
         scope.variable('state', 'false_count').value += 1
         return -scope.child(nn.dense)(x, 2)
 
       return lift.cond(pred, true_fn, false_fn, scope, x)
 
     x = jnp.ones((1, 3))
     y1, vars = init(f)(random.PRNGKey(0), x, True)
     self.assertEqual(vars['state'], {'true_count': 1, 'false_count': 0})
-    y2, vars = apply(f, mutable="state")(vars, x, False)
+    y2, vars = apply(f, mutable='state')(vars, x, False)
     self.assertEqual(vars['state'], {'true_count': 1, 'false_count': 1})
     np.testing.assert_allclose(y1, -y2)
 
-  @temp_flip_flag('return_frozendict', False)
   def test_switch(self):
     def f(scope, x, index):
       scope.variable('state', 'a_count', lambda: 0)
       scope.variable('state', 'b_count', lambda: 0)
       scope.variable('state', 'c_count', lambda: 0)
 
       def a_fn(scope, x):
@@ -184,37 +222,38 @@
         return scope.child(nn.dense)(x, 2)
 
       return lift.switch(index, [a_fn, b_fn, c_fn], scope, x)
 
     x = jnp.ones((1, 3))
     y1, vars = init(f)(random.PRNGKey(0), x, 0)
     self.assertEqual(vars['state'], {'a_count': 1, 'b_count': 0, 'c_count': 0})
-    y2, updates = apply(f, mutable="state")(vars, x, 1)
+    y2, updates = apply(f, mutable='state')(vars, x, 1)
     vars = copy(vars, updates)
     self.assertEqual(vars['state'], {'a_count': 1, 'b_count': 1, 'c_count': 0})
     np.testing.assert_allclose(y1, -y2)
-    y3, updates = apply(f, mutable="state")(vars, x, 2)
+    y3, updates = apply(f, mutable='state')(vars, x, 2)
     vars = copy(vars, updates)
     self.assertEqual(vars['state'], {'a_count': 1, 'b_count': 1, 'c_count': 1})
     np.testing.assert_allclose(y1, y3)
 
   def test_subscope_var_aliasing(self):
     def test(scope, x):
-      subscope = scope.push(name="a")
-      subscope.put_variable('state', 'x', 0.)
+      subscope = scope.push(name='a')
+      subscope.put_variable('state', 'x', 0.0)
       _ = lift.while_loop(
-        lambda scope, x: False,
-        lambda scope, x: x,
-        scope,
-        jnp.array(0, jnp.int32),
-        carry_variables=['state'],
+          lambda scope, x: False,
+          lambda scope, x: x,
+          scope,
+          jnp.array(0, jnp.int32),
+          carry_variables=['state'],
       )
-      subscope.put_variable('state', 'x', 1.)
+      subscope.put_variable('state', 'x', 1.0)
       val0 = scope.variables()['state']['a']['x']
       val1 = subscope.variables()['state']['x']
       self.assertEqual(val0, val1)
       return x
-    init(test)( random.PRNGKey(0), 1.)
+
+    init(test)(random.PRNGKey(0), 1.0)
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/core_meta_test.py` & `flax-0.7.1/tests/core/core_meta_test.py`

 * *Files 15% similar despite different names*

```diff
@@ -24,158 +24,201 @@
 
 
 class MetaTest(absltest.TestCase):
 
   def test_boxed_param(self):
     def f(scope, xs):
       def g(scope, x):
-        kernel_init = meta.with_partitioning(nn.initializers.ones_init(),
-                                             ('in', 'out'))
+        kernel_init = meta.with_partitioning(
+            nn.initializers.ones_init(), ('in', 'out')
+        )
         kernel = scope.param('kernel', kernel_init, (x.shape[-1], 2))
         kernel_box = scope.get_variable('params', 'kernel')
         self.assertIsInstance(kernel_box, meta.Partitioned)
         self.assertEqual(kernel_box.names, ('in', 'out'))
         return x @ kernel
 
       lift.vmap(
-          g, in_axes=0,
-          variable_axes={'params': 0}, split_rngs={'params': True},
-          metadata_params={meta.PARTITION_NAME: 'batch'})(scope, xs)
+          g,
+          in_axes=0,
+          variable_axes={'params': 0},
+          split_rngs={'params': True},
+          metadata_params={meta.PARTITION_NAME: 'batch'},
+      )(scope, xs)
 
     _, variables = init(f)(random.PRNGKey(0), jnp.zeros((8, 3)))
-    self.assertEqual(variables['params']['kernel'].names,
-                     ('batch', 'in', 'out'))
+    self.assertEqual(
+        variables['params']['kernel'].names, ('batch', 'in', 'out')
+    )
 
   def test_boxed_variable(self):
     def f(scope, xs):
       def g(scope, x):
-        kernel_init = meta.with_partitioning(nn.initializers.ones_init(),
-                                             ('in', 'out'))
-        kernel = scope.variable('params', 'kernel', kernel_init,
-                                scope.make_rng('params'), (x.shape[-1], 2))
-        kernel.value += 1.
+        kernel_init = meta.with_partitioning(
+            nn.initializers.ones_init(), ('in', 'out')
+        )
+        kernel = scope.variable(
+            'params',
+            'kernel',
+            kernel_init,
+            scope.make_rng('params'),
+            (x.shape[-1], 2),
+        )
+        kernel.value += 1.0
         self.assertEqual(kernel.value.sum(), kernel.value.size * 2)
         kernel_box = scope.get_variable('params', 'kernel')
         self.assertIsInstance(kernel_box, meta.Partitioned)
         self.assertEqual(kernel_box.names, ('in', 'out'))
         return x @ kernel.value
 
       lift.vmap(
-          g, in_axes=0,
-          variable_axes={'params': 0}, split_rngs={'params': True},
-          metadata_params={meta.PARTITION_NAME: 'batch'})(scope, xs)
+          g,
+          in_axes=0,
+          variable_axes={'params': 0},
+          split_rngs={'params': True},
+          metadata_params={meta.PARTITION_NAME: 'batch'},
+      )(scope, xs)
 
     _, variables = init(f)(random.PRNGKey(0), jnp.zeros((8, 3)))
-    self.assertEqual(variables['params']['kernel'].names,
-                     ('batch', 'in', 'out'))
+    self.assertEqual(
+        variables['params']['kernel'].names, ('batch', 'in', 'out')
+    )
 
   def test_partition_axis_unspecified(self):
     def f(scope, xs):
       def g(scope, x):
-        kernel_init = meta.with_partitioning(nn.initializers.ones_init(),
-                                             ('in', 'out'))
+        kernel_init = meta.with_partitioning(
+            nn.initializers.ones_init(), ('in', 'out')
+        )
         scope.param('kernel', kernel_init, (3, 2))
         return x
 
       with self.assertRaises(errors.PartitioningUnspecifiedError):
         lift.vmap(
-            g, in_axes=0,
-            variable_axes={'params': 0}, split_rngs={'params': True},
-            metadata_params={})(scope, xs)
+            g,
+            in_axes=0,
+            variable_axes={'params': 0},
+            split_rngs={'params': True},
+            metadata_params={},
+        )(scope, xs)
+
     init(f)(random.PRNGKey(0), jnp.zeros((8, 3)))
 
   def test_unbox(self):
-    xs = {'kernel': meta.Partitioned(jnp.zeros((3, 2)), ('in', 'out')),
-          'complex': meta.Partitioned(
-              {'K': jnp.zeros((3, 2)), 'b': jnp.zeros((3,))}, ('data',))}
+    xs = {
+        'kernel': meta.Partitioned(jnp.zeros((3, 2)), ('in', 'out')),
+        'complex': meta.Partitioned(
+            {'K': jnp.zeros((3, 2)), 'b': jnp.zeros((3,))}, ('data',)
+        ),
+    }
     unboxed = meta.unbox(xs)
     unboxed_shapes = jax.tree_map(jnp.shape, unboxed)
-    self.assertEqual(unboxed_shapes, {
-        'kernel': (3, 2),
-        'complex': {
-            'K': (3, 2), 'b': (3,),
-        }
-    })
+    self.assertEqual(
+        unboxed_shapes,
+        {
+            'kernel': (3, 2),
+            'complex': {
+                'K': (3, 2),
+                'b': (3,),
+            },
+        },
+    )
 
   def test_scan_over_layers(self):
     def f(scope, x):
       def body(scope, x):
-        kernel_init = meta.with_partitioning(nn.initializers.ones_init(),
-                                             ('in', 'out'))
+        kernel_init = meta.with_partitioning(
+            nn.initializers.ones_init(), ('in', 'out')
+        )
         y = nn.dense(scope, x, 3, kernel_init=kernel_init)
         return y, ()
 
       c, _ = lift.scan(
           body,
-          variable_axes={'params': 0}, split_rngs={'params': True},
+          variable_axes={'params': 0},
+          split_rngs={'params': True},
           length=8,
-          metadata_params={meta.PARTITION_NAME: 'layers'})(scope, x)
+          metadata_params={meta.PARTITION_NAME: 'layers'},
+      )(scope, x)
       return c
 
     _, variables = init(f)(random.PRNGKey(0), jnp.zeros((8, 3)))
     boxed_shapes = jax.tree_map(jnp.shape, variables['params'])
-    self.assertEqual(boxed_shapes, {
-        'kernel': meta.Partitioned((8, 3, 3), ('layers', 'in', 'out')),
-        'bias': (8, 3),
-    })
+    self.assertEqual(
+        boxed_shapes,
+        {
+            'kernel': meta.Partitioned((8, 3, 3), ('layers', 'in', 'out')),
+            'bias': (8, 3),
+        },
+    )
 
   def test_get_partition_spec(self):
-    xs = {'kernel': meta.Partitioned(jnp.zeros((8, 3, 3)),
-                                     ('layers', 'in', 'out')),
-          'bias': jnp.zeros((8, 3)),
-          'step': jnp.array(100)}
+    xs = {
+        'kernel': meta.Partitioned(
+            jnp.zeros((8, 3, 3)), ('layers', 'in', 'out')
+        ),
+        'bias': jnp.zeros((8, 3)),
+        'step': jnp.array(100),
+    }
     ps = meta.get_partition_spec(xs)
     self.assertEqual(
         ps,
         {
             'kernel': jax.sharding.PartitionSpec('layers', 'in', 'out'),
             'bias': jax.sharding.PartitionSpec(),
             'step': jax.sharding.PartitionSpec(),
         },
     )
 
   def test_get_sharding(self):
     devices = mesh_utils.create_device_mesh((jax.local_device_count(), 1))
     mesh = sharding.Mesh(devices, ('in', 'out'))
-    xs = {'kernel': meta.Partitioned(jnp.zeros((8, 3)),
-                                     ('in', 'out')),
-          'bias': jnp.zeros((8, 3)),
-          'step': jnp.array(100)}
+    xs = {
+        'kernel': meta.Partitioned(jnp.zeros((8, 3)), ('in', 'out')),
+        'bias': jnp.zeros((8, 3)),
+        'step': jnp.array(100),
+    }
     ps = meta.get_sharding(xs, mesh)
     self.assertEqual(
         ps,
         {
-            'kernel': jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec('in', 'out')),
-            'bias': jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec()),
-            'step': jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec()),
+            'kernel': jax.sharding.NamedSharding(
+                mesh, jax.sharding.PartitionSpec('in', 'out')
+            ),
+            'bias': jax.sharding.NamedSharding(
+                mesh, jax.sharding.PartitionSpec()
+            ),
+            'step': jax.sharding.NamedSharding(
+                mesh, jax.sharding.PartitionSpec()
+            ),
         },
     )
 
   def test_boxed_param_with_mesh(self):
     devices = mesh_utils.create_device_mesh((jax.local_device_count(), 1))
     mesh = sharding.Mesh(devices, ('in', 'out'))
 
     def f(scope, x):
-        kernel_init = meta.with_partitioning(
-          nn.initializers.ones_init(),('in', 'out'), mesh=mesh)
-        kernel = scope.param('kernel', kernel_init, (x.shape[-1], 2))
-        kernel_box = scope.get_variable('params', 'kernel')
-        self.assertIsInstance(kernel_box, meta.Partitioned)
-        self.assertEqual(kernel_box.names, ('in', 'out'))
-        return x @ kernel
+      kernel_init = meta.with_partitioning(
+          nn.initializers.ones_init(), ('in', 'out'), mesh=mesh
+      )
+      kernel = scope.param('kernel', kernel_init, (x.shape[-1], 2))
+      kernel_box = scope.get_variable('params', 'kernel')
+      self.assertIsInstance(kernel_box, meta.Partitioned)
+      self.assertEqual(kernel_box.names, ('in', 'out'))
+      return x @ kernel
 
     @jax.jit
     def create_state():
       y, variables = init(f)(random.PRNGKey(0), jnp.zeros((8, 4)))
       spec = meta.get_partition_spec(variables)
       shardings = jax.tree_map(lambda s: sharding.NamedSharding(mesh, s), spec)
       variables = jax.lax.with_sharding_constraint(variables, shardings)
       return variables
 
-
     variables = create_state()
-    self.assertEqual(variables['params']['kernel'].names,
-                     ('in', 'out'))
+    self.assertEqual(variables['params']['kernel'].names, ('in', 'out'))
     self.assertIs(variables['params']['kernel'].mesh, mesh)
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/core_scope_test.py` & `flax-0.7.1/tests/core/core_scope_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -24,27 +24,31 @@
 from jax import numpy as jnp
 
 import numpy as np
 
 
 from absl.testing import absltest
 
+
 class ScopeTest(absltest.TestCase):
 
   def test_rng(self):
     def f(scope):
       self.assertTrue(scope.has_rng('params'))
       self.assertFalse(scope.has_rng('dropout'))
       rng = scope.make_rng('params')
-      self.assertTrue(np.all(rng == LazyRng.create(random.PRNGKey(0), 1).as_jax_rng()))
+      self.assertTrue(
+          np.all(rng == LazyRng.create(random.PRNGKey(0), 1).as_jax_rng())
+      )
+
     init(f)(random.PRNGKey(0))
 
   def test_in_filter(self):
-    filter_true = lambda x, y : self.assertTrue(scope.in_filter(x, y))
-    filter_false = lambda x, y : self.assertFalse(scope.in_filter(x, y))
+    filter_true = lambda x, y: self.assertTrue(scope.in_filter(x, y))
+    filter_false = lambda x, y: self.assertFalse(scope.in_filter(x, y))
 
     filter_true(True, 'any_string1')
     filter_false(False, 'any_string2')
     filter_true('exact_match', 'exact_match')
     filter_false('no_match1', 'no_match2')
     filter_true(['one', 'two'], 'one')
     filter_false(['one', 'two'], 'three')
@@ -56,86 +60,109 @@
       self.assertEqual(scope.union_filters(a, b), ans)
       self.assertEqual(scope.union_filters(b, a), ans)
 
     union_check(['a', 'b'], ['b', 'c'], set(['a', 'b', 'c']))
     union_check(True, False, True)
     union_check(False, False, set())
     union_check(True, True, True)
-    union_check(scope.DenyList(['a', 'b']), scope.DenyList(['b', 'c']), scope.DenyList(set(['b'])))
-    union_check(scope.DenyList(['a', 'b']), ['b', 'c'], scope.DenyList(set(['a'])))
+    union_check(
+        scope.DenyList(['a', 'b']),
+        scope.DenyList(['b', 'c']),
+        scope.DenyList(set(['b'])),
+    )
+    union_check(
+        scope.DenyList(['a', 'b']), ['b', 'c'], scope.DenyList(set(['a']))
+    )
 
   def test_intersect_filter(self):
     def intersect_check(a, b, ans):
       self.assertEqual(scope.intersect_filters(a, b), ans)
       self.assertEqual(scope.intersect_filters(b, a), ans)
 
     intersect_check(['a', 'b'], ['b', 'c'], set(['b']))
     intersect_check(True, False, False)
     intersect_check(False, False, set())
     intersect_check(True, True, True)
-    intersect_check(scope.DenyList(['a', 'b']), scope.DenyList(['b', 'c']), scope.DenyList(set(['a', 'b', 'c'])))
+    intersect_check(
+        scope.DenyList(['a', 'b']),
+        scope.DenyList(['b', 'c']),
+        scope.DenyList(set(['a', 'b', 'c'])),
+    )
     intersect_check(scope.DenyList(['a', 'b']), ['b', 'c'], set(['c']))
 
   def test_subtract_filter(self):
     def subtract_check(a, b, ans):
       self.assertEqual(scope.subtract_filters(a, b), ans)
 
     subtract_check(['a', 'b'], ['b', 'c'], set(['a']))
     subtract_check(True, False, scope.DenyList(False))
     subtract_check(False, False, set())
     subtract_check(True, True, False)
     subtract_check(True, 'a', scope.DenyList('a'))
-    subtract_check(scope.DenyList(['a', 'b']), scope.DenyList(['b', 'c']), set(['c']))
-    subtract_check(scope.DenyList(['a', 'b']), ['b', 'c'], scope.DenyList(set(['a', 'b', 'c'])))
-
+    subtract_check(
+        scope.DenyList(['a', 'b']), scope.DenyList(['b', 'c']), set(['c'])
+    )
+    subtract_check(
+        scope.DenyList(['a', 'b']),
+        ['b', 'c'],
+        scope.DenyList(set(['a', 'b', 'c'])),
+    )
 
   def test_group_collections(self):
-    params = { 'dense1': { 'x': [10, 20] } }
-    batch_stats = { 'dense1': { 'ema': 5 } }
-    xs = { 'params': params, 'batch_stats': batch_stats }
+    params = {'dense1': {'x': [10, 20]}}
+    batch_stats = {'dense1': {'ema': 5}}
+    xs = {'params': params, 'batch_stats': batch_stats}
 
     # Retrieve all keys only once.
     group = scope.group_collections(xs, ['params', 'params'])
     self.assertEqual(group, ({'params': params}, {}))
 
     # Ignore non-existing keys.
     self.assertEqual(scope.group_collections(xs, ['vars']), ({},))
 
     # False gets nothing and True retrieves all keys once.
-    self.assertEqual(scope.group_collections(xs, [False, True, True]),
-                                             ({}, xs, {}))
+    self.assertEqual(
+        scope.group_collections(xs, [False, True, True]), ({}, xs, {})
+    )
 
   def test_inconsistent_param_shapes(self):
     def f(scope):
       scope.param('test', nn.initializers.ones_init(), (4,))
 
-    msg = r'Initializer expected to generate shape \(2,\) but got shape \(4,\) instead for parameter "test" in "/"'
+    msg = (
+        r'Initializer expected to generate shape \(2,\) but got shape \(4,\)'
+        r' instead for parameter "test" in "/"'
+    )
     with self.assertRaisesRegex(errors.ScopeParamShapeError, msg):
       apply(f)(freeze({'params': {'test': np.ones((2,))}}))
 
   def test_apply_variables_bad_pytree(self):
     def f(scope):
       scope.param('kernel', nn.initializers.ones_init(), (4,))
 
     params = freeze({
         'params': {
             'kernel': np.ones((4,)),
         },
     })
     apply(f)(params)  # Valid.
     msg = 'but got a dict with an extra params layer'
-    with self.assertRaisesRegex(errors.ApplyScopeInvalidVariablesStructureError,
-                                msg):
+    with self.assertRaisesRegex(
+        errors.ApplyScopeInvalidVariablesStructureError, msg
+    ):
       apply(f)({'params': params})
 
   def test_mutate_undefined_collection(self):
     def f(scope):
       scope.put_variable('state', 'test', 123)
 
-    msg = r'Cannot update variable "test" in "/" because collection "state" is immutable.'
+    msg = (
+        r'Cannot update variable "test" in "/" because collection "state" is'
+        r' immutable.'
+    )
     with self.assertRaisesRegex(errors.ModifyScopeVariableError, msg):
       init(f, mutable='params')(random.PRNGKey(0))
 
   def test_undefined_param(self):
     def f(scope):
       nn.dense(scope.push('dense'), np.ones((1, 2)), 2)
 
@@ -150,41 +177,48 @@
 
     _, variables = apply(f, mutable='state')({}, True)
     apply(f, mutable=False)(variables, False)
 
   def test_rngs_check_w_frozen_dict(self):
     def f(scope, x):
       return x
-    _ = apply(f)(
-        {}, np.array([0.]), rngs=freeze({'a':random.PRNGKey(0)}))
 
-  @unittest.skipIf(not hasattr(jax_config, 'jax_enable_custom_prng'),
-                   'custom PRNG tests require config.jax_enable_custom_prng')
+    _ = apply(f)({}, np.array([0.0]), rngs=freeze({'a': random.PRNGKey(0)}))
+
+  @unittest.skipIf(
+      not hasattr(jax_config, 'jax_enable_custom_prng'),
+      'custom PRNG tests require config.jax_enable_custom_prng',
+  )
   def test_rng_check_w_old_and_new_keys(self):
     old_setting = jax_config.jax_enable_custom_prng
     try:
       jax_config.update('jax_enable_custom_prng', False)
       self.assertTrue(scope._is_valid_rng(random.PRNGKey(0)))
       jax_config.update('jax_enable_custom_prng', True)
       self.assertTrue(scope._is_valid_rng(random.PRNGKey(0)))
     finally:
       jax_config.update('jax_enable_custom_prng', old_setting)
 
   def test_jax_leak_detector(self):
     with jax.check_tracer_leaks(True):
+
       def f(scope):
         def g(scope):
           pass
+
         scope.child(g)()
+
       jax.jit(init(f))(random.PRNGKey(0))
 
   def test_rng_counter_reuse(self):
     root = Scope({}, {'dropout': random.PRNGKey(0)})
+
     def f(scope):
       return scope.make_rng('dropout')
+
     a = root.child(f)()
     root = root.rewound()
     b = root.child(f)()
     self.assertFalse(jnp.allclose(a, b))
 
   def test_empty_col_error(self):
     root = Scope({})
@@ -209,38 +243,48 @@
     abc = root.variable('state', 'abc')
     self.assertEqual(abc.value, 1)
     with self.assertRaises(errors.ScopeVariableNotFoundError):
       root.variable('state', 'test')
 
   def test_variable_alias(self):
     scope = Scope({}, mutable='state')
-    subscope = scope.push(name="a")
-    subscope.put_variable('state', 'x', 0.)
-    scope.put_variable('state', 'a', {'x': jnp.array(1., jnp.float32)})
-    self.assertEqual(scope.variables()['state']['a']['x'], subscope.variables()['state']['x'])
+    subscope = scope.push(name='a')
+    subscope.put_variable('state', 'x', 0.0)
+    scope.put_variable('state', 'a', {'x': jnp.array(1.0, jnp.float32)})
+    self.assertEqual(
+        scope.variables()['state']['a']['x'], subscope.variables()['state']['x']
+    )
 
   def test_lazy_init(self):
     def f(scope, x):
-      k = scope.param("kernel", nn.initializers.lecun_normal(), (x.shape[-1], x.shape[-1]))
+      k = scope.param(
+          'kernel', nn.initializers.lecun_normal(), (x.shape[-1], x.shape[-1])
+      )
       return x @ k
+
     init_fn = lazy_init(f)
     # provide a massive input message which would OOM if any compute ops were actually executed
-    variables = init_fn(random.PRNGKey(0), jax.ShapeDtypeStruct((1024 * 1024 * 1024, 128), jnp.float32))
-    self.assertEqual(variables["params"]["kernel"].shape, (128, 128))
+    variables = init_fn(
+        random.PRNGKey(0),
+        jax.ShapeDtypeStruct((1024 * 1024 * 1024, 128), jnp.float32),
+    )
+    self.assertEqual(variables['params']['kernel'].shape, (128, 128))
 
   def test_lazy_init_fails_on_data_dependence(self):
     def f(scope, x):
       # kernel is initialized with x so params are now dependent on the input
-      k = scope.param("kernel", lambda _: x)
+      k = scope.param('kernel', lambda _: x)
       return x * k
+
     init_fn = lazy_init(f)
     with self.assertRaises(errors.LazyInitError):
       init_fn(random.PRNGKey(0), jax.ShapeDtypeStruct((8, 4), jnp.float32))
 
   @temp_flip_flag('fix_rng_separator', True)
   def test_fold_in_static_seperator(self):
-    x = LazyRng(random.PRNGKey(0), ("ab", "c"))
-    y = LazyRng(random.PRNGKey(0), ("a", "bc"))
+    x = LazyRng(random.PRNGKey(0), ('ab', 'c'))
+    y = LazyRng(random.PRNGKey(0), ('a', 'bc'))
     self.assertFalse(np.all(x.as_jax_rng() == y.as_jax_rng()))
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_attention_test.py` & `flax-0.7.1/tests/core/design/core_attention_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -26,135 +26,150 @@
 from flax.core import Scope, init, lift, Array, nn, unfreeze
 
 
 def softmax_attn(scope: Scope, weights: Array):
   del scope
   norm_dims = tuple(range(weights.ndim // 2, weights.ndim))
   log_norms = jax.scipy.special.logsumexp(
-      weights, axis=norm_dims, keepdims=True)
+      weights, axis=norm_dims, keepdims=True
+  )
   return jnp.exp(weights - log_norms)
 
+
 def with_dropout(fn, rate: float, deterministic: bool = False):
   def attn_fn(scope: Scope, weights: Array):
     attn_weights = fn(scope, weights)
-    return nn.dropout(scope, attn_weights, deterministic=deterministic, rate=rate)
+    return nn.dropout(
+        scope, attn_weights, deterministic=deterministic, rate=rate
+    )
+
   return attn_fn
 
+
 def _dot_product_attention(
     scope: Scope,
-    query: Array, key: Array, value: Array,
+    query: Array,
+    key: Array,
+    value: Array,
     bias: Optional[Array] = None,
     attn_fn: Callable = softmax_attn,
-    dtype=jnp.float32):
+    dtype=jnp.float32,
+):
   assert key.ndim == query.ndim
   assert key.ndim == value.ndim
 
   n = query.ndim
-  attn_weights = lax.dot_general(
-      query, key,
-      (((n-1,), (n - 1,)), ((), ())))
+  attn_weights = lax.dot_general(query, key, (((n - 1,), (n - 1,)), ((), ())))
   if bias is not None:
     attn_weights += bias
   attn_weights = attn_fn(scope, attn_weights)
   attn_weights = attn_weights.astype(dtype)
 
   contract_dims = (
       tuple(range(n - 1, attn_weights.ndim)),
-      tuple(range(0, n  - 1)))
-  y = lax.dot_general(
-      attn_weights, value,
-      (contract_dims, ((), ())))
+      tuple(range(0, n - 1)),
+  )
+  y = lax.dot_general(attn_weights, value, (contract_dims, ((), ())))
   return y
 
 
 def dot_product_attention(
     scope: Scope,
     inputs_q: Array,
     inputs_kv: Array,
     bias: Optional[Array] = None,
     qkv_features: Optional[int] = None,
     out_features: Optional[int] = None,
     attn_fn: Callable = softmax_attn,
-    dtype=jnp.float32):
+    dtype=jnp.float32,
+):
   if qkv_features is None:
     qkv_features = inputs_q.shape[-1]
   if out_features is None:
     out_features = inputs_q.shape[-1]
   dense = partial(nn.dense, features=qkv_features, bias=False, dtype=dtype)
 
   query = scope.child(dense, 'query')(inputs_q)
   key = scope.child(dense, 'key')(inputs_kv)
   value = scope.child(dense, 'value')(inputs_kv)
 
   y = _dot_product_attention(
-      scope, query, key, value,
-      bias=bias,
-      attn_fn=attn_fn, dtype=dtype)
+      scope, query, key, value, bias=bias, attn_fn=attn_fn, dtype=dtype
+  )
 
   return scope.child(nn.dense, 'out')(y, features=out_features, dtype=dtype)
 
 
-
 def multi_head_dot_product_attention(
     scope: Scope,
     inputs_q: Array,
     inputs_kv: Array,
     bias: Optional[Array] = None,
     qkv_features: Optional[int] = None,
     out_features: Optional[int] = None,
     attn_fn: Callable = softmax_attn,
     batch_axes: Sequence[int] = (0,),
     num_heads: int = 1,
     dtype=jnp.float32,
-    broadcast_dropout=False):
-
+    broadcast_dropout=False,
+):
   if qkv_features is None:
     qkv_features = inputs_q.shape[-1]
   if out_features is None:
     out_features = inputs_q.shape[-1]
 
   attn_fn = partial(
       dot_product_attention,
       attn_fn=attn_fn,
       qkv_features=qkv_features // num_heads,
       out_features=out_features,
-      dtype=dtype)
+      dtype=dtype,
+  )
   attn_fn = lift.vmap(
       attn_fn,
-      in_axes=(None, None, None), out_axes=-2,
+      in_axes=(None, None, None),
+      out_axes=-2,
       axis_size=num_heads,
       variable_axes={'params': 0},
-      split_rngs={'params': True, 'dropout': not broadcast_dropout})
+      split_rngs={'params': True, 'dropout': not broadcast_dropout},
+  )
   for axis in reversed(sorted(batch_axes)):
     attn_fn = lift.vmap(
         attn_fn,
-        in_axes=(axis, axis, axis), out_axes=axis,
+        in_axes=(axis, axis, axis),
+        out_axes=axis,
         variable_axes={'params': None},
-        split_rngs={'params': False, 'dropout': not broadcast_dropout})
+        split_rngs={'params': False, 'dropout': not broadcast_dropout},
+    )
 
   y = attn_fn(scope, inputs_q, inputs_kv, bias)
   return y.mean(axis=-2)
 
 
 class AttentionTest(absltest.TestCase):
 
   def test_attention(self):
     inputs = jnp.ones((2, 7, 16))
     model = partial(
         multi_head_dot_product_attention,
-        num_heads=2, batch_axes=(0,),
-        attn_fn=with_dropout(softmax_attn, 0.1, deterministic=False))
+        num_heads=2,
+        batch_axes=(0,),
+        attn_fn=with_dropout(softmax_attn, 0.1, deterministic=False),
+    )
 
     rngs = {'params': random.PRNGKey(0), 'dropout': random.PRNGKey(1)}
     y, variables = jax.jit(init(model))(rngs, inputs, inputs)
     variable_shapes = jax.tree_util.tree_map(jnp.shape, variables['params'])
     self.assertEqual(y.shape, (2, 7, 16))
-    self.assertEqual(unfreeze(variable_shapes), {
-        'key': {'kernel': (2, 16, 8)},
-        'value': {'kernel': (2, 16, 8)},
-        'query': {'kernel': (2, 16, 8)},
-        'out': {'bias': (2, 16), 'kernel': (2, 8, 16)},
-    })
+    self.assertEqual(
+        unfreeze(variable_shapes),
+        {
+            'key': {'kernel': (2, 16, 8)},
+            'value': {'kernel': (2, 16, 8)},
+            'query': {'kernel': (2, 16, 8)},
+            'out': {'bias': (2, 16), 'kernel': (2, 8, 16)},
+        },
+    )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_auto_encoder_test.py` & `flax-0.7.1/tests/core/design/core_auto_encoder_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -32,15 +32,14 @@
   x = scope.child(nn.dense, 'hidden')(x, hidden)
   x = nn.relu(x)
   return scope.child(nn.dense, 'out')(x, out)
 
 
 @dataclass
 class AutoEncoder:
-
   latents: int
   features: int
   hidden: int
 
   def __call__(self, scope, x):
     z = self.encode(scope, x)
     return self.decode(scope, z)
@@ -56,14 +55,15 @@
   if name is None:
     name = fn.__name__ if hasattr(fn, '__name__') else None
 
   def wrapper(self, *args, **kwargs):
     scope = self.scope.rewound()
     mod_fn = lambda scope: fn(self, scope, *args, **kwargs)
     return scope.child(mod_fn, name)()
+
   return wrapper
 
 
 @dataclass
 class AutoEncoder2:
   scope: Scope
   latents: int
@@ -102,58 +102,77 @@
 class AutoEncoderTest(absltest.TestCase):
 
   def test_auto_encoder_hp_struct(self):
     ae = AutoEncoder(latents=2, features=4, hidden=3)
     x = jnp.ones((1, 4))
     x_r, variables = init(ae)(random.PRNGKey(0), x)
     self.assertEqual(x.shape, x_r.shape)
-    variable_shapes = unfreeze(jax.tree_util.tree_map(jnp.shape, variables['params']))
-    self.assertEqual(variable_shapes, {
-        'encoder': {
-            'hidden': {'kernel': (4, 3), 'bias': (3,)},
-            'out': {'kernel': (3, 2), 'bias': (2,)},
-        },
-        'decoder': {
-            'hidden': {'kernel': (2, 3), 'bias': (3,)},
-            'out': {'kernel': (3, 4), 'bias': (4,)},
+    variable_shapes = unfreeze(
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
+    self.assertEqual(
+        variable_shapes,
+        {
+            'encoder': {
+                'hidden': {'kernel': (4, 3), 'bias': (3,)},
+                'out': {'kernel': (3, 2), 'bias': (2,)},
+            },
+            'decoder': {
+                'hidden': {'kernel': (2, 3), 'bias': (3,)},
+                'out': {'kernel': (3, 4), 'bias': (4,)},
+            },
         },
-    })
+    )
 
   def test_auto_encoder_with_scope(self):
-    ae = lambda scope, x: AutoEncoder2(scope, latents=2, features=4, hidden=3)(x)
+    ae = lambda scope, x: AutoEncoder2(scope, latents=2, features=4, hidden=3)(
+        x
+    )
     x = jnp.ones((1, 4))
 
     x_r, variables = init(ae)(random.PRNGKey(0), x)
     self.assertEqual(x.shape, x_r.shape)
-    variable_shapes = unfreeze(jax.tree_util.tree_map(jnp.shape, variables['params']))
-    self.assertEqual(variable_shapes, {
-        'encode': {
-            'hidden': {'kernel': (4, 3), 'bias': (3,)},
-            'out': {'kernel': (3, 2), 'bias': (2,)},
+    variable_shapes = unfreeze(
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
+    self.assertEqual(
+        variable_shapes,
+        {
+            'encode': {
+                'hidden': {'kernel': (4, 3), 'bias': (3,)},
+                'out': {'kernel': (3, 2), 'bias': (2,)},
+            },
+            'decode': {
+                'hidden': {'kernel': (2, 3), 'bias': (3,)},
+                'out': {'kernel': (3, 4), 'bias': (4,)},
+            },
         },
-        'decode': {
-            'hidden': {'kernel': (2, 3), 'bias': (3,)},
-            'out': {'kernel': (3, 4), 'bias': (4,)},
-        },
-    })
+    )
 
   def test_auto_encoder_bind_method(self):
-    ae = lambda scope, x: AutoEncoder3.create(scope, latents=2, features=4, hidden=3)(x)
+    ae = lambda scope, x: AutoEncoder3.create(
+        scope, latents=2, features=4, hidden=3
+    )(x)
     x = jnp.ones((1, 4))
 
     x_r, variables = init(ae)(random.PRNGKey(0), x)
     self.assertEqual(x.shape, x_r.shape)
-    variable_shapes = unfreeze(jax.tree_util.tree_map(jnp.shape, variables['params']))
-    self.assertEqual(variable_shapes, {
-        'encode': {
-            'hidden': {'kernel': (4, 3), 'bias': (3,)},
-            'out': {'kernel': (3, 2), 'bias': (2,)},
-        },
-        'decode': {
-            'hidden': {'kernel': (2, 3), 'bias': (3,)},
-            'out': {'kernel': (3, 4), 'bias': (4,)},
+    variable_shapes = unfreeze(
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
+    self.assertEqual(
+        variable_shapes,
+        {
+            'encode': {
+                'hidden': {'kernel': (4, 3), 'bias': (3,)},
+                'out': {'kernel': (3, 2), 'bias': (2,)},
+            },
+            'decode': {
+                'hidden': {'kernel': (2, 3), 'bias': (3,)},
+                'out': {'kernel': (3, 4), 'bias': (4,)},
+            },
         },
-    })
+    )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_big_resnets_test.py` & `flax-0.7.1/tests/core/design/core_big_resnets_test.py`

 * *Files 20% similar despite different names*

```diff
@@ -32,51 +32,68 @@
   x = scope.child(conv, 'conv_1')(x, features, (3, 3))
   x = scope.child(norm, 'bn_1')(x)
   x = act(x)
   x = scope.child(conv, 'conv_2')(x, features, (3, 3))
   x = scope.child(norm, 'bn_2')(x)
   return act(residual + x)
 
-def big_resnet(scope: Scope, x, blocks=(10, 5), dtype=jnp.float32,
-               norm=default_norm, act=nn.relu):
+
+def big_resnet(
+    scope: Scope,
+    x,
+    blocks=(10, 5),
+    dtype=jnp.float32,
+    norm=default_norm,
+    act=nn.relu,
+):
   conv = partial(nn.conv, bias=False, dtype=dtype)
   norm = partial(norm, dtype=dtype)
 
   # a two stage resnet where inner blocks are rematerialized to make sure
   # memory consumtion grows as O(sqrt(N)) and compute is O(N) where N is the number of blocks..
   # we use a double scan such that the compiled binary is of size O(1).
   print('total residual blocks:', np.prod(blocks))
 
   def body_fn(scope, x):
     return residual_block(scope, x, conv, norm, act, features=x.shape[-1])
 
   return lift.remat_scan(
-      body_fn, lengths=blocks,
+      body_fn,
+      lengths=blocks,
       variable_axes={'params': 0, 'batch_stats': 0},
       split_rngs={'params': True},
-      policy=None)(scope, x)
+      policy=None,
+  )(scope, x)
 
 
 class BigResnetTest(absltest.TestCase):
 
   def test_big_resnet(self):
     x = random.normal(random.PRNGKey(0), (1, 8, 8, 8))
     y, variables = init(big_resnet)(random.PRNGKey(1), x)
     self.assertEqual(y.shape, (1, 8, 8, 8))
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     batch_stats_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['batch_stats']))
-    self.assertEqual(param_shapes, {
-        'conv_1': {'kernel': (10, 5, 3, 3, 8, 8)},
-        'conv_2': {'kernel': (10, 5, 3, 3, 8, 8)},
-        'bn_1': {'scale': (10, 5, 8), 'bias': (10, 5, 8)},
-        'bn_2': {'scale': (10, 5, 8), 'bias': (10, 5, 8)}
-    })
-    self.assertEqual(batch_stats_shapes, {
-        'bn_1': {'var': (10, 5, 8), 'mean': (10, 5, 8)},
-        'bn_2': {'var': (10, 5, 8), 'mean': (10, 5, 8)}
-    })
+        jax.tree_util.tree_map(jnp.shape, variables['batch_stats'])
+    )
+    self.assertEqual(
+        param_shapes,
+        {
+            'conv_1': {'kernel': (10, 5, 3, 3, 8, 8)},
+            'conv_2': {'kernel': (10, 5, 3, 3, 8, 8)},
+            'bn_1': {'scale': (10, 5, 8), 'bias': (10, 5, 8)},
+            'bn_2': {'scale': (10, 5, 8), 'bias': (10, 5, 8)},
+        },
+    )
+    self.assertEqual(
+        batch_stats_shapes,
+        {
+            'bn_1': {'var': (10, 5, 8), 'mean': (10, 5, 8)},
+            'bn_2': {'var': (10, 5, 8), 'mean': (10, 5, 8)},
+        },
+    )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_custom_vjp_test.py` & `flax-0.7.1/tests/core/design/core_custom_vjp_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,47 +8,49 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 from typing import Sequence, Callable
 from functools import partial
 
 from absl.testing import absltest
 
 import numpy as np
 
 from flax.core import Scope, Array, init, apply, unfreeze, lift, nn
 
 import jax
 from jax import random, numpy as jnp
 
 
-def mlp_custom_grad(scope: Scope, x: Array,
-                    sizes: Sequence[int] = (8, 1),
-                    act_fn: Callable[[Array], Array] = nn.relu):
-
+def mlp_custom_grad(
+    scope: Scope,
+    x: Array,
+    sizes: Sequence[int] = (8, 1),
+    act_fn: Callable[[Array], Array] = nn.relu,
+):
   f = nn.dense
 
   def fwd(scope, x, features):
     y, vjp_fn = lift.vjp(partial(f, features=features), scope, x)
     return y, vjp_fn
 
   def bwd(features, res, y_t):
     del features
     vjp_fn = res
     params_t, *input_t = vjp_fn(y_t)
     params_t = jax.tree_util.tree_map(jnp.sign, params_t)
     return (params_t, *input_t)
 
   dense_custom_grad = lift.custom_vjp(
-      f, forward_fn=fwd, backward_fn=bwd, nondiff_argnums=(2,))
+      f, forward_fn=fwd, backward_fn=bwd, nondiff_argnums=(2,)
+  )
 
   # hidden layers
   for size in sizes[:-1]:
     x = scope.child(dense_custom_grad, prefix='hidden_')(x, size)
     x = act_fn(x)
 
   # output layer
@@ -57,19 +59,19 @@
 
 class CustomVJPTest(absltest.TestCase):
 
   def test_custom_vjp(self):
     x = random.normal(random.PRNGKey(0), (1, 4))
     y, variables = init(mlp_custom_grad)(random.PRNGKey(1), x)
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     loss_fn = lambda p, x: jnp.mean(apply(mlp_custom_grad)(p, x) ** 2)
     grad = jax.grad(loss_fn)(variables, x)
-    grad_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, grad['params']))
+    grad_shapes = unfreeze(jax.tree_util.tree_map(jnp.shape, grad['params']))
     self.assertEqual(y.shape, (1, 1))
     expected_param_shapes = {
         'hidden_0': {'kernel': (4, 8), 'bias': (8,)},
         'out': {'kernel': (8, 1), 'bias': (1,)},
     }
     self.assertEqual(param_shapes, expected_param_shapes)
     self.assertEqual(grad_shapes, expected_param_shapes)
```

### Comparing `flax-0.7.0/tests/core/design/core_dense_test.py` & `flax-0.7.1/tests/core/design/core_dense_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -29,118 +29,150 @@
 class Dense:
   features: int
   bias: bool = True
   kernel_init: Any = nn.linear.default_kernel_init
   bias_init: Any = nn.initializers.zeros_init()
 
   def __call__(self, scope, x):
-    kernel = scope.param('kernel', self.kernel_init, (x.shape[-1], self.features))
+    kernel = scope.param(
+        'kernel', self.kernel_init, (x.shape[-1], self.features)
+    )
     y = x @ kernel
     if self.bias:
       bias = scope.param('bias', self.bias_init, (self.features,))
       y += bias.reshape((1,) * (y.ndim - 1) + (-1,))
     return y
 
 
 @struct.dataclass
 class ExplicitDense:
   kernel: Array
   bias: Optional[Array]
 
   # a fully explicit "scope free" version
   @staticmethod
-  def create(rng, in_size, out_size, bias=True,
-             kernel_init=nn.linear.default_kernel_init,
-             bias_init=nn.initializers.zeros_init()):
+  def create(
+      rng,
+      in_size,
+      out_size,
+      bias=True,
+      kernel_init=nn.linear.default_kernel_init,
+      bias_init=nn.initializers.zeros_init(),
+  ):
     k1, k2 = random.split(rng, 2)
     kernel = kernel_init(k1, (in_size, out_size))
     if bias:
       bias = bias_init(k2, (out_size,))
     else:
       bias = None
     return ExplicitDense(kernel, bias)
 
   # a semi-explicit version where a scope is used to create explicit params
   @staticmethod
-  def create_in_scope(scope, in_size, out_size, bias=True,
-                      kernel_init=nn.linear.default_kernel_init,
-                      bias_init=nn.initializers.zeros_init()):
+  def create_in_scope(
+      scope,
+      in_size,
+      out_size,
+      bias=True,
+      kernel_init=nn.linear.default_kernel_init,
+      bias_init=nn.initializers.zeros_init(),
+  ):
     kernel = scope.param('kernel', kernel_init, (in_size, out_size))
     if bias:
       bias = scope.param('bias', bias_init, (out_size,))
     else:
       bias = None
     return ExplicitDense(kernel, bias)
 
   def __call__(self, x):
     y = x @ self.kernel
     if self.bias is not None:
       y += self.bias.reshape((1,) * (y.ndim - 1) + (-1,))
     return y
 
+
 def explicit_mlp(scope, x, sizes=(3, 1)):
   for i, size in enumerate(sizes):
     dense = scope.param(f'dense_{i}', ExplicitDense.create, x.shape[-1], size)
     x = dense(x)
     if i + 1 < len(sizes):
       x = nn.relu(x)
   return x
 
+
 def semi_explicit_mlp(scope, x, sizes=(3, 1)):
   for i, size in enumerate(sizes):
-    dense = scope.child(ExplicitDense.create_in_scope, prefix='dense_')(x.shape[-1], size)
+    dense = scope.child(ExplicitDense.create_in_scope, prefix='dense_')(
+        x.shape[-1], size
+    )
     x = dense(x)
     if i + 1 < len(sizes):
       x = nn.relu(x)
   return x
 
 
 class DenseTest(absltest.TestCase):
 
   def test_dense(self):
     model = Dense(features=4)
     x = jnp.ones((1, 3))
     y, variables = init(model)(random.PRNGKey(0), x)
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     self.assertEqual(y.shape, (1, 4))
-    self.assertEqual(param_shapes, {
-        'kernel': (3, 4),
-        'bias': (4,),
-    })
+    self.assertEqual(
+        param_shapes,
+        {
+            'kernel': (3, 4),
+            'bias': (4,),
+        },
+    )
 
   def test_explicit_dense(self):
     x = jnp.ones((1, 3))
     y, variables = init(explicit_mlp)(random.PRNGKey(0), x)
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     self.assertEqual(y.shape, (1, 4))
-    self.assertEqual(param_shapes, {
-        'kernel': (3, 4),
-        'bias': (4,),
-    })
+    self.assertEqual(
+        param_shapes,
+        {
+            'kernel': (3, 4),
+            'bias': (4,),
+        },
+    )
 
   def test_explicit_dense(self):
     x = jnp.ones((1, 4))
     y, variables = init(explicit_mlp)(random.PRNGKey(0), x)
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     self.assertEqual(y.shape, (1, 1))
-    self.assertEqual(param_shapes, {
-        'dense_0': ExplicitDense((4, 3), (3,)),
-        'dense_1': ExplicitDense((3, 1), (1,))
-    })
+    self.assertEqual(
+        param_shapes,
+        {
+            'dense_0': ExplicitDense((4, 3), (3,)),
+            'dense_1': ExplicitDense((3, 1), (1,)),
+        },
+    )
 
   def test_semi_explicit_dense(self):
     x = jnp.ones((1, 4))
     y, variables = init(semi_explicit_mlp)(random.PRNGKey(0), x)
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     self.assertEqual(y.shape, (1, 1))
-    self.assertEqual(param_shapes, {
-        'dense_0': {'kernel': (4, 3), 'bias': (3,)},
-        'dense_1': {'kernel': (3, 1), 'bias': (1,)}
-    })
+    self.assertEqual(
+        param_shapes,
+        {
+            'dense_0': {'kernel': (4, 3), 'bias': (3,)},
+            'dense_1': {'kernel': (3, 1), 'bias': (1,)},
+        },
+    )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_flow_test.py` & `flax-0.7.1/tests/core/design/core_flow_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -36,21 +36,19 @@
   def params(self, scope: Scope, features: int):
     kernel = scope.param('kernel', self.kernel_init, (features, features))
     bias = scope.param('bias', self.bias_init, (features,))
     return kernel, bias
 
   def forward(self, scope: Scope, x: Array):
     kernel, bias = self.params(scope, x.shape[-1])
-    return jnp.dot(
-      x, expm(kernel)) + bias.reshape((1,) * (x.ndim - 1) + (-1,))
+    return jnp.dot(x, expm(kernel)) + bias.reshape((1,) * (x.ndim - 1) + (-1,))
 
   def backward(self, scope: Scope, y: Array):
     kernel, bias = self.params(scope, y.shape[-1])
-    return jnp.dot(
-      y - bias.reshape((1,) * (y.ndim - 1) + (-1,)), expm(-kernel))
+    return jnp.dot(y - bias.reshape((1,) * (y.ndim - 1) + (-1,)), expm(-kernel))
 
 
 @dataclass
 class StackFlow:
   flows: Sequence[Flow]
 
   def forward(self, scope: Scope, x: Array):
@@ -67,20 +65,24 @@
 class FlowTest(absltest.TestCase):
 
   def test_flow(self):
     x = jnp.ones((1, 3))
     flow = StackFlow((DenseFlow(),) * 3)
     y, variables = init(flow.forward)(random.PRNGKey(0), x)
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     self.assertEqual(y.shape, (1, 3))
-    self.assertEqual(param_shapes, {
-        '0': {'kernel': (3, 3), 'bias': (3,)},
-        '1': {'kernel': (3, 3), 'bias': (3,)},
-        '2': {'kernel': (3, 3), 'bias': (3,)},
-    })
+    self.assertEqual(
+        param_shapes,
+        {
+            '0': {'kernel': (3, 3), 'bias': (3,)},
+            '1': {'kernel': (3, 3), 'bias': (3,)},
+            '2': {'kernel': (3, 3), 'bias': (3,)},
+        },
+    )
     x_restored = apply(flow.backward)(variables, y)
     self.assertTrue(jnp.allclose(x, x_restored))
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_resnet_test.py` & `flax-0.7.1/tests/core/design/core_resnet_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -21,117 +21,126 @@
 import jax
 from jax import random, numpy as jnp
 
 
 default_norm = partial(nn.batch_norm)
 
 
-def residual_block(scope: Scope, x: Array, conv, norm, act, features: int, strides=(1, 1)):
+def residual_block(
+    scope: Scope, x: Array, conv, norm, act, features: int, strides=(1, 1)
+):
   residual = x
   x = scope.child(conv, 'conv_1')(x, features, (1, 1))
   x = scope.child(norm, 'bn_1')(x)
   x = act(x)
   x = scope.child(conv, 'conv_2')(x, 4 * features, (3, 3), strides=strides)
   x = scope.child(norm, 'bn_2')(x)
   x = act(x)
   x = scope.child(conv, 'conv_3')(x, 4 * features, (1, 1))
   x = scope.child(norm, 'bn_3')(x)
 
   if x.shape != residual.shape:
-    residual = scope.child(conv, 'proj_conv')(residual, 4 * features, (1, 1), strides=strides)
+    residual = scope.child(conv, 'proj_conv')(
+        residual, 4 * features, (1, 1), strides=strides
+    )
     residual = scope.child(norm, 'proj_bn')(residual)
 
   return act(residual + x)
 
 
-def resnet(scope: Scope, x,
-           block_sizes=(3, 4, 6, 3),
-           features=16, num_classes=1000,
-           dtype=jnp.float32,
-           norm=default_norm,
-           act=nn.relu,
-           ):
+def resnet(
+    scope: Scope,
+    x,
+    block_sizes=(3, 4, 6, 3),
+    features=16,
+    num_classes=1000,
+    dtype=jnp.float32,
+    norm=default_norm,
+    act=nn.relu,
+):
   conv = partial(nn.conv, bias=False, dtype=dtype)
   norm = partial(norm, dtype=dtype)
 
   x = scope.child(conv, 'init_conv')(x, 16, (7, 7), padding=((3, 3), (3, 3)))
   x = scope.child(norm, 'init_bn')(x)
   x = act(x)
   x = nn.max_pool(x, (2, 2), (2, 2), 'SAME')
 
   for i, size in enumerate(block_sizes):
     for j in range(size):
       strides = (1, 1)
       if i > 0 and j == 0:
         strides = (2, 2)
-      block_features = features * 2 ** i
+      block_features = features * 2**i
       block_scope = scope.push(f'block_{i}_{j}')
-      x = residual_block(block_scope, x, conv, norm, act, block_features, strides)
+      x = residual_block(
+          block_scope, x, conv, norm, act, block_features, strides
+      )
       # we can access parameters of the sub module by operating on the scope
       # Example:
       # block_scope.get_kind('params')['conv_1']['kernel']
   x = jnp.mean(x, (1, 2))
   x = scope.child(nn.dense, 'out')(x, num_classes)
   return x
 
 
 class ResNetTest(absltest.TestCase):
 
   def test_resnet(self):
     block_sizes = (2, 2)
     x = random.normal(random.PRNGKey(0), (1, 64, 64, 3))
-    y, variables = init(resnet)(random.PRNGKey(1), x, block_sizes=block_sizes, features=16)
+    y, variables = init(resnet)(
+        random.PRNGKey(1), x, block_sizes=block_sizes, features=16
+    )
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     self.assertEqual(y.shape, (1, 1000))
 
-    self.assertEqual(param_shapes, {
-        'init_conv': {'kernel': (7, 7, 3, 16)},
-        'init_bn': {'bias': (16,), 'scale': (16,)},
-        'out': {'kernel': (128, 1000), 'bias': (1000,)},
-        'block_0_0': {
-            'conv_1': {'kernel': (1, 1, 16, 16)},
-            'conv_2': {'kernel': (3, 3, 16, 64)},
-            'conv_3': {'kernel': (1, 1, 64, 64)},
-
-            'bn_1': {'bias': (16,), 'scale': (16,)},
-            'bn_2': {'bias': (64,), 'scale': (64,)},
-            'bn_3': {'bias': (64,), 'scale': (64,)},
-
-            'proj_conv': {'kernel': (1, 1, 16, 64)},
-            'proj_bn': {'bias': (64,), 'scale': (64,)},
-        },
-        'block_0_1': {
-            'conv_1': {'kernel': (1, 1, 64, 16)},
-            'conv_2': {'kernel': (3, 3, 16, 64)},
-            'conv_3': {'kernel': (1, 1, 64, 64)},
-
-            'bn_1': {'bias': (16,), 'scale': (16,)},
-            'bn_2': {'bias': (64,), 'scale': (64,)},
-            'bn_3': {'bias': (64,), 'scale': (64,)},
-        },
-        'block_1_0': {
-            'conv_1': {'kernel': (1, 1, 64, 32)},
-            'conv_2': {'kernel': (3, 3, 32, 128)},
-            'conv_3': {'kernel': (1, 1, 128, 128)},
-
-            'bn_1': {'bias': (32,), 'scale': (32,)},
-            'bn_2': {'bias': (128,), 'scale': (128,)},
-            'bn_3': {'bias': (128,), 'scale': (128,)},
-
-            'proj_conv': {'kernel': (1, 1, 64, 128)},
-            'proj_bn': {'bias': (128,), 'scale': (128,)},
-        },
-        'block_1_1': {
-            'conv_1': {'kernel': (1, 1, 128, 32)},
-            'conv_2': {'kernel': (3, 3, 32, 128)},
-            'conv_3': {'kernel': (1, 1, 128, 128)},
-
-            'bn_1': {'bias': (32,), 'scale': (32,)},
-            'bn_2': {'bias': (128,), 'scale': (128,)},
-            'bn_3': {'bias': (128,), 'scale': (128,)},
+    self.assertEqual(
+        param_shapes,
+        {
+            'init_conv': {'kernel': (7, 7, 3, 16)},
+            'init_bn': {'bias': (16,), 'scale': (16,)},
+            'out': {'kernel': (128, 1000), 'bias': (1000,)},
+            'block_0_0': {
+                'conv_1': {'kernel': (1, 1, 16, 16)},
+                'conv_2': {'kernel': (3, 3, 16, 64)},
+                'conv_3': {'kernel': (1, 1, 64, 64)},
+                'bn_1': {'bias': (16,), 'scale': (16,)},
+                'bn_2': {'bias': (64,), 'scale': (64,)},
+                'bn_3': {'bias': (64,), 'scale': (64,)},
+                'proj_conv': {'kernel': (1, 1, 16, 64)},
+                'proj_bn': {'bias': (64,), 'scale': (64,)},
+            },
+            'block_0_1': {
+                'conv_1': {'kernel': (1, 1, 64, 16)},
+                'conv_2': {'kernel': (3, 3, 16, 64)},
+                'conv_3': {'kernel': (1, 1, 64, 64)},
+                'bn_1': {'bias': (16,), 'scale': (16,)},
+                'bn_2': {'bias': (64,), 'scale': (64,)},
+                'bn_3': {'bias': (64,), 'scale': (64,)},
+            },
+            'block_1_0': {
+                'conv_1': {'kernel': (1, 1, 64, 32)},
+                'conv_2': {'kernel': (3, 3, 32, 128)},
+                'conv_3': {'kernel': (1, 1, 128, 128)},
+                'bn_1': {'bias': (32,), 'scale': (32,)},
+                'bn_2': {'bias': (128,), 'scale': (128,)},
+                'bn_3': {'bias': (128,), 'scale': (128,)},
+                'proj_conv': {'kernel': (1, 1, 64, 128)},
+                'proj_bn': {'bias': (128,), 'scale': (128,)},
+            },
+            'block_1_1': {
+                'conv_1': {'kernel': (1, 1, 128, 32)},
+                'conv_2': {'kernel': (3, 3, 32, 128)},
+                'conv_3': {'kernel': (1, 1, 128, 128)},
+                'bn_1': {'bias': (32,), 'scale': (32,)},
+                'bn_2': {'bias': (128,), 'scale': (128,)},
+                'bn_3': {'bias': (128,), 'scale': (128,)},
+            },
         },
-    })
+    )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_scan_test.py` & `flax-0.7.1/tests/core/design/core_scan_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,78 +8,86 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 from flax.core import Scope, Array, init, unfreeze, lift, nn
 
 from absl.testing import absltest
 
 import jax
 from jax import random, numpy as jnp
 
 
-def mlp_scan(scope: Scope, xs: Array,
-             share_params: bool = False):
-
+def mlp_scan(scope: Scope, xs: Array, share_params: bool = False):
   scope.variable('counter', 'i', jnp.zeros, ())
+
   def body_fn(scope, c, x):
     counter = scope.variable('counter', 'i', jnp.zeros, ())
     counter.value += 1
     x = scope.child(nn.dense)(x, 1)
     return c, x
 
   if share_params:
     _, ys = lift.scan(
         body_fn,
         variable_carry='counter',
         variable_broadcast='params',
-        split_rngs={'params': False})(scope, (), xs)
+        split_rngs={'params': False},
+    )(scope, (), xs)
   else:
     _, ys = lift.scan(
         body_fn,
         variable_carry='counter',
         variable_axes={'params': 0},
-        split_rngs={'params': True})(scope, (), xs)
+        split_rngs={'params': True},
+    )(scope, (), xs)
 
   # output layer
   return ys
 
 
 class ScanTest(absltest.TestCase):
 
   def test_scan_unshared_params(self):
     x = random.normal(random.PRNGKey(0), (1, 4))
     x = jnp.concatenate([x, x], 0)
     y, variables = init(mlp_scan)(random.PRNGKey(1), x, share_params=False)
 
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     self.assertEqual(variables['counter']['i'], 2)
-    self.assertEqual(param_shapes, {
-      'dense_0': {'kernel': (2, 4, 1), 'bias': (2, 1)},
-    })
+    self.assertEqual(
+        param_shapes,
+        {
+            'dense_0': {'kernel': (2, 4, 1), 'bias': (2, 1)},
+        },
+    )
 
     self.assertNotEqual(y[0], y[1])
     k1, k2 = variables['params']['dense_0']['kernel']
     self.assertFalse(jnp.allclose(k1, k2))
 
   def test_scan_shared_params(self):
     x = random.normal(random.PRNGKey(0), (1, 4))
     x = jnp.concatenate([x, x], 0)
     y, variables = init(mlp_scan)(random.PRNGKey(1), x, share_params=True)
 
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
     self.assertEqual(variables['counter']['i'], 2)
-    self.assertEqual(param_shapes, {
-      'dense_0': {'kernel': (4, 1), 'bias': (1,)},
-    })
+    self.assertEqual(
+        param_shapes,
+        {
+            'dense_0': {'kernel': (4, 1), 'bias': (1,)},
+        },
+    )
 
     self.assertEqual(y[0], y[1])
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_tied_autoencoder_test.py` & `flax-0.7.1/tests/core/design/core_tied_autoencoder_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -24,58 +24,64 @@
 
 
 def transpose(fn):
   def trans(variables):
     return jax.tree_util.tree_map(lambda x: x.T, variables)
 
   return lift.map_variables(
-      fn, "params", map_in_fn=trans, map_out_fn=trans,
-      mutable=True)
+      fn, 'params', map_in_fn=trans, map_out_fn=trans, mutable=True
+  )
 
 
 @dataclass
 class TiedAutoEncoder:
-
   latents: int
   features: int
 
   def __call__(self, scope, x):
     z = self.encode(scope, x)
     return self.decode(scope, z)
 
   def encode(self, scope, x):
     return nn.dense(scope, x, self.latents, bias=False)
 
   def decode(self, scope, z):
-    return transpose(nn.dense)(
-        scope, z, self.features, bias=False)
+    return transpose(nn.dense)(scope, z, self.features, bias=False)
 
 
 class TiedAutoEncoderTest(absltest.TestCase):
 
   def test_tied_auto_encoder(self):
     ae = TiedAutoEncoder(latents=2, features=4)
     x = jnp.ones((1, ae.features))
     x_r, variables = init(ae)(random.PRNGKey(0), x)
 
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
-    self.assertEqual(param_shapes, {
-        'kernel': (4, 2),
-    })
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
+    self.assertEqual(
+        param_shapes,
+        {
+            'kernel': (4, 2),
+        },
+    )
     self.assertEqual(x.shape, x_r.shape)
 
   def test_init_from_decoder(self):
     ae = TiedAutoEncoder(latents=2, features=4)
     z = jnp.ones((1, ae.latents))
     x_r, variables = init(ae.decode)(random.PRNGKey(0), z)
 
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
-    self.assertEqual(param_shapes, {
-        'kernel': (4, 2),
-    })
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
+    self.assertEqual(
+        param_shapes,
+        {
+            'kernel': (4, 2),
+        },
+    )
     self.assertEqual(x_r.shape, (1, 4))
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_vmap_test.py` & `flax-0.7.1/tests/core/design/core_vmap_test.py`

 * *Files 17% similar despite different names*

```diff
@@ -18,28 +18,35 @@
 
 from absl.testing import absltest
 
 import jax
 from jax import random, numpy as jnp
 
 
-def mlp_vmap(scope: Scope, x: Array,
-             sizes: Sequence[int] = (8, 1),
-             act_fn: Callable[[Array], Array] = nn.relu,
-             share_params: bool = False):
+def mlp_vmap(
+    scope: Scope,
+    x: Array,
+    sizes: Sequence[int] = (8, 1),
+    act_fn: Callable[[Array], Array] = nn.relu,
+    share_params: bool = False,
+):
   if share_params:
-    dense_vmap = lift.vmap(nn.dense,
-                           in_axes=(0, None),
-                           variable_axes={'params': None},
-                           split_rngs={'params': False})
+    dense_vmap = lift.vmap(
+        nn.dense,
+        in_axes=(0, None),
+        variable_axes={'params': None},
+        split_rngs={'params': False},
+    )
   else:
-    dense_vmap = lift.vmap(nn.dense,
-                           in_axes=(0, None),
-                           variable_axes={'params': 0},
-                           split_rngs={'params': True})
+    dense_vmap = lift.vmap(
+        nn.dense,
+        in_axes=(0, None),
+        variable_axes={'params': 0},
+        split_rngs={'params': True},
+    )
 
   # hidden layers
   for size in sizes[:-1]:
     x = scope.child(dense_vmap, prefix='hidden_')(x, size)
     x = act_fn(x)
 
   # output layer
@@ -51,33 +58,41 @@
   def test_vmap_shared(self):
     x = random.normal(random.PRNGKey(0), (1, 4))
     x = jnp.concatenate([x, x], 0)
 
     y, variables = init(mlp_vmap)(random.PRNGKey(1), x, share_params=True)
 
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
-    self.assertEqual(param_shapes, {
-        'hidden_0' : {'kernel': (4, 8), 'bias': (8,)},
-        'out': {'kernel': (8, 1), 'bias': (1,)},
-    })
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
+    self.assertEqual(
+        param_shapes,
+        {
+            'hidden_0': {'kernel': (4, 8), 'bias': (8,)},
+            'out': {'kernel': (8, 1), 'bias': (1,)},
+        },
+    )
     self.assertEqual(y.shape, (2, 1))
     self.assertTrue(jnp.allclose(y[0], y[1]))
 
   def test_vmap_unshared(self):
     x = random.normal(random.PRNGKey(0), (1, 4))
     x = jnp.concatenate([x, x], 0)
 
     y, variables = init(mlp_vmap)(random.PRNGKey(1), x, share_params=False)
 
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
-    self.assertEqual(param_shapes, {
-        'hidden_0': {'kernel': (2, 4, 8), 'bias': (2, 8)},
-        'out': {'kernel': (2, 8, 1), 'bias': (2, 1)},
-    })
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
+    self.assertEqual(
+        param_shapes,
+        {
+            'hidden_0': {'kernel': (2, 4, 8), 'bias': (2, 8)},
+            'out': {'kernel': (2, 8, 1), 'bias': (2, 1)},
+        },
+    )
     self.assertEqual(y.shape, (2, 1))
     self.assertFalse(jnp.allclose(y[0], y[1]))
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/core/design/core_weight_std_test.py` & `flax-0.7.1/tests/core/design/core_weight_std_test.py`

 * *Files 15% similar despite different names*

```diff
@@ -35,39 +35,50 @@
     return variables
 
   # map_variables handles a few of nasty edge cases here...
   # the transformed kind will be immutable inside fn
   # this way we avoid lost mutations to param
   # map_variables also avoids accidental reuse of rngs
   # and it makes sure that other state is updated correctly (not twice during init!)
-  return lift.map_variables(fn, "params", std, init=True)
+  return lift.map_variables(fn, 'params', std, init=True)
 
-def mlp(scope: Scope, x: Array,
-        sizes: Sequence[int] = (8, 1)):
-  std_dense = weight_std(partial(
-      nn.dense, kernel_init=nn.initializers.normal(stddev=1e5)))
+
+def mlp(scope: Scope, x: Array, sizes: Sequence[int] = (8, 1)):
+  std_dense = weight_std(
+      partial(nn.dense, kernel_init=nn.initializers.normal(stddev=1e5))
+  )
   for size in sizes[:-1]:
     x = scope.child(std_dense, prefix='hidden_')(x, size)
   return scope.child(nn.dense, 'out')(x, sizes[-1])
 
 
 class WeightStdTest(absltest.TestCase):
 
   def test_weight_std(self):
-    x = random.normal(random.PRNGKey(0), (1, 4,))
+    x = random.normal(
+        random.PRNGKey(0),
+        (
+            1,
+            4,
+        ),
+    )
     y, variables = init(mlp)(random.PRNGKey(1), x)
 
     param_shapes = unfreeze(
-        jax.tree_util.tree_map(jnp.shape, variables['params']))
-    self.assertEqual(param_shapes, {
-        'hidden_0': {'kernel': (4, 8), 'bias': (8,)},
-        'out': {'kernel': (8, 1), 'bias': (1,)},
-    })
+        jax.tree_util.tree_map(jnp.shape, variables['params'])
+    )
+    self.assertEqual(
+        param_shapes,
+        {
+            'hidden_0': {'kernel': (4, 8), 'bias': (8,)},
+            'out': {'kernel': (8, 1), 'bias': (1,)},
+        },
+    )
     self.assertEqual(y.shape, (1, 1))
-    self.assertTrue(y.ravel() < 1.)
+    self.assertTrue(y.ravel() < 1.0)
 
     y2 = apply(mlp)(variables, x)
     self.assertTrue(jnp.allclose(y, y2))
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/download_dataset_metadata.sh` & `flax-0.7.1/tests/download_dataset_metadata.sh`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/tests/early_stopping_test.py` & `flax-0.7.1/tests/early_stopping_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -25,80 +25,84 @@
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 
 class EarlyStoppingTests(absltest.TestCase):
 
   def test_update(self):
-    es = early_stopping.EarlyStopping(min_delta=0,
-                                      patience=0)
+    es = early_stopping.EarlyStopping(min_delta=0, patience=0)
 
     for i in range(2):
       improve_steps = 0
       for step in range(10):
-        metric = 1.
+        metric = 1.0
         did_improve, es = es.update(metric)
         if not did_improve:
           improve_steps += 1
         if es.should_stop:
           break
 
       self.assertEqual(improve_steps, 1)
       self.assertEqual(step, 1)
 
       es = es.reset()  # ensure object is reusable if reset.
 
   def test_patience(self):
-    es = early_stopping.EarlyStopping(min_delta=0,
-                                      patience=0)
-    patient_es = early_stopping.EarlyStopping(min_delta=0,
-                                              patience=6)
+    es = early_stopping.EarlyStopping(min_delta=0, patience=0)
+    patient_es = early_stopping.EarlyStopping(min_delta=0, patience=6)
     for step in range(10):
-      metric = 1.
+      metric = 1.0
       did_improve, es = es.update(metric)
       if es.should_stop:
         break
 
     self.assertEqual(step, 1)
 
     for patient_step in range(10):
-      metric = 1.
+      metric = 1.0
       did_improve, patient_es = patient_es.update(metric)
       if patient_es.should_stop:
         break
 
     self.assertEqual(patient_step, 7)
 
   def test_delta(self):
-    es = early_stopping.EarlyStopping(min_delta=0,
-                                      patience=0)
-    delta_es = early_stopping.EarlyStopping(min_delta=1e-3,
-                                            patience=0)
-    delta_patient_es = early_stopping.EarlyStopping(min_delta=1e-3,
-                                                    patience=1)
-    metric = 1.
+    es = early_stopping.EarlyStopping(min_delta=0, patience=0)
+    delta_es = early_stopping.EarlyStopping(min_delta=1e-3, patience=0)
+    delta_patient_es = early_stopping.EarlyStopping(min_delta=1e-3, patience=1)
+    metric = 1.0
     for step in range(100):
       metric -= 1e-4
       did_improve, es = es.update(metric)
       if es.should_stop:
         break
 
     self.assertEqual(step, 99)
 
-    metric = 1.
+    metric = 1.0
     for step in range(100):
       metric -= 1e-4
       did_improve, delta_es = delta_es.update(metric)
       if delta_es.should_stop:
         break
 
     self.assertEqual(step, 1)
 
-    metrics = [0.01, 0.005, 0.0033, 0.0025, 0.002,
-               0.0017, 0.0014, 0.0012, 0.0011, 0.001]
+    metrics = [
+        0.01,
+        0.005,
+        0.0033,
+        0.0025,
+        0.002,
+        0.0017,
+        0.0014,
+        0.0012,
+        0.0011,
+        0.001,
+    ]
     improvement_steps = 0
     for step in range(10):
       metric = metrics[step]
       did_improve, delta_patient_es = delta_patient_es.update(metric)
       if did_improve:
         improvement_steps += 1
       if delta_patient_es.should_stop:
```

### Comparing `flax-0.7.0/tests/import_test.ipynb` & `flax-0.7.1/tests/import_test.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/tests/io_test.py` & `flax-0.7.1/tests/io_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -27,24 +27,24 @@
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 
 class IOTest(parameterized.TestCase):
 
   @parameterized.parameters(
-    {'backend_mode': io.BackendMode.DEFAULT},
-    {'backend_mode': io.BackendMode.TF}
+      {'backend_mode': io.BackendMode.DEFAULT},
+      {'backend_mode': io.BackendMode.TF},
   )
   def test_override(self, backend_mode):
     with io.override_mode(backend_mode):
       self.assertEqual(io.io_mode, backend_mode)
 
   @parameterized.parameters(
-    {'write_mode': io.BackendMode.DEFAULT, 'read_mode': io.BackendMode.TF},
-    {'write_mode': io.BackendMode.TF, 'read_mode': io.BackendMode.DEFAULT}
+      {'write_mode': io.BackendMode.DEFAULT, 'read_mode': io.BackendMode.TF},
+      {'write_mode': io.BackendMode.TF, 'read_mode': io.BackendMode.DEFAULT},
   )
   def test_GFile(self, write_mode, read_mode):
     test_string = b'testing write and read'
     with tempfile.TemporaryDirectory() as temp_dir_path:
       test_path = os.path.join(temp_dir_path, 'test')
 
       with io.override_mode(write_mode):
@@ -68,16 +68,16 @@
 
       with io.override_mode(io.BackendMode.TF):
         tf_dir_set = set(io.listdir(temp_dir_path))
 
       self.assertEqual(default_dir_set, tf_dir_set)
 
   @parameterized.parameters(
-    {'create_temp_fn': tempfile.TemporaryDirectory},
-    {'create_temp_fn': tempfile.NamedTemporaryFile}
+      {'create_temp_fn': tempfile.TemporaryDirectory},
+      {'create_temp_fn': tempfile.NamedTemporaryFile},
   )
   def test_isdir(self, create_temp_fn):
     with create_temp_fn() as temp:
       path = temp.name if hasattr(temp, 'name') else temp
 
       with io.override_mode(io.BackendMode.DEFAULT):
         default_isdir = io.isdir(path)
@@ -103,16 +103,22 @@
       with io.override_mode(io.BackendMode.TF):
         io.copy(copy1_path, copy2_path)
 
       with io.GFile(copy2_path, 'rb') as file:
         self.assertEqual(file.read(), test_string)
 
   @parameterized.parameters(
-    {'backend_mode': io.BackendMode.DEFAULT, 'error_type': errors.AlreadyExistsError},
-    {'backend_mode': io.BackendMode.TF, 'error_type': tf.errors.AlreadyExistsError},
+      {
+          'backend_mode': io.BackendMode.DEFAULT,
+          'error_type': errors.AlreadyExistsError,
+      },
+      {
+          'backend_mode': io.BackendMode.TF,
+          'error_type': tf.errors.AlreadyExistsError,
+      },
   )
   def test_copy_raises_error(self, backend_mode, error_type):
     with tempfile.NamedTemporaryFile() as temp_file:
       with io.override_mode(backend_mode):
         with self.assertRaises(error_type):
           io.copy(temp_file.name, temp_file.name)
 
@@ -131,45 +137,52 @@
       with io.override_mode(io.BackendMode.TF):
         io.rename(rename1_path, rename2_path)
 
       with io.GFile(rename2_path, 'rb') as file:
         self.assertTrue(os.path.exists(rename2_path))
 
   @parameterized.parameters(
-    {'backend_mode': io.BackendMode.DEFAULT, 'error_type': errors.AlreadyExistsError},
-    {'backend_mode': io.BackendMode.TF, 'error_type': tf.errors.AlreadyExistsError},
+      {
+          'backend_mode': io.BackendMode.DEFAULT,
+          'error_type': errors.AlreadyExistsError,
+      },
+      {
+          'backend_mode': io.BackendMode.TF,
+          'error_type': tf.errors.AlreadyExistsError,
+      },
   )
   def test_rename_raises_error(self, backend_mode, error_type):
     with tempfile.NamedTemporaryFile() as temp_file:
       with io.override_mode(backend_mode):
         with self.assertRaises(error_type):
           io.rename(temp_file.name, temp_file.name)
 
   def test_exists(self):
     with tempfile.NamedTemporaryFile() as temp_file:
-
       with io.override_mode(io.BackendMode.DEFAULT):
         default_exists = io.exists(temp_file.name)
 
       with io.override_mode(io.BackendMode.TF):
         tf_exists = io.exists(temp_file.name)
 
       self.assertEqual(default_exists, tf_exists)
 
   @parameterized.parameters(
-    {'backend_mode': io.BackendMode.DEFAULT},
-    {'backend_mode': io.BackendMode.TF}
+      {'backend_mode': io.BackendMode.DEFAULT},
+      {'backend_mode': io.BackendMode.TF},
   )
   def test_makedirs(self, backend_mode):
     with tempfile.TemporaryDirectory() as temp_dir_path:
       test_dir_path = os.path.join(temp_dir_path, 'test_dir')
 
       with io.override_mode(backend_mode):
         io.makedirs(test_dir_path)
-      self.assertTrue(os.path.exists(test_dir_path) and (os.path.isdir(test_dir_path)))
+      self.assertTrue(
+          os.path.exists(test_dir_path) and (os.path.isdir(test_dir_path))
+      )
 
   def test_glob(self):
     with tempfile.TemporaryDirectory() as temp_dir_path:
       os.mkdir(os.path.join(temp_dir_path, 'a'))
       os.mkdir(os.path.join(temp_dir_path, 'as'))
       os.mkdir(os.path.join(temp_dir_path, 'af'))
       os.mkdir(os.path.join(temp_dir_path, 'test'))
@@ -180,32 +193,32 @@
 
     with io.override_mode(io.BackendMode.TF):
       tf_glob_set = set(io.glob('a*/'))
 
     self.assertEqual(default_glob_set, tf_glob_set)
 
   @parameterized.parameters(
-    {'backend_mode': io.BackendMode.DEFAULT},
-    {'backend_mode': io.BackendMode.TF}
+      {'backend_mode': io.BackendMode.DEFAULT},
+      {'backend_mode': io.BackendMode.TF},
   )
   def test_remove(self, backend_mode):
     with tempfile.TemporaryDirectory() as temp_dir_path:
       test_path = os.path.join(temp_dir_path, 'test')
 
       with io.GFile(test_path, 'wb') as file:
         file.write(b'placeholder text')
 
       with io.override_mode(backend_mode):
         io.remove(test_path)
 
       self.assertTrue(not os.path.exists(test_path))
 
   @parameterized.parameters(
-    {'backend_mode': io.BackendMode.DEFAULT},
-    {'backend_mode': io.BackendMode.TF}
+      {'backend_mode': io.BackendMode.DEFAULT},
+      {'backend_mode': io.BackendMode.TF},
   )
   def test_rmtree(self, backend_mode):
     with tempfile.TemporaryDirectory() as temp_dir_path:
       dir0_path = os.path.join(temp_dir_path, 'dir0')
 
       os.mkdir(dir0_path)
       os.mkdir(os.path.join(dir0_path, 'dir1'))
@@ -216,18 +229,17 @@
       os.mkdir(os.path.join(dir0_path, 'dir6'))
 
       with io.override_mode(backend_mode):
         io.rmtree(dir0_path)
 
       self.assertTrue(not os.path.exists(dir0_path))
 
-
   @parameterized.parameters(
-    {'backend_mode': io.BackendMode.DEFAULT},
-    {'backend_mode': io.BackendMode.TF}
+      {'backend_mode': io.BackendMode.DEFAULT},
+      {'backend_mode': io.BackendMode.TF},
   )
   def test_getsize(self, backend_mode):
     with tempfile.TemporaryDirectory() as temp_dir_path:
       test_path = os.path.join(temp_dir_path, 'test')
 
       content = b'placeholder text'
       with io.GFile(test_path, 'wb') as file:
```

### Comparing `flax-0.7.0/tests/jax_utils_test.py` & `flax-0.7.1/tests/jax_utils_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -41,45 +41,45 @@
   def test_basics(self, dtype, bs):
     # Just tests that basic calling works without exploring caveats.
     @partial(jax_utils.pad_shard_unpad, static_argnums=())
     def add(a, b):
       return a + b
 
     x = np.arange(bs, dtype=dtype)
-    y = add(x, 10*x)
+    y = add(x, 10 * x)
     chex.assert_type(y.dtype, x.dtype)
-    np.testing.assert_allclose(np.float64(y), np.float64(x + 10*x))
+    np.testing.assert_allclose(np.float64(y), np.float64(x + 10 * x))
 
   @parameterized.product(dtype=DTYPES, bs=BATCH_SIZES)
   def test_trees(self, dtype, bs):
     # Just tests that basic calling works without exploring caveats.
     @partial(jax_utils.pad_shard_unpad, static_argnums=())
     def add(a, b):
       return a['a'] + b[0]
 
     x = np.arange(bs, dtype=dtype)
-    y = add(dict(a=x), (10*x, ))
+    y = add(dict(a=x), (10 * x,))
     chex.assert_type(y.dtype, x.dtype)
-    np.testing.assert_allclose(np.float64(y), np.float64(x + 10*x))
+    np.testing.assert_allclose(np.float64(y), np.float64(x + 10 * x))
 
   @parameterized.parameters(DTYPES)
   def test_min_device_batch_avoids_recompile(self, dtype):
     @partial(jax_utils.pad_shard_unpad, static_argnums=())
     @jax.jit
     @chex.assert_max_traces(n=1)
     def add(a, b):
       return a + b
 
     chex.clear_trace_counter()
 
     for bs in self.BATCH_SIZES:
       x = np.arange(bs, dtype=dtype)
-      y = add(x, 10*x, min_device_batch=9)  # pylint: disable=unexpected-keyword-arg
+      y = add(x, 10 * x, min_device_batch=9)  # pylint: disable=unexpected-keyword-arg
       chex.assert_type(y.dtype, x.dtype)
-      np.testing.assert_allclose(np.float64(y), np.float64(x + 10*x))
+      np.testing.assert_allclose(np.float64(y), np.float64(x + 10 * x))
 
   @parameterized.product(dtype=DTYPES, bs=BATCH_SIZES)
   def test_static_argnum(self, dtype, bs):
     @partial(jax_utils.pad_shard_unpad, static_argnums=(1,))
     def add(a, b):
       return a + b
```

### Comparing `flax-0.7.0/tests/linen/initializers_test.py` & `flax-0.7.1/tests/linen/initializers_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -29,36 +29,40 @@
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 
 class InitializersTest(parameterized.TestCase):
 
   @parameterized.parameters(
-    {
-      'builder_fn': initializers.zeros_init,
-      'params_shape': (2, 3),
-      'expected_params': jnp.zeros((2, 3)),
-    }, {
-      'builder_fn': initializers.ones_init,
-      'params_shape': (3, 2),
-      'expected_params': jnp.ones((3, 2)),
-    })
+      {
+          'builder_fn': initializers.zeros_init,
+          'params_shape': (2, 3),
+          'expected_params': jnp.zeros((2, 3)),
+      },
+      {
+          'builder_fn': initializers.ones_init,
+          'params_shape': (3, 2),
+          'expected_params': jnp.ones((3, 2)),
+      },
+  )
   def test_call_builder(self, builder_fn, params_shape, expected_params):
     params = builder_fn()(random.PRNGKey(42), params_shape, jnp.float32)
     np.testing.assert_allclose(params, expected_params)
 
   @parameterized.parameters(
-    {
-      'builder_fn': initializers.zeros_init,
-      'expected_params': jnp.zeros((2, 5)),
-    }, {
-      'builder_fn': initializers.ones_init,
-      'expected_params': jnp.ones((2, 5)),
-    })
+      {
+          'builder_fn': initializers.zeros_init,
+          'expected_params': jnp.zeros((2, 5)),
+      },
+      {
+          'builder_fn': initializers.ones_init,
+          'expected_params': jnp.ones((2, 5)),
+      },
+  )
   def test_kernel_builder(self, builder_fn, expected_params):
     layer = nn.Dense(5, kernel_init=builder_fn())
     params = layer.init(random.PRNGKey(42), jnp.empty((3, 2)))['params']
     np.testing.assert_allclose(params['kernel'], expected_params)
 
 
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/kw_only_dataclasses_test.py` & `flax-0.7.1/tests/linen/kw_only_dataclasses_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -20,15 +20,14 @@
 
 from flax.linen import kw_only_dataclasses
 
 
 class KwOnlyDataclassesTest(absltest.TestCase):
 
   def test_kwonly_args_moved_to_end(self):
-
     @kw_only_dataclasses.dataclass
     class TestClass:
       a: int = 1
       b: int = kw_only_dataclasses.field(default=2, kw_only=True)
       c: int = 3
 
     params = inspect.signature(TestClass.__init__).parameters
@@ -43,15 +42,14 @@
     v2 = TestClass(b=20)
     self.assertDictEqual(dataclasses.asdict(v2), dict(a=1, b=20, c=3))
 
     v3 = TestClass(1, 30)
     self.assertDictEqual(dataclasses.asdict(v3), dict(a=1, b=2, c=30))
 
   def test_base_optional_subclass_required(self):
-
     @kw_only_dataclasses.dataclass
     class Parent:
       a: int = kw_only_dataclasses.field(default=2, kw_only=True)
 
     @kw_only_dataclasses.dataclass
     class Child(Parent):
       b: int
@@ -100,18 +98,18 @@
     self.assertEqual(c_params['x'].default, 2)
     self.assertEqual(c_params['y'].default, 3)
     self.assertEqual(c_params['name'].default, inspect.Parameter.empty)
     self.assertEqual(c_params['size'].default, inspect.Parameter.empty)
 
     value = C(4, 'foo')  # pylint: disable=too-many-function-args
     self.assertDictEqual(
-        dataclasses.asdict(value), dict(name='foo', size=4, x=2, y=3))
+        dataclasses.asdict(value), dict(name='foo', size=4, x=2, y=3)
+    )
 
   def test_kwonly_marker(self):
-
     @kw_only_dataclasses.dataclass
     class A:
       x: float
       _: kw_only_dataclasses.KW_ONLY
       a: int = 5
       b: int = kw_only_dataclasses.field(default=2)
       c: int = kw_only_dataclasses.field(default=2, kw_only=True)
```

### Comparing `flax-0.7.0/tests/linen/linen_activation_test.py` & `flax-0.7.1/tests/linen/linen_activation_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/tests/linen/linen_attention_test.py` & `flax-0.7.1/tests/linen/linen_attention_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -29,14 +29,15 @@
 import jax.numpy as jnp
 
 import numpy as np
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
+
 class AttentionTest(parameterized.TestCase):
 
   def test_multihead_self_attention(self):
     rng = random.PRNGKey(0)
     x = jnp.ones((4, 6, 5))
     sa_module = nn.SelfAttention(
         num_heads=8,
@@ -137,47 +138,56 @@
   def test_causal_mask_1d(self):
     """Tests autoregresive masking for 1d attention."""
     x = jnp.ones((3, 16))  # (bs1, length)
     mask_1d = nn.attention.make_causal_mask(x)
     ts = np.arange(16)
     mask_1d_simple = (ts[:, None] >= ts[None, :])[None, None, :, :]
     mask_1d_simple = jnp.broadcast_to(mask_1d_simple, (3, 1, 16, 16))
-    np.testing.assert_allclose(mask_1d, mask_1d_simple,)
+    np.testing.assert_allclose(
+        mask_1d,
+        mask_1d_simple,
+    )
 
   @parameterized.parameters([((5,), (1,)), ((6, 5), (2,))])
-  @temp_flip_flag('return_frozendict', False)
   def test_decoding(self, spatial_shape, attn_dims):
     bs = 2
     num_heads = 3
     num_features = 4
     rng = random.PRNGKey(0)
     key1, key2 = random.split(rng)
     inputs = random.normal(
-        key1, (bs,) + spatial_shape + (num_heads * num_features,))
+        key1, (bs,) + spatial_shape + (num_heads * num_features,)
+    )
     module = nn.SelfAttention(
         num_heads=num_heads,
         qkv_features=num_heads * num_features,
         precision=lax.Precision.HIGHEST,
         deterministic=False,
-        decode=False)
+        decode=False,
+    )
     decode_module = module.clone(decode=True)
 
     initial_vars = decode_module.init(key2, inputs)
     state, params = pop(initial_vars, 'params')
     causal_mask = nn.attention.make_causal_mask(jnp.ones((bs,) + spatial_shape))
     y_ref = jax.jit(lambda x, y: module.apply(initial_vars, x, y))(
-        inputs, causal_mask)
+        inputs, causal_mask
+    )
+
     # feed the inputs sequentially to simulate decoding
     def body_fn(state, x):
       y, state = decode_module.apply(
-          {'params': params, **state}, x, mutable=['cache'])
+          {'params': params, **state}, x, mutable=['cache']
+      )
       return state, y
+
     # scan_in_dim supports scanning multiple dims
-    _, y = jax_utils.scan_in_dim(body_fn, state, inputs,
-                                 axis=attn_dims, keepdims=True)
+    _, y = jax_utils.scan_in_dim(
+        body_fn, state, inputs, axis=attn_dims, keepdims=True
+    )
 
     np.testing.assert_allclose(y_ref, y, atol=1e-5)
 
   def test_autoregresive_receptive_field_1d(self):
     """Tests the autoregresive self-attention receptive field."""
     rng = random.PRNGKey(0)
     rng1, rng2 = random.split(rng, num=2)
@@ -187,15 +197,16 @@
     num_heads = 1
     input_shape = (1, length, dim)
     inputs = random.normal(rng2, input_shape)
 
     module = nn.MultiHeadDotProductAttention(
         num_heads=num_heads,
         kernel_init=jax.nn.initializers.ones,
-        deterministic=False)
+        deterministic=False,
+    )
 
     initial_vars = module.init(rng1, inputs, inputs)
     causal_mask = nn.attention.make_causal_mask(jnp.ones(input_shape[:-1]))
 
     def model_loss(inputs, pos):
       out = module.apply(initial_vars, inputs, inputs, causal_mask)
       assert out.shape == input_shape
@@ -206,19 +217,23 @@
 
     def get_receptive_field_1d(pos):
       g = grad_fn(inputs, pos)[0, :, :]
       return jnp.any((jnp.abs(g) > 1e-5).astype(jnp.uint32), axis=-1)
 
     for i in range(length):
       deps = get_receptive_field_1d(i)
-      assert (deps[:i] == 1).all(), ('Receptive Field Error: Some of the '
-                                     'previous postions are not reachable '
-                                     'in autoregressive self-attention.')
+      assert (deps[:i] == 1).all(), (
+          'Receptive Field Error: Some of the '
+          'previous postions are not reachable '
+          'in autoregressive self-attention.'
+      )
       if i != length - 1:
         k = i + 1
-        assert (deps[k:] == 0).all(), ('Receptive Field Error: Some of the '
-                                       'future postions are reachable in '
-                                       'autoregressive self-attention.')
+        assert (deps[k:] == 0).all(), (
+            'Receptive Field Error: Some of the '
+            'future postions are reachable in '
+            'autoregressive self-attention.'
+        )
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/linen_combinators_test.py` & `flax-0.7.1/tests/linen/linen_combinators_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -33,46 +33,48 @@
   activation: Optional[Any] = None
   activation_final: Optional[Any] = None
 
   @nn.compact
   def __call__(self, inputs):
     x = inputs
     for layer_size in self.layer_sizes[:-1]:
-      x = nn.Dense(features=layer_size, kernel_init=nn.initializers.ones_init())(x)
+      x = nn.Dense(
+          features=layer_size, kernel_init=nn.initializers.ones_init()
+      )(x)
       if self.activation is not None:
         x = self.activation(x)
     x = nn.Dense(
-        features=self.layer_sizes[-1], kernel_init=nn.initializers.ones_init())(
-            x)
+        features=self.layer_sizes[-1], kernel_init=nn.initializers.ones_init()
+    )(x)
     if self.activation_final is None:
       return x
     return self.activation_final(x)
 
 
 class AttentionTuple(nn.Module):
   num_heads: int = 2
   qkv_features: int = 16
 
   @nn.compact
   def __call__(self, query, key_value):
     output = nn.MultiHeadDotProductAttention(
-        num_heads=self.num_heads, qkv_features=self.qkv_features)(query,
-                                                                  key_value)
+        num_heads=self.num_heads, qkv_features=self.qkv_features
+    )(query, key_value)
     return output, key_value
 
 
 class AttentionDict(nn.Module):
   num_heads: int = 2
   qkv_features: int = 16
 
   @nn.compact
   def __call__(self, query, key_value):
     output = nn.MultiHeadDotProductAttention(
-        num_heads=self.num_heads, qkv_features=self.qkv_features)(query,
-                                                                  key_value)
+        num_heads=self.num_heads, qkv_features=self.qkv_features
+    )(query, key_value)
     return dict(query=output, key_value=key_value)
 
 
 class SequentialTest(absltest.TestCase):
 
   def test_construction(self):
     sequential = nn.Sequential([nn.Dense(4), nn.Dense(2)])
@@ -80,57 +82,59 @@
     x = random.uniform(key1, (3, 1, 5))
     params = sequential.init(key2, x)
     output = sequential.apply(params, x)
     self.assertEqual(output.shape, (3, 1, 2))
 
   def test_fails_if_layers_empty(self):
     sequential = nn.Sequential([])
-    with self.assertRaisesRegex(ValueError,
-                                'Empty Sequential module'):
+    with self.assertRaisesRegex(ValueError, 'Empty Sequential module'):
       sequential.init(random.PRNGKey(42), jnp.ones((3, 5)))
 
   def test_same_output_as_mlp(self):
     sequential = nn.Sequential([
         nn.Dense(4, kernel_init=nn.initializers.ones_init()),
         nn.Dense(8, kernel_init=nn.initializers.ones_init()),
-        nn.Dense(2, kernel_init=nn.initializers.ones_init())
+        nn.Dense(2, kernel_init=nn.initializers.ones_init()),
     ])
     mlp = MLP(layer_sizes=[4, 8, 2])
 
     key1, key2 = random.split(random.PRNGKey(0), 2)
     x = random.uniform(key1, (3, 5))
     params_1 = sequential.init(key2, x)
     params_2 = mlp.init(key2, x)
 
     output_1 = sequential.apply(params_1, x)
     output_2 = mlp.apply(params_2, x)
     np.testing.assert_array_equal(output_1, output_2)
 
   def test_same_output_as_mlp_with_activation(self):
     sequential = nn.Sequential([
-        nn.Dense(4, kernel_init=nn.initializers.ones_init()), nn.relu,
-        nn.Dense(8, kernel_init=nn.initializers.ones_init()), nn.relu,
-        nn.Dense(2, kernel_init=nn.initializers.ones_init()), nn.log_softmax
+        nn.Dense(4, kernel_init=nn.initializers.ones_init()),
+        nn.relu,
+        nn.Dense(8, kernel_init=nn.initializers.ones_init()),
+        nn.relu,
+        nn.Dense(2, kernel_init=nn.initializers.ones_init()),
+        nn.log_softmax,
     ])
 
     mlp = MLP(
         layer_sizes=[4, 8, 2],
         activation=nn.relu,
-        activation_final=nn.log_softmax)
+        activation_final=nn.log_softmax,
+    )
 
     key1, key2 = random.split(random.PRNGKey(0), 2)
     x = random.uniform(key1, (3, 5))
     params_1 = sequential.init(key2, x)
     params_2 = mlp.init(key2, x)
 
     output_1 = sequential.apply(params_1, x)
     output_2 = mlp.apply(params_2, x)
     np.testing.assert_array_equal(output_1, output_2)
 
-
   def test_tuple_output(self):
     sequential = nn.Sequential([
         AttentionTuple(),
         AttentionTuple(),
     ])
 
     key1, key2 = random.split(random.PRNGKey(0), 2)
```

### Comparing `flax-0.7.0/tests/linen/linen_dtypes_test.py` & `flax-0.7.1/tests/linen/linen_dtypes_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -21,32 +21,33 @@
 
 from flax import linen as nn
 from flax.linen import dtypes
 
 import jax
 from jax import numpy as jnp
 
-default_float_dtype = jnp.result_type(1.)
+default_float_dtype = jnp.result_type(1.0)
+
 
 class DtypesTest(absltest.TestCase):
 
   def test_no_inexact_dtype(self):
-    i32 = jnp.int32(1.)
+    i32 = jnp.int32(1.0)
     self.assertEqual(dtypes.canonicalize_dtype(i32, inexact=False), jnp.int32)
 
   def test_inexact_dtype(self):
     with jax.experimental.enable_x64():
       i64 = jnp.int64(1)
       self.assertEqual(dtypes.canonicalize_dtype(i64), jnp.float32)
     i32 = jnp.int32(1)
     self.assertEqual(dtypes.canonicalize_dtype(i32), jnp.float32)
-    i16 = jnp.int16(1.)
+    i16 = jnp.int16(1.0)
     self.assertEqual(dtypes.canonicalize_dtype(i16), jnp.float32)
 
   def test_explicit_downcast(self):
-    f32 = jnp.float32(1.)
-    x, = dtypes.promote_dtype(f32, dtype=jnp.float16)
+    f32 = jnp.float32(1.0)
+    (x,) = dtypes.promote_dtype(f32, dtype=jnp.float16)
     self.assertEqual(x.dtype, jnp.float16)
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/linen_linear_test.py` & `flax-0.7.1/tests/linen/linen_linear_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -42,37 +42,37 @@
         features=4,
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, _ = dense_module.init_with_output(rng, x)
     self.assertEqual(y.shape, (1, 4))
     self.assertEqual(y.dtype, jnp.float32)
-    np.testing.assert_allclose(y, np.full((1, 4), 4.))
+    np.testing.assert_allclose(y, np.full((1, 4), 4.0))
 
   def test_dense_extra_batch_dims(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((1, 2, 3))
     dense_module = nn.Dense(
         features=4,
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, _ = dense_module.init_with_output(rng, x)
-    np.testing.assert_allclose(y, np.full((1, 2, 4), 4.))
+    np.testing.assert_allclose(y, np.full((1, 2, 4), 4.0))
 
   def test_dense_no_bias(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((1, 3))
     dense_module = nn.Dense(
         features=4,
         use_bias=False,
         kernel_init=initializers.ones,
     )
     y, _ = dense_module.init_with_output(rng, x)
-    np.testing.assert_allclose(y, np.full((1, 4), 3.))
+    np.testing.assert_allclose(y, np.full((1, 4), 3.0))
 
   def test_dense_is_dense_general(self):
     x = jax.random.normal(random.PRNGKey(0), (5, 3))
     dense_module = nn.Dense(
         features=4,
         use_bias=True,
         bias_init=initializers.normal(),
@@ -104,174 +104,172 @@
     x = jnp.ones((1, 3))
     dg_module = nn.DenseGeneral(
         features=(2, 2),
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, _ = dg_module.init_with_output(rng, x)
-    np.testing.assert_allclose(y, np.full((1, 2, 2), 4.))
+    np.testing.assert_allclose(y, np.full((1, 2, 2), 4.0))
 
   def test_dense_general_two_in(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((1, 2, 2))
     dg_module = nn.DenseGeneral(
         features=3,
         axis=(-2, 2),
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, _ = dg_module.init_with_output(rng, x)
-    np.testing.assert_allclose(y, np.full((1, 3), 5.))
+    np.testing.assert_allclose(y, np.full((1, 3), 5.0))
 
   def test_dense_general_batch_dim(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((2, 1, 3, 5))
 
-    state = {'counter': 0.}
+    state = {'counter': 0.0}
+
     def _counter_init(rng, shape, dtype, state):
       del rng, dtype
-      state['counter'] += 1.
+      state['counter'] += 1.0
       return jnp.full(shape, state['counter'])
+
     counter_init = functools.partial(_counter_init, state=state)
 
     dg_module = nn.DenseGeneral(
         features=7,
         axis=(3, -2),
         batch_dims=0,
         bias_init=initializers.ones,
         kernel_init=counter_init,
     )
     y, _ = dg_module.init_with_output(rng, x)
-    target = np.full((2, 1, 7), 16.)
+    target = np.full((2, 1, 7), 16.0)
     np.testing.assert_allclose(y, target)
 
-  @parameterized.parameters([((-2, 3), (), 'bijk,jklm->bilm'),
-                             ((3, -2), (), 'bijk,jklm->bilm'),
-                             ((-2, 3), (0,), 'bijk,bjklm->bilm')])
+  @parameterized.parameters([
+      ((-2, 3), (), 'bijk,jklm->bilm'),
+      ((3, -2), (), 'bijk,jklm->bilm'),
+      ((-2, 3), (0,), 'bijk,bjklm->bilm'),
+  ])
   def test_dense_general_vs_numpy(self, axis, batch_dims, einsum_expr):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((16, 8, 9, 10))
 
     dg_module = nn.DenseGeneral(
         features=(11, 12),
         axis=axis,
         batch_dims=batch_dims,
         bias_init=initializers.ones,
         kernel_init=initializers.normal(),
     )
     y, initial_params = dg_module.init_with_output(rng, x)
-    target = np.einsum(einsum_expr, x, initial_params['params']['kernel']) + 1.
+    target = np.einsum(einsum_expr, x, initial_params['params']['kernel']) + 1.0
     np.testing.assert_allclose(y, target, atol=1e-6)
 
   def test_complex_params_dense(self):
-    dense = nn.Dense(
-      features=2,
-      param_dtype=jnp.complex64)
+    dense = nn.Dense(features=2, param_dtype=jnp.complex64)
     x = jnp.ones((1, 2), jnp.float32)
     variables = dense.init(random.PRNGKey(0), x)
     self.assertEqual(variables['params']['kernel'].dtype, jnp.complex64)
     self.assertEqual(variables['params']['bias'].dtype, jnp.complex64)
     y = dense.apply(variables, x)
     self.assertEqual(y.dtype, jnp.complex64)
 
   def test_complex_input_dense(self):
-    dense = nn.Dense(
-      features=2)
+    dense = nn.Dense(features=2)
     x = jnp.ones((1, 2), jnp.complex64)
     variables = dense.init(random.PRNGKey(0), x)
     self.assertEqual(variables['params']['kernel'].dtype, jnp.float32)
     self.assertEqual(variables['params']['bias'].dtype, jnp.float32)
     y = dense.apply(variables, x)
     self.assertEqual(y.dtype, jnp.complex64)
 
-
-  @parameterized.product(
-      use_bias=(True, False))
+  @parameterized.product(use_bias=(True, False))
   def test_conv(self, use_bias):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((1, 8, 3))
     conv_module = nn.Conv(
         features=4,
         use_bias=use_bias,
         kernel_size=(3,),
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 4))
-    expected = 10. if use_bias else 9.
+    expected = 10.0 if use_bias else 9.0
     np.testing.assert_allclose(y, np.full((1, 6, 4), expected))
 
-
-  @parameterized.product(
-      use_bias=(True, False))
+  @parameterized.product(use_bias=(True, False))
   def test_multibatch_input_conv(self, use_bias):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((2, 5, 8, 3))
     conv_module = nn.Conv(
         features=4,
         use_bias=use_bias,
         kernel_size=(3,),
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 4))
-    expected = 10. if use_bias else 9.
+    expected = 10.0 if use_bias else 9.0
     np.testing.assert_allclose(y, np.full((2, 5, 6, 4), expected))
 
-
   def test_conv_local(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((1, 8, 2))
     conv_module = nn.ConvLocal(
         features=4,
         kernel_size=(3,),
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (6, 3 * 2, 4))
-    np.testing.assert_allclose(y, np.full((1, 6, 4), 7.))
+    np.testing.assert_allclose(y, np.full((1, 6, 4), 7.0))
 
   def test_single_input_conv(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((8, 3))
     conv_module = nn.Conv(
         features=4,
         kernel_size=(3,),
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 4))
-    np.testing.assert_allclose(y, np.full((6, 4), 10.))
+    np.testing.assert_allclose(y, np.full((6, 4), 10.0))
 
   def test_single_input_masked_conv(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((8, 3))
     m = jnp.tril(jnp.ones((3, 3, 4)))
     conv_module = nn.Conv(
         features=4,
         kernel_size=(3,),
         padding='VALID',
         mask=m,
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
-    expected = jnp.array([[10., 7., 4., 1.],
-                          [10., 7., 4., 1.],
-                          [10., 7., 4., 1.],
-                          [10., 7., 4., 1.],
-                          [10., 7., 4., 1.],
-                          [10., 7., 4., 1.]])
+    expected = jnp.array([
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+    ])
     y, initial_params = conv_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 4))
     np.testing.assert_allclose(y, expected)
 
   def test_single_input_conv_local(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((8, 2))
@@ -280,42 +278,47 @@
         kernel_size=(3,),
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (6, 3 * 2, 4))
-    np.testing.assert_allclose(y, np.full((6, 4), 7.))
+    np.testing.assert_allclose(y, np.full((6, 4), 7.0))
 
   def test_group_conv(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((1, 8, 4))
     conv_module = nn.Conv(
         features=4,
         kernel_size=(3,),
         feature_group_count=2,
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 2, 4))
-    np.testing.assert_allclose(y, np.full((1, 6, 4), 7.))
+    np.testing.assert_allclose(y, np.full((1, 6, 4), 7.0))
 
   @parameterized.product(
       n_batch=(1, 3),
       n_features=(1, 2),
       kernel_size=(1, 2, 3, 9),
       n_input_features=(1, 3),
       input_size=(1, 8, 16),
-      module=(nn.Conv, nn.ConvLocal)
+      module=(nn.Conv, nn.ConvLocal),
   )
   def test_circular_conv_1d_constant(
-      self, n_batch, n_features, kernel_size, n_input_features, input_size,
-      module
+      self,
+      n_batch,
+      n_features,
+      kernel_size,
+      n_input_features,
+      input_size,
+      module,
   ):
     """
     Test 1D convolution with circular padding: filter with all elements equal
     to 1 applied on an input with all elements equal to 1.
     Result should have the same shape as input (except for the feature
     dimension) and have all elements equal to
     `n_input_features * kernel_lin_size`.
@@ -327,58 +330,57 @@
         kernel_size=(kernel_size,),
         padding='CIRCULAR',
         kernel_init=initializers.ones,
         bias_init=initializers.zeros,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
 
-    kernel_shape = self._get_kernel_shape(x.shape, (kernel_size,), module,
-                                          n_features)
+    kernel_shape = self._get_kernel_shape(
+        x.shape, (kernel_size,), module, n_features
+    )
 
     self.assertEqual(
         initial_params['params']['kernel'].shape,
         kernel_shape,
     )
     correct_ans = np.full(
         (n_batch, input_size, n_features), kernel_size * n_input_features
     )
     np.testing.assert_allclose(y, correct_ans)
 
-  def _get_kernel_shape(self,
-                        input_shape,
-                        kernel_size,
-                        module,
-                        n_features):
+  def _get_kernel_shape(self, input_shape, kernel_size, module, n_features):
     if module == nn.Conv:
       kernel_shape = kernel_size + (input_shape[-1], n_features)
     elif module == nn.ConvLocal:
       kernel_shape = input_shape[1:-1] + (
-          input_shape[-1] * np.prod(kernel_size), n_features)
+          input_shape[-1] * np.prod(kernel_size),
+          n_features,
+      )
     else:
       raise ValueError(module)
     return kernel_shape
 
   @parameterized.product(
       n_batch=(1, 3),
       n_features=(1, 2, 10),
       kernel_lin_size=(1, 2, 3, 9),
       n_input_features=(1, 5),
       input_x_size=(14,),
       input_y_size=(5, 10),
-      module=(nn.Conv, nn.ConvLocal)
+      module=(nn.Conv, nn.ConvLocal),
   )
   def test_circular_conv_2d_constant(
       self,
       n_batch,
       n_features,
       kernel_lin_size,
       n_input_features,
       input_x_size,
       input_y_size,
-      module
+      module,
   ):
     """
     Test 2D convolution with circular padding: square filter with all elements
     equal to 1 applied on an input with all elements equal to 1.
     Result should have the same shape as input (except for the feature
     dimension) and have all elements equal to
     `n_input_features * kernel_lin_size^2`.
@@ -391,16 +393,17 @@
         kernel_size=kernel_size,
         padding='CIRCULAR',
         kernel_init=initializers.ones,
         bias_init=initializers.zeros,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
 
-    kernel_shape = self._get_kernel_shape(x.shape, kernel_size, module,
-                                            n_features)
+    kernel_shape = self._get_kernel_shape(
+        x.shape, kernel_size, module, n_features
+    )
 
     self.assertEqual(
         initial_params['params']['kernel'].shape,
         kernel_shape,
     )
     correct_ans = np.full(
         (n_batch, input_x_size, input_y_size, n_features),
@@ -467,57 +470,61 @@
 
     conv_module = nn.Conv(
         features=1,
         kernel_size=(3,),
         padding='CIRCULAR',
         kernel_init=lambda *_: kernel,
         bias_init=initializers.zeros,
-        kernel_dilation=(3,))
+        kernel_dilation=(3,),
+    )
     y, initial_params = conv_module.init_with_output(rng, x)
 
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 1, 1))
     # Compare with manually computed convolution
-    correct_ans = np.array((3 + 2 * 1 + 4, 4 + 2 * 2 + 5, 5 + 2 * 3 + 1,
-                            1 + 2 * 4 + 2, 2 + 2 * 5 + 3))
+    correct_ans = np.array((
+        3 + 2 * 1 + 4,
+        4 + 2 * 2 + 5,
+        5 + 2 * 3 + 1,
+        1 + 2 * 4 + 2,
+        2 + 2 * 5 + 3,
+    ))
     correct_ans = np.expand_dims(correct_ans, (0, 2))
     np.testing.assert_allclose(y, correct_ans)
 
   def test_circular_conv_local_1d_dilation(self):
     """
     Test 1d local convolution with circular padding and kernel dilation
     """
     rng = dict(params=random.PRNGKey(0))
     x = np.arange(1, 6)
     x = np.expand_dims(x, (0, 2))
-    kernel = np.array((
-        (1, 2, 1),
-        (3, 4, 5),
-        (-1, 1, 2),
-        (2, 3, 4),
-        (-1, -2, -3)
-    ))
+    kernel = np.array(
+        ((1, 2, 1), (3, 4, 5), (-1, 1, 2), (2, 3, 4), (-1, -2, -3))
+    )
     kernel = np.expand_dims(kernel, (2,))
 
     conv_module = nn.ConvLocal(
         features=1,
         kernel_size=(3,),
         padding='CIRCULAR',
         kernel_init=lambda *_: kernel,
         bias_init=initializers.zeros,
-        kernel_dilation=(3,)
+        kernel_dilation=(3,),
     )
     y, initial_params = conv_module.init_with_output(rng, x)
 
     self.assertEqual(initial_params['params']['kernel'].shape, (5, 3, 1))
     # Compare with manually computed convolution
-    correct_ans = np.array((1 * 3 + 2 * 1 + 1 * 4,
-                            3 * 4 + 4 * 2 + 5 * 5,
-                            -1 * 5 + 1 * 3 + 2 * 1,
-                            2 * 1 + 3 * 4 + 4 * 2,
-                            -1 * 2 + -2 * 5 + -3 * 3))
+    correct_ans = np.array((
+        1 * 3 + 2 * 1 + 1 * 4,
+        3 * 4 + 4 * 2 + 5 * 5,
+        -1 * 5 + 1 * 3 + 2 * 1,
+        2 * 1 + 3 * 4 + 4 * 2,
+        -1 * 2 + -2 * 5 + -3 * 3,
+    ))
     correct_ans = np.expand_dims(correct_ans, (0, 2))
     np.testing.assert_allclose(y, correct_ans)
 
   def test_circular_conv_2d_custom(self):
     """Test 2d convolution with circular padding on a 3x3 example."""
     rng = dict(params=random.PRNGKey(0))
     x = np.array(((1, 2, 3), (4, 5, 6), (7, 8, 9)))
@@ -545,51 +552,31 @@
     np.testing.assert_allclose(y, correct_ans)
 
   def test_circular_conv_local_2d_custom(self):
     """
     Test 2d local convolution with circular padding on a 3x3 example
     """
     rng = dict(params=random.PRNGKey(0))
-    x = np.array(((1, 2, 3),
-                  (4, 5, 6),
-                  (7, 8, 9)))
+    x = np.array(((1, 2, 3), (4, 5, 6), (7, 8, 9)))
     x = np.expand_dims(x, (0, 3))
     kernel = np.array((
         (
-            ((0, 1, 0),
-             (1, 2, 1),
-             (0, 1, 0)),
-            ((0, 1, 0),
-             (1, 3, 1),
-             (0, 1, 0)),
-            ((0, 1, 0),
-             (1, 4, 1),
-             (0, 1, 0))
+            ((0, 1, 0), (1, 2, 1), (0, 1, 0)),
+            ((0, 1, 0), (1, 3, 1), (0, 1, 0)),
+            ((0, 1, 0), (1, 4, 1), (0, 1, 0)),
         ),
         (
-            ((0, 1, 0),
-             (1, 5, 1),
-             (0, 1, 0)),
-            ((0, 1, 0),
-             (1, 6, 1),
-             (0, 1, 0)),
-            ((0, 1, 0),
-             (1, 7, 1),
-             (0, 1, 0))
+            ((0, 1, 0), (1, 5, 1), (0, 1, 0)),
+            ((0, 1, 0), (1, 6, 1), (0, 1, 0)),
+            ((0, 1, 0), (1, 7, 1), (0, 1, 0)),
         ),
         (
-            ((0, 1, 0),
-             (1, 8, 1),
-             (0, 1, 0)),
-            ((0, 1, 0),
-             (1, 9, 1),
-             (0, 1, 0)),
-            ((0, 1, 0),
-             (1, 10, 1),
-             (0, 1, 0))
+            ((0, 1, 0), (1, 8, 1), (0, 1, 0)),
+            ((0, 1, 0), (1, 9, 1), (0, 1, 0)),
+            ((0, 1, 0), (1, 10, 1), (0, 1, 0)),
         ),
     ))
     kernel = np.expand_dims(kernel, (3,))
     kernel = np.reshape(kernel, (3, 3, 9, 1))
 
     conv_module = nn.ConvLocal(
         features=1,
@@ -598,39 +585,43 @@
         kernel_init=lambda *_: kernel,
         bias_init=initializers.zeros,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
 
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 9, 1))
     # Compare with manually computed convolution
-    correct_ans = np.array(
-        (
-            (2 * 1 + 7 + 2 + 4 + 3, 3 * 2 + 8 + 3 + 5 + 1, 4 * 3 + 9 + 1 + 6 + 2),
-            (5 * 4 + 1 + 5 + 7 + 6, 6 * 5 + 2 + 6 + 8 + 4, 7 * 6 + 3 + 4 + 9 + 5),
-            (8 * 7 + 4 + 8 + 1 + 9, 9 * 8 + 5 + 9 + 2 + 7, 10 * 9 + 6 + 7 + 3 + 8),
-        )
-    )
+    correct_ans = np.array((
+        (2 * 1 + 7 + 2 + 4 + 3, 3 * 2 + 8 + 3 + 5 + 1, 4 * 3 + 9 + 1 + 6 + 2),
+        (5 * 4 + 1 + 5 + 7 + 6, 6 * 5 + 2 + 6 + 8 + 4, 7 * 6 + 3 + 4 + 9 + 5),
+        (8 * 7 + 4 + 8 + 1 + 9, 9 * 8 + 5 + 9 + 2 + 7, 10 * 9 + 6 + 7 + 3 + 8),
+    ))
     correct_ans = np.expand_dims(correct_ans, (0, 3))
     np.testing.assert_allclose(y, correct_ans)
 
   def test_causal_conv1d(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((1, 8, 4))
     conv_module = nn.Conv(
         features=4,
         kernel_size=(3,),
         padding='CAUSAL',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, _ = conv_module.init_with_output(rng, x)
-    correct_ans = np.array([[[5., 5., 5., 5.], [9., 9., 9., 9.],
-                             [13., 13., 13., 13.], [13., 13., 13., 13.],
-                             [13., 13., 13., 13.], [13., 13., 13., 13.],
-                             [13., 13., 13., 13.], [13., 13., 13., 13.]]])
+    correct_ans = np.array([[
+        [5.0, 5.0, 5.0, 5.0],
+        [9.0, 9.0, 9.0, 9.0],
+        [13.0, 13.0, 13.0, 13.0],
+        [13.0, 13.0, 13.0, 13.0],
+        [13.0, 13.0, 13.0, 13.0],
+        [13.0, 13.0, 13.0, 13.0],
+        [13.0, 13.0, 13.0, 13.0],
+        [13.0, 13.0, 13.0, 13.0],
+    ]])
     np.testing.assert_allclose(y, correct_ans)
     np.testing.assert_array_equal(correct_ans.shape, y.shape)
 
   @parameterized.product(
       use_bias=(True, False),
   )
   def test_conv_transpose(self, use_bias):
@@ -642,26 +633,28 @@
         kernel_size=(3,),
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_transpose_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 4))
-    correct_ans = np.array([[[ 4.,  4.,  4.,  4.],
-                             [ 7.,  7.,  7.,  7.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [ 7.,  7.,  7.,  7.],
-                             [ 4.,  4.,  4.,  4.]]])
+    correct_ans = np.array([[
+        [4.0, 4.0, 4.0, 4.0],
+        [7.0, 7.0, 7.0, 7.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [7.0, 7.0, 7.0, 7.0],
+        [4.0, 4.0, 4.0, 4.0],
+    ]])
     if not use_bias:
-      correct_ans -= 1.
+      correct_ans -= 1.0
     np.testing.assert_allclose(y, correct_ans)
 
   @parameterized.product(
       use_bias=(True, False),
   )
   def test_multibatch_input_conv_transpose(self, use_bias):
     rng = dict(params=random.PRNGKey(0))
@@ -672,52 +665,56 @@
         kernel_size=(3,),
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_transpose_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 4))
-    correct_ans = np.array([[[ 4.,  4.,  4.,  4.],
-                             [ 7.,  7.,  7.,  7.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [10., 10., 10., 10.],
-                             [ 7.,  7.,  7.,  7.],
-                             [ 4.,  4.,  4.,  4.]]])
+    correct_ans = np.array([[
+        [4.0, 4.0, 4.0, 4.0],
+        [7.0, 7.0, 7.0, 7.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [7.0, 7.0, 7.0, 7.0],
+        [4.0, 4.0, 4.0, 4.0],
+    ]])
     correct_ans = np.repeat(correct_ans[None], repeats=2, axis=0)
     correct_ans = np.repeat(correct_ans, repeats=5, axis=1)
     if not use_bias:
-      correct_ans -= 1.
+      correct_ans -= 1.0
     np.testing.assert_allclose(y, correct_ans)
 
   def test_single_input_conv_transpose(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((8, 3))
     conv_transpose_module = nn.ConvTranspose(
         features=4,
         kernel_size=(3,),
         padding='VALID',
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_transpose_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 4))
-    correct_ans = np.array([[ 4.,  4.,  4.,  4.],
-                            [ 7.,  7.,  7.,  7.],
-                            [10., 10., 10., 10.],
-                            [10., 10., 10., 10.],
-                            [10., 10., 10., 10.],
-                            [10., 10., 10., 10.],
-                            [10., 10., 10., 10.],
-                            [10., 10., 10., 10.],
-                            [ 7.,  7.,  7.,  7.],
-                            [ 4.,  4.,  4.,  4.]])
+    correct_ans = np.array([
+        [4.0, 4.0, 4.0, 4.0],
+        [7.0, 7.0, 7.0, 7.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [10.0, 10.0, 10.0, 10.0],
+        [7.0, 7.0, 7.0, 7.0],
+        [4.0, 4.0, 4.0, 4.0],
+    ])
     np.testing.assert_allclose(y, correct_ans)
 
   def test_single_input_masked_conv_transpose(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((8, 3))
     m = jnp.tril(jnp.ones((3, 3, 4)))
     conv_transpose_module = nn.ConvTranspose(
@@ -726,35 +723,37 @@
         padding='VALID',
         mask=m,
         kernel_init=initializers.ones,
         bias_init=initializers.ones,
     )
     y, initial_params = conv_transpose_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 3, 4))
-    correct_ans = np.array([[ 4., 3., 2., 1.],
-                            [ 7., 5., 3., 1.],
-                            [10., 7., 4., 1.],
-                            [10., 7., 4., 1.],
-                            [10., 7., 4., 1.],
-                            [10., 7., 4., 1.],
-                            [10., 7., 4., 1.],
-                            [10., 7., 4., 1.],
-                            [ 7., 5., 3., 1.],
-                            [ 4., 3., 2., 1.]])
+    correct_ans = np.array([
+        [4.0, 3.0, 2.0, 1.0],
+        [7.0, 5.0, 3.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [10.0, 7.0, 4.0, 1.0],
+        [7.0, 5.0, 3.0, 1.0],
+        [4.0, 3.0, 2.0, 1.0],
+    ])
     np.testing.assert_allclose(y, correct_ans)
 
   @parameterized.product(
       n_batch=(1, 3),
       n_features=(1, 2),
       kernel_size=(1, 2, 3, 9),
       n_input_features=(1, 3),
       input_size=(1, 8, 16),
   )
   def test_circular_conv_transpose_1d_constant(
-          self, n_batch, n_features, kernel_size, n_input_features, input_size
+      self, n_batch, n_features, kernel_size, n_input_features, input_size
   ):
     """
     Test 1D transposed convolution with circular padding: filter with all
     elements equal to 1 applied on an input with all elements equal to 1.
     Result should have the same shape as input (except for the feature
     dimension) and have all elements equal to
     `n_input_features * kernel_lin_size`.
@@ -770,16 +769,17 @@
     )
     y, initial_params = conv_module.init_with_output(rng, x)
 
     self.assertEqual(
         initial_params['params']['kernel'].shape,
         (kernel_size, n_input_features, n_features),
     )
-    correct_ans = np.full((n_batch, input_size, n_features),
-                          kernel_size * n_input_features)
+    correct_ans = np.full(
+        (n_batch, input_size, n_features), kernel_size * n_input_features
+    )
     np.testing.assert_allclose(y, correct_ans)
 
   @parameterized.product(
       n_batch=(1, 3),
       n_features=(1, 2, 10),
       kernel_lin_size=(1, 2, 3, 9),
       n_input_features=(1, 5),
@@ -820,15 +820,15 @@
     correct_ans = np.full(
         (n_batch, input_x_size, input_y_size, n_features),
         kernel_lin_size * kernel_lin_size * n_input_features,
     )
     np.testing.assert_allclose(y, correct_ans)
 
   def test_circular_conv_transpose_2d_with_vmap(self):
-    layer = nn.ConvTranspose(features=5, kernel_size=(3,), padding="CIRCULAR")
+    layer = nn.ConvTranspose(features=5, kernel_size=(3,), padding='CIRCULAR')
 
     # this is ok
     sample_input = jnp.ones((1, 32, 2))
     out, vars = layer.init_with_output(jax.random.PRNGKey(0), sample_input)
     self.assertEqual(out.shape, (1, 32, 5))
 
     batch_input = jnp.ones((8, 32, 2))
@@ -854,22 +854,31 @@
         kernel_init=lambda *_: kernel,
         bias_init=initializers.zeros,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
 
     self.assertEqual(initial_params['params']['kernel'].shape, (3, 1, 1))
     # Compare with manually computed convolution
-    correct_ans = np.array(  # pyformat: disable
-          (1 * 1, 1 * 2, 1 * 1,
-           2 * 1, 2 * 2, 2 * 1,
-           3 * 1, 3 * 2, 3 * 1,
-           4 * 1, 4 * 2, 4 * 1,
-           5 * 1, 5 * 2, 5 * 1,
-           )
-    )
+    correct_ans = np.array((  # pyformat: disable
+        1 * 1,
+        1 * 2,
+        1 * 1,
+        2 * 1,
+        2 * 2,
+        2 * 1,
+        3 * 1,
+        3 * 2,
+        3 * 1,
+        4 * 1,
+        4 * 2,
+        4 * 1,
+        5 * 1,
+        5 * 2,
+        5 * 1,
+    ))
     correct_ans = np.expand_dims(correct_ans, (0, 2))
     np.testing.assert_allclose(y, correct_ans)
 
   def test_circular_conv_transpose_2d_custom(self):
     """Test 2d transposed convolution with circular padding on a 3x3 example."""
     rng = dict(params=random.PRNGKey(0))
     x = np.array((
@@ -925,16 +934,15 @@
     correct_ans = np.array((
         (21, 23),
         (29, 31),
     ))
     correct_ans = np.expand_dims(correct_ans, (0, 3))
     np.testing.assert_allclose(y, correct_ans)
 
-  @parameterized.product(
-      use_bias=(True, False))
+  @parameterized.product(use_bias=(True, False))
   def test_transpose_kernel_conv_transpose(self, use_bias):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.ones((1, 15, 15, 3))
     conv_module = nn.ConvTranspose(
         features=4,
         use_bias=use_bias,
         strides=(2, 2),
@@ -942,54 +950,56 @@
         padding='CIRCULAR',
         transpose_kernel=True,
     )
     y, initial_params = conv_module.init_with_output(rng, x)
     self.assertEqual(initial_params['params']['kernel'].shape, (6, 6, 4, 3))
     self.assertEqual(y.shape, (1, 30, 30, 4))
 
-  @parameterized.product(
-      module=(nn.Conv, nn.ConvLocal)
-  )
+  @parameterized.product(module=(nn.Conv, nn.ConvLocal))
   def test_int_kernel_size(self, module):
     conv = module(features=4, kernel_size=3)
     x = jnp.ones((8, 3))
     with self.assertRaises(TypeError):
       conv.init(random.PRNGKey(0), x)
 
   def test_embed(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.arange(4)[None]
-    dummy_embedding = jnp.broadcast_to(
-        jnp.arange(4)[..., None], (4, 3)).astype(jnp.float32)
+    dummy_embedding = jnp.broadcast_to(jnp.arange(4)[..., None], (4, 3)).astype(
+        jnp.float32
+    )
     embed_module = nn.Embed(
         num_embeddings=4,
         features=3,
         embedding_init=lambda rng, shape, dtype: dummy_embedding,
     )
     y, initial_params = embed_module.init_with_output(rng, x)
     np.testing.assert_allclose(y, dummy_embedding[None])
-    z = embed_module.apply(initial_params, jnp.ones((3,)),
-                           method=embed_module.attend)
-    np.testing.assert_allclose(z, 3. * jnp.arange(4))
+    z = embed_module.apply(
+        initial_params, jnp.ones((3,)), method=embed_module.attend
+    )
+    np.testing.assert_allclose(z, 3.0 * jnp.arange(4))
 
   def test_embed_numpy(self):
     rng = dict(params=random.PRNGKey(0))
     x = jnp.arange(4)[None]
-    dummy_embedding = np.broadcast_to(
-        np.arange(4)[..., None], (4, 3)).astype(np.float32)
+    dummy_embedding = np.broadcast_to(np.arange(4)[..., None], (4, 3)).astype(
+        np.float32
+    )
     embed_module = nn.Embed(
         num_embeddings=4,
         features=3,
         embedding_init=lambda rng, shape, dtype: dummy_embedding,
     )
     y, initial_params = embed_module.init_with_output(rng, x)
     np.testing.assert_allclose(y, dummy_embedding[None])
-    z = embed_module.apply(initial_params, jnp.ones((3,)),
-                           method=embed_module.attend)
-    np.testing.assert_allclose(z, 3. * jnp.arange(4))
+    z = embed_module.apply(
+        initial_params, jnp.ones((3,)), method=embed_module.attend
+    )
+    np.testing.assert_allclose(z, 3.0 * jnp.arange(4))
 
   def test_embed_hash(self):
     self.assertEqual(hash(nn.Embed(2, 3)), hash(nn.Embed(2, 3)))
     self.assertNotEqual(hash(nn.Embed(3, 4)), hash(nn.Embed(2, 3)))
 
   def test_non_final_axis(self):
     class Foo(nn.Module):
@@ -998,46 +1008,44 @@
       def __call__(self, x):
         return nn.DenseGeneral(features=6, axis=1, name='dense')(x)
 
     x = jnp.ones((2, 4, 8))
     y, variables = Foo().init_with_output(random.PRNGKey(0), x)
     self.assertEqual(
         jax.tree_util.tree_map(jnp.shape, variables['params']),
-        {'dense': {
-            'kernel': (4, 6),
-            'bias': (6,)
-        }})
+        {'dense': {'kernel': (4, 6), 'bias': (6,)}},
+    )
     self.assertEqual(y.shape, (2, 8, 6))
 
   def test_non_final_axes(self):
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         return nn.DenseGeneral(features=6, axis=(0, 1), name='dense')(x)
 
     x = jnp.ones((2, 4, 8))
     y, variables = Foo().init_with_output(random.PRNGKey(0), x)
     self.assertEqual(
         jax.tree_util.tree_map(jnp.shape, variables['params']),
-        {'dense': {
-            'kernel': (2, 4, 6),
-            'bias': (6,)
-        }})
+        {'dense': {'kernel': (2, 4, 6), 'bias': (6,)}},
+    )
     self.assertEqual(y.shape, (8, 6))
 
   def test_canonicalize_padding(self):
     def test_pad(pad, rank, expected=None):
       if expected is None:
         with self.assertRaises(ValueError):
           nn.linear.canonicalize_padding(pad, rank)
       else:
         self.assertEqual(nn.linear.canonicalize_padding(pad, rank), expected)
-    test_pad("SAME", 2, "SAME")
+
+    test_pad('SAME', 2, 'SAME')
     test_pad(2, 3, [(2, 2), (2, 2), (2, 2)])
     test_pad((2, 2), 3)
     test_pad((2, 2), 1)
     test_pad([1, (2, 3)], 2, [(1, 1), (2, 3)])
     test_pad([None, (1, 2)], 2)
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/linen_meta_test.py` & `flax-0.7.1/tests/linen/linen_meta_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,82 +28,94 @@
 class LinenMetaTest(absltest.TestCase):
 
   def test_boxed_param(self):
     class Bar(nn.Module):
 
       @nn.compact
       def __call__(mdl_self, x):  # pylint: disable=no-self-argument
-        kernel_init = nn.with_partitioning(nn.initializers.ones_init(),
-                                           ('in', 'out'))
+        kernel_init = nn.with_partitioning(
+            nn.initializers.ones_init(), ('in', 'out')
+        )
         kernel = mdl_self.param('kernel', kernel_init, (x.shape[-1], 2))
         kernel_box = mdl_self.get_variable('params', 'kernel')
         self.assertIsInstance(kernel_box, nn.Partitioned)
         self.assertEqual(kernel_box.names, ('in', 'out'))
         return x @ kernel
 
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, xs):
         return nn.vmap(
-            Bar, in_axes=0,
-            variable_axes={'params': 0}, split_rngs={'params': True},
-            metadata_params={nn.PARTITION_NAME: 'batch'})(name='bar')(xs)
+            Bar,
+            in_axes=0,
+            variable_axes={'params': 0},
+            split_rngs={'params': True},
+            metadata_params={nn.PARTITION_NAME: 'batch'},
+        )(name='bar')(xs)
 
     m = Foo()
     variables = m.init(random.PRNGKey(0), jnp.zeros((8, 3)))
-    self.assertEqual(variables['params']['bar']['kernel'].names,
-                     ('batch', 'in', 'out'))
-
+    self.assertEqual(
+        variables['params']['bar']['kernel'].names, ('batch', 'in', 'out')
+    )
 
   def test_boxed_variable(self):
     class Bar(nn.Module):
 
       @nn.compact
       def __call__(mdl_self, x):  # pylint: disable=no-self-argument
-        kernel_init = nn.with_partitioning(nn.initializers.ones_init(),
-                                           ('in', 'out'))
+        kernel_init = nn.with_partitioning(
+            nn.initializers.ones_init(), ('in', 'out')
+        )
         kernel = mdl_self.variable(
-            'params', 'kernel', kernel_init,
-            mdl_self.make_rng('params'), (x.shape[-1], 2))
-        kernel.value += 1.
+            'params',
+            'kernel',
+            kernel_init,
+            mdl_self.make_rng('params'),
+            (x.shape[-1], 2),
+        )
+        kernel.value += 1.0
         self.assertEqual(kernel.value.sum(), kernel.value.size * 2)
         kernel_box = mdl_self.get_variable('params', 'kernel')
         self.assertIsInstance(kernel_box, nn.Partitioned)
         self.assertEqual(kernel_box.names, ('in', 'out'))
         return x @ kernel.value
 
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, xs):
         return nn.vmap(
-            Bar, in_axes=0,
-            variable_axes={'params': 0}, split_rngs={'params': True},
-            metadata_params={nn.PARTITION_NAME: 'batch'})(name='bar')(xs)
+            Bar,
+            in_axes=0,
+            variable_axes={'params': 0},
+            split_rngs={'params': True},
+            metadata_params={nn.PARTITION_NAME: 'batch'},
+        )(name='bar')(xs)
 
     m = Foo()
     variables = m.init(random.PRNGKey(0), jnp.zeros((8, 3)))
-    self.assertEqual(variables['params']['bar']['kernel'].names,
-                     ('batch', 'in', 'out'))
-
+    self.assertEqual(
+        variables['params']['bar']['kernel'].names, ('batch', 'in', 'out')
+    )
 
   # def test_boxed_variable(self):
   #   def f(scope, xs):
   #     def g(scope, x):
-        # kernel_init = nn.with_partitioning(nn.initializers.ones_init(),
-        #                                      ('in', 'out'))
-        # kernel = scope.variable('params', 'kernel', kernel_init,
-        #                         scope.make_rng('params'), (x.shape[-1], 2))
-        # kernel.value += 1.
-        # self.assertEqual(kernel.value.sum(), kernel.value.size * 2)
-        # kernel_box = scope.get_variable('params', 'kernel')
-        # self.assertIsInstance(kernel_box, nn.Partitioned)
-        # self.assertEqual(kernel_box.names, ('in', 'out'))
-        # return x @ kernel.value
+  # kernel_init = nn.with_partitioning(nn.initializers.ones_init(),
+  #                                      ('in', 'out'))
+  # kernel = scope.variable('params', 'kernel', kernel_init,
+  #                         scope.make_rng('params'), (x.shape[-1], 2))
+  # kernel.value += 1.
+  # self.assertEqual(kernel.value.sum(), kernel.value.size * 2)
+  # kernel_box = scope.get_variable('params', 'kernel')
+  # self.assertIsInstance(kernel_box, nn.Partitioned)
+  # self.assertEqual(kernel_box.names, ('in', 'out'))
+  # return x @ kernel.value
 
   #     nn.vmap(
   #         g, in_axes=0,
   #         variable_axes={'params': 0}, split_rngs={'params': True},
   #         metadata_params={nn.PARTITION_NAME: 'batch'})(scope, xs)
 
   #   _, variables = init(f)(random.PRNGKey(0), jnp.zeros((8, 3)))
@@ -115,64 +127,78 @@
       hidden_size: int
 
       @nn.compact
       def __call__(self, x):
         ki = nn.linear.default_kernel_init
         h = nn.Dense(
             self.hidden_size,
-            kernel_init=nn.with_partitioning(ki, ('data', 'model')))(x)
+            kernel_init=nn.with_partitioning(ki, ('data', 'model')),
+        )(x)
         h = nn.relu(h)
         return nn.Dense(
-            x.shape[-1],
-            kernel_init=nn.with_partitioning(ki, ('model', 'data')))(h)
+            x.shape[-1], kernel_init=nn.with_partitioning(ki, ('model', 'data'))
+        )(h)
 
     class Model(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         def body(_, c):
           c = MLP(512)(c)
           return c, ()
+
         c, _ = nn.scan(
-            body, variable_axes={'params': 0}, split_rngs={'params': 0},
-            length=8, metadata_params={nn.PARTITION_NAME: None})(
-                self, x)
+            body,
+            variable_axes={'params': 0},
+            split_rngs={'params': 0},
+            length=8,
+            metadata_params={nn.PARTITION_NAME: None},
+        )(self, x)
         return c
 
     devs = mesh_utils.create_device_mesh((jax.device_count(), 1))
     mesh = Mesh(devs, ['data', 'model'])
     model = Model()
     x = jnp.ones((8, 128))
     spec = nn.get_partition_spec(
-        jax.eval_shape(model.init, random.PRNGKey(0), x))
-    self.assertEqual(spec, {
-        'params': {
-            'MLP_0': {
-                'Dense_0': {
-                    'bias': PartitionSpec(),
-                    'kernel': PartitionSpec(None, 'data', 'model'),
-                },
-                'Dense_1': {
-                    'bias': PartitionSpec(),
-                    'kernel': PartitionSpec(None, 'model', 'data'),
+        jax.eval_shape(model.init, random.PRNGKey(0), x)
+    )
+    self.assertEqual(
+        spec,
+        {
+            'params': {
+                'MLP_0': {
+                    'Dense_0': {
+                        'bias': PartitionSpec(),
+                        'kernel': PartitionSpec(None, 'data', 'model'),
+                    },
+                    'Dense_1': {
+                        'bias': PartitionSpec(),
+                        'kernel': PartitionSpec(None, 'model', 'data'),
+                    },
                 },
             },
         },
-    })
+    )
     x_spec = PartitionSpec('data', 'model')
     f = lambda x: jax.sharding.NamedSharding(mesh, x)
     if jax.config.jax_enable_custom_prng:
       key_spec = PartitionSpec()
     else:
       key_spec = PartitionSpec(None)
-    init_fn = jax.jit(model.init,
-                      in_shardings=jax.tree_map(f, (key_spec, x_spec)),
-                      out_shardings=jax.tree_map(f, spec))
+    init_fn = jax.jit(
+        model.init,
+        in_shardings=jax.tree_map(f, (key_spec, x_spec)),
+        out_shardings=jax.tree_map(f, spec),
+    )
     variables = init_fn(random.PRNGKey(0), x)
-    apply_fn = jax.jit(model.apply,
-                       in_shardings=jax.tree_map(f, (spec, x_spec)),
-                       out_shardings=jax.tree_map(f, x_spec))
+    apply_fn = jax.jit(
+        model.apply,
+        in_shardings=jax.tree_map(f, (spec, x_spec)),
+        out_shardings=jax.tree_map(f, x_spec),
+    )
     y = apply_fn(variables, x)
     self.assertEqual(y.shape, (8, 128))
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/linen_module_test.py` & `flax-0.7.1/tests/linen/linen_module_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,35 +18,46 @@
 import copy
 import dataclasses
 import functools
 import gc
 import inspect
 import operator
 import sys
-from typing import (Any, Callable, Generic, Mapping, NamedTuple, Sequence,
-                    Tuple, TypeVar, get_type_hints, Optional)
+from typing import (
+    Any,
+    Callable,
+    Generic,
+    Mapping,
+    NamedTuple,
+    Optional,
+    Sequence,
+    Tuple,
+    TypeVar,
+    get_type_hints,
+)
+from unittest.mock import patch
 
 from absl.testing import absltest
 from flax import config
 from flax import errors
 from flax import linen as nn
 from flax import struct
-from flax.core import Scope, freeze, FrozenDict, tracers
-from flax.linen import compact
 from flax.configurations import temp_flip_flag
+from flax.core import FrozenDict, Scope, freeze, tracers
+from flax.linen import compact
 import jax
 from jax import random
 from jax.nn import initializers
 import jax.numpy as jnp
 import numpy as np
-from unittest.mock import patch
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
+
 def tree_equals(x, y):
   return jax.tree_util.tree_all(jax.tree_util.tree_map(operator.eq, x, y))
 
 
 @contextlib.contextmanager
 def set_config(option: str, value: bool):
   old_value = getattr(config, option)
@@ -66,53 +77,63 @@
 
 
 class Dense(nn.Module):
   features: int
 
   @compact
   def __call__(self, x):
-    kernel = self.param('kernel', initializers.lecun_normal(),
-                        (x.shape[-1], self.features))
+    kernel = self.param(
+        'kernel', initializers.lecun_normal(), (x.shape[-1], self.features)
+    )
     y = jnp.dot(x, kernel)
     return y
 
 
 class ModuleTest(absltest.TestCase):
 
   def test_init_module(self):
     rngkey = jax.random.PRNGKey(0)
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     y = DummyModule(parent=scope)(x)
     params = scope.variables()['params']
     y2 = DummyModule(parent=scope.rewound())(x)
     np.testing.assert_allclose(y, y2)
-    np.testing.assert_allclose(y, jnp.array([2.]))
-    self.assertEqual(params, {'bias': jnp.array([1.])})
+    np.testing.assert_allclose(y, jnp.array([2.0]))
+    self.assertEqual(params, {'bias': jnp.array([1.0])})
 
   def test_lazy_init(self):
-
     class Foo(nn.Module):
+
       @compact
       def __call__(self, x):
-        k = self.param("kernel", nn.initializers.lecun_normal(), (x.shape[-1], x.shape[-1]))
+        k = self.param(
+            'kernel', nn.initializers.lecun_normal(), (x.shape[-1], x.shape[-1])
+        )
         return x @ k
+
     # provide a massive input message which would OOM if any compute ops were actually executed
-    variables = Foo().lazy_init(random.PRNGKey(0), jax.ShapeDtypeStruct((1024 * 1024 * 1024, 128), jnp.float32))
-    self.assertEqual(variables["params"]["kernel"].shape, (128, 128))
+    variables = Foo().lazy_init(
+        random.PRNGKey(0),
+        jax.ShapeDtypeStruct((1024 * 1024 * 1024, 128), jnp.float32),
+    )
+    self.assertEqual(variables['params']['kernel'].shape, (128, 128))
 
   def test_lazy_init_fails_on_data_dependence(self):
     class Foo(nn.Module):
+
       @compact
       def __call__(self, x):
-        k = self.param("kernel", lambda _: x)
+        k = self.param('kernel', lambda _: x)
         return x * k
 
     with self.assertRaises(errors.LazyInitError):
-      Foo().lazy_init(random.PRNGKey(0), jax.ShapeDtypeStruct((8, 4), jnp.float32))
+      Foo().lazy_init(
+          random.PRNGKey(0), jax.ShapeDtypeStruct((8, 4), jnp.float32)
+      )
 
   def test_arg_module(self):
     rngkey = jax.random.PRNGKey(0)
     x = jnp.ones((10,))
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     y = Dense(3, parent=scope)(x)
     params = scope.variables()['params']
@@ -137,22 +158,18 @@
     x = jnp.ones((10,))
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     y = MLP(parent=scope)(x)
     params = scope.variables()['params']
     y2 = MLP(parent=scope.rewound())(x)
     np.testing.assert_allclose(y, y2)
     param_shape = jax.tree_util.tree_map(jnp.shape, params)
-    self.assertEqual(param_shape, {
-        'Dense_0': {
-            'kernel': (10, 3)
-        },
-        'Dense_1': {
-            'kernel': (3, 3)
-        }
-    })
+    self.assertEqual(
+        param_shape,
+        {'Dense_0': {'kernel': (10, 3)}, 'Dense_1': {'kernel': (3, 3)}},
+    )
 
   def test_nested_module_reuse(self):
     rngkey = jax.random.PRNGKey(0)
 
     class MLP(nn.Module):
 
       @compact
@@ -176,24 +193,23 @@
     x = jnp.ones((10,))
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     y = Top(parent=scope)(x)
     params = scope.variables()['params']
     y2 = Top(parent=scope.rewound())(x)
     np.testing.assert_allclose(y, y2)
     param_shape = jax.tree_util.tree_map(jnp.shape, params)
-    self.assertEqual(param_shape, {
-        'MLP_0': {
-            'Dense_0': {
-                'kernel': (10, 3)
-            },
-            'Dense_1': {
-                'kernel': (3, 3)
+    self.assertEqual(
+        param_shape,
+        {
+            'MLP_0': {
+                'Dense_0': {'kernel': (10, 3)},
+                'Dense_1': {'kernel': (3, 3)},
             }
-        }
-    })
+        },
+    )
 
   def test_setup_dict_assignment(self):
     rngkey = jax.random.PRNGKey(0)
 
     class MLP(nn.Module):
 
       def setup(self):
@@ -211,46 +227,38 @@
     x = jnp.ones((10,))
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     y = MLP(parent=scope)(x)
     params = scope.variables()['params']
     y2 = MLP(parent=scope.rewound())(x)
     np.testing.assert_allclose(y, y2)
     param_shape = jax.tree_util.tree_map(jnp.shape, params)
-    self.assertEqual(param_shape, {
-        'lyrs1_a': {
-            'kernel': (10, 3)
-        },
-        'lyrs1_b': {
-            'kernel': (3, 3)
-        }
-    })
+    self.assertEqual(
+        param_shape,
+        {'lyrs1_a': {'kernel': (10, 3)}, 'lyrs1_b': {'kernel': (3, 3)}},
+    )
 
   def test_setup_dict_nonstring_keys(self):
-
     class Foo(nn.Module):
 
       def setup(self):
         self.a = {(1, 2): nn.Dense(2)}  # Tuple as key.
 
       @nn.compact
       def __call__(self, x):
         return self.a[(1, 2)](x)
 
     foo = Foo()
     x = jnp.ones(shape=(1, 3))
     params = foo.init(random.PRNGKey(0), x)['params']
     param_shape = jax.tree_util.tree_map(jnp.shape, params)
-    self.assertEqual(param_shape,
-                     {'a_(1, 2)': {
-                         'kernel': (3, 2),
-                         'bias': (2,)
-                     }})
+    self.assertEqual(
+        param_shape, {'a_(1, 2)': {'kernel': (3, 2), 'bias': (2,)}}
+    )
 
   def test_setup_cloning(self):
-
     class MLP(nn.Module):
 
       def setup(self):
         self.dense = Dense(3)
 
     scope = Scope({})
     unused_clone = MLP(parent=scope).clone()
@@ -296,33 +304,33 @@
 
       def setup(self):
         self.bias = self.param('bias', initializers.ones, self.xshape)
 
       def __call__(self, x):
         return x + self.bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     y = DummyModuleWithoutCompact(x.shape, parent=scope)(x)
     params = scope.variables()['params']
     y2 = DummyModuleWithoutCompact(x.shape, parent=scope.rewound())(x)
     np.testing.assert_allclose(y, y2)
-    np.testing.assert_allclose(y, jnp.array([2.]))
-    self.assertEqual(params, {'bias': jnp.array([1.])})
+    np.testing.assert_allclose(y, jnp.array([2.0]))
+    self.assertEqual(params, {'bias': jnp.array([1.0])})
 
   def test_init_outside_setup_without_compact(self):
     rngkey = jax.random.PRNGKey(0)
 
     class DummyModuleWithoutCompact(nn.Module):
 
       def __call__(self, x):
         bias = self.param('bias', initializers.ones, x.shape)
         return x + bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     with self.assertRaisesRegex(ValueError, 'must be initialized.*setup'):
       unused_y = DummyModuleWithoutCompact(parent=scope)(x)
 
   def test_init_outside_call(self):
     rngkey = jax.random.PRNGKey(0)
 
@@ -333,15 +341,15 @@
         bias = self.param('bias', initializers.ones, x.shape)
         return x + bias
 
       def foo(self, x):
         bias = self.param('bias', initializers.ones, x.shape)
         return x + bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     with self.assertRaisesRegex(ValueError, 'must be initialized.*setup'):
       unused_y = Dummy(parent=scope).foo(x)
 
   def test_setup_call_var_collision(self):
     rngkey = jax.random.PRNGKey(0)
 
@@ -352,15 +360,15 @@
         self.bias = self.param('bias', initializers.ones, self.xshape)
 
       @compact
       def __call__(self, x):
         unused_bias = self.param('bias', initializers.ones, x.shape)
         return x + self.bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     msg = 'Could not create param "bias" in Module Dummy: Name in use'
     with self.assertRaisesRegex(errors.NameInUseError, msg):
       unused_y = Dummy(x.shape, parent=scope)(x)
 
   def test_call_var_collision(self):
     rngkey = jax.random.PRNGKey(0)
@@ -370,15 +378,15 @@
 
       @compact
       def __call__(self, x):
         bias = self.param('bias', initializers.ones, self.xshape)
         bias = self.param('bias', initializers.ones, self.xshape)
         return x + bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     msg = 'Could not create param "bias" in Module Dummy: Name in use'
     with self.assertRaisesRegex(errors.NameInUseError, msg):
       unused_y = Dummy(x.shape, parent=scope)(x)
 
   def test_setup_var_collision(self):
     rngkey = jax.random.PRNGKey(0)
@@ -389,15 +397,15 @@
       def setup(self):
         self.bias = self.param('bias', initializers.ones, self.xshape)
         self.bias = self.param('bias', initializers.ones, self.xshape)
 
       def __call__(self, x):
         return x + self.bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     msg = 'Could not create param "bias" in Module Dummy: Name in use'
     with self.assertRaisesRegex(errors.NameInUseError, msg):
       unused_y = Dummy(x.shape, parent=scope)(x)
 
   def test_setattr_name_var_disagreement_allowed_in_lists(self):
     rngkey = jax.random.PRNGKey(0)
@@ -410,18 +418,18 @@
             self.param(f'bias_{i}', initializers.ones, self.xshape)
             for i in range(4)
         ]
 
       def __call__(self, x):
         return x + self.biases[0]
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     y = Dummy(x.shape, parent=scope)(x)
-    self.assertEqual(y, jnp.array([2.]))
+    self.assertEqual(y, jnp.array([2.0]))
 
   def test_setattr_name_var_disagreement_allowed_in_dicts(self):
     rngkey = jax.random.PRNGKey(0)
 
     class Dummy(nn.Module):
       xshape: Tuple[int, ...]
 
@@ -435,33 +443,33 @@
             str(i): self.param(f'bias_{i}', initializers.ones, self.xshape)
             for i in range(4)
         }
 
       def __call__(self, x):
         return x + self.biases['0']
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
     y = Dummy(x.shape, parent=scope)(x)
-    self.assertEqual(y, jnp.array([2.]))
+    self.assertEqual(y, jnp.array([2.0]))
 
   def test_submodule_var_collision_with_scope(self):
     rngkey = jax.random.PRNGKey(0)
 
     class Dummy(nn.Module):
       xshape: Tuple[int, ...]
 
       def setup(self):
         self.bias = self.param('bias', initializers.ones, self.xshape)
         self.bias = DummyModule()
 
       def __call__(self, x):
         return x + self.bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
 
     with self.assertRaises(errors.NameInUseError):
       unused_y = Dummy(x.shape, parent=scope)(x)
 
   def test_submodule_var_collision_with_submodule(self):
     rngkey = jax.random.PRNGKey(0)
@@ -473,15 +481,15 @@
         self.bias = self.param('bias', initializers.ones, self.xshape)
 
       @compact
       def __call__(self, x):
         unused_bias = DummyModule(name='bias')
         return x + self.bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
 
     msg = 'Could not create submodule "bias" in Module Dummy: Name in use'
     with self.assertRaisesRegex(errors.NameInUseError, msg):
       unused_y = Dummy(x.shape, parent=scope)(x)
 
   def test_submodule_var_collision_with_params(self):
@@ -494,23 +502,22 @@
         self.bias = DummyModule()
 
       @compact
       def __call__(self, x):
         unused_bias = self.param('bias', initializers.ones, self.xshape)
         return x + self.bias
 
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
     scope = Scope({}, {'params': rngkey}, mutable=['params'])
 
     msg = 'Could not create param "bias" in Module Dummy: Name in use'
     with self.assertRaisesRegex(errors.NameInUseError, msg):
       unused_y = Dummy(x.shape, parent=scope)(x)
 
   def test_attr_empty_container(self):
-
     class Foo(nn.Module):
       bar: Mapping[str, Any]
 
       @compact
       def __call__(self):
         pass
 
@@ -527,15 +534,14 @@
           pass
 
         @compact
         def call2(self):
           pass
 
   def test_only_one_compact_method_subclass(self):
-
     class Dummy(nn.Module):
 
       @nn.compact
       def __call__(self):
         pass
 
     class SubDummy(Dummy):
@@ -548,15 +554,14 @@
 
     subdummy = SubDummy(parent=scope)
     # Make sure the @compact annotation is valid on both base class and
     # subclass, as long as its on the same method.
     subdummy()
 
   def test_forgotten_compact_annotation(self):
-
     class Bar(nn.Module):
 
       # user forgot to add @compact
       def __call__(self, x):
         return nn.Dense(1)(x)
 
     class Foo(nn.Module):
@@ -564,57 +569,58 @@
       @nn.compact
       def __call__(self, x):
         bar = Bar()
         x = bar(x)
         x = bar(x)
         return x
 
-    msg = (r'Submodule Dense must be defined in `setup\(\)` or in a method '
-           'wrapped in `@compact`')
+    msg = (
+        r'Submodule Dense must be defined in `setup\(\)` or in a method '
+        'wrapped in `@compact`'
+    )
     with self.assertRaisesRegex(errors.AssignSubModuleError, msg):
       Foo().init(random.PRNGKey(0), jnp.ones((1, 3)))
 
   def test_forgotten_compact_annotation_with_explicit_parent(self):
-
     class Bar(nn.Module):
 
       def __call__(self, x):
         return nn.Dense(1, parent=self)(x)
 
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         bar = Bar()
         x = bar(x)
         x = bar(x)
         return x
 
-    msg = (r'Submodule Dense must be defined in `setup\(\)` or in a method '
-           'wrapped in `@compact`')
+    msg = (
+        r'Submodule Dense must be defined in `setup\(\)` or in a method '
+        'wrapped in `@compact`'
+    )
     with self.assertRaisesRegex(errors.AssignSubModuleError, msg):
       Foo().init(random.PRNGKey(0), jnp.ones((1, 3)))
 
   def test_numpy_array_shape_class_args(self):
-
     class MLP(nn.Module):
       widths: Sequence[int]
 
       @nn.compact
       def __call__(self, x):
         for width in self.widths[:-1]:
           x = nn.relu(nn.Dense(width)(x))
         return nn.Dense(self.widths[-1])(x)
 
     test = MLP(np.array([3, 3], np.int32))
     params = test.init({'params': random.PRNGKey(42)}, jnp.ones((3, 3)))
     _ = test.apply(params, jnp.ones((3, 3)))
 
   def test_get_local_methods(self):
-
     class Base:
 
       @staticmethod
       def bar(x):
         return x
 
       @classmethod
@@ -639,19 +645,19 @@
 
     class Derived2(Derived1):
       pass
 
     self.assertEqual(nn.module._get_local_method_names(Base), ('bleep',))
     self.assertEqual(nn.module._get_local_method_names(Derived1), ('bloop',))
     self.assertEqual(
-        nn.module._get_local_method_names(Derived1, exclude=('bloop',)), ())
+        nn.module._get_local_method_names(Derived1, exclude=('bloop',)), ()
+    )
     self.assertEqual(nn.module._get_local_method_names(Derived2), ())
 
   def test_inheritance_dataclass_attribs(self):
-
     class Test(nn.Module):
       bar: int
 
       def __call__(self, x):
         return x
 
     class Test2(Test):
@@ -693,46 +699,50 @@
     self.assertTrue(hasattr(test3, 'name'))
     self.assertTrue(hasattr(test3, 'parent'))
     self.assertTrue(hasattr(test4, 'bar'))
     self.assertTrue(hasattr(test4, 'baz'))
     self.assertTrue(hasattr(test4, 'name'))
     self.assertTrue(hasattr(test4, 'parent'))
     self.assertEqual(
-        list(Test.__dataclass_fields__.keys()), ['bar', 'parent', 'name'])
+        list(Test.__dataclass_fields__.keys()), ['bar', 'parent', 'name']
+    )
     self.assertEqual(
         list(Test2.__dataclass_fields__.keys()),
-        ['bar', 'baz', 'parent', 'name'])
+        ['bar', 'baz', 'parent', 'name'],
+    )
     self.assertEqual(
         list(Test3.__dataclass_fields__.keys()),
-        ['bar', 'baz', 'parent', 'name'])
+        ['bar', 'baz', 'parent', 'name'],
+    )
     self.assertEqual(
         list(Test4.__dataclass_fields__.keys()),
-        ['bar', 'baz', 'parent', 'name'])
+        ['bar', 'baz', 'parent', 'name'],
+    )
 
   def test_get_suffix_value_pairs(self):
     for x in [(), [], {}, None, 0, set()]:
       self.assertEqual(nn.module._get_suffix_value_pairs(x), [('', x)])
     self.assertEqual(
-        nn.module._get_suffix_value_pairs({
-            'a': 1,
-            'b': 2
-        }), [('_a', 1), ('_b', 2)])
+        nn.module._get_suffix_value_pairs({'a': 1, 'b': 2}),
+        [('_a', 1), ('_b', 2)],
+    )
     self.assertEqual(
-        nn.module._get_suffix_value_pairs([1, 2, 3]), [('_0', 1), ('_1', 2),
-                                                       ('_2', 3)])
+        nn.module._get_suffix_value_pairs([1, 2, 3]),
+        [('_0', 1), ('_1', 2), ('_2', 3)],
+    )
     x1 = [nn.Dense(10), nn.relu, nn.Dense(10)]
     y1 = nn.module._get_suffix_value_pairs(x1)
     self.assertEqual(y1, [('_0', x1[0]), ('_1', x1[1]), ('_2', x1[2])])
     x2 = {'a': 1, 'b': {'c': nn.Dense(10), 'd': nn.relu}}
     y2 = nn.module._get_suffix_value_pairs(x2)
-    self.assertEqual(y2, [('_a', 1), ('_b_c', x2['b']['c']),
-                          ('_b_d', x2['b']['d'])])
+    self.assertEqual(
+        y2, [('_a', 1), ('_b_c', x2['b']['c']), ('_b_d', x2['b']['d'])]
+    )
 
   def test_mixed_list_assignment_in_setup(self):
-
     class Test(nn.Module):
 
       def setup(self):
         self.layers = [nn.Dense(10), nn.relu, nn.Dense(10)]
 
       def __call__(self, x):
         for lyr in self.layers:
@@ -750,15 +760,14 @@
     module_a = nn.Dense(10)
     module_a_2 = nn.Dense(10)
     module_b = nn.Dense(5)
     self.assertEqual(hash(module_a), hash(module_a_2))
     self.assertNotEqual(hash(module_a), hash(module_b))
 
   def test_module_custom_hash(self):
-
     class Test(nn.Module):
       x: int = 3
       y: int = 5
 
       def __hash__(self):
         return 42 + self.x
 
@@ -766,20 +775,19 @@
     module_a_2 = Test(1, 5)
     module_b = Test(2, 2)
     self.assertEqual(hash(module_a), hash(module_a_2))
     self.assertNotEqual(hash(module_a), hash(module_b))
 
   def test_module_with_scope_is_not_hashable(self):
     module_a = nn.Dense(10, parent=Scope({}))
-    msg = 'Can\'t call __hash__ on modules that hold variables.'
+    msg = "Can't call __hash__ on modules that hold variables."
     with self.assertRaisesWithLiteralMatch(TypeError, msg):
       hash(module_a)
 
   def test_module_trace(self):
-
     class MLP(nn.Module):
       act: Callable = nn.relu
       sizes: Sequence[int] = (3, 2)
 
       @nn.compact
       def __call__(self, x):
         for size in self.sizes:
@@ -819,15 +827,14 @@
     x = jnp.ones((1, 2))
     trace, variables = mlp.init_with_output(random.PRNGKey(0), x)
     self.assertEqual(trace, expected_trace)
     trace = mlp.apply(variables, x)
     self.assertEqual(trace, expected_trace)
 
   def test_module_apply_method(self):
-
     class Foo(nn.Module):
       not_callable: int = 1
 
       @nn.compact
       def __call__(self):
         pass
 
@@ -851,94 +858,90 @@
 
     # string method names are also allowed.
     Foo().apply({}, method='test')
     # test same for init.
     Foo().init({}, method='test')
 
     # non-existent attribute names will yield AttributeError.
-    with self.assertRaisesRegex(AttributeError, "allowed_apply_fn"):
+    with self.assertRaisesRegex(AttributeError, 'allowed_apply_fn'):
       Foo().apply({}, method='allowed_apply_fn')
       # test same for init.
       Foo().init({}, method='allowed_apply_fn')
 
     # attributes which are not callables yield TypeError.
-    with self.assertRaisesRegex(TypeError, "'Foo.not_callable' must be a callable"):
+    with self.assertRaisesRegex(
+        TypeError, "'Foo.not_callable' must be a callable"
+    ):
       Foo().apply({}, method='not_callable')
       # test same for init.
       Foo().init({}, method='not_callable')
 
   def test_call_unbound_compact_module_methods(self):
     dense = Dense(3)
     msg = r'Can\'t call compact methods on unbound modules'
     with self.assertRaisesRegex(errors.CallCompactUnboundModuleError, msg):
       dense(jnp.ones((1,)))
 
   def test_call_unbound_has_variable(self):
-
     class EmptyModule(nn.Module):
 
       def foo(self):
         self.has_variable('bar', 'baz')
 
     empty = EmptyModule()
     with self.assertRaisesRegex(ValueError, 'variable.*unbound module'):
       empty.foo()
 
   def test_call_unbound_make_rng(self):
-
     class EmptyModule(nn.Module):
 
       def foo(self):
         self.make_rng('bar')
 
     empty = EmptyModule()
     with self.assertRaisesRegex(ValueError, 'RNGs.*unbound module'):
       empty.foo()
 
   def test_call_unbound_variables(self):
-
     class EmptyModule(nn.Module):
 
       def foo(self):
         self.variables
 
     empty = EmptyModule()
     with self.assertRaisesRegex(ValueError, 'variables.*unbound module'):
       empty.foo()
 
   def test_call_unbound_noncompact_module_methods(self):
-
     class EmptyModule(nn.Module):
       foo: int = 3
 
       def bar(self):
         return self.foo
 
     empty = EmptyModule()
     # It's fine to call methods of unbound methods that don't depend on
     # attributes defined during `setup`.
     self.assertEqual(empty.bar(), 3)
 
   def test_call_unbound_noncompact_module_methods_depending_on_setup(self):
-
     class EmptyModule(nn.Module):
 
       def setup(self):
         self.foo = 2
 
       def bar(self):
         return self.foo
 
     empty = EmptyModule()
     msg = r'"EmptyModule" object has no attribute "foo"'
     with self.assertRaisesRegex(AttributeError, msg):
       empty.bar()
 
   def test_module_with_attrs(self):
-
     class Foo(nn.Module):
       bar: nn.Dense = dataclasses.field(init=False)
 
       def setup(self):
         self.bar = nn.Dense(3)
 
       def __call__(self, x):
@@ -946,77 +949,77 @@
 
     foo = Foo()
     x = jnp.ones((2,))
     variables = foo.init(random.PRNGKey(0), x)
     self.assertEqual(variables['params']['bar']['kernel'].shape, (2, 3))
 
   def test_noncompact_module_frozen(self):
-
     class Foo(nn.Module):
 
       def setup(self):
         self.i = 1  # This is allowed (for assigning submodules).
 
       def __call__(self):
         self.i = 2  # This is not allowed.
 
-    msg = ('Can\'t set i=2 for Module of type Foo: Module instance is frozen '
-           'outside of setup method.')
+    msg = (
+        "Can't set i=2 for Module of type Foo: Module instance is frozen "
+        'outside of setup method.'
+    )
     with self.assertRaisesRegex(errors.SetAttributeFrozenModuleError, msg):
       Foo().init(random.PRNGKey(0))
 
   def test_compact_module_frozen(self):
-
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self):
         self.i = 2
 
-    msg = ('Can\'t set i=2 for Module of type Foo: Module instance is frozen '
-           'outside of setup method.')
+    msg = (
+        "Can't set i=2 for Module of type Foo: Module instance is frozen "
+        'outside of setup method.'
+    )
     with self.assertRaisesRegex(errors.SetAttributeFrozenModuleError, msg):
       Foo().init(random.PRNGKey(0))
 
   def test_submodule_frozen(self):
-
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self):
         dense = nn.Dense(10)
         dense.features = 20  # <--- This is not allowed
 
-    msg = ('Can\'t set features=20 for Module of type Dense: Module instance '
-           'is frozen outside of setup method.')
+    msg = (
+        "Can't set features=20 for Module of type Dense: Module instance "
+        'is frozen outside of setup method.'
+    )
     with self.assertRaisesRegex(errors.SetAttributeFrozenModuleError, msg):
       Foo().init(random.PRNGKey(0))
 
   def test_module_call_not_implemented(self):
-
     class Foo(nn.Module):
       pass
 
     msg = '"Foo" object has no attribute "__call__"'
     with self.assertRaisesRegex(AttributeError, msg):
       Foo().init(random.PRNGKey(0))
 
   def test_is_mutable_collection(self):
-
     class EmptyModule(nn.Module):
 
       def __call__(self):
         return self.is_mutable_collection('test')
 
     empty = EmptyModule()
     self.assertTrue(empty.apply({}, mutable=['test'])[0])
     self.assertFalse(empty.apply({}, mutable=False))
 
   def test_module_lazy_getattr_setup(self):
-
     class A(nn.Module):
 
       def setup(self):
         self.d = nn.Dense(2)
 
       def __call__(self, x):
         return self.d(x)
@@ -1034,15 +1037,14 @@
     key = random.PRNGKey(0)
     x = jnp.ones((2,))
 
     (y1, y2), unused_vars = B().init_with_output(key, x)
     np.testing.assert_array_equal(y1, y2)
 
   def test_module_lazy_dir_setup(self):
-
     class A(nn.Module):
 
       def setup(self):
         self.d = nn.Dense(2)
 
       def __call__(self, x):
         return self.d(x)
@@ -1059,15 +1061,14 @@
         return y1, y2
 
     key = random.PRNGKey(0)
     x = jnp.ones((2,))
     _ = B().init_with_output(key, x)
 
   def test_module_unbound_getattr(self):
-
     class A(nn.Module):
 
       def setup(self):
         b = B()
         b.c  # B is unbound because it is not yet assigned to an attribute.
         self.b = b
 
@@ -1094,17 +1095,15 @@
 
       def test(self):
         pass
 
     A().test()
     self.assertFalse(setup_called)
 
-  @temp_flip_flag('return_frozendict', False)
   def test_module_pass_as_attr(self):
-
     class A(nn.Module):
 
       def setup(self):
         self.b = B(nn.Dense(2))
 
       def __call__(self, x):
         return self.b(x)
@@ -1125,15 +1124,14 @@
                     'kernel': (1, 2),
                 }
             },
         },
     }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
 
-  @temp_flip_flag('return_frozendict', False)
   def test_module_pass_in_closure(self):
     a = nn.Dense(2)
 
     class B(nn.Module):
 
       def setup(self):
         self.foo = a
@@ -1150,17 +1148,15 @@
                 'kernel': (1, 2),
             }
         },
     }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
     self.assertIsNone(a.name)
 
-  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_adoption(self):
-
     class Encoder(nn.Module):
       n_layers: int
       ch: int
 
       def setup(self):
         self.layers = [nn.Dense(self.ch) for _ in range(self.n_layers)]
 
@@ -1206,17 +1202,15 @@
                     'kernel': (4, 8),
                 },
             },
         },
     }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
 
-  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_adoption_pytree(self):
-
     class A(nn.Module):
 
       @nn.compact
       def __call__(self, c, x):
         counter = self.variable('counter', 'i', jnp.zeros, ())
         counter.value += 1
         x = nn.Dense(1)(x)
@@ -1248,17 +1242,20 @@
             },
         },
     }
     self.assertTrue(
         jax.tree_util.tree_all(
             jax.tree_util.tree_map(
                 lambda x, y: np.testing.assert_allclose(x, y, atol=1e-7),
-                counters, ref_counters)))
+                counters,
+                ref_counters,
+            )
+        )
+    )
 
-  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_adoption_sharing(self):
     dense = functools.partial(nn.Dense, use_bias=False)
 
     class A(nn.Module):
 
       @nn.compact
       def __call__(self, x):
@@ -1301,15 +1298,14 @@
                     'kernel': (2, 2),
                 },
             },
         },
     }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
 
-  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_named_submodule_adoption(self):
     dense = functools.partial(nn.Dense, use_bias=False)
 
     class A(nn.Module):
 
       def setup(self):
         self.dense = dense(4)
@@ -1356,17 +1352,15 @@
               'proj': {
                   'kernel': (4, 6),
               },
           },
       }
     self.assertTrue(tree_equals(var_shapes, ref_var_shapes))
 
-  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_pytree_adoption_sharing(self):
-
     class A(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         counter = self.variable('counter', 'i', jnp.zeros, ())
         counter.value += 1
         x = nn.Dense(1)(x)
@@ -1394,30 +1388,28 @@
                 'i': jnp.array(6.0),
             },
         },
     }
     self.assertTrue(tree_equals(counters, ref_counters))
 
   def test_inner_class_def(self):
-
     class X(nn.Module):
 
       class Hyper(struct.PyTreeNode):
         a: int
 
       hyper: Hyper
 
       @nn.compact
       def __call__(self, x):
         return x + 1
 
     self.assertIsInstance(X.Hyper(a=1), X.Hyper)
 
   def test_sow(self):
-
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, x, **sow_args):
         self.sow('intermediates', 'h', x, **sow_args)
         self.sow('intermediates', 'h', 2 * x, **sow_args)
         return 3 * x
@@ -1427,24 +1419,25 @@
     self.assertNotIn('intermediates', variables)
     # ...unless we override mutable.
     variables = Foo().init(random.PRNGKey(0), 1, mutable=True)
     self.assertEqual(variables, {'intermediates': {'h': (1, 2)}})
 
     _, state = Foo().apply({}, 1, mutable=['intermediates'])
     self.assertEqual(state, {'intermediates': {'h': (1, 2)}})
-    _, state = Foo().apply({},
-                           1,
-                           init_fn=lambda: 0,
-                           reduce_fn=lambda a, b: a + b,
-                           mutable=['intermediates'])
+    _, state = Foo().apply(
+        {},
+        1,
+        init_fn=lambda: 0,
+        reduce_fn=lambda a, b: a + b,
+        mutable=['intermediates'],
+    )
     self.assertEqual(state, {'intermediates': {'h': 3}})
     self.assertEqual(Foo().apply({}, 1), 3)
 
   def test_capture_intermediates(self):
-
     class Bar(nn.Module):
 
       def test(self, x):
         return x + 1
 
     class Foo(nn.Module):
 
@@ -1455,15 +1448,14 @@
     _, state = Foo().apply({}, 1, capture_intermediates=True)
     self.assertEqual(state, {'intermediates': {'__call__': (3,)}})
     fn = lambda mdl, _: isinstance(mdl, Bar)
     _, state = Foo().apply({}, 1, capture_intermediates=fn)
     self.assertEqual(state, {'intermediates': {'Bar_0': {'test': (2,)}}})
 
   def test_perturb(self):
-
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         x = nn.Dense(10)(x)
         x = self.perturb('before_multiply', x)
         x = 4 * x
@@ -1474,23 +1466,25 @@
       variables = {'params': params, 'perturbations': perturbations}
       preds = Foo().apply(variables, inputs)
       return jnp.square(preds - targets).mean()
 
     x = jax.random.uniform(jax.random.PRNGKey(1), shape=(10,))
     y = jax.random.uniform(jax.random.PRNGKey(2), shape=(10,))
     variables = Foo().init(jax.random.PRNGKey(0), x)
-    intm_grads = jax.grad(
-        loss, argnums=1)(variables['params'], variables['perturbations'], x, y)
+    intm_grads = jax.grad(loss, argnums=1)(
+        variables['params'], variables['perturbations'], x, y
+    )
     # activation * 4 so reverse gradient also * 4
     self.assertTrue(
-        all(intm_grads['after_multiply'] * 4 == intm_grads['before_multiply']))
+        all(intm_grads['after_multiply'] * 4 == intm_grads['before_multiply'])
+    )
 
   def test_perturb_noop(self):
-
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         x = nn.Dense(10)(x)
         x = self.perturb('before_multiply', x)
         x = 4 * x
         x = self.perturb('after_multiply', x)
         return x
@@ -1501,22 +1495,23 @@
     params = variables['params']
     perturbations = variables['perturbations']
 
     # check no error if perturbations is not passed
     module.apply({'params': params}, x)
 
     # check errors if perturbations is passed but empty
-    with self.assertRaisesRegex(errors.ScopeCollectionNotFound, 'Tried to access'):
+    with self.assertRaisesRegex(
+        errors.ScopeCollectionNotFound, 'Tried to access'
+    ):
       module.apply({'params': params, 'perturbations': {}}, x)
 
     # check no error if perturbations is passed and not empty
     module.apply({'params': params, 'perturbations': perturbations}, x)
 
   def test_functional_apply(self):
-
     class Foo(nn.Module):
 
       def setup(self):
         self.a = nn.Dense(3)
         self.b = nn.Dense(1)
 
     def f(foo, x):
@@ -1528,15 +1523,14 @@
     f_init = nn.init_with_output(f, foo)
     f_apply = nn.apply(f, foo)
     y1, variables = f_init(random.PRNGKey(0), x)
     y2 = f_apply(variables, x)
     self.assertEqual(y1, y2)
 
   def test_bind(self):
-
     class Foo(nn.Module):
 
       def setup(self):
         self.a = nn.Dense(3)
         self.b = nn.Dense(1)
 
     def f(foo, x):
@@ -1547,15 +1541,14 @@
     x = jnp.ones((4,))
     f_init = nn.init_with_output(f, foo)
     y1, variables = f_init(random.PRNGKey(0), x)
     y2 = f(foo.bind(variables), x)
     self.assertEqual(y1, y2)
 
   def test_bind_stateful(self):
-
     class Foo(nn.Module):
 
       def setup(self):
         self.a = nn.Dense(3)
         self.bn = nn.BatchNorm()
         self.b = nn.Dense(1)
 
@@ -1572,20 +1565,21 @@
     y2 = f(foo_b, x)
     y3, new_state = nn.apply(f, foo, mutable='batch_stats')(variables, x)
     self.assertEqual(y1, y2)
     self.assertEqual(y2, y3)
     bs_1 = new_state['batch_stats']
     bs_2 = foo_b.variables['batch_stats']
     for x, y in zip(
-        jax.tree_util.tree_leaves(bs_1), jax.tree_util.tree_leaves(bs_2)):
+        jax.tree_util.tree_leaves(bs_1), jax.tree_util.tree_leaves(bs_2)
+    ):
       np.testing.assert_allclose(x, y)
 
   def test_unbind(self):
-
     class Foo(nn.Module):
+
       def setup(self):
         self.encoder = nn.Dense(4)
         self.decoder = nn.Dense(2)
 
       def __call__(self, x):
         x = self.encoder(x)
         return self.decoder(x)
@@ -1598,32 +1592,34 @@
     decoder, decoder_vars = foo.bind(variables).decoder.unbind()
 
     self.assertIsInstance(encoder, nn.Dense)
     self.assertEqual(encoder.features, 4)
     self.assertIsInstance(decoder, nn.Dense)
     self.assertEqual(decoder.features, 2)
 
-    np.testing.assert_equal(variables['params']['encoder'], encoder_vars['params'])
-    np.testing.assert_equal(variables['params']['decoder'], decoder_vars['params'])
+    np.testing.assert_equal(
+        variables['params']['encoder'], encoder_vars['params']
+    )
+    np.testing.assert_equal(
+        variables['params']['decoder'], decoder_vars['params']
+    )
 
   def test_passing_mutable_variables(self):
-
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         return nn.Dense(2)(x)
 
     x = jnp.ones((3,))
     variables = Foo().init(random.PRNGKey(0), x)
     y = Foo().apply(variables, x)
     self.assertEqual(y.shape, (2,))
 
   def test_super_compact(self):
-
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         return nn.Dense(4)(x)
 
     class Bar(Foo):
@@ -1635,29 +1631,24 @@
 
     k = random.PRNGKey(0)
     x = jnp.ones((4, 7))
 
     variables = Bar().init(k, x)
     shapes = jax.tree_util.tree_map(np.shape, variables['params'])
     self.assertEqual(
-        shapes, {
-            'Dense_0': {
-                'kernel': (7, 4),
-                'bias': (4,)
-            },
-            'Dense_1': {
-                'kernel': (4, 3),
-                'bias': (3,)
-            },
-        })
+        shapes,
+        {
+            'Dense_0': {'kernel': (7, 4), 'bias': (4,)},
+            'Dense_1': {'kernel': (4, 3), 'bias': (3,)},
+        },
+    )
     y = Bar().apply(variables, x)
     self.assertEqual(y.shape, (4, 3))
 
   def test_super_setup(self):
-
     class Foo(nn.Module):
 
       def setup(self):
         self.a = nn.Dense(4)
 
     class Bar(Foo):
 
@@ -1673,24 +1664,24 @@
     x = jnp.ones((4, 7))
 
     variables = Bar().init(k, x)
     y = Bar().apply(variables, x)
     self.assertEqual(y.shape, (4, 3))
 
   def test_freeze_attr(self):
-
     class Foo(NamedTuple):
       a: int
       b: int
 
     self.assertEqual(nn.module._freeze_attr([1, 2]), (1, 2))
     xs = nn.module._freeze_attr(Foo(1, 2))
     self.assertEqual(xs, (1, 2))
-    self.assertEqual(type(xs),
-                     Foo)  # equality test for NamedTuple doesn't check class!
+    self.assertEqual(
+        type(xs), Foo
+    )  # equality test for NamedTuple doesn't check class!
 
   def test_generic_multiple_inheritance(self):
     T = TypeVar('T')
 
     class MyComponent(nn.Module, Generic[T]):
       pass
 
@@ -1703,20 +1694,20 @@
     class MyModule2(nn.Module):
       submodule: MyComponent2[jnp.ndarray]
 
   def test_jit_rng_equivalance(self):
     model = nn.Dense(1, use_bias=False)
     jit_model = nn.jit(nn.Dense)(1, use_bias=False)
     param = model.init(random.PRNGKey(0), np.ones((1, 1)))['params']['kernel']
-    param_2 = jit_model.init(random.PRNGKey(0), np.ones(
-        (1, 1)))['params']['kernel']
+    param_2 = jit_model.init(random.PRNGKey(0), np.ones((1, 1)))['params'][
+        'kernel'
+    ]
     self.assertEqual(param, param_2)
 
   def test_rng_reuse_after_rewind(self):
-
     class C(nn.Module):
 
       @nn.compact
       def __call__(self):
         # Some module that has dropouts in it, in general,
         # it does more than just dropout!
         return self.make_rng('dropout')
@@ -1739,15 +1730,14 @@
         return jnp.all(x0 == x1)
 
     k = random.PRNGKey(0)
     rng_equals = B().apply({}, rngs={'dropout': k})
     self.assertFalse(rng_equals)
 
   def test_module_get_put_has_variable(self):
-
     class A(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         self.put_variable('test_col', 'a', x)
         assert self.has_variable('test_col', 'a')
         return self.get_variable('test_col', 'a')
@@ -1758,15 +1748,21 @@
         self.put_variable('test_col', 'a', x)
         assert self.has_variable('test_col', 'a')
         return self.get_variable('test_col', 'a')
 
     class C(nn.Module):
 
       def setup(self):
-        self.put_variable('test_col', 'a', jnp.ones(2,))
+        self.put_variable(
+            'test_col',
+            'a',
+            jnp.ones(
+                2,
+            ),
+        )
         assert self.has_variable('test_col', 'a')
 
       def __call__(self):
         return self.get_variable('test_col', 'a')
 
     x = jnp.ones((2,))
 
@@ -1799,15 +1795,14 @@
       def __call__(self) -> None:
         pass
 
     rngs = {}
     D().init(rngs)
 
   def test_modifying_attribs_in_post_init(self):
-
     class Foo(nn.Module):
       love: int = 99
 
       def __post_init__(self):
         self.hate = 100 - self.love
         super().__post_init__()
 
@@ -1822,63 +1817,61 @@
         self.love = 101
         super().__post_init__()
 
     bar = Bar()
     self.assertEqual(bar.love, 101)
 
   def test_has_rng(self):
-
     class Foo(nn.Module):
 
       def __call__(self):
         return self.has_rng('bar')
 
     foo = Foo()
     with self.assertRaisesRegex(ValueError, 'RNGs.*unbound module'):
       foo()
     k = random.PRNGKey(0)
     self.assertTrue(foo.apply({}, rngs={'bar': k}))
     self.assertFalse(foo.apply({}, rngs={'baz': k}))
 
   def test_is_initializing(self):
-
     class Foo(nn.Module):
 
       def __call__(self):
         return self.is_initializing()
 
     foo = Foo()
     k = random.PRNGKey(0)
     self.assertTrue(foo.init_with_output(k)[0])
     self.assertFalse(foo.apply({}))
 
   def test_throws_invalid_instance_module_error(self):
-
     class B(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         return x
 
     k = random.PRNGKey(0)
     x = random.uniform(random.PRNGKey(1), (2,))
 
     with self.assertRaises(errors.InvalidInstanceModuleError):
       B.init(k, x)  # B is module class, not B() a module instance
     with self.assertRaises(errors.InvalidInstanceModuleError):
       B.init_with_output(k, x)
     with self.assertRaises(errors.InvalidInstanceModuleError):
-      B.apply({},
-              x)  # similar issue w. apply called on class instead of instance.
+      B.apply(
+          {}, x
+      )  # similar issue w. apply called on class instead of instance.
     with self.assertRaises(errors.InvalidInstanceModuleError):
-      B.bind({},
-             x)  # similar issue w. apply called on class instead of instance.
+      B.bind(
+          {}, x
+      )  # similar issue w. apply called on class instead of instance.
 
   def test_throws_incorrect_post_init_override_error(self):
-
     class A(nn.Module):
       x: float
 
       def __post_init__(self):
         self.x_square = self.x**2
 
       @nn.compact
@@ -1895,39 +1888,37 @@
     unspecified_parent = parent_parameter.default
 
     self.assertIs(unspecified_parent, copy.copy(unspecified_parent))
 
     self.assertIs(unspecified_parent, copy.deepcopy(unspecified_parent))
 
   def test_type_hints(self):
-
     class Network(nn.Module):
       layers: int
 
     type_hints = get_type_hints(Network)
     self.assertEqual(type_hints['layers'], int)
 
   def test_incorrect_property(self):
-
     class Foo(nn.Module):
 
       @property
       def prop(self):
         return self.non_existent
 
       def __call__(self):
         return self.prop
 
     foo = Foo()
-    with self.assertRaisesRegex(errors.DescriptorAttributeError,
-                                'Trying to access a property that'):
+    with self.assertRaisesRegex(
+        errors.DescriptorAttributeError, 'Trying to access a property that'
+    ):
       foo.apply({})
 
   def test_custom_descriptor(self):
-
     class Descriptor:
 
       def __get__(self, obj, objtype=None):
         return 10
 
     class Foo(nn.Module):
       prop = Descriptor()
@@ -1936,29 +1927,29 @@
         return self.prop
 
     foo = Foo()
     res = foo.apply({})
     self.assertEqual(res, 10)
 
   def test_custom_descriptor_error(self):
-
     class Descriptor:
 
       def __get__(self, obj, objtype=None):
         return obj.non_existent
 
     class Foo(nn.Module):
       prop = Descriptor()
 
       def __call__(self):
         return self.prop
 
     foo = Foo()
-    with self.assertRaisesRegex(errors.DescriptorAttributeError,
-                                'Trying to access a property that'):
+    with self.assertRaisesRegex(
+        errors.DescriptorAttributeError, 'Trying to access a property that'
+    ):
       foo.apply({})
 
   def test_nested_external_modules(self):
     class Baz(nn.Module):
       a: int
 
       def setup(self):
@@ -1970,62 +1961,69 @@
     class Bar(nn.Module):
       baz: Baz
 
       def __call__(self, x):
         return self.baz(x)
 
     class Foo(nn.Module):
+
       def setup(self):
         self.bar = Bar(baz=Baz(a=1))
 
       def __call__(self, x):
         return self.bar.baz(x)
 
     module = Foo()
     y, variables = module.init_with_output(jax.random.PRNGKey(0), 1)
     self.assertEqual(y, 3)
 
   def test_getattribute_triggers_setup(self):
     class B(nn.Module):
+
       def setup(self):
         self.p1 = self.param('p1', lambda k: jnp.ones((2,)))
+
       def fn1(self, x):
         return self.p1 + x
+
     class A(nn.Module):
       b: nn.Module
+
       def __call__(self, x):
         return self.b.fn1(x)
+
     a = A(b=B())
     k = random.PRNGKey(0)
     x = jnp.zeros((2,))
-    vs = nn.init(lambda a,x: a(x), a)(k, x)
-    y = nn.apply(lambda a,x: a.b.fn1(x), a)(vs, x)
+    vs = nn.init(lambda a, x: a(x), a)(k, x)
+    y = nn.apply(lambda a, x: a.b.fn1(x), a)(vs, x)
     np.testing.assert_array_equal(y, jnp.ones((2,)))
 
   def test_nested_sequential_in_call(self):
     class Foo(nn.Module):
+
       def setup(self):
         self.seq = nn.Sequential([nn.Dense(10) for i in range(10)])
 
       def __call__(self, x):
         # try calling only the first layer
         return self.seq.layers[0](x)
 
     module = Foo()
     variables = module.init(jax.random.PRNGKey(0), jnp.ones((1, 10)))
 
   def test_setup_called_bounded_submodules(self):
     module = nn.Sequential([
-      nn.Sequential([
-        nn.Dense(2),
+        nn.Sequential([
+            nn.Dense(2),
+            nn.relu,
+            nn.Dense(2),
+        ]),
         nn.relu,
         nn.Dense(2),
-      ]),
-      nn.relu,
-      nn.Dense(2),
     ])
     x = jnp.ones((1, 3))
     variables = module.init(jax.random.PRNGKey(0), x)
     bound_module = module.bind(variables)
 
     self.assertIsNotNone(bound_module.layers[0].layers[0].scope)
     self.assertIsNotNone(bound_module.layers[0].layers[2].scope)
@@ -2045,15 +2043,14 @@
       bars: Sequence[Bar]
 
       def __call__(self, x):
         for bar in self.bars:
           x = bar(x)
         return x
 
-
     module = Foo(bars=[])
     module.bars = [Bar(a=1)]
 
     variables = module.init(jax.random.PRNGKey(0), jnp.ones(()))
     bound_module = module.bind(variables)
 
     bar1 = bound_module.bars[0]
@@ -2089,54 +2086,62 @@
         return y, bar_vars
 
     # create foo
     module = Foo()
 
     # run foo
     (y, bar_vars), variables = module.init_with_output(
-      jax.random.PRNGKey(0), jnp.ones(()))
+        jax.random.PRNGKey(0), jnp.ones(())
+    )
 
     self.assertIn('params', bar_vars)
 
   def test_nested_shared(self):
     class Shared(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return nn.Dense(1)(x)
 
     class Unshared(nn.Module):
       shared: nn.Module
+
       def __call__(self, x):
         return self.shared(x)
 
     class Super(nn.Module):
       a: nn.Module
       b: nn.Module
+
       def run_a(self, x):
         return self.a(x)
+
       def run_b(self, x):
         return self.b(x)
+
       def __call__(self, x):
         return self.a(x) + self.b(x)
 
-
     sh = Shared()
     a = Unshared(shared=sh)
     b = Unshared(shared=sh)
     module = Super(a=a, b=b)
 
     rng = jax.random.PRNGKey(0)
-    params = module.init(rng, jnp.ones(1))["params"]
+    params = module.init(rng, jnp.ones(1))['params']
 
-    module.apply({"params": params}, jnp.ones(1))  # works as expected
-    module.apply({"params": params}, jnp.ones(1), method="run_a")  # works as expected
-    module.apply({"params": params}, jnp.ones(1), method="run_b")  # ScopeParamNotFoundError: Could not find parameter named "kernel" in scope "/b/shared/Dense_0"
+    module.apply({'params': params}, jnp.ones(1))  # works as expected
+    module.apply(
+        {'params': params}, jnp.ones(1), method='run_a'
+    )  # works as expected
+    module.apply(
+        {'params': params}, jnp.ones(1), method='run_b'
+    )  # ScopeParamNotFoundError: Could not find parameter named "kernel" in scope "/b/shared/Dense_0"
 
   def test_repr(self):
-
     class Base1(nn.Module):
       a: int
 
     class Base2(nn.Module):
       b: str
 
     class Foo(Base2, Base1):
@@ -2175,16 +2180,18 @@
   def test_kw_only(self):
     def create_kw_layers():
       class BaseLayer(nn.Module, kw_only=True):
         base_multiplier: Optional[int] = -1
 
       class ChildLayer(BaseLayer):
         child_multiplier: int  # Don't want to have to set a default argument!
+
         def __call__(self, x):
           return x * self.child_multiplier * self.base_multiplier
+
       return BaseLayer, ChildLayer
 
     if tuple(sys.version_info)[:3] < (3, 10, 0):
       with self.assertRaisesRegex(TypeError, 'not available before Py 3.10'):
         BaseLayer, ChildLayer = create_kw_layers()
     else:
       BaseLayer, ChildLayer = create_kw_layers()
@@ -2221,193 +2228,204 @@
           del out, rngs
 
 
 class RelaxedNamingTests(absltest.TestCase):
 
   def test_relaxed_adoption(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         p = self.param('p', nn.initializers.zeros, x.shape)
         return x + p
 
     class Bar(nn.Module):
       sub: nn.Module
+
       def __call__(self, x):
         return self.sub(x)
 
     with set_config('flax_preserve_adopted_names', True):
       foo = Foo(name='foo')
       bar = Bar(sub=foo)
       k = random.PRNGKey(0)
       x = jnp.zeros((1,))
       vs = bar.init(k, x)
-      self.assertTrue("foo" in vs['params'], "relaxed naming failure")
+      self.assertTrue('foo' in vs['params'], 'relaxed naming failure')
       y = bar.apply(vs, x)
 
     with set_config('flax_preserve_adopted_names', False):
       foo = Foo(name='foo')
       bar = Bar(sub=foo)
       k = random.PRNGKey(0)
       x = jnp.zeros((1,))
       vs = bar.init(k, x)
-      self.assertTrue("sub" in vs['params'], "old policy naming failure")
+      self.assertTrue('sub' in vs['params'], 'old policy naming failure')
       y = bar.apply(vs, x)
 
   def test_class_optional_adoption_name_preservation(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         p = self.param('p', nn.initializers.zeros, x.shape)
         return x + p
 
     class Bar1(nn.Module):
       sub: nn.Module
       preserve_adopted_names = True
+
       def __call__(self, x):
         return self.sub(x)
 
     class Bar2(nn.Module):
       sub: nn.Module
       preserve_adopted_names = False
+
       def __call__(self, x):
         return self.sub(x)
 
     with set_config('flax_preserve_adopted_names', False):
       foo = Foo(name='foo')
       bar = Bar1(sub=foo)
       k = random.PRNGKey(0)
       x = jnp.zeros((1,))
       vs = bar.init(k, x)
-      self.assertTrue("foo" in vs['params'], "adoption naming failure")
+      self.assertTrue('foo' in vs['params'], 'adoption naming failure')
       y = bar.apply(vs, x)
 
     with set_config('flax_preserve_adopted_names', True):
       foo = Foo(name='foo')
       bar = Bar2(sub=foo)
       k = random.PRNGKey(0)
       x = jnp.zeros((1,))
       vs = bar.init(k, x)
-      self.assertTrue("sub" in vs['params'], "adoption naming failure")
+      self.assertTrue('sub' in vs['params'], 'adoption naming failure')
       y = bar.apply(vs, x)
 
   def test_nested_class_optional_adoption_name_preservation(self):
-
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         p = self.param('p', nn.initializers.zeros, x.shape)
         return x + p
 
     class Bar(nn.Module):
       sub: nn.Module
       preserve_adopted_names = True
+
       def __call__(self, x):
         return self.sub(x)
 
     class Baz(nn.Module):
       sub: nn.Module
       preserve_adopted_names = True
+
       def __call__(self, x):
         return self.sub(x)
 
     with set_config('flax_preserve_adopted_names', False):
       foo = Foo(name='foo')
       bar = Bar(sub=foo, name='bar')
       baz = Baz(sub=bar)
       k = random.PRNGKey(0)
       x = jnp.zeros((1,))
       vs = baz.init(k, x)
-      self.assertTrue("bar" in vs['params'], "adoption naming failure")
-      self.assertTrue("foo" in vs['params']['bar'], "adoption naming failure")
+      self.assertTrue('bar' in vs['params'], 'adoption naming failure')
+      self.assertTrue('foo' in vs['params']['bar'], 'adoption naming failure')
       y = baz.apply(vs, x)
 
   def test_relaxed_adoption_still_conflict_checks(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         p = self.param('p', nn.initializers.zeros, x.shape)
         return x + p
 
     class Bar(nn.Module):
       sub1: nn.Module
       sub2: nn.Module
+
       def __call__(self, x):
         return self.sub(x)
 
     with set_config('flax_preserve_adopted_names', True):
       foo1 = Foo(name='foo')
       foo2 = Foo(name='foo')
       bar = Bar(sub1=foo1, sub2=foo2)
       k = random.PRNGKey(0)
       x = jnp.zeros((1,))
       with self.assertRaises(errors.NameInUseError):
         vs = bar.init(k, x)
 
   def test_relaxed_adoption_unnamed_adoptee(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         p = self.param('p', nn.initializers.zeros, x.shape)
         return x + p
 
     class Bar(nn.Module):
       sub: nn.Module
+
       def __call__(self, x):
         return self.sub(x)
 
     with set_config('flax_preserve_adopted_names', True):
       foo = Foo(name=None)
       bar = Bar(sub=foo)
       k = random.PRNGKey(0)
       x = jnp.zeros((1,))
       vs = bar.init(k, x)
-      self.assertTrue("sub" in vs['params'], "relaxed naming failure")
+      self.assertTrue('sub' in vs['params'], 'relaxed naming failure')
       y = bar.apply(vs, x)
 
     with set_config('flax_preserve_adopted_names', False):
       foo = Foo(name='foo')
       bar = Bar(sub=foo)
       k = random.PRNGKey(0)
       x = jnp.zeros((1,))
       vs = bar.init(k, x)
-      self.assertTrue("sub" in vs['params'], "old policy naming failure")
+      self.assertTrue('sub' in vs['params'], 'old policy naming failure')
       y = bar.apply(vs, x)
 
   def test_relaxed_python_conflict(self):
-
     class Foo(nn.Module):
       dummy = 0
+
       @nn.compact
       def __call__(self, x):
         p = self.param('dummy', nn.initializers.zeros, x.shape)
         return x + p
 
     foo = Foo(name='foo')
     k = random.PRNGKey(0)
     x = jnp.zeros((1,))
     vs = foo.init(k, x)
 
   def test_relaxed_intercollection_conflict(self):
-
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         v1 = self.variable('col1', 'v', lambda x: jnp.zeros(x), x.shape)
         v2 = self.variable('col2', 'v', lambda x: jnp.zeros(x), x.shape)
         return x + v1.value + v2.value
 
     foo = Foo(name='foo')
     k = random.PRNGKey(0)
     x = jnp.zeros((1,))
     vs = foo.init(k, x)
 
   def test_relaxed_intercollection_conflict_set(self):
-
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         v1 = self.variable('col1', 'v', lambda x: jnp.zeros(x), x.shape)
         v2 = self.variable('col2', 'v', lambda x: jnp.zeros(x), x.shape)
         v3 = self.variable('col1', 'v', lambda x: jnp.zeros(x), x.shape)
         return x + v1.value + v2.value + v3.value
 
@@ -2417,23 +2435,22 @@
     with self.assertRaises(errors.NameInUseError):
       vs = foo.init(k, x)
 
 
 class FrozenDictTests(absltest.TestCase):
 
   def test_frozendict_flag(self):
-
     with set_config('flax_return_frozendict', True):
-      x = jnp.zeros((2,3))
+      x = jnp.zeros((2, 3))
       layer = nn.Dense(5)
       params = layer.init(random.PRNGKey(0), x)
       self.assertTrue(isinstance(params, FrozenDict))
 
     with set_config('flax_return_frozendict', False):
-      x = jnp.zeros((2,3))
+      x = jnp.zeros((2, 3))
       layer = nn.Dense(5)
       params = layer.init(random.PRNGKey(0), x)
       self.assertTrue(isinstance(params, dict))
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/linen_recurrent_test.py` & `flax-0.7.1/tests/linen/linen_recurrent_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -26,14 +26,15 @@
 from flax.linen.recurrent import flip_sequences
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 
 class RNNTest(absltest.TestCase):
+
   def test_rnn_basic_forward(self):
     batch_size = 10
     seq_len = 40
     channels_in = 5
     channels_out = 15
 
     rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True)
@@ -44,15 +45,17 @@
     carry, ys = rnn.apply(variables, xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out))
 
     for layer_params in variables['params']['cell'].values():
       if 'bias' in layer_params:
         self.assertEqual(layer_params['bias'].shape, (channels_out,))
-      self.assertIn(layer_params['kernel'].shape[0], [channels_in, channels_out])
+      self.assertIn(
+          layer_params['kernel'].shape[0], [channels_in, channels_out]
+      )
       self.assertEqual(layer_params['kernel'].shape[1], channels_out)
 
   def test_rnn_multiple_batch_dims(self):
     batch_dims = (10, 11)
     seq_len = 40
     channels_in = 5
     channels_out = 15
@@ -65,15 +68,17 @@
     carry, ys = rnn.apply(variables, xs)
 
     self.assertEqual(ys.shape, (*batch_dims, seq_len, channels_out))
 
     for layer_params in variables['params']['cell'].values():
       if 'bias' in layer_params:
         self.assertEqual(layer_params['bias'].shape, (channels_out,))
-      self.assertIn(layer_params['kernel'].shape[0], [channels_in, channels_out])
+      self.assertIn(
+          layer_params['kernel'].shape[0], [channels_in, channels_out]
+      )
       self.assertEqual(layer_params['kernel'].shape[1], channels_out)
 
   def test_rnn_unroll(self):
     batch_size = 10
     seq_len = 40
     channels_in = 5
     channels_out = 15
@@ -86,15 +91,17 @@
     carry, ys = rnn.apply(variables, xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out))
 
     for layer_params in variables['params']['cell'].values():
       if 'bias' in layer_params:
         self.assertEqual(layer_params['bias'].shape, (channels_out,))
-      self.assertIn(layer_params['kernel'].shape[0], [channels_in, channels_out])
+      self.assertIn(
+          layer_params['kernel'].shape[0], [channels_in, channels_out]
+      )
       self.assertEqual(layer_params['kernel'].shape[1], channels_out)
 
   def test_rnn_time_major(self):
     seq_len = 40
     batch_size = 10
     channels_in = 5
     channels_out = 15
@@ -113,27 +120,29 @@
       self.assertEqual(leaf.shape, (batch_size, channels_out))
 
     self.assertEqual(ys.shape, (seq_len, batch_size, channels_out))
 
     for layer_params in variables['params']['cell'].values():
       if 'bias' in layer_params:
         self.assertEqual(layer_params['bias'].shape, (channels_out,))
-      self.assertIn(layer_params['kernel'].shape[0], [channels_in, channels_out])
+      self.assertIn(
+          layer_params['kernel'].shape[0], [channels_in, channels_out]
+      )
       self.assertEqual(layer_params['kernel'].shape[1], channels_out)
 
   def test_rnn_with_spatial_dimensions(self):
     batch_size = 10
     seq_len = 40
     kernel_size = (3, 3)
     image_size = (32, 32)
     channels_in = 5
     channels_out = 15
 
     rnn = nn.RNN(
-      nn.ConvLSTMCell(channels_out, kernel_size),
+        nn.ConvLSTMCell(channels_out, kernel_size),
     )
 
     xs = jnp.ones((batch_size, seq_len, *image_size, channels_in))
     variables = rnn.init(jax.random.PRNGKey(0), xs)
 
     ys: jnp.ndarray
     carry, ys = rnn.apply(variables, xs, return_carry=True)
@@ -144,67 +153,84 @@
       self.assertEqual(leaf.shape[:-1], (batch_size, *image_size))
       self.assertIn(leaf.shape[-1], [channels_in, channels_out])
 
     self.assertEqual(ys.shape, (batch_size, seq_len, *image_size, channels_out))
 
     for layer_params in variables['params']['cell'].values():
       if 'bias' in layer_params:
-          self.assertEqual(layer_params['bias'].shape, (channels_out * 4,))
-      self.assertIn(layer_params['kernel'].shape[2], [channels_in, channels_out, channels_out * 4])
+        self.assertEqual(layer_params['bias'].shape, (channels_out * 4,))
+      self.assertIn(
+          layer_params['kernel'].shape[2],
+          [channels_in, channels_out, channels_out * 4],
+      )
       self.assertEqual(layer_params['kernel'].shape[3], channels_out * 4)
 
   def test_numerical_equivalence(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs)
 
-    cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
+    cell_carry = rnn.cell.initialize_carry(
+        jax.random.PRNGKey(0), xs[:, 0].shape
+    )
     cell_params = variables['params']['cell']
 
     for i in range(seq_len):
-      cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[:, i, :])
+      cell_carry, y = rnn.cell.apply(
+          {'params': cell_params}, cell_carry, xs[:, i, :]
+      )
       np.testing.assert_allclose(y, ys[:, i, :], rtol=1e-5)
 
     np.testing.assert_allclose(cell_carry, carry, rtol=1e-5)
 
   def test_numerical_equivalence_with_mask(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     key = jax.random.PRNGKey(0)
-    seq_lengths = jax.random.randint(key, (batch_size,), minval=1, maxval=seq_len + 1)
+    seq_lengths = jax.random.randint(
+        key, (batch_size,), minval=1, maxval=seq_len + 1
+    )
 
     rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
-    (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs, seq_lengths=seq_lengths)
+    (carry, ys), variables = rnn.init_with_output(
+        jax.random.PRNGKey(0), xs, seq_lengths=seq_lengths
+    )
 
-    cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
+    cell_carry = rnn.cell.initialize_carry(
+        jax.random.PRNGKey(0), xs[:, 0].shape
+    )
     cell_params = variables['params']['cell']
     carries = []
 
     for i in range(seq_len):
-      cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[:, i, :])
+      cell_carry, y = rnn.cell.apply(
+          {'params': cell_params}, cell_carry, xs[:, i, :]
+      )
       np.testing.assert_allclose(y, ys[:, i, :], rtol=1e-5)
       carries.append(cell_carry)
 
     for batch_idx, length in enumerate(seq_lengths):
       t = int(length) - 1
       for carries_t_, carry_ in zip(carries[t], carry):
-        np.testing.assert_allclose(carries_t_[batch_idx], carry_[batch_idx], rtol=1e-5)
+        np.testing.assert_allclose(
+            carries_t_[batch_idx], carry_[batch_idx], rtol=1e-5
+        )
 
   def test_numerical_equivalence_single_batch(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
@@ -213,58 +239,74 @@
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs)
 
     cell_params = variables['params']['cell']
 
     for batch_idx in range(batch_size):
-      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:1, 0].shape)
+      cell_carry = rnn.cell.initialize_carry(
+          jax.random.PRNGKey(0), xs[:1, 0].shape
+      )
 
       for i in range(seq_len):
-        cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[batch_idx, i, :][None])
+        cell_carry, y = rnn.cell.apply(
+            {'params': cell_params}, cell_carry, xs[batch_idx, i, :][None]
+        )
         np.testing.assert_allclose(y[0], ys[batch_idx, i, :], rtol=1e-6)
 
-      carry_i = jax.tree_map(lambda x: x[batch_idx:batch_idx+1], carry)
+      carry_i = jax.tree_map(lambda x: x[batch_idx : batch_idx + 1], carry)
       np.testing.assert_allclose(cell_carry, carry_i, rtol=1e-6)
 
   def test_numerical_equivalence_single_batch_nn_scan(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     cell: nn.LSTMCell = nn.LSTMCell(channels_out)
-    rnn: nn.LSTMCell = nn.scan(nn.LSTMCell, in_axes=1, out_axes=1,
-                   variable_broadcast='params',
-                   split_rngs={'params': False})(channels_out)
+    rnn: nn.LSTMCell = nn.scan(
+        nn.LSTMCell,
+        in_axes=1,
+        out_axes=1,
+        variable_broadcast='params',
+        split_rngs={'params': False},
+    )(channels_out)
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     carry = rnn.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
     ys: jnp.ndarray
-    (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), carry, xs)
+    (carry, ys), variables = rnn.init_with_output(
+        jax.random.PRNGKey(0), carry, xs
+    )
 
     cell_params = variables['params']
 
     for batch_idx in range(batch_size):
       cell_carry = cell.initialize_carry(jax.random.PRNGKey(0), xs[:1, 0].shape)
 
       for i in range(seq_len):
-        cell_carry, y = cell.apply({'params': cell_params}, cell_carry, xs[batch_idx:batch_idx+1, i, :])
+        cell_carry, y = cell.apply(
+            {'params': cell_params},
+            cell_carry,
+            xs[batch_idx : batch_idx + 1, i, :],
+        )
         np.testing.assert_allclose(y[0], ys[batch_idx, i, :], rtol=1e-5)
 
-      carry_i = jax.tree_map(lambda x: x[batch_idx:batch_idx+1], carry)
+      carry_i = jax.tree_map(lambda x: x[batch_idx : batch_idx + 1], carry)
       np.testing.assert_allclose(cell_carry, carry_i, rtol=1e-5)
 
   def test_numerical_equivalence_single_batch_jax_scan(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
-    xs = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, seq_len, channels_in))
+    xs = jax.random.uniform(
+        jax.random.PRNGKey(0), (batch_size, seq_len, channels_in)
+    )
     cell: nn.LSTMCell = nn.LSTMCell(channels_out)
     carry = cell.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
     variables = cell.init(jax.random.PRNGKey(0), carry, xs[:, 0])
     cell_params = variables['params']
 
     def scan_fn(carry, x):
       return cell.apply({'params': cell_params}, carry, x)
@@ -272,15 +314,17 @@
     ys: jnp.ndarray
     carry, ys = jax.lax.scan(scan_fn, carry, xs.swapaxes(0, 1))
     ys = ys.swapaxes(0, 1)
 
     cell_carry = cell.initialize_carry(jax.random.PRNGKey(0), xs[:, 0].shape)
 
     for i in range(seq_len):
-      cell_carry, y = cell.apply({'params': cell_params}, cell_carry, xs[:, i, :])
+      cell_carry, y = cell.apply(
+          {'params': cell_params}, cell_carry, xs[:, i, :]
+      )
       np.testing.assert_allclose(y, ys[:, i, :], rtol=1e-4)
 
     np.testing.assert_allclose(cell_carry, carry, rtol=1e-4)
 
   def test_reverse(self):
     batch_size = 3
     seq_len = 4
@@ -292,46 +336,71 @@
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs)
 
     cell_params = variables['params']['cell']
 
     for batch_idx in range(batch_size):
-      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:1, 0].shape)
+      cell_carry = rnn.cell.initialize_carry(
+          jax.random.PRNGKey(0), xs[:1, 0].shape
+      )
 
       for i in range(seq_len):
-        cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[batch_idx, seq_len - i - 1, :][None])
+        cell_carry, y = rnn.cell.apply(
+            {'params': cell_params},
+            cell_carry,
+            xs[batch_idx, seq_len - i - 1, :][None],
+        )
         np.testing.assert_allclose(y[0], ys[batch_idx, i, :], rtol=1e-5)
 
       np.testing.assert_allclose(
-        cell_carry, jax.tree_map(lambda x: x[batch_idx:batch_idx+1], carry), rtol=1e-5)
+          cell_carry,
+          jax.tree_map(lambda x: x[batch_idx : batch_idx + 1], carry),
+          rtol=1e-5,
+      )
 
   def test_reverse_but_keep_order(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
-    rnn = nn.RNN(nn.LSTMCell(channels_out), return_carry=True, reverse=True, keep_order=True)
+    rnn = nn.RNN(
+        nn.LSTMCell(channels_out),
+        return_carry=True,
+        reverse=True,
+        keep_order=True,
+    )
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     (carry, ys), variables = rnn.init_with_output(jax.random.PRNGKey(0), xs)
 
     cell_params = variables['params']['cell']
 
     for batch_idx in range(batch_size):
-      cell_carry = rnn.cell.initialize_carry(jax.random.PRNGKey(0), xs[:1, 0].shape)
+      cell_carry = rnn.cell.initialize_carry(
+          jax.random.PRNGKey(0), xs[:1, 0].shape
+      )
 
       for i in range(seq_len):
-        cell_carry, y = rnn.cell.apply({'params': cell_params}, cell_carry, xs[batch_idx, seq_len - i - 1, :][None])
-        np.testing.assert_allclose(y[0], ys[batch_idx, seq_len - i - 1, :], rtol=1e-5)
+        cell_carry, y = rnn.cell.apply(
+            {'params': cell_params},
+            cell_carry,
+            xs[batch_idx, seq_len - i - 1, :][None],
+        )
+        np.testing.assert_allclose(
+            y[0], ys[batch_idx, seq_len - i - 1, :], rtol=1e-5
+        )
 
       np.testing.assert_allclose(
-        cell_carry, jax.tree_map(lambda x: x[batch_idx:batch_idx+1], carry), rtol=1e-5)
+          cell_carry,
+          jax.tree_map(lambda x: x[batch_idx : batch_idx + 1], carry),
+          rtol=1e-5,
+      )
 
   def test_flip_sequence(self):
     x = jnp.arange(2 * 5).reshape((2, 5))
     seq_lengths = jnp.array([4, 2])
 
     flipped = flip_sequences(x, seq_lengths, num_batch_dims=1, time_major=False)
 
@@ -365,25 +434,31 @@
 
     flipped = flip_sequences(x, seq_lengths, num_batch_dims=1, time_major=True)
 
     self.assertEqual(flipped.shape, (5, 2, 3))
     np.testing.assert_allclose(flipped[:4, 0], x[:4, 0][::-1])
     np.testing.assert_allclose(flipped[:2, 1], x[:2, 1][::-1])
 
+  def test_basic_seq_lengths(self):
+    x = jnp.ones((2, 10, 6))
+    lstm = nn.RNN(nn.LSTMCell(265))
+    variables = lstm.init(jax.random.PRNGKey(0), x)
+    y = lstm.apply(variables, x, seq_lengths=jnp.array([5, 5]))
+
+
 class BidirectionalTest(absltest.TestCase):
 
   def test_bidirectional(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     bdirectional = nn.Bidirectional(
-      nn.RNN(nn.LSTMCell(channels_out)),
-      nn.RNN(nn.LSTMCell(channels_out))
+        nn.RNN(nn.LSTMCell(channels_out)), nn.RNN(nn.LSTMCell(channels_out))
     )
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     ys, variables = bdirectional.init_with_output(jax.random.PRNGKey(0), xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out * 2))
@@ -391,35 +466,32 @@
   def test_shared_cell(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     cell = nn.LSTMCell(channels_out)
-    bdirectional = nn.Bidirectional(
-      nn.RNN(cell),
-      nn.RNN(cell)
-    )
+    bdirectional = nn.Bidirectional(nn.RNN(cell), nn.RNN(cell))
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     ys, variables = bdirectional.init_with_output(jax.random.PRNGKey(0), xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out * 2))
 
   def test_custom_merge_fn(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     bdirectional = nn.Bidirectional(
-      nn.RNN(nn.LSTMCell(channels_out)),
-      nn.RNN(nn.LSTMCell(channels_out)),
-      merge_fn=lambda x, y: x + y
+        nn.RNN(nn.LSTMCell(channels_out)),
+        nn.RNN(nn.LSTMCell(channels_out)),
+        merge_fn=lambda x, y: x + y,
     )
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
     ys, variables = bdirectional.init_with_output(jax.random.PRNGKey(0), xs)
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out))
@@ -427,62 +499,55 @@
   def test_return_carry(self):
     batch_size = 3
     seq_len = 4
     channels_in = 5
     channels_out = 6
 
     bdirectional = nn.Bidirectional(
-      nn.RNN(nn.LSTMCell(channels_out)),
-      nn.RNN(nn.LSTMCell(channels_out)),
-      return_carry=True
+        nn.RNN(nn.LSTMCell(channels_out)),
+        nn.RNN(nn.LSTMCell(channels_out)),
+        return_carry=True,
     )
 
     xs = jnp.ones((batch_size, seq_len, channels_in))
     ys: jnp.ndarray
-    (carry, ys), variables = bdirectional.init_with_output(jax.random.PRNGKey(0), xs)
+    (carry, ys), variables = bdirectional.init_with_output(
+        jax.random.PRNGKey(0), xs
+    )
     carry_forward, carry_backward = carry
 
     self.assertEqual(ys.shape, (batch_size, seq_len, channels_out * 2))
     self.assertEqual(
-      jax.tree_map(jnp.shape, carry_forward),
-      ((batch_size, channels_out), (batch_size, channels_out))
+        jax.tree_map(jnp.shape, carry_forward),
+        ((batch_size, channels_out), (batch_size, channels_out)),
     )
     self.assertEqual(
-      jax.tree_map(jnp.shape, carry_backward),
-      ((batch_size, channels_out), (batch_size, channels_out))
+        jax.tree_map(jnp.shape, carry_backward),
+        ((batch_size, channels_out), (batch_size, channels_out)),
     )
 
+
 class TestRecurrentDeprecation(parameterized.TestCase):
 
   @parameterized.product(
-    cell_type=[nn.LSTMCell, nn.GRUCell, nn.OptimizedLSTMCell]
+      cell_type=[nn.LSTMCell, nn.GRUCell, nn.OptimizedLSTMCell]
   )
   def test_constructor(self, cell_type):
-
-    with self.assertRaisesRegex(
-      TypeError,
-      "The RNNCellBase API has changed"
-    ):
+    with self.assertRaisesRegex(TypeError, 'The RNNCellBase API has changed'):
       cell_type()
 
   @parameterized.product(
-    cell_type=[nn.LSTMCell, nn.GRUCell, nn.OptimizedLSTMCell]
+      cell_type=[nn.LSTMCell, nn.GRUCell, nn.OptimizedLSTMCell]
   )
   def test_initialize_carry(self, cell_type):
     key = jax.random.PRNGKey(0)
-    with self.assertRaisesRegex(
-      TypeError,
-      "The RNNCellBase API has changed"
-    ):
+    with self.assertRaisesRegex(TypeError, 'The RNNCellBase API has changed'):
       cell_type.initialize_carry(key, (2,), 3)
 
   def test_rnn(self):
     cell = nn.LSTMCell(3)
-    with self.assertRaisesRegex(
-      TypeError,
-      "The RNNCellBase API has changed"
-    ):
+    with self.assertRaisesRegex(TypeError, 'The RNNCellBase API has changed'):
       nn.RNN(cell, cell_size=8)
 
 
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/linen_transforms_test.py` & `flax-0.7.1/tests/linen/linen_transforms_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -31,22 +31,23 @@
 from flax.configurations import temp_flip_flag
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 # pylint: disable=attribute-defined-outside-init,unused-variable,g-wrong-blank-lines,g-bare-generic
 
+
 def tree_equals(x, y):
-  return jax.tree_util.tree_all(
-      jax.tree_util.tree_map(operator.eq, x, y))
+  return jax.tree_util.tree_all(jax.tree_util.tree_map(operator.eq, x, y))
 
 
 def tree_allclose(x, y):
   return jax.tree_util.tree_all(
-      jax.tree_util.tree_map(lambda x,y: np.all(np.isclose(x,y)), x, y))
+      jax.tree_util.tree_map(lambda x, y: np.all(np.isclose(x, y)), x, y)
+  )
 
 
 id_fn = lambda x: x
 
 
 class TransformedMLP(nn.Module):
   features: Sequence[int]
@@ -73,14 +74,15 @@
       x = inputs
       for i, feat in enumerate(self.features):
         # JIT the Module (it's __call__ fn by default.)
         x = nn.Dense(feat, name=f'layers_{i}')(x)
         if i != len(self.features) - 1:
           x = nn.relu(x)
       return x
+
   return MLP
 
 
 class TransformTest(absltest.TestCase):
 
   def test_jit(self):
     key1, key2 = random.split(random.PRNGKey(3), 2)
@@ -127,19 +129,22 @@
     init_variables = normal_model.init(key2, x)
     y1 = normal_model.apply(init_variables, x)
     y2 = remat_model.apply(init_variables, x)
 
     self.assertTrue(np.all(y1 == y2))
 
   def test_remat_kwargs(self):
-    raise unittest.SkipTest("test breaks with grad")
+    raise unittest.SkipTest('test breaks with grad')
+
     class ConditionalReLU(nn.Module):
+
       @nn.compact
-      def __call__(self, input, apply_relu : bool = False):
+      def __call__(self, input, apply_relu: bool = False):
         return nn.relu(input) if apply_relu else input
+
     key = random.PRNGKey(0)
     x = jnp.ones((4, 4)) * -1
     remat_model = nn.remat(ConditionalReLU)()
     p = remat_model.init(key, x)
     y = remat_model.apply(p, x, apply_relu=True)
 
     self.assertTrue(np.all(y == jnp.zeros_like(x)))
@@ -179,14 +184,15 @@
     y = foo.apply(variables, x, False)
     self.assertEqual(y.shape, (1, 3))
 
   def test_remat_decorator_static_argnums(self):
     test = self
 
     class FooTrainStatic(nn.Module):
+
       @partial(nn.remat, static_argnums=(2,))
       @nn.compact
       def __call__(self, inputs, train: bool):
         test.assertTrue(isinstance(train, bool))
 
         return nn.Dense(3, use_bias=False)(inputs)
 
@@ -195,546 +201,663 @@
 
     x = jnp.empty((1, 2))
     variables = foo.init(random.PRNGKey(0), x, True)
     y = foo.apply(variables, x, False)
     self.assertEqual(y.shape, (1, 3))
 
     class FooTrainDynamic(nn.Module):
+
       @partial(nn.remat, static_argnums=())
       @nn.compact
       def __call__(self, inputs, train: bool):
         test.assertTrue(isinstance(train, jnp.ndarray))
 
         return nn.Dense(3, use_bias=False)(inputs)
 
     # set train as a non-static arguments
     foo = FooTrainDynamic()
 
     variables = foo.init(random.PRNGKey(0), x, True)
     y = foo.apply(variables, x, False)
     self.assertEqual(y.shape, (1, 3))
 
-
   def test_vmap(self):
     key1, key2 = random.split(random.PRNGKey(3), 2)
     x = random.uniform(key1, (4, 4))
     x2 = random.uniform(key1, (5, 4, 4))
 
     def vmap(cls):
-      return nn.vmap(cls,
-                     in_axes=(0,),
-                     variable_axes={'params': None},
-                     split_rngs={'params': False})
+      return nn.vmap(
+          cls,
+          in_axes=(0,),
+          variable_axes={'params': None},
+          split_rngs={'params': False},
+      )
+
     normal_model = TransformedMLP(features=[3, 4, 5])
     vmap_model = TransformedMLP(features=[3, 4, 5], transform=vmap)
     init_variables = normal_model.init(key2, x)
     # simulate vmap in python for comparison:
-    y1 = jnp.vstack([normal_model.apply(init_variables, x2[i])[None, ...]
-                     for i in np.arange(x2.shape[0])])
+    y1 = jnp.vstack(
+        [
+            normal_model.apply(init_variables, x2[i])[None, ...]
+            for i in np.arange(x2.shape[0])
+        ]
+    )
     y2 = vmap_model.apply(init_variables, x2)
     np.testing.assert_allclose(y1, y2, atol=1e-7)
 
   def test_vmap_decorated(self):
     key1, key2 = random.split(random.PRNGKey(3), 2)
     x = random.uniform(key1, (4, 4))
     x2 = random.uniform(key1, (5, 4, 4))
 
     def vmap(fn):
-      return nn.vmap(fn,
-                     in_axes=(0,),
-                     variable_axes={'params': None},
-                     split_rngs={'params': False})
+      return nn.vmap(
+          fn,
+          in_axes=(0,),
+          variable_axes={'params': None},
+          split_rngs={'params': False},
+      )
+
     normal_model = decorated_MLP()(features=[3, 4, 5])
     vmap_model = decorated_MLP(vmap)(features=[3, 4, 5])
     init_variables = normal_model.init(key2, x)
     # simulate vmap in python for comparison:
-    y1 = jnp.vstack([normal_model.apply(init_variables, x2[i])[None, ...]
-                     for i in np.arange(x2.shape[0])])
+    y1 = jnp.vstack(
+        [
+            normal_model.apply(init_variables, x2[i])[None, ...]
+            for i in np.arange(x2.shape[0])
+        ]
+    )
     y2 = vmap_model.apply(init_variables, x2)
     np.testing.assert_allclose(y1, y2, atol=1e-7)
 
   def test_vmap_batchnorm(self):
     key1, key2 = random.split(random.PRNGKey(3), 2)
     x = random.uniform(key1, (4, 4))
     x2 = random.uniform(key1, (5, 4, 4))
 
     def vmap(cls):
-      return nn.vmap(cls,
-                     in_axes=(0,),
-                     variable_axes={'params': None, 'batch_stats': None},
-                     split_rngs={'params': False},
-                     axis_name='batch')
+      return nn.vmap(
+          cls,
+          in_axes=(0,),
+          variable_axes={'params': None, 'batch_stats': None},
+          split_rngs={'params': False},
+          axis_name='batch',
+      )
+
     class MlpBn(nn.Module):
       axis_name: Any = None
 
       @nn.compact
       def __call__(self, x):
         x = nn.Dense(3)(x)
         x = nn.BatchNorm(axis_name=self.axis_name, use_running_average=False)(x)
         return x
 
     normal_model = MlpBn()
     vmap_model = vmap(MlpBn)(axis_name='batch')
     init_variables = normal_model.init(key2, x)
-    y1 = normal_model.apply(init_variables, x2.reshape((-1, 4)), mutable=['batch_stats'])[0]
+    y1 = normal_model.apply(
+        init_variables, x2.reshape((-1, 4)), mutable=['batch_stats']
+    )[0]
     y1 = y1.reshape((5, 4, 3))
     y2 = vmap_model.apply(init_variables, x2, mutable=['batch_stats'])[0]
     np.testing.assert_allclose(y1, y2, atol=1e-5)
 
   def test_scan(self):
     class SimpleScan(nn.Module):
       features: int
+
       @nn.compact
       def __call__(self, c, xs):
-        LSTM = nn.scan(nn.LSTMCell,
-                       variable_broadcast='params',
-                       split_rngs={'params': False})
-        return LSTM(self.features, name="lstm_cell")(c, xs)
+        LSTM = nn.scan(
+            nn.LSTMCell,
+            variable_broadcast='params',
+            split_rngs={'params': False},
+        )
+        return LSTM(self.features, name='lstm_cell')(c, xs)
 
     key1, key2 = random.split(random.PRNGKey(0), 2)
     xs = random.uniform(key1, (5, 3, 2))
     dummy_rng = random.PRNGKey(0)
     init_carry = nn.LSTMCell(2).initialize_carry(dummy_rng, xs[0].shape)
     model = SimpleScan(2)
     init_variables = model.init(key2, init_carry, xs)
     # simulate scan in python for comparison:
     c = init_carry
     ys = []
-    lstmcell_variables = freeze({'params': init_variables['params']['lstm_cell']})
+    lstmcell_variables = freeze(
+        {'params': init_variables['params']['lstm_cell']}
+    )
     for i in range(xs.shape[0]):
       c, y = nn.LSTMCell(2).apply(lstmcell_variables, c, xs[i])
       ys.append(y[None, ...])
     y1 = jnp.vstack(ys)
 
     c2, y2 = model.apply(init_variables, init_carry, xs)
     np.testing.assert_allclose(y1, y2, atol=1e-7)
     np.testing.assert_allclose(c[0], c2[0], atol=1e-7)
     np.testing.assert_allclose(c[1], c2[1], atol=1e-7)
 
   def test_scan_decorated(self):
     class SimpleScan(nn.Module):
       features: int
-      @partial(nn.scan,
-               variable_broadcast='params',
-               in_axes=(nn.broadcast, 0),
-               split_rngs={'params': False})
+
+      @partial(
+          nn.scan,
+          variable_broadcast='params',
+          in_axes=(nn.broadcast, 0),
+          split_rngs={'params': False},
+      )
       @nn.compact
       def __call__(self, c, b, xs):
         assert b.shape == (4,)
-        return nn.LSTMCell(self.features, name="lstm_cell")(c, xs)
+        return nn.LSTMCell(self.features, name='lstm_cell')(c, xs)
 
     key1, key2 = random.split(random.PRNGKey(0), 2)
     xs = random.uniform(key1, (4, 3, 2))
     b = jnp.ones((4,))
     dummy_rng = random.PRNGKey(0)
     init_carry = nn.LSTMCell(2).initialize_carry(dummy_rng, xs[0].shape)
     model = SimpleScan(2)
     init_variables = model.init(key2, init_carry, b, xs)
     # simulate scan in python for comparison:
     c = init_carry
     ys = []
-    lstmcell_variables = freeze({'params': init_variables['params']['lstm_cell']})
+    lstmcell_variables = freeze(
+        {'params': init_variables['params']['lstm_cell']}
+    )
     for i in range(xs.shape[0]):
       c, y = nn.LSTMCell(2).apply(lstmcell_variables, c, xs[i])
       ys.append(y[None, ...])
     y1 = jnp.vstack(ys)
 
     c2, y2 = model.apply(init_variables, init_carry, b, xs)
     np.testing.assert_allclose(y1, y2, atol=1e-7)
     np.testing.assert_allclose(c[0], c2[0], atol=1e-7)
     np.testing.assert_allclose(c[1], c2[1], atol=1e-7)
 
   def test_multiscope_lifting_simple(self):
     class Counter(nn.Module):
+
       @nn.compact
       def __call__(self):
         v = self.variable('counter', 'foo', lambda: jnp.array([0]))
         v.value += jnp.array([1])
         return v.value
+
     class Outer(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         cntr = nn.jit(Counter)(name='cntr')()
         return x
+
     class Inner(nn.Module):
       outer_module: nn.Module
+
       @nn.compact
       def __call__(self, x):
         return self.outer_module(x)
+
     class Test(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         outer_dense = nn.jit(Outer)(name='outer')
         # we share stateful outer module as arg to two different, transformed modules:
         inner = nn.jit(Inner)(outer_dense, name='inner1')
         inner2 = nn.jit(Inner)(outer_dense, name='inner2')
         res = inner(x) + inner2(x)
         return res
 
     x = jnp.ones((10, 10))
     rngs = random.PRNGKey(0)
     init_vars = Test(None).init(rngs, x)
     _, new_vars = Test(None).apply(init_vars, x, mutable=['counter'])
-    self.assertEqual(init_vars['counter']['outer']['cntr']['foo'],
-                     jnp.array([2], jnp.int32))
-    self.assertEqual(new_vars['counter']['outer']['cntr']['foo'],
-                     jnp.array([4], jnp.int32))
+    self.assertEqual(
+        init_vars['counter']['outer']['cntr']['foo'], jnp.array([2], jnp.int32)
+    )
+    self.assertEqual(
+        new_vars['counter']['outer']['cntr']['foo'], jnp.array([4], jnp.int32)
+    )
 
   def test_multiscope_lifting_simple_decorator(self):
     class Counter(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self):
         v = self.variable('counter', 'foo', lambda: jnp.array([0]))
         v.value += jnp.array([1])
         return v.value
+
     class Outer(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         cntr = Counter(name='cntr')()
         return x
+
     class Inner(nn.Module):
       outer_module: nn.Module
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         return self.outer_module(x)
+
     class Test(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         outer_dense = Outer(name='outer')
         # we share stateful outer module as arg to two different, transformed modules:
         inner = Inner(outer_dense, name='inner1')
         inner2 = Inner(outer_dense, name='inner2')
         res = inner(x) + inner2(x)
         return res
 
     x = jnp.ones((1, 1))
     rngs = random.PRNGKey(0)
     init_vars = Test(None).init(rngs, x)
     _, new_vars = Test(None).apply(init_vars, x, mutable=['counter'])
-    self.assertEqual(init_vars['counter']['outer']['cntr']['foo'],
-                     jnp.array([2], jnp.int32))
-    self.assertEqual(new_vars['counter']['outer']['cntr']['foo'],
-                     jnp.array([4], jnp.int32))
+    self.assertEqual(
+        init_vars['counter']['outer']['cntr']['foo'], jnp.array([2], jnp.int32)
+    )
+    self.assertEqual(
+        new_vars['counter']['outer']['cntr']['foo'], jnp.array([4], jnp.int32)
+    )
 
   def test_multiscope_lifting_argtree(self):
     class Counter(nn.Module):
+
       @nn.compact
       def __call__(self):
         v = self.variable('counter', 'foo', lambda: jnp.array([0]))
         v.value += jnp.array([1])
         return v.value
+
     class Outer(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         cntr = nn.jit(Counter)(name='cntr')()
         return x
+
     class Inner(nn.Module):
       outer_module: Sequence[nn.Module]
+
       @nn.compact
       def __call__(self, x):
         return self.outer_module[0](x) + self.outer_module[1](x)
+
     class Test(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         outer_dense1 = nn.jit(Outer)(name='outer1')
         outer_dense2 = nn.jit(Outer)(name='outer2')
         # we share stateful outer module as arg to two different, transformed modules:
         inner1 = nn.jit(Inner)((outer_dense1, outer_dense2), name='inner1')
         inner2 = nn.jit(Inner)((outer_dense1, outer_dense2), name='inner2')
         res = inner1(x) + inner2(x)
         return res
 
     x = jnp.ones((1, 1))
     rngs = random.PRNGKey(0)
     init_vars = Test(None).init(rngs, x)
     _, new_vars = Test(None).apply(init_vars, x, mutable=['counter'])
-    self.assertEqual(init_vars['counter']['outer1']['cntr']['foo'],
-                     jnp.array([2], jnp.int32))
-    self.assertEqual(new_vars['counter']['outer1']['cntr']['foo'],
-                     jnp.array([4], jnp.int32))
-    self.assertEqual(init_vars['counter']['outer2']['cntr']['foo'],
-                     jnp.array([2], jnp.int32))
-    self.assertEqual(new_vars['counter']['outer2']['cntr']['foo'],
-                     jnp.array([4], jnp.int32))
+    self.assertEqual(
+        init_vars['counter']['outer1']['cntr']['foo'], jnp.array([2], jnp.int32)
+    )
+    self.assertEqual(
+        new_vars['counter']['outer1']['cntr']['foo'], jnp.array([4], jnp.int32)
+    )
+    self.assertEqual(
+        init_vars['counter']['outer2']['cntr']['foo'], jnp.array([2], jnp.int32)
+    )
+    self.assertEqual(
+        new_vars['counter']['outer2']['cntr']['foo'], jnp.array([4], jnp.int32)
+    )
 
   def test_multiscope_lifting_argtree_decorator(self):
     class Counter(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self):
         v = self.variable('counter', 'foo', lambda: jnp.array([0]))
         v.value += jnp.array([1])
         return v.value
+
     class Outer(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         cntr = nn.jit(Counter)(name='cntr')()
         return x
+
     class Inner(nn.Module):
       outer_module: Sequence[nn.Module]
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         return self.outer_module[0](x) + self.outer_module[1](x)
+
     class Test(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         outer_dense1 = Outer(name='outer1')
         outer_dense2 = Outer(name='outer2')
         # we share stateful outer module as arg to two different, transformed modules:
         inner1 = Inner((outer_dense1, outer_dense2), name='inner1')
         inner2 = Inner((outer_dense1, outer_dense2), name='inner2')
         res = inner1(x) + inner2(x)
         return res
 
     x = jnp.ones((1, 1))
     rngs = random.PRNGKey(0)
     init_vars = Test(None).init(rngs, x)
     _, new_vars = Test(None).apply(init_vars, x, mutable=['counter'])
-    self.assertEqual(init_vars['counter']['outer1']['cntr']['foo'],
-                     jnp.array([2], jnp.int32))
-    self.assertEqual(new_vars['counter']['outer1']['cntr']['foo'],
-                     jnp.array([4], jnp.int32))
-    self.assertEqual(init_vars['counter']['outer2']['cntr']['foo'],
-                     jnp.array([2], jnp.int32))
-    self.assertEqual(new_vars['counter']['outer2']['cntr']['foo'],
-                     jnp.array([4], jnp.int32))
+    self.assertEqual(
+        init_vars['counter']['outer1']['cntr']['foo'], jnp.array([2], jnp.int32)
+    )
+    self.assertEqual(
+        new_vars['counter']['outer1']['cntr']['foo'], jnp.array([4], jnp.int32)
+    )
+    self.assertEqual(
+        init_vars['counter']['outer2']['cntr']['foo'], jnp.array([2], jnp.int32)
+    )
+    self.assertEqual(
+        new_vars['counter']['outer2']['cntr']['foo'], jnp.array([4], jnp.int32)
+    )
 
   def test_multiscope_lifting_simple_decorator_w_jit(self):
     # TODO: actually test jaxpr on a simpler module.
     class Counter(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self):
         v = self.variable('counter', 'foo', lambda: jnp.array([0]))
         v.value += jnp.array([1])
         return v.value
+
     class Outer(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         cntr = Counter(name='cntr')()
         return x
+
     class Inner(nn.Module):
       outer_module: nn.Module
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         return self.outer_module(x)
+
     class Test(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         outer_dense = Outer(name='outer')
         # we share stateful outer module as arg to two different, transformed modules:
         inner = Inner(outer_dense, name='inner1')
         inner2 = Inner(outer_dense, name='inner2')
         res = inner(x) + inner2(x)
         return res
 
     x = jnp.ones((1, 1))
     rngs = random.PRNGKey(0)
     init_vars = Test(None).init(rngs, x)
     _, new_vars = Test(None).apply(init_vars, x, mutable=['counter'])
-    self.assertEqual(init_vars['counter']['outer']['cntr']['foo'],
-                    jnp.array([2], jnp.int32))
-    self.assertEqual(new_vars['counter']['outer']['cntr']['foo'],
-                    jnp.array([4], jnp.int32))
+    self.assertEqual(
+        init_vars['counter']['outer']['cntr']['foo'], jnp.array([2], jnp.int32)
+    )
+    self.assertEqual(
+        new_vars['counter']['outer']['cntr']['foo'], jnp.array([4], jnp.int32)
+    )
 
   def test_vmapped_outer_module(self):
     class Outer(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         return nn.Dense(5)(x)
+
     class Inner(nn.Module):
       outer_module: nn.Module
-      @partial(nn.vmap,
-               in_axes=(0,),
-               variable_axes={'params': 0},
-               split_rngs={'params': True})
+
+      @partial(
+          nn.vmap,
+          in_axes=(0,),
+          variable_axes={'params': 0},
+          split_rngs={'params': True},
+      )
       @nn.jit
       @nn.compact
       def __call__(self, x):
         return self.outer_module(x)
+
     class Test(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         outer_dense = Outer(name='outer')
         inner = Inner(outer_dense, name='inner1')
         inner2 = Inner(outer_dense, name='inner2')
         res = inner(x) + inner2(x)
         return res
 
     x = jnp.ones((3, 1, 2))
     rngs = random.PRNGKey(0)
     init_vars = Test(None).init(rngs, x)
     y = Test(None).apply(init_vars, x)
     self.assertEqual(
-        init_vars['params']['outer']['Dense_0']['kernel'].shape,
-        (3, 2, 5))
+        init_vars['params']['outer']['Dense_0']['kernel'].shape, (3, 2, 5)
+    )
     self.assertEqual(
-        init_vars['params']['outer']['Dense_0']['bias'].shape,
-        (3, 5))
+        init_vars['params']['outer']['Dense_0']['bias'].shape, (3, 5)
+    )
     self.assertEqual(y.shape, (3, 1, 5))
 
   def test_module_transform_with_setup(self):
     class Foo(nn.Module):
+
       def setup(self):
         self.test = self.param('test', nn.initializers.ones_init(), ())
 
       def __call__(self, x):
         return x * self.test
 
-    FooVmap = nn.vmap(Foo, in_axes=0, out_axes=0,
-                      variable_axes={'params': 0}, split_rngs={'params': True})
+    FooVmap = nn.vmap(
+        Foo,
+        in_axes=0,
+        out_axes=0,
+        variable_axes={'params': 0},
+        split_rngs={'params': True},
+    )
     variables = FooVmap().init(random.PRNGKey(0), jnp.ones((4,)))
     self.assertEqual(variables['params']['test'].shape, (4,))
 
-
   def test_nested_module_args_vmap(self):
     class A(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return nn.Dense(3)(x)
+
     class B(nn.Module):
       A: nn.Module
+
       @nn.compact
       def __call__(self, x):
         return self.A(x)
+
     class C(nn.Module):
       B: nn.Module
-      @partial(nn.vmap,
-               variable_axes={'params': 0},
-               split_rngs={'params': True})
+
+      @partial(
+          nn.vmap, variable_axes={'params': 0}, split_rngs={'params': True}
+      )
       @nn.compact
       def __call__(self, x):
         return self.B(x)
+
     class D(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         a = A()
         b = B(a)
         c = C(b)
         return c(x)
 
     key = random.PRNGKey(0)
     x = jnp.ones((10, 10))
     p = D().init(key, x)
 
     variable_shapes = jax.tree_util.tree_map(jnp.shape, p)
     self.assertEqual(
-        variable_shapes['params']['A_0']['Dense_0']['kernel'],
-        (10, 10, 3))
+        variable_shapes['params']['A_0']['Dense_0']['kernel'], (10, 10, 3)
+    )
     self.assertEqual(
-        variable_shapes['params']['A_0']['Dense_0']['bias'],
-        (10, 3))
+        variable_shapes['params']['A_0']['Dense_0']['bias'], (10, 3)
+    )
 
   def test_nested_module_args_vmap_2(self):
     class A(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return nn.Dense(3)(x)
+
     class B(nn.Module):
       A: nn.Module
+
       @nn.compact
       def __call__(self, x):
         return self.A(x)
+
     class C(nn.Module):
       A: nn.Module
       B: nn.Module
+
       @partial(
-          nn.vmap,
-          variable_axes={'params': 0},
-          split_rngs={'params': True})
+          nn.vmap, variable_axes={'params': 0}, split_rngs={'params': True}
+      )
       @nn.compact
       def __call__(self, x):
         return self.B(x) + self.A(x)
+
     class D(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         a1 = A()
         a2 = A()
         b = B(a1)
         c = C(a2, b)
         return c(x)
 
     key = random.PRNGKey(0)
     x = jnp.ones((10, 10))
     p = D().init(key, x)
 
     variable_shapes = jax.tree_util.tree_map(jnp.shape, p)
     self.assertEqual(
-        variable_shapes['params']['A_0']['Dense_0']['kernel'],
-        (10, 10, 3))
+        variable_shapes['params']['A_0']['Dense_0']['kernel'], (10, 10, 3)
+    )
     self.assertEqual(
-        variable_shapes['params']['A_0']['Dense_0']['bias'],
-        (10, 3))
+        variable_shapes['params']['A_0']['Dense_0']['bias'], (10, 3)
+    )
     self.assertEqual(
-        variable_shapes['params']['A_1']['Dense_0']['kernel'],
-        (10, 10, 3))
+        variable_shapes['params']['A_1']['Dense_0']['kernel'], (10, 10, 3)
+    )
     self.assertEqual(
-        variable_shapes['params']['A_1']['Dense_0']['bias'],
-        (10, 3))
+        variable_shapes['params']['A_1']['Dense_0']['bias'], (10, 3)
+    )
 
   def test_nested_setup_calls_count(self):
     D = 3
     N = 4
     setup_cntr = 0
     call_cntr = 0
+
     class Repeat(nn.Module):
       mdl_def: Any
+
       def setup(self):
         self.lyrs = [self.mdl_def() for _ in range(N)]
+
       @nn.remat  # we just use remat as a convenient test of transform logic
       def __call__(self, x):
         for lyr in self.lyrs:
           lyr(x)
         return x
+
     class Counter(nn.Module):
+
       def setup(self):
         nonlocal setup_cntr
         setup_cntr += 1
         self.dense = nn.Dense(2, use_bias=False)
+
       @nn.remat
       def __call__(self, x):
         nonlocal call_cntr
         call_cntr += 1
         return self.dense(x)
 
     def nested_repeat(mdl):
       for _ in range(D):
         mdl = partial(Repeat, mdl)
       return mdl()
+
     _ = nested_repeat(Counter).init(random.PRNGKey(0), jnp.ones((2,)))
     # setup_cntr == 128 due to 1 call in Counter.setup by _validate_setup
     # and 1 further "real" call.
     self.assertEqual(setup_cntr, 128)
     self.assertEqual(call_cntr, 64)
 
   def test_multimethod_setup_calls(self):
-    cntr=0
+    cntr = 0
+
     class A(nn.Module):
+
       def setup(self):
         nonlocal cntr
-        cntr+=1
+        cntr += 1
         self.d = nn.Dense(2)
+
       @nn.remat
       def foo(self, x):
         return self.d(x)
+
       @nn.remat
       def bar(self, x):
         return self.d(x)
+
     class B(nn.Module):
+
       def setup(self):
         self.a = A()
+
       def __call__(self, x):
         y1 = self.a.foo(x)
         y2 = self.a.bar(x)
         return y1, y2
 
     key = random.PRNGKey(0)
     x = jnp.ones((2,))
@@ -744,39 +867,47 @@
     # 1 call by _validate_setup
     # 1 call for the setup() outside transform boundary
     # and two further "real" calls in transform boundaries
     self.assertEqual(cntr, 4)
 
   def test_toplevel_submodule_adoption_transform(self):
     class A(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return nn.Dense(3)(x)
+
     class B(nn.Module):
       A: nn.Module
+
       @nn.compact
       def __call__(self, x):
         return self.A(x)
+
     class C(nn.Module):
       A: nn.Module
       B: nn.Module
+
       @partial(
-          nn.vmap,
-          variable_axes={'params': 0},
-          split_rngs={'params': True})
+          nn.vmap, variable_axes={'params': 0}, split_rngs={'params': True}
+      )
       @nn.compact
       def __call__(self, x):
         return self.B(x) + self.A(x)
+
     class Csimple(nn.Module):
       A: nn.Module
       B: nn.Module
+
       @nn.compact
       def __call__(self, x):
         return self.B(x) + self.A(x)
+
     class D(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         a1 = A()
         a2 = A()
         b = B(a1)
         c = C(a2, b)
         return c(x)
@@ -785,55 +916,62 @@
     x = jnp.ones((10, 10))
     p1 = D().init(key, x)
     y1 = D().apply(p1, x)
 
     a1 = A()
     a2 = A()
     b = B(a1)
-    p2 = freeze({'params': {
-        'A': p1['params']['A_0'],
-        'B': {
-            'A': p1['params']['A_1'],
+    p2 = freeze(
+        {
+            'params': {
+                'A': p1['params']['A_0'],
+                'B': {
+                    'A': p1['params']['A_1'],
+                },
+            }
         }
-    }})
+    )
 
     # Test method wrapper transform.
     y2 = C(a2, b).apply(p2, x)
     np.testing.assert_allclose(y1, y2, atol=1e-7)
     # Test class transform.
-    Ctrafo = nn.vmap(Csimple,
-                     variable_axes={'params': 0},
-                     split_rngs={'params': True})
+    Ctrafo = nn.vmap(
+        Csimple, variable_axes={'params': 0}, split_rngs={'params': True}
+    )
 
     y3 = Ctrafo(a2, b).apply(p2, x)
     np.testing.assert_allclose(y1, y3, atol=1e-7)
 
-  @temp_flip_flag('return_frozendict', False)
   def test_toplevel_submodule_adoption_pytree_transform(self):
     class A(nn.Module):
+
       @nn.compact
       def __call__(self, c, x):
         counter = self.variable('counter', 'i', jnp.zeros, ())
         counter.value += 1
         x = nn.Dense(1)(x)
         return c, x
 
     class B(nn.Module):
       A: Any
+
       @nn.compact
       def __call__(self, c, x):
         return self.A['foo'](*self.A['bar'](c, x))
 
     a = A()
     As = {'foo': A(), 'bar': A()}
-    b = nn.scan(B,
-                in_axes=0,
-                variable_carry='counter',
-                variable_broadcast='params',
-                split_rngs={'params': False})(As)
+    b = nn.scan(
+        B,
+        in_axes=0,
+        variable_carry='counter',
+        variable_broadcast='params',
+        split_rngs={'params': False},
+    )(As)
 
     key = random.PRNGKey(0)
     x = jnp.ones((10, 2))
 
     p = B(As).init(key, x, x)
     y, cntrs = b.apply(p, x, x, mutable='counter')
     ref_cntrs = {
@@ -841,682 +979,857 @@
             'A_bar': {
                 'i': jnp.array(11.0),
             },
             'A_foo': {
                 'i': jnp.array(11.0),
             },
         },
-      }
-    self.assertTrue(jax.tree_util.tree_all(
-        jax.tree_util.tree_map(
-            lambda x, y: np.testing.assert_allclose(x, y, atol=1e-7),
-            cntrs, ref_cntrs)
-        ))
+    }
+    self.assertTrue(
+        jax.tree_util.tree_all(
+            jax.tree_util.tree_map(
+                lambda x, y: np.testing.assert_allclose(x, y, atol=1e-7),
+                cntrs,
+                ref_cntrs,
+            )
+        )
+    )
 
-  @temp_flip_flag('return_frozendict', False)
   def test_partially_applied_module_constructor_transform(self):
     k = random.PRNGKey(0)
-    x = jnp.ones((3,4,4))
+    x = jnp.ones((3, 4, 4))
     dense = partial(nn.Dense, use_bias=False)
     vmap_dense = nn.vmap(
-        dense,
-        variable_axes={'params':0},
-        split_rngs={'params':True})(4)
+        dense, variable_axes={'params': 0}, split_rngs={'params': True}
+    )(4)
     init_vars = vmap_dense.init(k, x)
     init_vars_shapes = jax.tree_util.tree_map(jnp.shape, init_vars)
     ref_var_shapes = {
         'params': {
             'kernel': (3, 4, 4),
         },
     }
     self.assertTrue(tree_equals(init_vars_shapes, ref_var_shapes))
 
-  @temp_flip_flag('return_frozendict', False)
   def test_partial_module_method(self):
     k = random.PRNGKey(0)
-    x = jnp.ones((3,4,4))
+    x = jnp.ones((3, 4, 4))
+
     class Foo(nn.Module):
 
       @nn.compact
       def inner(self, x):
         return nn.Dense(2, use_bias=False)(x)
 
       def __call__(self, x):
         return nn.vmap(
             partial(Foo.inner),
-            variable_axes={'params':0},
-            split_rngs={'params':True})(self, x)
+            variable_axes={'params': 0},
+            split_rngs={'params': True},
+        )(self, x)
 
     init_vars = Foo().init(k, x)
     init_vars_shapes = jax.tree_util.tree_map(jnp.shape, init_vars)
     ref_var_shapes = {
-        'params': {
-          'Dense_0': {'kernel': (3, 4, 2)}
-        },
+        'params': {'Dense_0': {'kernel': (3, 4, 2)}},
     }
     self.assertTrue(tree_equals(init_vars_shapes, ref_var_shapes))
 
   def test_variable_in_args_transform(self):
     class Test(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         baz = self.variable('test', 'baz', jnp.zeros, x.shape)
         y = self.mutate_variable_in_method(x, baz)
         return y
+
       @nn.jit
       def mutate_variable_in_method(self, x, baz):
         baz.value += x
         return baz.value
 
     k = random.PRNGKey(0)
     x = jnp.ones((1,))
     variables = Test().init(k, x)
-    np.testing.assert_allclose(variables['test']['baz'],
-                               jnp.array([1.0,]), atol=1e-7)
+    np.testing.assert_allclose(
+        variables['test']['baz'],
+        jnp.array([
+            1.0,
+        ]),
+        atol=1e-7,
+    )
     y, variables = Test().apply(variables, x, mutable=['test'])
-    np.testing.assert_allclose(variables['test']['baz'],
-                               jnp.array([2.0,]), atol=1e-7)
+    np.testing.assert_allclose(
+        variables['test']['baz'],
+        jnp.array([
+            2.0,
+        ]),
+        atol=1e-7,
+    )
 
   def test_module_instance_in_args_transform(self):
     class Inner(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         baz = self.variable('test', 'baz', jnp.zeros, x.shape)
         baz.value += x
         return baz.value
 
     class Test(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
-        inner = Inner(name="inner")
+        inner = Inner(name='inner')
         y = self.call_instance_arg_in_method(x, inner)
         return y
+
       @nn.jit
       def call_instance_arg_in_method(self, x, inner):
         return inner(x)
 
     k = random.PRNGKey(0)
     x = jnp.ones((1,))
     variables = Test().init(k, x)
-    np.testing.assert_allclose(variables['test']['inner']['baz'],
-                                jnp.array([1.0,]), atol=1e-7)
+    np.testing.assert_allclose(
+        variables['test']['inner']['baz'],
+        jnp.array([
+            1.0,
+        ]),
+        atol=1e-7,
+    )
     y, variables = Test().apply(variables, x, mutable=['test'])
-    np.testing.assert_allclose(variables['test']['inner']['baz'],
-                                jnp.array([2.0,]), atol=1e-7)
+    np.testing.assert_allclose(
+        variables['test']['inner']['baz'],
+        jnp.array([
+            2.0,
+        ]),
+        atol=1e-7,
+    )
 
   def test_module_instance_in_args_transform_nested(self):
     class Inner(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         baz = self.variable('test', 'baz', jnp.zeros, x.shape)
         baz.value += x
         return baz.value
 
     class Outer(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, inner, x):
         y = self.call_instance_arg_in_method(x, inner)
         return y
+
       @nn.jit
       def call_instance_arg_in_method(self, x, inner):
         return inner(x)
 
     class Test(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
-        inner = Inner(name="inner")
-        outer = Outer(name="outer")
+        inner = Inner(name='inner')
+        outer = Outer(name='outer')
         return outer(inner, x)
 
     k = random.PRNGKey(0)
     x = jnp.ones((1,))
     variables = Test().init(k, x)
-    np.testing.assert_allclose(variables['test']['inner']['baz'],
-                                jnp.array([1.0,]), atol=1e-7)
+    np.testing.assert_allclose(
+        variables['test']['inner']['baz'],
+        jnp.array([
+            1.0,
+        ]),
+        atol=1e-7,
+    )
     y, variables = Test().apply(variables, x, mutable=['test'])
-    np.testing.assert_allclose(variables['test']['inner']['baz'],
-                                jnp.array([2.0,]), atol=1e-7)
-
+    np.testing.assert_allclose(
+        variables['test']['inner']['baz'],
+        jnp.array([
+            2.0,
+        ]),
+        atol=1e-7,
+    )
 
   def test_nested_variable_passing(self):
     class NestedVarUser(nn.Module):
       somevar: nn.Variable
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         self.somevar.value += x
         return x
+
     class VarUser(nn.Module):
       somevar: nn.Variable
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         return NestedVarUser(self.somevar)(x)
+
     class VarPasser(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         baz = self.variable('test', 'baz', jnp.zeros, x.shape)
         y = VarUser(baz)(x)
         return y
 
     k = random.PRNGKey(0)
     x = jnp.ones((1,))
     variables = VarPasser().init(k, x)
-    np.testing.assert_allclose(variables['test']['baz'],
-                               jnp.array([1.0,]), atol=1e-7)
+    np.testing.assert_allclose(
+        variables['test']['baz'],
+        jnp.array([
+            1.0,
+        ]),
+        atol=1e-7,
+    )
     y, variables = VarPasser().apply(variables, x, mutable=['test'])
-    np.testing.assert_allclose(variables['test']['baz'],
-                               jnp.array([2.0,]), atol=1e-7)
+    np.testing.assert_allclose(
+        variables['test']['baz'],
+        jnp.array([
+            2.0,
+        ]),
+        atol=1e-7,
+    )
 
   def test_returned_module_warning(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return x
+
     class Bar(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         f = self._helper()
         return f(x)
+
       @nn.jit
       def _helper(self):
         return Foo()
+
     b = Bar()
     with self.assertRaises(errors.TransformedMethodReturnValueError):
       b.apply({}, jnp.ones(2))
 
   def test_returned_variable_warning(self):
     class Bar(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         f = self._helper()
         return f(x)
+
       @nn.jit
       def _helper(self):
         return nn.Variable(None, None, None, False)
+
     b = Bar()
     with self.assertRaises(errors.TransformedMethodReturnValueError):
       b.apply({}, jnp.ones(2))
 
   def test_nowrap(self):
     class Bar(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return self._helper(x)
+
       @nn.nowrap
       def _helper(self, x):
         if len(nn.module._context.module_stack) > 2:  # pylint: disable=protected-access
           raise ValueError('Module stack too deep.')
         return x
 
     b = Bar()
     b.apply({}, jnp.ones(2))
 
   def test_map_variables_tied_autoencoder(self):
     def trans(variables):
       return jax.tree_util.tree_map(lambda x: x.T, variables)
 
     class TiedAutencoder(nn.Module):
-
       features: int
       latents: int
 
       @nn.compact
       def _call(self, x, decode):
         def f(self):
-          return nn.Dense(self.features if decode else self.latents, use_bias=False)(x)
+          return nn.Dense(
+              self.features if decode else self.latents, use_bias=False
+          )(x)
 
         if decode:
           map_fn = trans
         else:
           map_fn = lambda x: x
-        return nn.map_variables(f, "params", map_fn, map_fn, mutable=True)(self)
+        return nn.map_variables(f, 'params', map_fn, map_fn, mutable=True)(self)
 
       def encode(self, x):
         return self._call(x, False)
 
       def decode(self, x):
         return self._call(x, True)
 
       def __call__(self, x):
         return self.decode(self.encode(x))
 
     x = jnp.ones((2, 4))
     ae = TiedAutencoder(4, 5)
     variables = ae.init(random.PRNGKey(0), x)
-    param_shapes = jax.tree_util.tree_map(jnp.shape, variables["params"])
-    self.assertEqual(param_shapes, {
-      "Dense_0": {"kernel": (4, 5)}
-    })
-
+    param_shapes = jax.tree_util.tree_map(jnp.shape, variables['params'])
+    self.assertEqual(param_shapes, {'Dense_0': {'kernel': (4, 5)}})
 
   def test_map_variables_bit_weights(self):
     class BitWeights(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         def sign(x):
           return jax.tree_util.tree_map(jnp.sign, x)
-        BitDense = nn.map_variables(nn.Dense, "params", sign, init=True)
+
+        BitDense = nn.map_variables(nn.Dense, 'params', sign, init=True)
         return BitDense(4)(x)
+
     bw = BitWeights()
     x = jnp.ones((2, 4))
     y, variables = bw.init_with_output(random.PRNGKey(0), x)
     y_2 = bw.apply(variables, x)
     np.testing.assert_allclose(y, y_2)
 
-
   def test_remat_scan(self):
     class BigModel(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         DenseStack = nn.remat_scan(nn.Dense, lengths=(100,))
-        return DenseStack(8, name="dense_stack")(x)
+        return DenseStack(8, name='dense_stack')(x)
 
     x = jnp.ones((2, 8))
     model = BigModel()
     variables = model.init(random.PRNGKey(0), x)
     param_shapes = jax.tree_util.tree_map(jnp.shape, variables['params'])
-    self.assertEqual(param_shapes["dense_stack"]["kernel"], (100, 8, 8))
-    self.assertEqual(param_shapes["dense_stack"]["bias"], (100, 8))
+    self.assertEqual(param_shapes['dense_stack']['kernel'], (100, 8, 8))
+    self.assertEqual(param_shapes['dense_stack']['bias'], (100, 8))
     y = model.apply(variables, x)
     self.assertEqual(y.shape, (2, 8))
 
-
   def test_vjp(self):
     class Bar(nn.Module):
+
       @nn.compact
       def __call__(self, x, y):
         p = self.param('test', nn.initializers.constant(0.5), ())
         self.variable('state', 'counter', lambda: 0)
         return p * x * y
 
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x, y):
         z, bwd = nn.vjp(Bar.__call__, Bar(), x, y)
         return bwd(jnp.ones(z.shape))
 
-    x = jnp.array([1., 2., 3.])
-    y = jnp.array([4., 5., 6.])
+    x = jnp.array([1.0, 2.0, 3.0])
+    y = jnp.array([4.0, 5.0, 6.0])
     params = Foo().init(random.PRNGKey(0), x, y)
     params_grad, x_grad, y_grad = Foo().apply(params, x, y)
-    self.assertEqual(params_grad, {
-      'params': nn.FrozenDict({'test': 32.}),
-    })
-    np.testing.assert_allclose(x_grad, [2., 2.5, 3.])
-    np.testing.assert_allclose(y_grad, [0.5, 1., 1.5])
+    self.assertEqual(
+        params_grad,
+        {
+            'params': nn.FrozenDict({'test': 32.0}),
+        },
+    )
+    np.testing.assert_allclose(x_grad, [2.0, 2.5, 3.0])
+    np.testing.assert_allclose(y_grad, [0.5, 1.0, 1.5])
 
   def test_jvp(self):
     class Bar(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         p = self.param('test', nn.initializers.zeros, ())
         self.variable('state', 'counter', lambda: 0)
         return p * x
 
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         bar = Bar()
-        vars_t = jax.tree_util.tree_map(jnp.ones_like, bar.variables.get('params', {}))
-        _, out_t = nn.jvp(Bar.__call__, bar, (x,), (jnp.zeros_like(x),), {'params': vars_t})
+        vars_t = jax.tree_util.tree_map(
+            jnp.ones_like, bar.variables.get('params', {})
+        )
+        _, out_t = nn.jvp(
+            Bar.__call__, bar, (x,), (jnp.zeros_like(x),), {'params': vars_t}
+        )
         return out_t
 
     x = jnp.ones((3,))
     params = Foo().init(random.PRNGKey(0), x)
     y_t = Foo().apply(params, x)
     np.testing.assert_allclose(y_t, jnp.ones_like(x))
 
   def test_complicated_alias_mutation(self):
     class A(nn.Module):
       b: nn.Module
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         return self.b(x)
+
     class B(nn.Module):
       c: nn.Module
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         y = C(name='outer_c')(x)
         z = self.c(x)
         return z
+
     class C(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         initialized = self.has_variable('muts', 'v')
         v = self.variable('muts', 'v', lambda: jnp.zeros_like(x))
         if initialized:
           v.value += x
         return x
 
     a = A(b=B(c=C()))
     k = random.PRNGKey(0)
     x = jnp.ones((1,), jnp.float32)
     vs = a.init(k, x)
-    y, vs_new = a.apply(vs, x, mutable=['muts',])
-    np.testing.assert_array_equal(vs_new['muts']['b']['c']['v'],
-                                  jnp.array([1.], jnp.float32))
-    np.testing.assert_array_equal(vs_new['muts']['b']['outer_c']['v'],
-                                  jnp.array([1.], jnp.float32))
+    y, vs_new = a.apply(
+        vs,
+        x,
+        mutable=[
+            'muts',
+        ],
+    )
+    np.testing.assert_array_equal(
+        vs_new['muts']['b']['c']['v'], jnp.array([1.0], jnp.float32)
+    )
+    np.testing.assert_array_equal(
+        vs_new['muts']['b']['outer_c']['v'], jnp.array([1.0], jnp.float32)
+    )
 
   def test_custom_vjp(self):
-
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         def f(mdl, x):
           return mdl(x)
 
         def fwd(mdl, x):
           return nn.vjp(f, mdl, x)
 
         def bwd(vjp_fn, y_t):
           params_t, input_t = vjp_fn(y_t)
           params_t = jax.tree_util.tree_map(jnp.sign, params_t)
           return params_t, input_t
 
-        sign_grad = nn.custom_vjp(
-            f, forward_fn=fwd, backward_fn=bwd)
+        sign_grad = nn.custom_vjp(f, forward_fn=fwd, backward_fn=bwd)
         return sign_grad(nn.Dense(1), x).reshape(())
+
     x = jnp.ones((2,))
     variables = Foo().init(random.PRNGKey(0), x)
     grad = jax.grad(Foo().apply)(variables, x)
     for grad_leaf in jax.tree_util.tree_leaves(grad):
-      self.assertTrue(jnp.all(jnp.abs(grad_leaf) == 1.))
+      self.assertTrue(jnp.all(jnp.abs(grad_leaf) == 1.0))
 
   def test_transform_with_setup_and_methods_on_submodules(self):
     # This is the archetypal example motivating the introduction of
     # SetupState as a triple-enum to handle multiple setup() calls
     # across transform boundaries and scope reuse.
     class Foo(nn.Module):
+
       def setup(self):
         self.inner = nn.Dense(2)
+
       def helper(self, x, m):
         return m(x)
+
       def __call__(self, x):
         return self.helper(x, self.inner)
+
     k = random.PRNGKey(0)
     x = jnp.ones((2,))
     vs_foo = Foo().init(k, x)
 
     class Bar(nn.Module):
+
       def setup(self):
         self.inner = nn.Dense(2)
+
       @nn.jit
       def helper(self, x, m):
         return m(x)
+
       @nn.jit
       def __call__(self, x):
         return self.helper(x, self.inner)
+
     vs_bar = Bar().init(k, x)
-    self.assertTrue(tree_equals(
-      jax.tree_util.tree_map(jnp.shape, vs_foo),
-      jax.tree_util.tree_map(jnp.shape, vs_bar)))
+    self.assertTrue(
+        tree_equals(
+            jax.tree_util.tree_map(jnp.shape, vs_foo),
+            jax.tree_util.tree_map(jnp.shape, vs_bar),
+        )
+    )
 
   def test_transform_methods_on_submodules_still_reserve_names(self):
     class Foo(nn.Module):
+
       @nn.jit
       def helper(self, x, m):
-        conflicting_a = nn.Dense(2, name="a")
+        conflicting_a = nn.Dense(2, name='a')
         return m(x)
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
-        a = nn.Dense(2, name="a")
+        a = nn.Dense(2, name='a')
         return self.helper(x, a)
+
     k = random.PRNGKey(0)
     x = jnp.ones((2,))
     with self.assertRaises(errors.NameInUseError):
       vs = Foo().init(k, x)
 
   def test_transform_setup_still_reserve_names(self):
     class Identity(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return x
+
     class Test(nn.Module):
+
       def setup(self):
         self.sub = Identity()
         self.sub = Identity()
+
       @nn.jit
       def __call__(self, x):
         return x
 
     k = random.PRNGKey(0)
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
 
     with self.assertRaises(errors.NameInUseError):
       y = Test().init(k, x)
 
   def test_transform_with_setup_and_methods_on_submodule_pytrees(self):
     class Foo(nn.Module):
+
       def setup(self):
         self.inners = [nn.Dense(2), nn.Dense(2)]
+
       def helper(self, x, ms):
         return ms[0](x) + ms[1](x)
+
       def __call__(self, x):
         return self.helper(x, self.inners)
+
     class JitFoo(nn.Module):
+
       def setup(self):
         self.inners = [nn.Dense(2), nn.Dense(2)]
+
       @nn.jit
       def helper(self, x, ms):
         return ms[0](x) + ms[1](x)
+
       @nn.jit
       def __call__(self, x):
         return self.helper(x, self.inners)
 
     k = random.PRNGKey(0)
     x = jnp.ones((2,))
 
     vs_0 = Foo().init(k, x)
     vs_1 = JitFoo().init(k, x)
 
     self.assertTrue(tree_allclose(vs_0, vs_1))
 
   def test_transform_setup_still_reserve_names_pytrees(self):
     class Identity(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return x
+
     class Test(nn.Module):
+
       def setup(self):
         self.subs = [Identity(), Identity()]
         self.subs = [Identity(), Identity()]
+
       @nn.jit
       def __call__(self, x):
         return x
 
     k = random.PRNGKey(0)
-    x = jnp.array([1.])
+    x = jnp.array([1.0])
 
     msg = r'Could not create submodule "subs_0".*'
     with self.assertRaisesRegex(errors.NameInUseError, msg):
       y = Test().init(k, x)
 
   def test_scan_of_setup_parameter(self):
     class Body(nn.Module):
+
       def setup(self):
         self.dense = nn.Dense(1)
         self.p = self.param('p', lambda k: jnp.ones((1,)))
+
       def __call__(self, x):
         return self.dense(x) + self.p, None
+
     scanbody = nn.scan(
-      Body,
-      variable_axes={'params': 0},
-      split_rngs={'params': True},
-      length=2)
+        Body, variable_axes={'params': 0}, split_rngs={'params': True}, length=2
+    )
     k = random.PRNGKey(0)
     x = jnp.ones((1,))
     vs = scanbody().init(k, x)
     y = scanbody().apply(vs, x)
 
   def test_multi_method_class_transform(self):
     class Foo(nn.Module):
+
       def setup(self):
         self.dense0 = nn.Dense(2)
         self.dense1 = nn.Dense(2)
+
       def method_0(self, x):
         return self.dense0(x), x
+
       def method_1(self, x, y):
         return self.dense1(x) + y, None
+
     class Bar(nn.Module):
+
       @nn.compact
       def __call__(self, x):
-        ScanFoo = nn.scan(Foo,
-                          methods={
-                            'method_0': dict(
-                              variable_axes={'params': 0},
-                              split_rngs={'params': True},
-                              in_axes=nn.broadcast, out_axes=0,
-                              length=3),
-                            'method_1': dict(
-                              variable_axes={'params': 0},
-                              split_rngs={'params': True},
-                              in_axes=0,
-                              length=3)
-                          })
+        ScanFoo = nn.scan(
+            Foo,
+            methods={
+                'method_0': dict(
+                    variable_axes={'params': 0},
+                    split_rngs={'params': True},
+                    in_axes=nn.broadcast,
+                    out_axes=0,
+                    length=3,
+                ),
+                'method_1': dict(
+                    variable_axes={'params': 0},
+                    split_rngs={'params': True},
+                    in_axes=0,
+                    length=3,
+                ),
+            },
+        )
         sf = ScanFoo()
         y, ys = sf.method_0(x)
         z, _ = sf.method_1(y, ys)
         return z
 
     k = random.PRNGKey(0)
-    x = random.uniform(random.PRNGKey(1), (2,2))
+    x = random.uniform(random.PRNGKey(1), (2, 2))
     vs = Bar().init(k, x)
     y = Bar().apply(vs, x)
 
   def test_compact_aliasing_collision(self):
     class Foo(nn.Module):
       m1: nn.Module
       m2: nn.Module
+
       @nn.compact
       def __call__(self, x):
         x = self.m2(self.m1(x))
         return x
+
     class Bar(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         dense = nn.Dense(2)
         x = nn.jit(Foo)(dense, dense)(x)
         return x
+
     k = random.PRNGKey(0)
     x = jnp.zeros((2, 2))
     _ = Bar().init(k, x)
 
   def test_compact_aliasing_collision_arg_and_attrib(self):
     class Foo(nn.Module):
       m1: nn.Module
+
       @nn.compact
       def __call__(self, x, m2):
         x = m2(self.m1(x))
         return x
+
     class Bar(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         dense = nn.Dense(2)
         x = nn.jit(Foo)(dense)(x, dense)
         return x
+
     k = random.PRNGKey(0)
     x = jnp.zeros((2, 2))
     _ = Bar().init(k, x)
 
   def test_jit_with_setup_helpers(self):
     class Foo(nn.Module):
+
       def setup(self):
         self.a = nn.Dense(2)
         self.setup_helper()
+
       def setup_helper(self):
         self.b = nn.Dense(2)
+
       def __call__(self, x):
         return self.b(self.a(x))
+
     class JitFoo(nn.Module):
+
       def setup(self):
         self.a = nn.Dense(2)
         self.setup_helper()
+
       def setup_helper(self):
         self.b = nn.Dense(2)
+
       @nn.jit
       def __call__(self, x):
         return self.b(self.a(x))
+
     k = random.PRNGKey(0)
-    x = jnp.ones((2,2))
+    x = jnp.ones((2, 2))
     vs = JitFoo().init(k, x)
     y0 = JitFoo().apply(vs, x)
     vs = Foo().init(k, x)
     y1 = Foo().apply(vs, x)
     np.testing.assert_array_equal(y0, y1)
 
   def test_while_loop(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         key_zero = random.PRNGKey(0)
         key_zero = jnp.broadcast_to(key_zero, (2, *key_zero.shape))
         self.param('inc', lambda _: 1)
         self.put_variable('state', 'acc', 0)
         self.put_variable('state', 'rng_params', key_zero)
         self.put_variable('state', 'rng_loop', key_zero)
 
         def cond_fn(mdl, c):
           acc = mdl.get_variable('state', 'acc')
           return acc < x
+
         def body_fn(mdl, c):
           i = mdl.get_variable('state', 'acc')
           p_rng = mdl.make_rng('params')
           l_rng = mdl.make_rng('loop')
-          mdl.put_variable('state', 'rng_params', mdl.get_variable('state', 'rng_params').at[i].set(p_rng))
-          mdl.put_variable('state', 'rng_loop', mdl.get_variable('state', 'rng_loop').at[i].set(l_rng))
+          mdl.put_variable(
+              'state',
+              'rng_params',
+              mdl.get_variable('state', 'rng_params').at[i].set(p_rng),
+          )
+          mdl.put_variable(
+              'state',
+              'rng_loop',
+              mdl.get_variable('state', 'rng_loop').at[i].set(l_rng),
+          )
           inc = mdl.get_variable('params', 'inc')
           mdl.put_variable('state', 'acc', i + inc)
           return c
+
         return nn.while_loop(
-            cond_fn, body_fn, self, (),
-            carry_variables='state', split_rngs={'params': False, 'loop': True})
+            cond_fn,
+            body_fn,
+            self,
+            (),
+            carry_variables='state',
+            split_rngs={'params': False, 'loop': True},
+        )
+
     x = 2
     mdl = Foo()
-    _, vars = mdl.apply({}, x, mutable=True, rngs={'params': random.PRNGKey(1), 'loop': random.PRNGKey(2)})
+    _, vars = mdl.apply(
+        {},
+        x,
+        mutable=True,
+        rngs={'params': random.PRNGKey(1), 'loop': random.PRNGKey(2)},
+    )
     self.assertEqual(vars['state']['acc'], x)
-    np.testing.assert_array_equal(vars['state']['rng_params'][0], vars['state']['rng_params'][1])
-    np.testing.assert_array_compare(operator.__ne__, vars['state']['rng_loop'][0], vars['state']['rng_loop'][1])
+    np.testing.assert_array_equal(
+        vars['state']['rng_params'][0], vars['state']['rng_params'][1]
+    )
+    np.testing.assert_array_compare(
+        operator.__ne__,
+        vars['state']['rng_loop'][0],
+        vars['state']['rng_loop'][1],
+    )
 
   def test_cond(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x, pred):
         self.variable('state', 'true_count', lambda: 0)
         self.variable('state', 'false_count', lambda: 0)
+
         def true_fn(mdl, x):
           mdl.variable('state', 'true_count').value += 1
           return nn.Dense(2, name='dense')(x)
 
         def false_fn(mdl, x):
           mdl.variable('state', 'false_count').value += 1
           return -nn.Dense(2, name='dense')(x)
 
         return nn.cond(pred, true_fn, false_fn, self, x)
 
-  @temp_flip_flag('return_frozendict', False)
   def test_switch(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x, pred):
         self.variable('state', 'a_count', lambda: 0)
         self.variable('state', 'b_count', lambda: 0)
         self.variable('state', 'c_count', lambda: 0)
+
         def a_fn(mdl, x):
           mdl.variable('state', 'a_count').value += 1
           return nn.Dense(2, name='dense')(x)
 
         def b_fn(mdl, x):
           mdl.variable('state', 'b_count').value += 1
           return -nn.Dense(2, name='dense')(x)
@@ -1527,104 +1840,122 @@
 
         return nn.switch(pred, [a_fn, b_fn, c_fn], self, x)
 
     x = jnp.ones((1, 3))
     foo = Foo()
     y1, vars = foo.init_with_output(random.PRNGKey(0), x, 0)
     self.assertEqual(vars['state'], {'a_count': 1, 'b_count': 0, 'c_count': 0})
-    y2, updates = foo.apply(vars, x, 1, mutable="state")
+    y2, updates = foo.apply(vars, x, 1, mutable='state')
     vars = copy(vars, updates)
     self.assertEqual(vars['state'], {'a_count': 1, 'b_count': 1, 'c_count': 0})
     np.testing.assert_allclose(y1, -y2)
-    y3, updates = foo.apply(vars, x, 2, mutable="state")
+    y3, updates = foo.apply(vars, x, 2, mutable='state')
     vars = copy(vars, updates)
     self.assertEqual(vars['state'], {'a_count': 1, 'b_count': 1, 'c_count': 1})
     np.testing.assert_allclose(y1, y3)
 
-  @temp_flip_flag('return_frozendict', False)
   def test_switch_multihead(self):
     class Foo(nn.Module):
+
       def setup(self) -> None:
         self.heads = [
-          nn.Sequential([nn.Dense(10), nn.Dense(7), nn.Dense(5)]),
-          nn.Sequential([nn.Dense(11), nn.Dense(5)]),
-          nn.Dense(5),
+            nn.Sequential([nn.Dense(10), nn.Dense(7), nn.Dense(5)]),
+            nn.Sequential([nn.Dense(11), nn.Dense(5)]),
+            nn.Dense(5),
         ]
 
       @nn.compact
       def __call__(self, x, index):
         def head_fn(i):
           def fn(mdl, x):
             mdl.variable('state', f'{i}_count', lambda: -1).value += 1
             return mdl.heads[i](x)
+
           return fn
 
         branches = [head_fn(i) for i in range(len(self.heads))]
 
         if self.is_mutable_collection('params'):
           for branch in branches:
             _ = branch(self, x)
 
         return nn.switch(index, branches, self, x)
 
     x = jnp.ones((1, 3))
     foo = Foo()
     y1, vars = foo.init_with_output(random.PRNGKey(0), x, 0)
     self.assertEqual(vars['state'], {'0_count': 1, '1_count': 0, '2_count': 0})
-    y2, updates = foo.apply(vars, x, 1, mutable="state")
+    y2, updates = foo.apply(vars, x, 1, mutable='state')
     vars = copy(vars, updates)
     self.assertEqual(vars['state'], {'0_count': 1, '1_count': 1, '2_count': 0})
-    y3, updates = foo.apply(vars, x, 2, mutable="state")
+    y3, updates = foo.apply(vars, x, 2, mutable='state')
     vars = copy(vars, updates)
     self.assertEqual(vars['state'], {'0_count': 1, '1_count': 1, '2_count': 1})
 
-    self.assertEqual(vars['params']['heads_0']['layers_0']['kernel'].shape, (3, 10))
+    self.assertEqual(
+        vars['params']['heads_0']['layers_0']['kernel'].shape, (3, 10)
+    )
     self.assertEqual(vars['params']['heads_0']['layers_0']['bias'].shape, (10,))
-    self.assertEqual(vars['params']['heads_0']['layers_1']['kernel'].shape, (10, 7))
+    self.assertEqual(
+        vars['params']['heads_0']['layers_1']['kernel'].shape, (10, 7)
+    )
     self.assertEqual(vars['params']['heads_0']['layers_1']['bias'].shape, (7,))
-    self.assertEqual(vars['params']['heads_0']['layers_2']['kernel'].shape, (7, 5))
+    self.assertEqual(
+        vars['params']['heads_0']['layers_2']['kernel'].shape, (7, 5)
+    )
     self.assertEqual(vars['params']['heads_0']['layers_2']['bias'].shape, (5,))
 
-    self.assertEqual(vars['params']['heads_1']['layers_0']['kernel'].shape, (3, 11))
+    self.assertEqual(
+        vars['params']['heads_1']['layers_0']['kernel'].shape, (3, 11)
+    )
     self.assertEqual(vars['params']['heads_1']['layers_0']['bias'].shape, (11,))
-    self.assertEqual(vars['params']['heads_1']['layers_1']['kernel'].shape, (11, 5))
+    self.assertEqual(
+        vars['params']['heads_1']['layers_1']['kernel'].shape, (11, 5)
+    )
     self.assertEqual(vars['params']['heads_1']['layers_1']['bias'].shape, (5,))
 
     self.assertEqual(vars['params']['heads_2']['kernel'].shape, (3, 5))
     self.assertEqual(vars['params']['heads_2']['bias'].shape, (5,))
 
-
-
   def test_lift_instance_error(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         return nn.checkpoint(nn.Dense(2))(x)
+
     with self.assertRaises(errors.TransformTargetError):
       Foo().init(random.PRNGKey(0), jnp.zeros((2, 3)))
 
   def test_scan_compact_count(self):
     class Foo(nn.Module):
       num_layers: int = 5
 
       @nn.compact
       def __call__(self, x):
         def body_fn(mdl, x):
           return nn.Dense(features=x.shape[-1])(x), ()
-        x, _ = nn.scan(body_fn, length=self.num_layers, variable_axes={"params": 0}, split_rngs={"params": True})(self, x)
+
+        x, _ = nn.scan(
+            body_fn,
+            length=self.num_layers,
+            variable_axes={'params': 0},
+            split_rngs={'params': True},
+        )(self, x)
         return x
 
     m = Foo()
     x = jnp.ones((3,))
     v = m.init(jax.random.PRNGKey(0), x)
     self.assertEqual(v['params']['Dense_0']['kernel'].shape, (5, 3, 3))
     m.apply(v, x)
 
   def test_bound_methods_in_direct_transforms(self):
     class CondModel(nn.Module):
+
       def setup(self):
         self.dense = nn.Dense(3)
 
       def f1(self, arr):
         arr = self.dense(arr)
         return arr
 
@@ -1634,73 +1965,113 @@
 
       def __call__(self, x):
         return nn.cond(x.sum() > 0, self.f1, self.f2, self, x)
 
     cond_model = CondModel()
 
     output, init_params = jax.jit(cond_model.init_with_output)(
-        jax.random.PRNGKey(0),
-        x=jnp.ones(3))
+        jax.random.PRNGKey(0), x=jnp.ones(3)
+    )
 
   def test_add_metadata_axis(self):
     vars_copy = None
+
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         nonlocal vars_copy
-        kernel_init=nn.with_partitioning(
-            nn.initializers.lecun_normal(), ('foo', 'bar'))
+        kernel_init = nn.with_partitioning(
+            nn.initializers.lecun_normal(), ('foo', 'bar')
+        )
         vars_copy = self.variables
-        return nn.Dense(4, kernel_init=kernel_init, use_bias=False, name="dense")(x)
+        return nn.Dense(
+            4, kernel_init=kernel_init, use_bias=False, name='dense'
+        )(x)
+
     class Test(nn.Module):
-      @partial(nn.add_metadata_axis,
-               variable_axes={'params': 0},
-               metadata_params={nn.PARTITION_NAME: 'baz'})
+
+      @partial(
+          nn.add_metadata_axis,
+          variable_axes={'params': 0},
+          metadata_params={nn.PARTITION_NAME: 'baz'},
+      )
       @nn.compact
       def __call__(self, x):
-        return Foo(name="foo")(x)
+        return Foo(name='foo')(x)
 
     k = random.PRNGKey(0)
-    x = jnp.ones((4,4), dtype=jnp.float32)
+    x = jnp.ones((4, 4), dtype=jnp.float32)
     vs = Test().init(k, x)
     y = Test().apply(vs, x)
-    outer_expect = jax.tree_map(jnp.shape,
-        freeze({'params': {'foo': {'dense': {'kernel':
-            nn.Partitioned(jnp.ones((4, 4)), names=('baz', 'foo', 'bar'))}}}}))
-    inner_expect = jax.tree_map(jnp.shape,
-        freeze({'params': {'dense': {'kernel':
-            nn.Partitioned(jnp.ones((4, 4)), names=('foo', 'bar'))}}}))
+    outer_expect = jax.tree_map(
+        jnp.shape,
+        freeze(
+            {
+                'params': {
+                    'foo': {
+                        'dense': {
+                            'kernel': nn.Partitioned(
+                                jnp.ones((4, 4)), names=('baz', 'foo', 'bar')
+                            )
+                        }
+                    }
+                }
+            }
+        ),
+    )
+    inner_expect = jax.tree_map(
+        jnp.shape,
+        freeze(
+            {
+                'params': {
+                    'dense': {
+                        'kernel': nn.Partitioned(
+                            jnp.ones((4, 4)), names=('foo', 'bar')
+                        )
+                    }
+                }
+            }
+        ),
+    )
     self.assertEqual(jax.tree_map(jnp.shape, vs), outer_expect)
     self.assertEqual(jax.tree_map(jnp.shape, vars_copy), inner_expect)
 
-
   def test_outer_setup_called_with_sharing_across_transforms(self):
     class A(nn.Module):
+
       def setup(self):
-        self.foo = self.param(
-            'foo', nn.initializers.zeros, (2, 2), jnp.float32)
+        self.foo = self.param('foo', nn.initializers.zeros, (2, 2), jnp.float32)
+
       def __call__(self, x):
         return self.foo
+
     class B(nn.Module):
       a: Any
+
       @nn.compact
       def __call__(self, x):
         return self.a(x)
+
     class C(nn.Module):
+
       def setup(self):
         self.a = A()
         self.b = nn.jit(B)(self.a)
+
       def __call__(self, x):
         b = self.b(x)
         a = self.a(x)
         return a + b
+
     k = random.PRNGKey(0)
     x = random.randint(k, (2, 2), minval=0, maxval=10)
     vs = C().init(k, x)
     y = C().apply(vs, x)
-    outer_expect = jax.tree_map(jnp.shape,
-        freeze({'params': {'a': {'foo': jnp.zeros((2, 2))}}}))
+    outer_expect = jax.tree_map(
+        jnp.shape, freeze({'params': {'a': {'foo': jnp.zeros((2, 2))}}})
+    )
     self.assertEqual(jax.tree_map(jnp.shape, vs), outer_expect)
 
 
 if __name__ == '__main__':
-  absltest.main()
+  absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/partitioning_test.py` & `flax-0.7.1/tests/linen/partitioning_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -53,73 +53,90 @@
     self.assertEqual(partitioning.get_axis_rules(), AXIS_RULES_1)
 
   def test_logical_to_mesh_axes(self):
     axes_0 = ('foo', 'bar')
     # direct rule assignment
     self.assertEqual(
         partitioning.logical_to_mesh_axes(axes_0, rules=AXIS_RULES_1),
-        ('data', 'model'))
+        ('data', 'model'),
+    )
     # axis rules context
     with partitioning.axis_rules(AXIS_RULES_1):
       self.assertEqual(
-          partitioning.logical_to_mesh_axes(axes_0), ('data', 'model'))
+          partitioning.logical_to_mesh_axes(axes_0), ('data', 'model')
+      )
       # nested context
       with partitioning.axis_rules(AXIS_RULES_2):
         self.assertEqual(
-            partitioning.logical_to_mesh_axes(axes_0), ('model', None))
+            partitioning.logical_to_mesh_axes(axes_0), ('model', None)
+        )
     # duplicated logical names
     with partitioning.axis_rules(AXIS_RULES_1):
       with self.assertRaises(ValueError):
         partitioning.logical_to_mesh_axes(('foo', 'foo', 'baz'))
 
   def test_logical_to_mesh_axes_priorities(self):
-    p_rules = (
-        ('foo', 'model'),
-        ('bar', 'model'),
-        ('baz', 'data'))
+    p_rules = (('foo', 'model'), ('bar', 'model'), ('baz', 'data'))
     with partitioning.axis_rules(p_rules):
       self.assertEqual(
           partitioning.logical_to_mesh_axes(('foo', 'bar', 'baz')),
-          ('model', None, 'data'))
+          ('model', None, 'data'),
+      )
       self.assertEqual(
           partitioning.logical_to_mesh_axes(('bar', 'foo', 'baz')),
-          (None, 'model', 'data'))
+          (None, 'model', 'data'),
+      )
       self.assertEqual(
           partitioning.logical_to_mesh_axes(('baz', 'bar', 'foo')),
-          ('data', None, 'model'))
+          ('data', None, 'model'),
+      )
       self.assertEqual(
           partitioning.logical_to_mesh_axes(
-              ('baz', 'bar', 'foo', 'unassigned')),
-          ('data', None, 'model', None))
+              ('baz', 'bar', 'foo', 'unassigned')
+          ),
+          ('data', None, 'model', None),
+      )
 
   @parameterized.parameters(
-      dict(rules=(('a', ('model', 'data')), ('b', 'data')),
-           axes=('a', 'b'),
-           expected=(('model', 'data'), None)),
-      dict(rules=(('a', ('model', 'replica')), ('b', 'data')),
-           axes=('a', 'b'),
-           expected=(('model', 'replica'), 'data')),
-      dict(rules=(('a', ('model', 'replica')), ('b', ('data', 'model'))),
-           axes=('a', 'b'),
-           expected=(('model', 'replica'), None)),
-      dict(rules=(('a', ('model', 'replica')), ('b', 'model')),
-           axes=('a', 'b', 'c'),
-           expected=(('model', 'replica'), None, None)),
-      dict(rules=(),
-           axes=('a', 'b', 'c'),
-           expected=(None, None, None)),
-      dict(rules=(('a', None), ('a', 'model')),
-           axes=('a', 'b'),
-           expected=(None, None)),
-      dict(rules=(('baz', 'data'),
-                  ('bar', None),
-                  ('foo', 'model'),
-                  ('foo', 'data')),
-           axes=('baz', 'bar', 'foo'),
-           expected=('data', None, 'model')),
+      dict(
+          rules=(('a', ('model', 'data')), ('b', 'data')),
+          axes=('a', 'b'),
+          expected=(('model', 'data'), None),
+      ),
+      dict(
+          rules=(('a', ('model', 'replica')), ('b', 'data')),
+          axes=('a', 'b'),
+          expected=(('model', 'replica'), 'data'),
+      ),
+      dict(
+          rules=(('a', ('model', 'replica')), ('b', ('data', 'model'))),
+          axes=('a', 'b'),
+          expected=(('model', 'replica'), None),
+      ),
+      dict(
+          rules=(('a', ('model', 'replica')), ('b', 'model')),
+          axes=('a', 'b', 'c'),
+          expected=(('model', 'replica'), None, None),
+      ),
+      dict(rules=(), axes=('a', 'b', 'c'), expected=(None, None, None)),
+      dict(
+          rules=(('a', None), ('a', 'model')),
+          axes=('a', 'b'),
+          expected=(None, None),
+      ),
+      dict(
+          rules=(
+              ('baz', 'data'),
+              ('bar', None),
+              ('foo', 'model'),
+              ('foo', 'data'),
+          ),
+          axes=('baz', 'bar', 'foo'),
+          expected=('data', None, 'model'),
+      ),
   )
   def test_logical_to_mesh_axes_cases(self, rules, axes, expected):
     with partitioning.axis_rules(rules):
       result = partitioning.logical_to_mesh_axes(axes)
     self.assertEqual(result, expected)
 
   @mock.patch('flax.linen.spmd._with_sharding_constraint')
@@ -138,188 +155,226 @@
       )
 
   @mock.patch('flax.linen.spmd._with_sharding_constraint')
   def test_with_sharding_constraint_fallback(self, wsc_fn):
     arr = jnp.ones((2, 2))
     with partitioning.axis_rules(AXIS_RULES_1):
       _ = partitioning.with_sharding_constraint(arr, ('foo', 'not_recognized'))
-      wsc_fn.assert_called_with(arr, jax.sharding.PartitionSpec('data', None), mesh=None)
+      wsc_fn.assert_called_with(
+          arr, jax.sharding.PartitionSpec('data', None), mesh=None
+      )
       wsc_fn.reset_mock()
       _ = partitioning.with_sharding_constraint(
-          arr, ('foo', 'not_recognized'),
-          fallback=partitioning.RulesFallback.AXIS_IS_UNSHARDED)
-      wsc_fn.assert_called_with(arr, jax.sharding.PartitionSpec('data', None), mesh=None)
+          arr,
+          ('foo', 'not_recognized'),
+          fallback=partitioning.RulesFallback.AXIS_IS_UNSHARDED,
+      )
+      wsc_fn.assert_called_with(
+          arr, jax.sharding.PartitionSpec('data', None), mesh=None
+      )
       wsc_fn.reset_mock()
       with self.assertRaises(ValueError):
         _ = partitioning.with_sharding_constraint(
-            arr, ('foo', 'not_recognized'),
-            fallback=partitioning.RulesFallback.RAISE_ERROR)
+            arr,
+            ('foo', 'not_recognized'),
+            fallback=partitioning.RulesFallback.RAISE_ERROR,
+        )
       wsc_fn.assert_not_called()
       _ = partitioning.with_sharding_constraint(
-          arr, ('foo', 'not_recognized'),
-          fallback=partitioning.RulesFallback.NO_CONSTRAINT)
+          arr,
+          ('foo', 'not_recognized'),
+          fallback=partitioning.RulesFallback.NO_CONSTRAINT,
+      )
       wsc_fn.assert_not_called()
 
   @parameterized.parameters(dict(axes_spec=None), dict(axes_spec=()))
   def test_param_with_axes_no_axes(self, axes_spec):
     class ParamTest(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         foo = partitioning.param_with_axes(
-            'foo', lambda k, s, d: jnp.zeros(s, d),
-            (2, 2), x.dtype, axes=axes_spec)
+            'foo',
+            lambda k, s, d: jnp.zeros(s, d),
+            (2, 2),
+            x.dtype,
+            axes=axes_spec,
+        )
         return x + foo
 
     k = random.PRNGKey(0)
     x = jnp.ones((2, 2))
     _ = ParamTest().init(k, x)
 
   def test_param_with_axes(self):
     class ParamTest(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         foo = partitioning.param_with_axes(
-            'foo', lambda k, s, d: jnp.zeros(s, d),
-            (2, 2), x.dtype, axes=('foo', 'bar'))
+            'foo',
+            lambda k, s, d: jnp.zeros(s, d),
+            (2, 2),
+            x.dtype,
+            axes=('foo', 'bar'),
+        )
         return x + foo
 
-    p_rules = (
-        ('foo', 'model'),
-        ('bar', 'data'),
-        ('baz', None))
+    p_rules = (('foo', 'model'), ('bar', 'data'), ('baz', None))
     k = random.PRNGKey(0)
     x = jnp.ones((2, 2))
     with partitioning.axis_rules(p_rules):
       variables = ParamTest().init(k, x)
     self.assertIn('params', variables)
     self.assertIn('params_axes', variables)
-    self.assertEqual(variables['params_axes']['foo_axes'],
-                     partitioning.AxisMetadata(names=('foo', 'bar')))
+    self.assertEqual(
+        variables['params_axes']['foo_axes'],
+        partitioning.AxisMetadata(names=('foo', 'bar')),
+    )
     logical_axis_names = partitioning.get_axis_names(variables['params_axes'])
-    self.assertEqual(logical_axis_names,
-                     {'foo': jax.sharding.PartitionSpec('foo', 'bar')})
+    self.assertEqual(
+        logical_axis_names, {'foo': jax.sharding.PartitionSpec('foo', 'bar')}
+    )
 
   def test_param_pytree_with_axes(self):
     def init_fn(k, s, d):
       del k
       return {'a': jnp.zeros(s, d), 'b': (jnp.zeros(s, d), jnp.zeros(s, d))}
+
     axes = {'a': ('foo', 'bar'), 'b': (('foo', 'bar'), ('bar', 'foo'))}
+
     class ParamTest(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         foo = partitioning.param_with_axes(
-            'foo', init_fn, (2, 2), x.dtype, axes=axes)
+            'foo', init_fn, (2, 2), x.dtype, axes=axes
+        )
         return x + foo['a']
 
-    p_rules = (
-        ('foo', 'model'),
-        ('bar', 'data'),
-        ('baz', None))
+    p_rules = (('foo', 'model'), ('bar', 'data'), ('baz', None))
     k = random.PRNGKey(0)
     x = jnp.ones((2, 2))
     with partitioning.axis_rules(p_rules):
       variables = ParamTest().init(k, x)
     self.assertIn('params', variables)
     self.assertIn('params_axes', variables)
-    self.assertEqual(variables['params_axes']['foo_axes'],
-                     partitioning.AxisMetadata(names=axes))
+    self.assertEqual(
+        variables['params_axes']['foo_axes'],
+        partitioning.AxisMetadata(names=axes),
+    )
     logical_axis_names = partitioning.get_axis_names(variables['params_axes'])
     expected = freeze(
-        {'foo':
-             {'a': jax.sharding.PartitionSpec('foo', 'bar'),
-              'b': (jax.sharding.PartitionSpec('foo', 'bar'),
-                    jax.sharding.PartitionSpec('bar', 'foo'))}})
+        {
+            'foo': {
+                'a': jax.sharding.PartitionSpec('foo', 'bar'),
+                'b': (
+                    jax.sharding.PartitionSpec('foo', 'bar'),
+                    jax.sharding.PartitionSpec('bar', 'foo'),
+                ),
+            }
+        }
+    )
     self.assertEqual(logical_axis_names, expected)
 
   @parameterized.parameters(dict(axes_spec=None), dict(axes_spec=()))
   def test_variable_with_axes_no_axes(self, axes_spec):
     class VarTest(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         foo = partitioning.variable_with_axes(
-            'test', 'foo', jnp.zeros, (2, 2), x.dtype, axes=axes_spec)
+            'test', 'foo', jnp.zeros, (2, 2), x.dtype, axes=axes_spec
+        )
         return x + foo.value
 
     k = random.PRNGKey(0)
     x = jnp.ones((2, 2))
     _ = VarTest().init(k, x)
 
   def test_variable_with_empty_tuple_has_empty_axes(self):
-
     class VarTest(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         foo = partitioning.variable_with_axes(
-            'test', 'foo', jnp.zeros, (2, 2), x.dtype, axes=())
+            'test', 'foo', jnp.zeros, (2, 2), x.dtype, axes=()
+        )
         return x + foo.value
 
     k = random.PRNGKey(0)
     x = jnp.ones((2, 2))
     variables = VarTest().init(k, x)
     logical_axis_names = partitioning.get_axis_names(variables['test_axes'])
     self.assertEqual(logical_axis_names, {'foo': jax.sharding.PartitionSpec()})
 
   def test_variable_with_axes(self):
     class VarTest(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         foo = partitioning.variable_with_axes(
-            'test', 'foo', jnp.zeros, (2, 2), x.dtype, axes=('foo', 'bar'))
+            'test', 'foo', jnp.zeros, (2, 2), x.dtype, axes=('foo', 'bar')
+        )
         return x + foo.value
 
-    p_rules = (
-        ('foo', 'model'),
-        ('bar', 'data'),
-        ('baz', None))
+    p_rules = (('foo', 'model'), ('bar', 'data'), ('baz', None))
     k = random.PRNGKey(0)
     x = jnp.ones((2, 2))
     with partitioning.axis_rules(p_rules):
       variables = VarTest().init(k, x)
     self.assertIn('test', variables)
     self.assertIn('test_axes', variables)
-    self.assertEqual(variables['test_axes']['foo_axes'],
-                     partitioning.AxisMetadata(names=('foo', 'bar')))
+    self.assertEqual(
+        variables['test_axes']['foo_axes'],
+        partitioning.AxisMetadata(names=('foo', 'bar')),
+    )
     logical_axis_names = partitioning.get_axis_names(variables['test_axes'])
-    self.assertEqual(logical_axis_names,
-                     {'foo': jax.sharding.PartitionSpec('foo', 'bar')})
+    self.assertEqual(
+        logical_axis_names, {'foo': jax.sharding.PartitionSpec('foo', 'bar')}
+    )
 
   @mock.patch('flax.linen.partitioning._with_sharding_constraint')
   def test_variable_with_axes_fallback(self, wsc_fn):
     class VarTest(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         foo = partitioning.variable_with_axes(
-            'test', 'foo', jnp.zeros, (2, 2), x.dtype, axes=('foo', 'bar'),
-            fallback=partitioning.RulesFallback.NO_CONSTRAINT)
+            'test',
+            'foo',
+            jnp.zeros,
+            (2, 2),
+            x.dtype,
+            axes=('foo', 'bar'),
+            fallback=partitioning.RulesFallback.NO_CONSTRAINT,
+        )
         return x + foo.value
 
     p_rules = (
         # No rule for 'foo':
         ('bar', 'data'),
-        ('baz', None))
+        ('baz', None),
+    )
     k = random.PRNGKey(0)
     x = jnp.ones((2, 2))
     with partitioning.axis_rules(p_rules):
       variables = VarTest().init(k, x)
 
     wsc_fn.assert_not_called()
     self.assertIn('test', variables)
     self.assertIn('test_axes', variables)
-    self.assertEqual(variables['test_axes']['foo_axes'],
-                     partitioning.AxisMetadata(names=('foo', 'bar')))
+    self.assertEqual(
+        variables['test_axes']['foo_axes'],
+        partitioning.AxisMetadata(names=('foo', 'bar')),
+    )
     logical_axis_names = partitioning.get_axis_names(variables['test_axes'])
-    self.assertEqual(logical_axis_names,
-                     {'foo': jax.sharding.PartitionSpec('foo', 'bar')})
+    self.assertEqual(
+        logical_axis_names, {'foo': jax.sharding.PartitionSpec('foo', 'bar')}
+    )
 
   def test_scan_with_axes(self):
     # MLP Hparams
     B, L, E = 8, 4, 32  # pylint: disable=invalid-name
     # fake inputs
     x = jnp.ones((B, E))
     k = random.PRNGKey(0)
@@ -329,23 +384,26 @@
 
       @nn.compact
       def __call__(self, x):
         W1 = partitioning.param_with_axes(  # pylint: disable=invalid-name
             'W1',
             nn.initializers.xavier_normal(),
             (x.shape[-1], self.depth),
-            axes=('emb', 'mlp'))
+            axes=('emb', 'mlp'),
+        )
         W2 = partitioning.param_with_axes(  # pylint: disable=invalid-name
             'W2',
             nn.initializers.xavier_normal(),
             (self.depth, x.shape[-1]),
-            axes=('mlp', 'emb'))
+            axes=('mlp', 'emb'),
+        )
         y = jnp.dot(jnp.sin(jnp.dot(x, W1)), W2)
         _ = partitioning.variable_with_axes(
-            'stats', 'y_st', lambda: y, axes=('batch', 'emb'))
+            'stats', 'y_st', lambda: y, axes=('batch', 'emb')
+        )
         # scan expects a (carry, out) return signature.
         return y, None
 
     class Scanned(nn.Module):
       num_layers: int
       depth: int
 
@@ -354,16 +412,16 @@
         scanned_sindot = partitioning.scan_with_axes(
             SinDot,
             in_axes=(),
             variable_axes={'params': 0, 'stats': 1},
             split_rngs={'params': True},
             axis_name='layer',
             axes_collections=('params', 'stats'),
-            length=self.num_layers)(self.depth,
-                                    name='scanned_layer')
+            length=self.num_layers,
+        )(self.depth, name='scanned_layer')
         y, _ = scanned_sindot(x)
         # test calling again to test metadata compatibility across calls
         _, _ = scanned_sindot(x)
         return y
 
     p_rules = (('emb', 'data'), ('mlp', 'model'), ('batch', 'data'))
     with partitioning.axis_rules(p_rules):
@@ -373,99 +431,117 @@
       Scanned(L, E).apply(variables, x)
     self.assertIn('params', variables)
     self.assertIn('params_axes', variables)
     self.assertIn('stats', variables)
     self.assertIn('stats_axes', variables)
     self.assertEqual(
         variables['params_axes']['scanned_layer']['W1_axes'],
-        partitioning.AxisMetadata(names=('layer', 'emb', 'mlp')))
+        partitioning.AxisMetadata(names=('layer', 'emb', 'mlp')),
+    )
     logical_axis_names = partitioning.get_axis_names(variables['params_axes'])
     self.assertEqual(
         logical_axis_names,
-        {'scanned_layer': {
-            'W1': jax.sharding.PartitionSpec('layer', 'emb', 'mlp'),
-            'W2': jax.sharding.PartitionSpec('layer', 'mlp', 'emb')}})
+        {
+            'scanned_layer': {
+                'W1': jax.sharding.PartitionSpec('layer', 'emb', 'mlp'),
+                'W2': jax.sharding.PartitionSpec('layer', 'mlp', 'emb'),
+            }
+        },
+    )
     logical_axis_names = partitioning.get_axis_names(variables['stats_axes'])
     self.assertEqual(
         logical_axis_names,
-        {'scanned_layer': {
-            'y_st': jax.sharding.PartitionSpec('batch', 'layer', 'emb')}})
+        {
+            'scanned_layer': {
+                'y_st': jax.sharding.PartitionSpec('batch', 'layer', 'emb')
+            }
+        },
+    )
 
   def test_vmap_with_axes(self):
-
     class Foo(nn.Module):
 
       @nn.compact
       def __call__(self, x):
-        return partitioning.param_with_axes(
-            'w', jax.nn.initializers.uniform(), [4, 3], axes=('out', 'in')) @ x
+        return (
+            partitioning.param_with_axes(
+                'w', jax.nn.initializers.uniform(), [4, 3], axes=('out', 'in')
+            )
+            @ x
+        )
 
     class Vmapped(nn.Module):
 
       @nn.compact
       def __call__(self, x):
         FooVmapped = partitioning.vmap_with_axes(  # pylint: disable=invalid-name
             Foo,
             variable_axes={
                 'params': 1,
             },
             split_rngs={'params': True},
-            partitioning_axis_names={'params': 'vmap_axis'})
+            partitioning_axis_names={'params': 'vmap_axis'},
+        )
         return FooVmapped(name='foo_vmapped')(x)
 
     p_rules = (('out', None), ('in', 'data'), ('vmap_axis', 'model'))
 
     # check that regular Food module is correct
     with partitioning.axis_rules(p_rules):
       variables = Foo().init(jax.random.PRNGKey(0), jnp.array([1, 2, 3]))
     variables = unfreeze(variables)
-    variables['params'] = jax.tree_util.tree_map(lambda x: x.shape, variables['params'])
+    variables['params'] = jax.tree_util.tree_map(
+        lambda x: x.shape, variables['params']
+    )
     self.assertDictEqual(
-        variables, {
-            'params': {
-                'w': (4, 3)
-            },
+        variables,
+        {
+            'params': {'w': (4, 3)},
             'params_axes': {
                 'w_axes': partitioning.AxisMetadata(names=('out', 'in'))
-            }
-        })
+            },
+        },
+    )
 
     # check that FooVmapped adds 'vmap_axis' to axis 1
     with partitioning.axis_rules(p_rules):
       variables = Vmapped().init(
-          jax.random.PRNGKey(0), jnp.array([[1, 2, 3], [4, 5, 6]]))
+          jax.random.PRNGKey(0), jnp.array([[1, 2, 3], [4, 5, 6]])
+      )
     variables = unfreeze(variables)
-    variables['params'] = jax.tree_util.tree_map(lambda x: x.shape, variables['params'])
+    variables['params'] = jax.tree_util.tree_map(
+        lambda x: x.shape, variables['params']
+    )
     self.assertDictEqual(
-        variables, {
-            'params': {
-                'foo_vmapped': {
-                    'w': (4, 2, 3)
-                }
-            },
+        variables,
+        {
+            'params': {'foo_vmapped': {'w': (4, 2, 3)}},
             'params_axes': {
                 'foo_vmapped': {
-                    'w_axes':
-                        partitioning.AxisMetadata(
-                            names=('out', 'vmap_axis', 'in'))
+                    'w_axes': partitioning.AxisMetadata(
+                        names=('out', 'vmap_axis', 'in')
+                    )
                 }
-            }
-        })
+            },
+        },
+    )
 
   def test_logical_with_mesh_and_rules(self):
     devices = mesh_utils.create_device_mesh((jax.local_device_count(), 1))
     mesh = sharding.Mesh(devices, ('in', 'out'))
     test = self
     rules = (('a', 'in'), ('b', 'out'))
 
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         kernel_init = nn.with_logical_partitioning(
-          nn.initializers.ones_init(), ('a', 'b'), mesh=mesh, rules=rules)
+            nn.initializers.ones_init(), ('a', 'b'), mesh=mesh, rules=rules
+        )
         kernel = self.param('kernel', kernel_init, (x.shape[-1], 2))
         kernel_box = self.get_variable('params', 'kernel')
         test.assertIsInstance(kernel_box, nn.Partitioned)
         test.assertEqual(kernel_box.names, ('a', 'b'))
         return x @ kernel
 
     @jax.jit
@@ -473,16 +549,15 @@
       module = Foo()
       variables = module.init(random.PRNGKey(0), jnp.zeros((8, 4)))
       logical_spec = nn.get_partition_spec(variables)
       shardings = nn.logical_to_mesh_sharding(logical_spec, mesh, rules)
       variables = jax.lax.with_sharding_constraint(variables, shardings)
       return variables
 
-
     variables = create_state()
-    self.assertEqual(variables['params']['kernel'].names,
-                     ('a', 'b'))
+    self.assertEqual(variables['params']['kernel'].names, ('a', 'b'))
     self.assertIs(variables['params']['kernel'].mesh, mesh)
     self.assertEqual(variables['params']['kernel'].rules, rules)
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/summary_test.py` & `flax-0.7.1/tests/linen/summary_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -27,85 +27,92 @@
 from flax.configurations import temp_flip_flag
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 CONSOLE_TEST_KWARGS = dict(force_terminal=False, no_color=True, width=10_000)
 
+
 def _get_shapes(pytree):
-  return jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, pytree)
+  return jax.tree_util.tree_map(
+      lambda x: x.shape if hasattr(x, "shape") else x, pytree
+  )
+
 
 def _get_obj_repr_value(x):
   if isinstance(x, summary._ObjectRepresentation):
     return x.obj
   return x
 
+
 class ConvBlock(nn.Module):
   features: int
   kernel_size: List[int]
   test_sow: bool
 
   def setup(self) -> None:
     self.conv = nn.Conv(self.features, self.kernel_size)
     self.bn = nn.BatchNorm()
     self.dropout = nn.Dropout(0.5)
 
   def block_method(self, x: Array, training: bool) -> Array:
     x = self.conv(x)
 
     if self.test_sow:
-      self.sow('intermediates', 'INTERM', x)
+      self.sow("intermediates", "INTERM", x)
 
     x = self.bn(x, use_running_average=not training)
     x = self.dropout(x, deterministic=not training)
     x = nn.relu(x)
     return x
 
   def __call__(self, x: Array, training: bool) -> Array:
     x = self.conv(x)
 
     if self.test_sow:
-      self.sow('intermediates', 'INTERM', x)
+      self.sow("intermediates", "INTERM", x)
 
     x = self.bn(x, use_running_average=not training)
     x = self.dropout(x, deterministic=not training)
     x = nn.relu(x)
     return x
 
+
 class CNN(nn.Module):
   test_sow: bool
 
   def setup(self) -> None:
     self.block1 = ConvBlock(32, [3, 3], test_sow=self.test_sow)
     self.block2 = ConvBlock(64, [3, 3], test_sow=self.test_sow)
     self.dense = nn.Dense(10)
 
   def cnn_method(self, x: Array, training: bool) -> Array:
     x = self.block1.block_method(x, training=training)
     x = self.block2.block_method(x, training=training)
     x = x.mean(axis=(1, 2))
 
     if self.test_sow:
-      self.sow('intermediates', 'INTERM', x)
+      self.sow("intermediates", "INTERM", x)
 
     x = self.dense(x)
 
-    return x, dict(a=x, b=x+1.0)
+    return x, dict(a=x, b=x + 1.0)
 
   def __call__(self, x: Array, training: bool) -> Array:
     x = self.block1.block_method(x, training=training)
     x = self.block2.block_method(x, training=training)
     x = x.mean(axis=(1, 2))
 
     if self.test_sow:
-      self.sow('intermediates', 'INTERM', x)
+      self.sow("intermediates", "INTERM", x)
 
     x = self.dense(x)
 
-    return x, dict(a=x, b=x+1.0)
+    return x, dict(a=x, b=x + 1.0)
+
 
 class SummaryTest(absltest.TestCase):
 
   def test_module_summary(self):
     """
     This test creates a Table using `module_summary` and checks that it
     matches the expected output given the CNN model defined in `_get_tabulate_cnn`.
@@ -113,16 +120,18 @@
 
     batch_size = 32
 
     x = jnp.ones((batch_size, 28, 28, 1))
     module = CNN(test_sow=False)
 
     table = summary._get_module_table(module, depth=None, show_repeated=True)(
-      {"dropout":random.PRNGKey(0), "params": random.PRNGKey(1)},
-      x, training=True, mutable=True,
+        {"dropout": random.PRNGKey(0), "params": random.PRNGKey(1)},
+        x,
+        training=True,
+        mutable=True,
     )
     # get values for inputs and outputs from their _ValueRepresentation
     for row in table:
       row.inputs = jax.tree_util.tree_map(_get_obj_repr_value, row.inputs)
       row.outputs = jax.tree_util.tree_map(_get_obj_repr_value, row.outputs)
 
     # 10 rows = 1 CNN + 4 ConvBlock_0 + 4 ConvBlock_1 + 1 Dense_0
@@ -141,64 +150,83 @@
     self.assertEqual(table[7].path, ("block2", "bn"))
     self.assertEqual(table[8].path, ("block2", "dropout"))
 
     self.assertEqual(table[9].path, ("dense",))
 
     # check outputs shapes
     self.assertEqual(
-      (table[0].inputs[0].shape, table[0].inputs[1]),
-      (x.shape, dict(training=True)),
+        (table[0].inputs[0].shape, table[0].inputs[1]),
+        (x.shape, dict(training=True)),
     )
     self.assertEqual(
-      _get_shapes(table[0].outputs),
-      ((batch_size, 10), dict(a=(batch_size, 10), b=(batch_size, 10))),
+        _get_shapes(table[0].outputs),
+        ((batch_size, 10), dict(a=(batch_size, 10), b=(batch_size, 10))),
     )
 
-    self.assertEqual(_get_shapes(table[1].inputs), ((batch_size, 28, 28, 1), {'training': True}))
+    self.assertEqual(
+        _get_shapes(table[1].inputs),
+        ((batch_size, 28, 28, 1), {"training": True}),
+    )
     self.assertEqual(table[1].outputs.shape, (batch_size, 28, 28, 32))
     self.assertEqual(table[2].inputs.shape, (batch_size, 28, 28, 1))
     self.assertEqual(table[2].outputs.shape, (batch_size, 28, 28, 32))
-    self.assertEqual(_get_shapes(table[3].inputs), ((batch_size, 28, 28, 32), {'use_running_average': False}))
+    self.assertEqual(
+        _get_shapes(table[3].inputs),
+        ((batch_size, 28, 28, 32), {"use_running_average": False}),
+    )
     self.assertEqual(table[3].outputs.shape, (batch_size, 28, 28, 32))
-    self.assertEqual(_get_shapes(table[4].inputs), ((batch_size, 28, 28, 32), {'deterministic': False}))
+    self.assertEqual(
+        _get_shapes(table[4].inputs),
+        ((batch_size, 28, 28, 32), {"deterministic": False}),
+    )
     self.assertEqual(table[4].outputs.shape, (batch_size, 28, 28, 32))
 
-    self.assertEqual(_get_shapes(table[5].inputs), ((batch_size, 28, 28, 32), {'training': True}))
+    self.assertEqual(
+        _get_shapes(table[5].inputs),
+        ((batch_size, 28, 28, 32), {"training": True}),
+    )
     self.assertEqual(table[5].outputs.shape, (batch_size, 28, 28, 64))
     self.assertEqual(table[6].inputs.shape, (batch_size, 28, 28, 32))
     self.assertEqual(table[6].outputs.shape, (batch_size, 28, 28, 64))
-    self.assertEqual(_get_shapes(table[7].inputs), ((batch_size, 28, 28, 64), {'use_running_average': False}))
+    self.assertEqual(
+        _get_shapes(table[7].inputs),
+        ((batch_size, 28, 28, 64), {"use_running_average": False}),
+    )
     self.assertEqual(table[7].outputs.shape, (batch_size, 28, 28, 64))
-    self.assertEqual(_get_shapes(table[8].inputs), ((batch_size, 28, 28, 64), {'deterministic': False}))
+    self.assertEqual(
+        _get_shapes(table[8].inputs),
+        ((batch_size, 28, 28, 64), {"deterministic": False}),
+    )
     self.assertEqual(table[8].outputs.shape, (batch_size, 28, 28, 64))
 
     self.assertEqual(table[9].inputs.shape, (batch_size, 64))
     self.assertEqual(table[9].outputs.shape, (batch_size, 10))
 
     # check no summary is performed
     for row in table:
       self.assertEqual(
-        row.module_variables,
-        row.counted_variables,
+          row.module_variables,
+          row.counted_variables,
       )
 
-  @temp_flip_flag('return_frozendict', False)
   def test_module_summary_with_depth(self):
     """
     This test creates a Table using `module_summary` set the `depth` argument to `1`,
     table should have less rows as a consequence.
     """
     batch_size = 32
 
     x = jnp.ones((batch_size, 28, 28, 1))
     module = CNN(test_sow=False)
 
     table = summary._get_module_table(module, depth=1, show_repeated=True)(
-      {"dropout":random.PRNGKey(0), "params": random.PRNGKey(1)},
-      x, training=True, mutable=True,
+        {"dropout": random.PRNGKey(0), "params": random.PRNGKey(1)},
+        x,
+        training=True,
+        mutable=True,
     )
     # get values for inputs and outputs from their _ValueRepresentation
 
     for row in table:
       row.inputs = jax.tree_util.tree_map(_get_obj_repr_value, row.inputs)
       row.outputs = jax.tree_util.tree_map(_get_obj_repr_value, row.outputs)
 
@@ -210,53 +238,57 @@
 
     self.assertEqual(table[1].path, ("block1",))
     self.assertEqual(table[2].path, ("block2",))
     self.assertEqual(table[3].path, ("dense",))
 
     # check outputs shapes
     self.assertEqual(
-      (table[0].inputs[0].shape, table[0].inputs[1]),
-      (x.shape, dict(training=True)),
+        (table[0].inputs[0].shape, table[0].inputs[1]),
+        (x.shape, dict(training=True)),
     )
     self.assertEqual(
-      _get_shapes(table[0].outputs),
-      ((batch_size, 10), dict(a=(batch_size, 10), b=(batch_size, 10))),
+        _get_shapes(table[0].outputs),
+        ((batch_size, 10), dict(a=(batch_size, 10), b=(batch_size, 10))),
     )
 
-    self.assertEqual(_get_shapes(table[1].inputs), ((batch_size, 28, 28, 1), {'training': True}))
+    self.assertEqual(
+        _get_shapes(table[1].inputs),
+        ((batch_size, 28, 28, 1), {"training": True}),
+    )
     self.assertEqual(table[1].outputs.shape, (batch_size, 28, 28, 32))
 
-    self.assertEqual(_get_shapes(table[2].inputs), ((batch_size, 28, 28, 32), {'training': True}))
+    self.assertEqual(
+        _get_shapes(table[2].inputs),
+        ((batch_size, 28, 28, 32), {"training": True}),
+    )
     self.assertEqual(table[2].outputs.shape, (batch_size, 28, 28, 64))
 
     self.assertEqual(table[3].inputs.shape, (batch_size, 64))
     self.assertEqual(table[3].outputs.shape, (batch_size, 10))
 
     # check ConvBlock_0 and ConvBlock_1 are summarized
     self.assertNotEqual(table[1].module_variables, table[1].counted_variables)
     self.assertNotEqual(table[2].module_variables, table[2].counted_variables)
 
     # check CNN and Dense_0 output are not summarized
     self.assertEqual(table[0].module_variables, table[0].counted_variables)
     self.assertEqual(table[3].module_variables, table[3].counted_variables)
 
-
-  @temp_flip_flag('return_frozendict', False)
   def test_tabulate(self):
     """
     This test creates a string representation of a Module using `Module.tabulate`
     and checks that it matches the expected output given the CNN model defined in `_get_tabulate_cnn`.
     """
     batch_size = 32
 
     x = jnp.ones((batch_size, 28, 28, 1))
     module = CNN(test_sow=False)
 
     module_repr = module.tabulate(
-        {"dropout":random.PRNGKey(0), "params": random.PRNGKey(1)},
+        {"dropout": random.PRNGKey(0), "params": random.PRNGKey(1)},
         x,
         training=True,
         console_kwargs=CONSOLE_TEST_KWARGS,
     )
 
     # NOTE: its tricky to validate the content of lines
     # because it seems to be shell-dependent, so we will
@@ -283,68 +315,64 @@
     self.assertIn("78.6 KB", lines[-6])
 
     # total counts
     self.assertIn("Total Parameters", lines[-3])
     self.assertIn("19,850", lines[-3])
     self.assertIn("79.4 KB", lines[-3])
 
-
   def test_tabulate_with_sow(self):
-
     batch_size = 32
 
     x = jnp.ones((batch_size, 28, 28, 1))
     module = CNN(test_sow=True)
 
     module_repr = module.tabulate(
-      {"dropout":random.PRNGKey(0), "params": random.PRNGKey(1)},
-      x,
-      training=True,
-      console_kwargs=CONSOLE_TEST_KWARGS,
+        {"dropout": random.PRNGKey(0), "params": random.PRNGKey(1)},
+        x,
+        training=True,
+        console_kwargs=CONSOLE_TEST_KWARGS,
     )
 
     self.assertIn("intermediates", module_repr)
     self.assertIn("INTERM", module_repr)
 
   def test_tabulate_with_method(self):
-
     batch_size = 32
 
     x = jnp.ones((batch_size, 28, 28, 1))
     module = CNN(test_sow=False)
 
     module_repr = module.tabulate(
-      {"dropout":random.PRNGKey(0), "params": random.PRNGKey(1)},
-      x,
-      training=True,
-      method=CNN.cnn_method,
-      console_kwargs=CONSOLE_TEST_KWARGS,
+        {"dropout": random.PRNGKey(0), "params": random.PRNGKey(1)},
+        x,
+        training=True,
+        method=CNN.cnn_method,
+        console_kwargs=CONSOLE_TEST_KWARGS,
     )
 
     self.assertIn("(block_method)", module_repr)
     self.assertIn("(cnn_method)", module_repr)
 
-  @temp_flip_flag('return_frozendict', False)
   def test_tabulate_function(self):
     """
     This test creates a string representation of a Module using `Module.tabulate`
     and checks that it matches the expected output given the CNN model defined in `_get_tabulate_cnn`.
     """
     batch_size = 32
 
     x = jnp.ones((batch_size, 28, 28, 1))
     module = CNN(test_sow=False)
 
     module_repr = nn.tabulate(
-      module,
-      {"dropout":random.PRNGKey(0), "params": random.PRNGKey(1)},
-      console_kwargs=CONSOLE_TEST_KWARGS,
+        module,
+        {"dropout": random.PRNGKey(0), "params": random.PRNGKey(1)},
+        console_kwargs=CONSOLE_TEST_KWARGS,
     )(
-      x,
-      training=True,
+        x,
+        training=True,
     )
 
     lines = module_repr.split("\n")
 
     # check title
     module_name = module.__class__.__name__
     self.assertIn(f"{module_name} Summary", lines[1])
@@ -365,113 +393,112 @@
     self.assertIn("78.6 KB", lines[-6])
 
     # total counts
     self.assertIn("Total Parameters", lines[-3])
     self.assertIn("19,850", lines[-3])
     self.assertIn("79.4 KB", lines[-3])
 
-
-  @temp_flip_flag('return_frozendict', False)
   def test_lifted_transform(self):
     class LSTM(nn.Module):
       features: int
 
       @nn.compact
       def __call__(self, x):
-          carry = nn.LSTMCell(self.features).initialize_carry(
-              random.PRNGKey(0), x[:, 0].shape
-          )
-          ScanLSTM = nn.scan(
-              nn.LSTMCell,
-              variable_broadcast="params",
-              split_rngs={"params": False},
-              in_axes=1,
-              out_axes=1,
-          )
-          return ScanLSTM(self.features, name="ScanLSTM")(carry, x)
-
+        carry = nn.LSTMCell(self.features).initialize_carry(
+            random.PRNGKey(0), x[:, 0].shape
+        )
+        ScanLSTM = nn.scan(
+            nn.LSTMCell,
+            variable_broadcast="params",
+            split_rngs={"params": False},
+            in_axes=1,
+            out_axes=1,
+        )
+        return ScanLSTM(self.features, name="ScanLSTM")(carry, x)
 
     lstm = LSTM(features=128)
 
     with jax.check_tracer_leaks(True):
       module_repr = lstm.tabulate(
-        random.PRNGKey(0),
-        x=jnp.ones((32, 128, 64)),
-        console_kwargs=CONSOLE_TEST_KWARGS)
+          random.PRNGKey(0),
+          x=jnp.ones((32, 128, 64)),
+          console_kwargs=CONSOLE_TEST_KWARGS,
+      )
 
     lines = module_repr.splitlines()
 
     self.assertIn("LSTM", lines[5])
     self.assertIn("ScanLSTM", lines[9])
     self.assertIn("LSTMCell", lines[9])
     self.assertIn("ScanLSTM/ii", lines[13])
     self.assertIn("Dense", lines[13])
 
-  @temp_flip_flag('return_frozendict', False)
   def test_lifted_transform_no_rename(self):
     class LSTM(nn.Module):
       features: int
 
       @nn.compact
       def __call__(self, x):
-          carry = nn.LSTMCell(self.features).initialize_carry(
-              random.PRNGKey(0), x[:, 0].shape
-          )
-          ScanLSTM = nn.scan(
-              nn.LSTMCell,
-              variable_broadcast="params",
-              split_rngs={"params": False},
-              in_axes=1,
-              out_axes=1,
-          )
-          return ScanLSTM(self.features)(carry, x)
-
+        carry = nn.LSTMCell(self.features).initialize_carry(
+            random.PRNGKey(0), x[:, 0].shape
+        )
+        ScanLSTM = nn.scan(
+            nn.LSTMCell,
+            variable_broadcast="params",
+            split_rngs={"params": False},
+            in_axes=1,
+            out_axes=1,
+        )
+        return ScanLSTM(self.features)(carry, x)
 
     lstm = LSTM(features=128)
 
     with jax.check_tracer_leaks(True):
       module_repr = lstm.tabulate(
-        random.PRNGKey(0),
-        x=jnp.ones((32, 128, 64)),
-        console_kwargs=CONSOLE_TEST_KWARGS)
+          random.PRNGKey(0),
+          x=jnp.ones((32, 128, 64)),
+          console_kwargs=CONSOLE_TEST_KWARGS,
+      )
 
     lines = module_repr.splitlines()
 
     self.assertIn("LSTM", lines[5])
     self.assertIn("ScanLSTMCell_0", lines[9])
     self.assertIn("LSTMCell", lines[9])
     self.assertIn("ScanLSTMCell_0/ii", lines[13])
     self.assertIn("Dense", lines[13])
 
-  @temp_flip_flag('return_frozendict', False)
   def test_module_reuse(self):
     class ConvBlock(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         x = nn.Conv(32, [3, 3])(x)
         x = nn.BatchNorm(use_running_average=True)(x)
         x = nn.Dropout(0.5, deterministic=True)(x)
         x = nn.relu(x)
         return x
 
     class CNN(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         block = ConvBlock()
         x = block(x)
         x = block(x)
         x = block(x)
         return x
 
     x = jnp.ones((4, 28, 28, 32))
     module_repr = CNN().tabulate(
-      jax.random.PRNGKey(0),
-      x=x,
-      show_repeated=True,
-      console_kwargs=CONSOLE_TEST_KWARGS)
+        jax.random.PRNGKey(0),
+        x=x,
+        show_repeated=True,
+        console_kwargs=CONSOLE_TEST_KWARGS,
+    )
     lines = module_repr.splitlines()
 
     # first call
     self.assertIn("ConvBlock_0/Conv_0", lines[9])
     self.assertIn("bias", lines[9])
     self.assertIn("ConvBlock_0/BatchNorm_0", lines[14])
     self.assertIn("mean", lines[14])
@@ -492,95 +519,100 @@
     self.assertIn("ConvBlock_0/BatchNorm_0", lines[33])
     self.assertNotIn("mean", lines[33])
     self.assertNotIn("bias", lines[33])
     self.assertIn("ConvBlock_0/Dropout_0", lines[35])
 
   def test_empty_input(self):
     class EmptyInput(nn.Module):
+
       @nn.compact
       def __call__(self):
         return 1
 
     module = EmptyInput()
     module_repr = module.tabulate({}, console_kwargs=CONSOLE_TEST_KWARGS)
     lines = module_repr.splitlines()
 
-    self.assertRegex(lines[5], r'|\s*|\s*EmptyInput\s*|\s*|\s*1\s*|')
+    self.assertRegex(lines[5], r"|\s*|\s*EmptyInput\s*|\s*|\s*1\s*|")
 
   def test_numpy_scalar(self):
     class Submodule(nn.Module):
+
       def __call__(self, x):
         return x + 1
 
     class EmptyInput(nn.Module):
+
       @nn.compact
       def __call__(self):
         return Submodule()(x=np.pi)
 
     module = EmptyInput()
     module_repr = module.tabulate({}, console_kwargs=CONSOLE_TEST_KWARGS)
     lines = module_repr.splitlines()
 
-    self.assertIn('4.141592', lines[5])
-    self.assertIn('x: 3.141592', lines[7])
-    self.assertIn('4.141592', lines[7])
+    self.assertIn("4.141592", lines[5])
+    self.assertIn("x: 3.141592", lines[7])
+    self.assertIn("4.141592", lines[7])
 
-  @temp_flip_flag('return_frozendict', False)
   def test_partitioned_params(self):
-
     class Classifier(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         hidden = nn.Dense(
-          features=1024,
-          kernel_init=nn.with_partitioning(
-            nn.initializers.lecun_normal(), (None, 'data')
-          ),
-          bias_init=nn.with_partitioning(
-            nn.initializers.zeros, (None,)
-          ),
-          name='hidden',
+            features=1024,
+            kernel_init=nn.with_partitioning(
+                nn.initializers.lecun_normal(), (None, "data")
+            ),
+            bias_init=nn.with_partitioning(nn.initializers.zeros, (None,)),
+            name="hidden",
         )
         x = x / 255.0
         x = x.reshape((x.shape[0], -1))  # flatten
         x = nn.relu(hidden(x))
-        x = nn.Dense(features=10, name='head')(x)
+        x = nn.Dense(features=10, name="head")(x)
         return x
 
     module = Classifier()
-    lines = module.tabulate(jax.random.PRNGKey(0), jnp.empty((1, 28, 28, 1)),
-                            console_kwargs=CONSOLE_TEST_KWARGS).splitlines()
-    self.assertIn('P(None,)', lines[7])
-    self.assertIn('P(None, data)', lines[8])
+    lines = module.tabulate(
+        jax.random.PRNGKey(0),
+        jnp.empty((1, 28, 28, 1)),
+        console_kwargs=CONSOLE_TEST_KWARGS,
+    ).splitlines()
+    self.assertIn("P(None,)", lines[7])
+    self.assertIn("P(None, data)", lines[8])
 
   def test_non_array_variables(self):
-
     class Metadata(struct.PyTreeNode):
       names: tuple = struct.field(pytree_node=False)
 
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self):
-        self.sow('foo', 'bar', Metadata(('baz', 'qux')))
+        self.sow("foo", "bar", Metadata(("baz", "qux")))
 
     module = Foo()
-    lines = module.tabulate({},
-                            console_kwargs=CONSOLE_TEST_KWARGS).splitlines()
-    self.assertIn('names', lines[6])
-    self.assertIn('baz', lines[7])
-    self.assertIn('qux', lines[8])
+    lines = module.tabulate({}, console_kwargs=CONSOLE_TEST_KWARGS).splitlines()
+    self.assertIn("names", lines[6])
+    self.assertIn("baz", lines[7])
+    self.assertIn("qux", lines[8])
 
   def test_tabulate_param_count(self):
     class Foo(nn.Module):
+
       @nn.compact
       def __call__(self, x):
         h = nn.Dense(4)(x)
         return nn.Dense(2)(h)
 
     x = jnp.ones((16, 9))
-    rep = Foo().tabulate(jax.random.PRNGKey(0), x, console_kwargs=CONSOLE_TEST_KWARGS)
+    rep = Foo().tabulate(
+        jax.random.PRNGKey(0), x, console_kwargs=CONSOLE_TEST_KWARGS
+    )
     lines = rep.splitlines()
-    self.assertIn('Total Parameters: 50', lines[-2])
+    self.assertIn("Total Parameters: 50", lines[-2])
 
 
-if __name__ == '__main__':
-  absltest.main()
+if __name__ == "__main__":
+  absltest.main()
```

### Comparing `flax-0.7.0/tests/linen/toplevel_test.py` & `flax-0.7.1/tests/linen/toplevel_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,19 +24,22 @@
 
 from flax import linen as nn
 from flax.core import Scope
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
+
 class Dummy(nn.Module):
+
   @nn.compact
   def __call__(self):
     self.param('foo', lambda rng: 1)
 
+
 class ModuleTopLevelTest(absltest.TestCase):
   pass
   # def test_toplevel_immutable(self):
   #   d = Dummy(parent=None)
   #   with self.assertRaisesRegex(BaseException, "orphaned module"):
   #     d()
```

### Comparing `flax-0.7.0/tests/run_all_tests.sh` & `flax-0.7.1/tests/run_all_tests.sh`

 * *Files identical despite different names*

### Comparing `flax-0.7.0/tests/serialization_test.py` & `flax-0.7.1/tests/serialization_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -51,37 +51,42 @@
 
 
 class WrongTuple(NamedTuple):
   wrong_field: Any
 
 
 class OriginalModule(nn.Module):
+
   @nn.compact
   def __call__(self, x):
     x = nn.Dense(10)(x)
     return x
 
 
 class WrongModule(nn.Module):
+
   @nn.compact
   def __call__(self, x):
     x = nn.Dense(10)(x)
     x = nn.Dense(10)(x)
     return x
 
 
 class SerializationTest(parameterized.TestCase):
 
   def test_dataclass_serialization(self):
     p = Point(x=1, y=2, meta={'dummy': True})
     state_dict = serialization.to_state_dict(p)
-    self.assertEqual(state_dict, {
-        'x': 1,
-        'y': 2,
-    })
+    self.assertEqual(
+        state_dict,
+        {
+            'x': 1,
+            'y': 2,
+        },
+    )
     restored_p = serialization.from_state_dict(p, {'x': 3, 'y': 4})
     expected_p = Point(x=3, y=4, meta={'dummy': True})
     self.assertEqual(restored_p, expected_p)
 
     with self.assertRaises(ValueError):  # invalid field
       serialization.from_state_dict(p, {'z': 3})
     with self.assertRaises(ValueError):  # missing field
@@ -89,36 +94,36 @@
 
   def test_model_serialization(self):
     rng = random.PRNGKey(0)
     module = nn.Dense(features=1, kernel_init=nn.initializers.ones_init())
     x = jnp.ones((1, 1), jnp.float32)
     initial_params = module.init(rng, x)
     state = serialization.to_state_dict(initial_params)
-    self.assertEqual(state, {
-        'params': {
-            'kernel': np.ones((1, 1)),
-            'bias': np.zeros((1,)),
-        }
-    })
+    self.assertEqual(
+        state,
+        {
+            'params': {
+                'kernel': np.ones((1, 1)),
+                'bias': np.zeros((1,)),
+            }
+        },
+    )
     state = {
         'params': {
             'kernel': np.zeros((1, 1)),
             'bias': np.zeros((1,)),
         }
     }
     restored_model = serialization.from_state_dict(initial_params, state)
     self.assertEqual(restored_model, freeze(state))
 
   def test_partial_serialization(self):
     add_one = Partial(jnp.add, 1)
     state = serialization.to_state_dict(add_one)
-    self.assertEqual(state, {
-        'args': {'0': 1},
-        'keywords': {}
-    })
+    self.assertEqual(state, {'args': {'0': 1}, 'keywords': {}})
     restored_add_one = serialization.from_state_dict(add_one, state)
     self.assertEqual(add_one.args, restored_add_one.args)
 
   def test_optimizer_serialization(self):
     rng = random.PRNGKey(0)
     module = nn.Dense(features=1, kernel_init=nn.initializers.ones_init())
     x = jnp.ones((1, 1), jnp.float32)
@@ -126,114 +131,194 @@
     tx = optax.sgd(0.1, momentum=0.1)
     tx_state = tx.init(initial_params)
     state = serialization.to_state_dict(tx_state)
     expected_state = {
         '0': {
             'trace': {
                 'params': {
-                    'bias': np.array([0.], dtype=jnp.float32),
-                    'kernel': np.array([[0.]], dtype=jnp.float32)
-                    }
+                    'bias': np.array([0.0], dtype=jnp.float32),
+                    'kernel': np.array([[0.0]], dtype=jnp.float32),
                 }
-            },
-        '1': {}
-        }
+            }
+        },
+        '1': {},
+    }
     self.assertEqual(state, expected_state)
     state = jax.tree_map(lambda x: x + 1, expected_state)
     restored_tx_state = serialization.from_state_dict(tx_state, state)
     tx_state_plus1 = jax.tree_map(lambda x: x + 1, tx_state)
     self.assertEqual(restored_tx_state, tx_state_plus1)
 
   def test_collection_serialization(self):
-
     @struct.dataclass
     class DummyDataClass:
       x: float
 
       @classmethod
       def initializer(cls, shape):
         del shape
-        return cls(x=0.)
+        return cls(x=0.0)
 
     class StatefulModule(nn.Module):
+
       @nn.compact
       def __call__(self):
         state = self.variable('state', 'dummy', DummyDataClass.initializer, ())
-        state.value = state.value.replace(x=state.value.x + 1.)
+        state.value = state.value.replace(x=state.value.x + 1.0)
 
     initial_variables = StatefulModule().init(random.PRNGKey(0))
     _, variables = StatefulModule().apply(initial_variables, mutable=['state'])
     serialized_state_dict = serialization.to_state_dict(variables)
-    self.assertEqual(serialized_state_dict,
-                     {'state': {'dummy': {'x': 2.0}}})
-    deserialized_state = serialization.from_state_dict(variables,
-                                                       serialized_state_dict)
+    self.assertEqual(serialized_state_dict, {'state': {'dummy': {'x': 2.0}}})
+    deserialized_state = serialization.from_state_dict(
+        variables, serialized_state_dict
+    )
     self.assertEqual(variables, deserialized_state)
 
-  @parameterized.parameters(
-    ['byte', 'b', 'ubyte', 'short',
-    'h', 'ushort', 'i', 'uint', 'intp',
-    'p', 'uintp', 'long', 'l', 'longlong',
-    'q', 'ulonglong', 'half', 'e', 'f',
-    'double', 'd', 'longdouble', 'g',
-    'cfloat', 'cdouble', 'clongdouble', 'm',
-    'bool8', 'b1', 'int64', 'i8', 'uint64', 'u8',
-    'float16', 'f2', 'float32', 'f4', 'float64',
-    'f8', 'float128', 'f16', 'complex64', 'c8',
-    'complex128', 'c16', 'complex256', 'c32',
-    'm8', 'int32', 'i4', 'uint32', 'u4', 'int16',
-    'i2', 'uint16', 'u2', 'int8', 'i1', 'uint8',
-    'u1', 'complex_', 'int0', 'uint0', 'single',
-    'csingle', 'singlecomplex', 'float_', 'intc',
-    'uintc', 'int_', 'longfloat', 'clongfloat',
-    'longcomplex', 'bool_', 'int', 'float',
-    'complex', 'bool']
-  )
+  @parameterized.parameters([
+      'byte',
+      'b',
+      'ubyte',
+      'short',
+      'h',
+      'ushort',
+      'i',
+      'uint',
+      'intp',
+      'p',
+      'uintp',
+      'long',
+      'l',
+      'longlong',
+      'q',
+      'ulonglong',
+      'half',
+      'e',
+      'f',
+      'double',
+      'd',
+      'longdouble',
+      'g',
+      'cfloat',
+      'cdouble',
+      'clongdouble',
+      'm',
+      'bool8',
+      'b1',
+      'int64',
+      'i8',
+      'uint64',
+      'u8',
+      'float16',
+      'f2',
+      'float32',
+      'f4',
+      'float64',
+      'f8',
+      'float128',
+      'f16',
+      'complex64',
+      'c8',
+      'complex128',
+      'c16',
+      'complex256',
+      'c32',
+      'm8',
+      'int32',
+      'i4',
+      'uint32',
+      'u4',
+      'int16',
+      'i2',
+      'uint16',
+      'u2',
+      'int8',
+      'i1',
+      'uint8',
+      'u1',
+      'complex_',
+      'int0',
+      'uint0',
+      'single',
+      'csingle',
+      'singlecomplex',
+      'float_',
+      'intc',
+      'uintc',
+      'int_',
+      'longfloat',
+      'clongfloat',
+      'longcomplex',
+      'bool_',
+      'int',
+      'float',
+      'complex',
+      'bool',
+  ])
   def test_numpy_serialization(self, dtype):
     np.random.seed(0)
-    if (dtype in {'float128', 'f16', 'complex256', 'c32'}) and (platform.system() == 'Darwin') and (platform.machine() == 'arm64'):
-      pytest.skip(f'Mac M1 does not support dtype {dtype}') # skip testing these dtypes if user is on Mac M1
+    if (
+        (dtype in {'float128', 'f16', 'complex256', 'c32'})
+        and (platform.system() == 'Darwin')
+        and (platform.machine() == 'arm64')
+    ):
+      pytest.skip(
+          f'Mac M1 does not support dtype {dtype}'
+      )  # skip testing these dtypes if user is on Mac M1
 
     v = np.random.uniform(-100, 100, size=()).astype(dtype)[()]
     restored_v = serialization.msgpack_restore(
-        serialization.msgpack_serialize(v))
+        serialization.msgpack_serialize(v)
+    )
     self.assertEqual(restored_v.dtype, v.dtype)
     np.testing.assert_array_equal(restored_v, v)
 
     for shape in [(), (5,), (10, 10), (1, 20, 30, 1)]:
       arr = np.random.uniform(-100, 100, size=shape).astype(dtype)
       restored_arr = serialization.msgpack_restore(
-          serialization.msgpack_serialize(arr))
+          serialization.msgpack_serialize(arr)
+      )
       self.assertEqual(restored_arr.dtype, arr.dtype)
       np.testing.assert_array_equal(restored_arr, arr)
 
   def test_jax_numpy_serialization(self):
-    jax_dtypes = [jnp.bool_, jnp.uint8, jnp.uint16, jnp.uint32,
-                  jnp.int8, jnp.int16, jnp.int32,
-                  jnp.bfloat16, jnp.float16, jnp.float32,
-                  jnp.complex64]
+    jax_dtypes = [
+        jnp.bool_,
+        jnp.uint8,
+        jnp.uint16,
+        jnp.uint32,
+        jnp.int8,
+        jnp.int16,
+        jnp.int32,
+        jnp.bfloat16,
+        jnp.float16,
+        jnp.float32,
+        jnp.complex64,
+    ]
     for dtype in jax_dtypes:
       v = jnp.array(np.random.uniform(-100, 100, size=())).astype(dtype)[()]
       restored_v = serialization.msgpack_restore(
-          serialization.msgpack_serialize(v))
+          serialization.msgpack_serialize(v)
+      )
       self.assertEqual(restored_v.dtype, v.dtype)
       np.testing.assert_array_equal(restored_v, v)
 
       for shape in [(), (5,), (10, 10), (1, 20, 30, 1)]:
-        arr = jnp.array(
-            np.random.uniform(-100, 100, size=shape)).astype(dtype)
+        arr = jnp.array(np.random.uniform(-100, 100, size=shape)).astype(dtype)
         restored_arr = serialization.msgpack_restore(
-            serialization.msgpack_serialize(arr))
+            serialization.msgpack_serialize(arr)
+        )
         self.assertEqual(restored_arr.dtype, arr.dtype)
         np.testing.assert_array_equal(restored_arr, arr)
 
   def test_complex_serialization(self):
-    for x in [1j, 1+2j]:
+    for x in [1j, 1 + 2j]:
       restored_x = serialization.msgpack_restore(
-          serialization.msgpack_serialize(x))
+          serialization.msgpack_serialize(x)
+      )
       self.assertEqual(x, restored_x)
 
   def test_restore_chunked(self):
     old_chunksize = serialization.MAX_CHUNK_SIZE
     serialization.MAX_CHUNK_SIZE = 91 * 8
     try:
       tmp = np.random.uniform(-100, 100, size=(21, 37))
@@ -242,18 +327,20 @@
     finally:
       serialization.MAX_CHUNK_SIZE = old_chunksize
 
     np.testing.assert_array_equal(restored, tmp)
 
   def test_restore_unchunked(self):
     """Check if mgspack_restore works for unchunked inputs."""
+
     def msgpack_serialize_legacy(pytree):
       """Old implementation that was not chunking."""
-      return msgpack.packb(pytree, default=serialization._msgpack_ext_pack,
-                           strict_types=True)
+      return msgpack.packb(
+          pytree, default=serialization._msgpack_ext_pack, strict_types=True
+      )
 
     tmp = np.random.uniform(-100, 100, size=(21, 37))
     serialized = msgpack_serialize_legacy(tmp)
     old_chunksize = serialization.MAX_CHUNK_SIZE
     serialization.MAX_CHUNK_SIZE = 91 * 8
     try:
       restored = serialization.msgpack_restore(serialized)
@@ -272,24 +359,16 @@
     self.assertEqual(x1, restored_x1)
 
   def test_namedtuple_restore_legacy(self):
     foo_class = collections.namedtuple('Foo', 'a b c')
     x1 = foo_class(a=1, b=2, c=3)
     legacy_encoding = {
         'name': 'Foo',
-        'fields': {
-            '0': 'a',
-            '1': 'b',
-            '2': 'c'
-        },
-        'values': {
-            '0': 1,
-            '1': 2,
-            '2': 3
-        },
+        'fields': {'0': 'a', '1': 'b', '2': 'c'},
+        'values': {'0': 1, '1': 2, '2': 3},
     }
     x2 = foo_class(a=0, b=0, c=0)
     restored_x1 = serialization.from_state_dict(x2, legacy_encoding)
     self.assertEqual(type(x1), type(restored_x1))
     self.assertEqual(x1, restored_x1)
 
   def test_model_serialization_to_bytes(self):
@@ -319,22 +398,16 @@
       tmp = serialization._chunk_array_leaves_in_place(tmp)
     finally:
       serialization.MAX_CHUNK_SIZE = old_chunksize
     test = jax.tree_map(jnp.shape, tmp)
     ref = {
         'a': {
             '__msgpack_chunked_array__': (),
-            'chunks': {
-                '0': (91,),
-                '1': (9,)
-            },
-            'shape': {
-                '0': (),
-                '1': ()
-            }
+            'chunks': {'0': (91,), '1': (9,)},
+            'shape': {'0': (), '1': ()},
         }
     }
     self.assertEqual(test, ref)
 
   def test_serialization_chunking2(self):
     old_chunksize = serialization.MAX_CHUNK_SIZE
     serialization.MAX_CHUNK_SIZE = 91 * 8
@@ -355,41 +428,73 @@
       newtmp = serialization.msgpack_restore(tmpbytes)
     finally:
       serialization.MAX_CHUNK_SIZE = old_chunksize
 
     jax.tree_map(np.testing.assert_array_equal, tmp, newtmp)
 
   @parameterized.parameters(
-      {'target': [[[1, 2, 3], [4, 5]]], 'wrong_target': [[[1, 2, 3], [4]]],
-       'msg': ('The size of the list and the state dict do not match,'
-               ' got 1 and 2 at path ./0/1')},
-      {'target': (((1, 2, 3), (4, 5)),),
-       'wrong_target': (((1, 2, 3), (4,)),),
-       'msg': ('The size of the list and the state dict do not match,'
-               ' got 1 and 2 at path ./0/1')},
-      {'target': (((1, 2, 3), (OriginalTuple([4, 5]), 6)),),
-       'wrong_target': (((1, 2, 3), (WrongTuple([4, 5]), 6)),),
-       'msg': ("The field names of the state dict and the named tuple do "
-               "not match, got {'value'} and {'wrong_field'} at path ./0/1/0")},
-      {'target': {'a': {'b': {'c': [1, 2, 3], 'd': [4, 5]}}},
-       'wrong_target': {'a': {'b': {'c': [1, 2, 3], 'd': [4]}}},
-       'msg': ('The size of the list and the state dict do not match,'
-               ' got 1 and 2 at path ./a/b/d')},
-      {'target': {'a': {'b': {'c': [1, 2, 3], 'd': [4, 5]}}},
-       'wrong_target': {'a': {'b': {'c': [1, 2, 3], 'e': [4, 5]}}},
-       'msg': ("The target dict keys and state dict keys do not match, "
-               "target dict contains keys {'e'} which are not present in state dict at path ./a/b")},
-      {'target': 'original_params',
-       'wrong_target': 'wrong_params',
-       'msg': ("The target dict keys and state dict keys do not match, "
-               "target dict contains keys {'Dense_1'} which are not present in state dict at path ./params")},
-      {'target': 'original_train_state',
-       'wrong_target': 'wrong_train_state',
-       'msg': ("The target dict keys and state dict keys do not match, "
-               "target dict contains keys {'Dense_1'} which are not present in state dict at path ./params/params")}
+      {
+          'target': [[[1, 2, 3], [4, 5]]],
+          'wrong_target': [[[1, 2, 3], [4]]],
+          'msg': (
+              'The size of the list and the state dict do not match,'
+              ' got 1 and 2 at path ./0/1'
+          ),
+      },
+      {
+          'target': (((1, 2, 3), (4, 5)),),
+          'wrong_target': (((1, 2, 3), (4,)),),
+          'msg': (
+              'The size of the list and the state dict do not match,'
+              ' got 1 and 2 at path ./0/1'
+          ),
+      },
+      {
+          'target': (((1, 2, 3), (OriginalTuple([4, 5]), 6)),),
+          'wrong_target': (((1, 2, 3), (WrongTuple([4, 5]), 6)),),
+          'msg': (
+              'The field names of the state dict and the named tuple do '
+              "not match, got {'value'} and {'wrong_field'} at path ./0/1/0"
+          ),
+      },
+      {
+          'target': {'a': {'b': {'c': [1, 2, 3], 'd': [4, 5]}}},
+          'wrong_target': {'a': {'b': {'c': [1, 2, 3], 'd': [4]}}},
+          'msg': (
+              'The size of the list and the state dict do not match,'
+              ' got 1 and 2 at path ./a/b/d'
+          ),
+      },
+      {
+          'target': {'a': {'b': {'c': [1, 2, 3], 'd': [4, 5]}}},
+          'wrong_target': {'a': {'b': {'c': [1, 2, 3], 'e': [4, 5]}}},
+          'msg': (
+              'The target dict keys and state dict keys do not match, target'
+              " dict contains keys {'e'} which are not present in state dict at"
+              ' path ./a/b'
+          ),
+      },
+      {
+          'target': 'original_params',
+          'wrong_target': 'wrong_params',
+          'msg': (
+              'The target dict keys and state dict keys do not match, target'
+              " dict contains keys {'Dense_1'} which are not present in state"
+              ' dict at path ./params'
+          ),
+      },
+      {
+          'target': 'original_train_state',
+          'wrong_target': 'wrong_train_state',
+          'msg': (
+              'The target dict keys and state dict keys do not match, target'
+              " dict contains keys {'Dense_1'} which are not present in state"
+              ' dict at path ./params/params'
+          ),
+      },
   )
   def test_serialization_errors(self, target, wrong_target, msg):
     if target == 'original_params':
       x = jnp.ones((1, 28, 28, 1))
       rng = jax.random.PRNGKey(1)
       original_module = OriginalModule()
       target = original_module.init(rng, x)
@@ -402,17 +507,19 @@
       original_module = OriginalModule()
       original_params = original_module.init(rng, x)
       wrong_module = WrongModule()
       wrong_params = wrong_module.init(rng, x)
 
       tx = optax.sgd(learning_rate=0.1, momentum=0.9)
       target = train_state.TrainState.create(
-          apply_fn=original_module.apply, params=original_params, tx=tx)
+          apply_fn=original_module.apply, params=original_params, tx=tx
+      )
       wrong_target = train_state.TrainState.create(
-          apply_fn=wrong_module.apply, params=wrong_params, tx=tx)
+          apply_fn=wrong_module.apply, params=wrong_params, tx=tx
+      )
 
     encoded_bytes = serialization.to_bytes(target)
     with self.assertRaisesWithLiteralMatch(ValueError, msg):
       serialization.from_bytes(wrong_target, encoded_bytes)
 
 
 if __name__ == '__main__':
```

### Comparing `flax-0.7.0/tests/struct_test.py` & `flax-0.7.1/tests/struct_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,31 +56,31 @@
     leaves = jax.tree_util.tree_leaves(p)
     self.assertEqual(leaves, [1, 2])
     new_p = jax.tree_util.tree_map(lambda x: x + x, p)
     self.assertEqual(new_p, Point(x=2, y=4, meta={'abc': True}))
 
   def test_keypath_error(self):
     # TODO(mattjj): avoid using internal prefix_errors by testing vmap error msg
-    e, = prefix_errors(Point(1., [2.],  meta={}), Point(1., 2., meta={}))
+    (e,) = prefix_errors(Point(1.0, [2.0], meta={}), Point(1.0, 2.0, meta={}))
     with self.assertRaisesRegex(ValueError, r'in_axes\.y'):
       raise e('in_axes')
 
   def test_double_wrap_no_op(self):
-
     class A:
       a: int
 
     self.assertFalse(hasattr(A, '_flax_dataclass'))
 
     A = struct.dataclass(A)
     self.assertTrue(hasattr(A, '_flax_dataclass'))
 
-    A = struct.dataclass(A) # no-op
+    A = struct.dataclass(A)  # no-op
     self.assertTrue(hasattr(A, '_flax_dataclass'))
 
   def test_wrap_pytree_node_no_error(self):
     @struct.dataclass
     class A(struct.PyTreeNode):
       a: int
 
+
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/tensorboard_test.py` & `flax-0.7.1/tests/tensorboard_test.py`

 * *Files 15% similar despite different names*

```diff
@@ -23,17 +23,18 @@
 from tensorboard.backend.event_processing import directory_watcher
 from tensorboard.backend.event_processing import event_file_loader
 from tensorboard.util import tensor_util
 import tensorflow as tf
 
 from flax.metrics.tensorboard import SummaryWriter, _flatten_dict
 
+
 def _process_event(event):
   for value in event.summary.value:
-    yield {'wall_time': event.wall_time, 'step': event.step, 'value': value}
+    yield {"wall_time": event.wall_time, "step": event.step, "value": value}
 
 
 def _disk_usage(path: pathlib.Path):
   """Recursively computes the disk usage of a directory."""
   if path.is_file():
     return path.stat().st_size
   elif path.is_dir():
@@ -48,287 +49,333 @@
 class TensorboardTest(absltest.TestCase):
 
   def parse_and_return_summary_value(self, path):
     """Parse the event file in the given path and return the
     only summary value."""
     event_value_list = []
     event_file_generator = directory_watcher.DirectoryWatcher(
-        path, event_file_loader.EventFileLoader).Load()
+        path, event_file_loader.EventFileLoader
+    ).Load()
     event_values = itertools.chain.from_iterable(
-        map(_process_event, event_file_generator))
+        map(_process_event, event_file_generator)
+    )
     for value_dict in event_values:
       event_value_list.append(value_dict)
 
     self.assertLen(event_value_list, 1)
-    self.assertEqual(event_value_list[0]['step'], 1)
-    self.assertGreater(event_value_list[0]['wall_time'], 0.0)
-    return event_value_list[0]['value']
+    self.assertEqual(event_value_list[0]["step"], 1)
+    self.assertGreater(event_value_list[0]["wall_time"], 0.0)
+    return event_value_list[0]["value"]
 
   def test_summarywriter_flush_after_close(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
     summary_writer.close()
     with self.assertRaises(AttributeError):
       summary_writer.flush()
 
   def test_summarywriter_scalar(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
     # Write the scalar and check if the event exists and check data.
     float_value = 99.1232
-    summary_writer.scalar(tag='scalar_test', value=float_value, step=1)
+    summary_writer.scalar(tag="scalar_test", value=float_value, step=1)
 
     summary_value = self.parse_and_return_summary_value(path=log_dir)
-    self.assertEqual(summary_value.tag, 'scalar_test')
-    self.assertTrue(np.allclose(
-        tensor_util.make_ndarray(summary_value.tensor).item(),
-        float_value))
+    self.assertEqual(summary_value.tag, "scalar_test")
+    self.assertTrue(
+        np.allclose(
+            tensor_util.make_ndarray(summary_value.tensor).item(), float_value
+        )
+    )
 
   def test_summarywriter_text(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
-    text = 'hello world.'
-    summary_writer.text(tag='text_test', textdata=text, step=1)
+    text = "hello world."
+    summary_writer.text(tag="text_test", textdata=text, step=1)
 
     summary_value = self.parse_and_return_summary_value(path=log_dir)
-    self.assertEqual(summary_value.tag, 'text_test')
+    self.assertEqual(summary_value.tag, "text_test")
     self.assertEqual(
-        tensor_util.make_ndarray(summary_value.tensor).item().decode('utf-8'),
-        text)
+        tensor_util.make_ndarray(summary_value.tensor).item().decode("utf-8"),
+        text,
+    )
 
   def test_summarywriter_image(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
-    expected_img = np.random.uniform(low=0., high=255., size=(30, 30, 3))
+    expected_img = np.random.uniform(low=0.0, high=255.0, size=(30, 30, 3))
     expected_img = expected_img.astype(np.uint8)
-    summary_writer.image(tag='image_test', image=expected_img, step=1)
+    summary_writer.image(tag="image_test", image=expected_img, step=1)
     summary_value = self.parse_and_return_summary_value(path=log_dir)
 
-    self.assertEqual(summary_value.tag, 'image_test')
+    self.assertEqual(summary_value.tag, "image_test")
     actual_img = tf.image.decode_image(summary_value.tensor.string_val[2])
     self.assertTrue(np.allclose(actual_img, expected_img))
 
   def test_summarywriter_image_float_pixel_values(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
-    expected_img = np.random.uniform(low=0., high=1., size=(30, 30, 3))
-    summary_writer.image(tag='image_test', image=expected_img, step=1)
+    expected_img = np.random.uniform(low=0.0, high=1.0, size=(30, 30, 3))
+    summary_writer.image(tag="image_test", image=expected_img, step=1)
     summary_value = self.parse_and_return_summary_value(path=log_dir)
 
     # convert and scale expected_img appropriately to numpy uint8.
     expected_img = tf.image.convert_image_dtype(
-        image=expected_img, dtype=np.uint8)
+        image=expected_img, dtype=np.uint8
+    )
 
-    self.assertEqual(summary_value.tag, 'image_test')
+    self.assertEqual(summary_value.tag, "image_test")
     actual_img = tf.image.decode_image(summary_value.tensor.string_val[2])
     self.assertTrue(np.allclose(actual_img, expected_img))
 
   def test_summarywriter_2dimage_scaled(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
-    img = np.random.uniform(low=0., high=255., size=(30, 30))
+    img = np.random.uniform(low=0.0, high=255.0, size=(30, 30))
     img = img.astype(np.uint8)
-    summary_writer.image(tag='2dimage_test', image=img, step=1)
+    summary_writer.image(tag="2dimage_test", image=img, step=1)
     summary_value = self.parse_and_return_summary_value(path=log_dir)
 
-    self.assertEqual(summary_value.tag, '2dimage_test')
+    self.assertEqual(summary_value.tag, "2dimage_test")
     actual_img = tf.image.decode_image(summary_value.tensor.string_val[2])
     # assert the image was increased in dimension
     self.assertEqual(actual_img.shape, (30, 30, 3))
 
   def test_summarywriter_single_channel_image_scaled(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
-    img = np.random.uniform(low=0., high=255., size=(30, 30, 1))
+    img = np.random.uniform(low=0.0, high=255.0, size=(30, 30, 1))
     img = img.astype(np.uint8)
-    summary_writer.image(tag='2dimage_1channel_test', image=img, step=1)
+    summary_writer.image(tag="2dimage_1channel_test", image=img, step=1)
     summary_value = self.parse_and_return_summary_value(path=log_dir)
 
-    self.assertEqual(summary_value.tag, '2dimage_1channel_test')
+    self.assertEqual(summary_value.tag, "2dimage_1channel_test")
     actual_img = tf.image.decode_image(summary_value.tensor.string_val[2])
     # assert the image was increased in dimension
     self.assertEqual(actual_img.shape, (30, 30, 3))
 
   def test_summarywriter_multiple_images(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
-    expected_img = np.random.uniform(low=0., high=255., size=(2, 30, 30, 3))
+    expected_img = np.random.uniform(low=0.0, high=255.0, size=(2, 30, 30, 3))
     expected_img = expected_img.astype(np.uint8)
-    summary_writer.image(tag='multiple_images_test', image=expected_img, step=1)
+    summary_writer.image(tag="multiple_images_test", image=expected_img, step=1)
     summary_value = self.parse_and_return_summary_value(path=log_dir)
 
-    self.assertEqual(summary_value.tag, 'multiple_images_test')
-    actual_imgs = [tf.image.decode_image(s)
-                   for s in summary_value.tensor.string_val[2:]]
+    self.assertEqual(summary_value.tag, "multiple_images_test")
+    actual_imgs = [
+        tf.image.decode_image(s) for s in summary_value.tensor.string_val[2:]
+    ]
     self.assertTrue(np.allclose(np.stack(actual_imgs, axis=0), expected_img))
 
   def test_summarywriter_multiple_2dimages_scaled(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
-    img = np.random.uniform(low=0., high=255., size=(2, 30, 30))
+    img = np.random.uniform(low=0.0, high=255.0, size=(2, 30, 30))
     img = img.astype(np.uint8)
-    summary_writer.image(tag='multiple_2dimages_test', image=img, step=1)
+    summary_writer.image(tag="multiple_2dimages_test", image=img, step=1)
     summary_value = self.parse_and_return_summary_value(path=log_dir)
 
-    self.assertEqual(summary_value.tag, 'multiple_2dimages_test')
-    actual_imgs = [tf.image.decode_image(s)
-                   for s in summary_value.tensor.string_val[2:]]
+    self.assertEqual(summary_value.tag, "multiple_2dimages_test")
+    actual_imgs = [
+        tf.image.decode_image(s) for s in summary_value.tensor.string_val[2:]
+    ]
     # assert the images were increased in dimension
     self.assertEqual(np.stack(actual_imgs, axis=0).shape, (2, 30, 30, 3))
 
   def test_summarywriter_audio(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
     expected_audio_samples = np.random.uniform(
-        low=-1., high=1., size=(2, 48000, 2))
+        low=-1.0, high=1.0, size=(2, 48000, 2)
+    )
     summary_writer.audio(
-        tag='audio_test', audiodata=expected_audio_samples, step=1)
+        tag="audio_test", audiodata=expected_audio_samples, step=1
+    )
 
     summary_value = self.parse_and_return_summary_value(path=log_dir)
-    self.assertEqual(summary_value.tag, 'audio_test')
+    self.assertEqual(summary_value.tag, "audio_test")
 
     # Assert two audio files are parsed.
     self.assertLen(summary_value.tensor.string_val, 2)
 
     # Assert values.
     actual_audio_1 = tf.audio.decode_wav(
-        summary_value.tensor.string_val[0]).audio
-    self.assertTrue(np.allclose(
-        expected_audio_samples[0], actual_audio_1, atol=1e-04))
+        summary_value.tensor.string_val[0]
+    ).audio
+    self.assertTrue(
+        np.allclose(expected_audio_samples[0], actual_audio_1, atol=1e-04)
+    )
 
     actual_audio_2 = tf.audio.decode_wav(
-        summary_value.tensor.string_val[1]).audio
-    self.assertTrue(np.allclose(
-        expected_audio_samples[1], actual_audio_2, atol=1e-04))
+        summary_value.tensor.string_val[1]
+    ).audio
+    self.assertTrue(
+        np.allclose(expected_audio_samples[1], actual_audio_2, atol=1e-04)
+    )
 
   def test_summarywriter_audio_sampled_output(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
     expected_audio_samples = np.random.uniform(
-        low=-1., high=1., size=(2, 48000, 2))
+        low=-1.0, high=1.0, size=(2, 48000, 2)
+    )
     summary_writer.audio(
-        tag='audio_test', audiodata=expected_audio_samples, step=1,
-        max_outputs=1)
+        tag="audio_test",
+        audiodata=expected_audio_samples,
+        step=1,
+        max_outputs=1,
+    )
 
     summary_value = self.parse_and_return_summary_value(path=log_dir)
-    self.assertEqual(summary_value.tag, 'audio_test')
+    self.assertEqual(summary_value.tag, "audio_test")
 
     # Assert only the first audio clip is available.
     self.assertLen(summary_value.tensor.string_val, 1)
 
     # Assert values.
     actual_audio = tf.audio.decode_wav(summary_value.tensor.string_val[0]).audio
 
-    self.assertTrue(np.allclose(
-        expected_audio_samples[0], actual_audio, atol=1e-04))
+    self.assertTrue(
+        np.allclose(expected_audio_samples[0], actual_audio, atol=1e-04)
+    )
 
   def test_summarywriter_clipped_audio(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
     expected_audio_samples = np.random.uniform(
-        low=-2., high=2., size=(2, 48000, 2))
+        low=-2.0, high=2.0, size=(2, 48000, 2)
+    )
     summary_writer.audio(
-        tag='audio_test', audiodata=expected_audio_samples, step=1,
-        max_outputs=1)
+        tag="audio_test",
+        audiodata=expected_audio_samples,
+        step=1,
+        max_outputs=1,
+    )
 
     summary_value = self.parse_and_return_summary_value(path=log_dir)
-    self.assertEqual(summary_value.tag, 'audio_test')
+    self.assertEqual(summary_value.tag, "audio_test")
 
     # Assert one audio files is parsed.
     self.assertLen(summary_value.tensor.string_val, 1)
 
     # actual_audio is clipped.
-    actual_audio = tf.audio.decode_wav(
-        summary_value.tensor.string_val[0]).audio
-    self.assertFalse(np.allclose(
-        expected_audio_samples[0], actual_audio, atol=1e-04))
+    actual_audio = tf.audio.decode_wav(summary_value.tensor.string_val[0]).audio
+    self.assertFalse(
+        np.allclose(expected_audio_samples[0], actual_audio, atol=1e-04)
+    )
 
     clipped_audio = np.clip(np.array(expected_audio_samples[0]), -1, 1)
-    self.assertTrue(
-        np.allclose(clipped_audio, actual_audio, atol=1e-04))
+    self.assertTrue(np.allclose(clipped_audio, actual_audio, atol=1e-04))
 
   def test_summarywriter_histogram_defaultbins(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
     histogram = np.arange(1000)
     # Histogram will be created for 30 (default) bins.
-    summary_writer.histogram(tag='histogram_test', values=histogram, step=1)
+    summary_writer.histogram(tag="histogram_test", values=histogram, step=1)
 
     summary_value = self.parse_and_return_summary_value(path=log_dir)
-    self.assertEqual(summary_value.tag, 'histogram_test')
+    self.assertEqual(summary_value.tag, "histogram_test")
     actual_histogram = tensor_util.make_ndarray(summary_value.tensor)
     self.assertTrue(actual_histogram.shape, (30, 3))
     self.assertTrue(
-        np.allclose(actual_histogram[0], (0.0, 33.3, 34.0), atol=1e-01))
+        np.allclose(actual_histogram[0], (0.0, 33.3, 34.0), atol=1e-01)
+    )
 
   def test_summarywriter_histogram_2bins(self):
     log_dir = tempfile.mkdtemp()
     summary_writer = SummaryWriter(log_dir=log_dir)
     histogram = np.arange(1000)
     summary_writer.histogram(
-        tag='histogram_test', values=histogram, step=1, bins=2)
+        tag="histogram_test", values=histogram, step=1, bins=2
+    )
 
     summary_value = self.parse_and_return_summary_value(path=log_dir)
-    self.assertEqual(summary_value.tag, 'histogram_test')
+    self.assertEqual(summary_value.tag, "histogram_test")
     actual_histogram = tensor_util.make_ndarray(summary_value.tensor)
     self.assertTrue(actual_histogram.shape, (2, 3))
     self.assertTrue(
-        np.allclose(actual_histogram[0], (0.0, 499.5, 500.0), atol=1e-01))
+        np.allclose(actual_histogram[0], (0.0, 499.5, 500.0), atol=1e-01)
+    )
     self.assertTrue(
-        np.allclose(actual_histogram[1], (499.5, 999.0, 500.0), atol=1e-01))
+        np.allclose(actual_histogram[1], (499.5, 999.0, 500.0), atol=1e-01)
+    )
 
   def test_flatten_dict(self):
     # Valid types according to https://github.com/tensorflow/tensorboard/blob/1204566da5437af55109f7a4af18f9f8b7c4f864/tensorboard/plugins/hparams/summary_v2.py
-    input_hparams={
-      # Example Invalid Types
-      "None": None, "List": [1, 2, 3], "Tuple": (1, 2, 3), "Complex": complex("1+1j"), "np.complex_": np.complex_("1+1j"),
-      # Valid Python Types
-      "Bool": True, "Int": 1, "Float": 1.0, "Str": "test",
-      # Valid Numpy Types
-      "np.bool_": np.bool_(1), "np.integer": np.int_(1),  "np.floating": np.float_(1.0), "np.character": np.str_("test"),
-      # Nested dict to flatten
-      "Nested_Dict": {
+    input_hparams = {
+        # Example Invalid Types
         "None": None,
         "List": [1, 2, 3],
         "Tuple": (1, 2, 3),
         "Complex": complex("1+1j"),
         "np.complex_": np.complex_("1+1j"),
+        # Valid Python Types
         "Bool": True,
         "Int": 1,
         "Float": 1.0,
         "Str": "test",
+        # Valid Numpy Types
         "np.bool_": np.bool_(1),
         "np.integer": np.int_(1),
         "np.floating": np.float_(1.0),
-        "np.character": np.str_("test")
-      }
+        "np.character": np.str_("test"),
+        # Nested dict to flatten
+        "Nested_Dict": {
+            "None": None,
+            "List": [1, 2, 3],
+            "Tuple": (1, 2, 3),
+            "Complex": complex("1+1j"),
+            "np.complex_": np.complex_("1+1j"),
+            "Bool": True,
+            "Int": 1,
+            "Float": 1.0,
+            "Str": "test",
+            "np.bool_": np.bool_(1),
+            "np.integer": np.int_(1),
+            "np.floating": np.float_(1.0),
+            "np.character": np.str_("test"),
+        },
     }
 
     result_hparams = _flatten_dict(input_hparams)
 
-    expected_hparams={
-      "None": "None", "List": "[1, 2, 3]", "Tuple": "(1, 2, 3)", "Complex": "(1+1j)", "np.complex_": "(1+1j)",
-      # Valid Python Types
-      "Bool": True, "Int": 1, "Float": 1.0, "Str": "test",
-      # Valid Numpy Types
-      "np.bool_": np.bool_(1), "np.integer": np.int_(1),  "np.floating": np.float_(1.0), "np.character": np.str_("test"),
-      # Nested Dict
-      "Nested_Dict.None": "None",
-      "Nested_Dict.List": "[1, 2, 3]",
-      "Nested_Dict.Tuple": "(1, 2, 3)",
-      "Nested_Dict.Complex": "(1+1j)",
-      "Nested_Dict.np.complex_": "(1+1j)",
-      "Nested_Dict.Bool": True,
-      "Nested_Dict.Int": 1,
-      "Nested_Dict.Float": 1.0,
-      "Nested_Dict.Str": "test",
-      "Nested_Dict.np.bool_": np.bool_(1),
-      "Nested_Dict.np.integer": np.int_(1),
-      "Nested_Dict.np.floating": np.float_(1.0),
-      "Nested_Dict.np.character": np.str_("test")
+    expected_hparams = {
+        "None": "None",
+        "List": "[1, 2, 3]",
+        "Tuple": "(1, 2, 3)",
+        "Complex": "(1+1j)",
+        "np.complex_": "(1+1j)",
+        # Valid Python Types
+        "Bool": True,
+        "Int": 1,
+        "Float": 1.0,
+        "Str": "test",
+        # Valid Numpy Types
+        "np.bool_": np.bool_(1),
+        "np.integer": np.int_(1),
+        "np.floating": np.float_(1.0),
+        "np.character": np.str_("test"),
+        # Nested Dict
+        "Nested_Dict.None": "None",
+        "Nested_Dict.List": "[1, 2, 3]",
+        "Nested_Dict.Tuple": "(1, 2, 3)",
+        "Nested_Dict.Complex": "(1+1j)",
+        "Nested_Dict.np.complex_": "(1+1j)",
+        "Nested_Dict.Bool": True,
+        "Nested_Dict.Int": 1,
+        "Nested_Dict.Float": 1.0,
+        "Nested_Dict.Str": "test",
+        "Nested_Dict.np.bool_": np.bool_(1),
+        "Nested_Dict.np.integer": np.int_(1),
+        "Nested_Dict.np.floating": np.float_(1.0),
+        "Nested_Dict.np.character": np.str_("test"),
     }
 
     self.assertDictEqual(result_hparams, expected_hparams)
 
   def test_auto_flush(self):
     tmp_dir = pathlib.Path(self.create_tempdir().full_path)
     summary_writer = SummaryWriter(tmp_dir, auto_flush=True)
@@ -344,9 +391,9 @@
     summary_writer.scalar("metric", 123, 1)
     filesize_before_flush = _disk_usage(tmp_dir)
     summary_writer.flush()
     filesize_after_flush = _disk_usage(tmp_dir)
     self.assertLess(filesize_before_flush, filesize_after_flush)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
   absltest.main()
```

### Comparing `flax-0.7.0/tests/traceback_util_test.py` & `flax-0.7.1/tests/traceback_util_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -39,25 +39,30 @@
   def test_exclusion_list(self):
     traceback_util.show_flax_in_tracebacks()
     exclusion_len_wo_flax = len(jax_traceback_util._exclude_paths)
     traceback_util.hide_flax_in_tracebacks()
     exclusion_len_w_flax = len(jax_traceback_util._exclude_paths)
     self.assertLen(
         traceback_util._flax_exclusions,
-        exclusion_len_w_flax - exclusion_len_wo_flax)
+        exclusion_len_w_flax - exclusion_len_wo_flax,
+    )
 
   def test_simple_exclusion_tracebackhide(self):
     if not TRACEBACKHIDE_SUPPORTED:
       return
+
     class Test1(nn.Module):
+
       @nn.remat
       @nn.compact
       def __call__(self, x):
         return Test2()(x)
+
     class Test2(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         raise ValueError('error here.')
         return x  # pylint: disable=unreachable
 
     traceback_util.hide_flax_in_tracebacks()
@@ -77,22 +82,24 @@
         self.assertIn(f.f_code.co_filename, EXPECTED_FILES)
         filtered_frames += 1
       unfiltered_frames += 1
 
     self.assertEqual(filtered_frames, 3)
     self.assertGreater(unfiltered_frames, filtered_frames)
 
-
   def test_simple_exclusion_remove_frames(self):
     class Test1(nn.Module):
+
       @nn.remat
       @nn.compact
       def __call__(self, x):
         return Test2()(x)
+
     class Test2(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         raise ValueError('error here.')
         return x  # pylint: disable=unreachable
 
     traceback_util.hide_flax_in_tracebacks()
@@ -115,26 +122,27 @@
     unfiltered_frames = 0
     for _, _ in traceback.walk_tb(tb_unfiltered):
       unfiltered_frames += 1
 
     self.assertEqual(filtered_frames, 3)
     self.assertGreater(unfiltered_frames, filtered_frames)
 
-
   def test_dynamic_exclusion(self):
-
     if not TRACEBACKHIDE_SUPPORTED:
       return
 
     class Test1(nn.Module):
+
       @nn.remat
       @nn.compact
       def __call__(self, x):
         return Test2()(x)
+
     class Test2(nn.Module):
+
       @nn.jit
       @nn.compact
       def __call__(self, x):
         raise ValueError('error here.')
         return x  # pylint: disable=unreachable
 
     key = random.PRNGKey(0)
@@ -181,18 +189,22 @@
     unfiltered_frames_w_flax = 0
     for f, _ in traceback.walk_tb(tb_w_flax):
       if '__tracebackhide__' not in f.f_locals:
         unfiltered_frames_w_flax += 1
       else:
         filtered_frames_w_flax += 1
 
-    self.assertEqual(unfiltered_frames_all + filtered_frames_all,
-                     unfiltered_frames_w_flax + filtered_frames_w_flax)
-    self.assertEqual(unfiltered_frames_all + filtered_frames_all,
-                     unfiltered_frames_no_flax + filtered_frames_no_flax)
+    self.assertEqual(
+        unfiltered_frames_all + filtered_frames_all,
+        unfiltered_frames_w_flax + filtered_frames_w_flax,
+    )
+    self.assertEqual(
+        unfiltered_frames_all + filtered_frames_all,
+        unfiltered_frames_no_flax + filtered_frames_no_flax,
+    )
     self.assertEqual(unfiltered_frames_no_flax, 3)
     self.assertGreater(unfiltered_frames_all, unfiltered_frames_w_flax)
     self.assertGreater(unfiltered_frames_w_flax, unfiltered_frames_no_flax)
 
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.7.0/tests/traverse_util_test.py` & `flax-0.7.1/tests/traverse_util_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -99,16 +99,17 @@
     self.assertEqual(list(traversal.iterate(x)), [2])
     y = traversal.update(lambda x: x + x, x)
     self.assertEqual(y, Point(x=1, y=4))
 
   def test_traverse_merge(self):
     x = [{'foo': 1, 'bar': 2}, {'foo': 3, 'bar': 4}]
     traversal_base = traverse_util.t_identity.each()
-    traversal = traversal_base.merge(traverse_util.TraverseItem('foo'),
-                                     traverse_util.TraverseItem('bar'))
+    traversal = traversal_base.merge(
+        traverse_util.TraverseItem('foo'), traverse_util.TraverseItem('bar')
+    )
     self.assertEqual(list(traversal.iterate(x)), [1, 2, 3, 4])
     y = traversal.update(lambda x: x + x, x)
     self.assertEqual(y, [{'foo': 2, 'bar': 4}, {'foo': 6, 'bar': 8}])
 
   def test_traverse_each(self):
     x = [{'foo': 1}, {'foo': 2}]
     traversal = traverse_util.t_identity.each()['foo']
@@ -146,68 +147,80 @@
       traversal.set([3], x)  # too few values
     with self.assertRaises(ValueError):
       traversal.set([3, 4, 5], x)  # too many values
 
   def test_flatten_dict(self):
     xs = {'foo': 1, 'bar': {'a': 2, 'b': {}}}
     flat_xs = traverse_util.flatten_dict(xs)
-    self.assertEqual(flat_xs, {
-        ('foo',): 1,
-        ('bar', 'a'): 2,
-    })
+    self.assertEqual(
+        flat_xs,
+        {
+            ('foo',): 1,
+            ('bar', 'a'): 2,
+        },
+    )
     flat_xs = traverse_util.flatten_dict(freeze(xs))
-    self.assertEqual(flat_xs, {
-      ('foo',): 1,
-      ('bar', 'a'): 2,
-    })
+    self.assertEqual(
+        flat_xs,
+        {
+            ('foo',): 1,
+            ('bar', 'a'): 2,
+        },
+    )
     flat_xs = traverse_util.flatten_dict(xs, sep='/')
-    self.assertEqual(flat_xs, {
-      'foo': 1,
-      'bar/a': 2,
-    })
+    self.assertEqual(
+        flat_xs,
+        {
+            'foo': 1,
+            'bar/a': 2,
+        },
+    )
 
   def test_unflatten_dict(self):
-    expected_xs = {
-      'foo': 1,
-      'bar': {'a': 2}
-    }
+    expected_xs = {'foo': 1, 'bar': {'a': 2}}
     xs = traverse_util.unflatten_dict({
-      ('foo',): 1,
-      ('bar', 'a'): 2,
+        ('foo',): 1,
+        ('bar', 'a'): 2,
     })
     self.assertEqual(xs, expected_xs)
-    xs = traverse_util.unflatten_dict({
-      'foo': 1,
-      'bar/a': 2,
-    }, sep='/')
+    xs = traverse_util.unflatten_dict(
+        {
+            'foo': 1,
+            'bar/a': 2,
+        },
+        sep='/',
+    )
     self.assertEqual(xs, expected_xs)
 
   def test_flatten_dict_keep_empty(self):
     xs = {'foo': 1, 'bar': {'a': 2, 'b': {}}}
     flat_xs = traverse_util.flatten_dict(xs, keep_empty_nodes=True)
-    self.assertEqual(flat_xs, {
-        ('foo',): 1,
-        ('bar', 'a'): 2,
-        ('bar', 'b'): traverse_util.empty_node,
-    })
+    self.assertEqual(
+        flat_xs,
+        {
+            ('foo',): 1,
+            ('bar', 'a'): 2,
+            ('bar', 'b'): traverse_util.empty_node,
+        },
+    )
     xs_restore = traverse_util.unflatten_dict(flat_xs)
     self.assertEqual(xs, xs_restore)
 
   def test_flatten_dict_is_leaf(self):
     xs = {'foo': {'c': 4}, 'bar': {'a': 2, 'b': {}}}
     flat_xs = traverse_util.flatten_dict(
-        xs,
-        is_leaf=lambda k, x: len(k) == 1 and len(x) == 2)
-    self.assertEqual(flat_xs, {
-        ('foo', 'c'): 4,
-        ('bar',): {
-            'a': 2,
-            'b': {}
+        xs, is_leaf=lambda k, x: len(k) == 1 and len(x) == 2
+    )
+    self.assertEqual(
+        flat_xs,
+        {
+            ('foo', 'c'): 4,
+            ('bar',): {'a': 2, 'b': {}},
         },
-    })
+    )
     xs_restore = traverse_util.unflatten_dict(flat_xs)
     self.assertEqual(xs, xs_restore)
 
 
 class ModelParamTraversalTest(absltest.TestCase):
 
   def test_only_works_on_model_params(self):
@@ -231,82 +244,108 @@
         'x': {
             'kernel': 2,
             'bias': 2,
             'y': {
                 'kernel': 6,
                 'bias': 4,
             },
-            'z': {}
+            'z': {},
         },
     }
     names = []
+
     def filter_fn(name, _):
       names.append(name)  # track names passed to filter_fn for testing
       return 'kernel' in name
+
     traversal = traverse_util.ModelParamTraversal(filter_fn)
 
     values = list(traversal.iterate(params))
     configs = [
         (params, expected_params),
         (flax.core.FrozenDict(params), flax.core.FrozenDict(expected_params)),
     ]
     for model, expected_model in configs:
       self.assertEqual(values, [1, 3])
-      self.assertEqual(set(names), set([
-          '/x/kernel', '/x/bias', '/x/y/kernel', '/x/y/bias']))
+      self.assertEqual(
+          set(names), set(['/x/kernel', '/x/bias', '/x/y/kernel', '/x/y/bias'])
+      )
       new_model = traversal.update(lambda x: x + x, model)
       self.assertEqual(new_model, expected_model)
 
   def test_path_value(self):
     params_in = {'a': {'b': 10, 'c': 2}}
     params_out = traverse_util.path_aware_map(
-      lambda path, x: x + 1 if 'b' in path else -x, params_in)
+        lambda path, x: x + 1 if 'b' in path else -x, params_in
+    )
 
     self.assertEqual(params_out, {'a': {'b': 11, 'c': -2}})
 
   def test_path_aware_map_with_multi_transform(self):
-    params = {'linear_1': {'w': jnp.zeros((5, 6)), 'b': jnp.zeros(5)},
-            'linear_2': {'w': jnp.zeros((6, 1)), 'b': jnp.zeros(1)}}
+    params = {
+        'linear_1': {'w': jnp.zeros((5, 6)), 'b': jnp.zeros(5)},
+        'linear_2': {'w': jnp.zeros((6, 1)), 'b': jnp.zeros(1)},
+    }
     gradients = jax.tree_util.tree_map(jnp.ones_like, params)  # dummy gradients
 
     param_labels = traverse_util.path_aware_map(
-      lambda path, x: 'kernel' if 'w' in path else 'bias', params)
+        lambda path, x: 'kernel' if 'w' in path else 'bias', params
+    )
     tx = optax.multi_transform(
-      {'kernel': optax.sgd(1.0), 'bias': optax.set_to_zero()}, param_labels)
+        {'kernel': optax.sgd(1.0), 'bias': optax.set_to_zero()}, param_labels
+    )
     state = tx.init(params)
     updates, new_state = tx.update(gradients, state, params)
     new_params = optax.apply_updates(params, updates)
 
-
-    self.assertTrue(np.allclose(new_params['linear_1']['b'], params['linear_1']['b']))
-    self.assertTrue(np.allclose(new_params['linear_2']['b'], params['linear_2']['b']))
-    self.assertFalse(np.allclose(new_params['linear_1']['w'], params['linear_1']['w']))
-    self.assertFalse(np.allclose(new_params['linear_2']['w'], params['linear_2']['w']))
+    self.assertTrue(
+        np.allclose(new_params['linear_1']['b'], params['linear_1']['b'])
+    )
+    self.assertTrue(
+        np.allclose(new_params['linear_2']['b'], params['linear_2']['b'])
+    )
+    self.assertFalse(
+        np.allclose(new_params['linear_1']['w'], params['linear_1']['w'])
+    )
+    self.assertFalse(
+        np.allclose(new_params['linear_2']['w'], params['linear_2']['w'])
+    )
 
   def test_path_aware_map_with_masked(self):
-    params = {'linear_1': {'w': jnp.zeros((5, 6)), 'b': jnp.zeros(5)},
-            'linear_2': {'w': jnp.zeros((6, 1)), 'b': jnp.zeros(1)}}
+    params = {
+        'linear_1': {'w': jnp.zeros((5, 6)), 'b': jnp.zeros(5)},
+        'linear_2': {'w': jnp.zeros((6, 1)), 'b': jnp.zeros(1)},
+    }
     gradients = jax.tree_util.tree_map(jnp.ones_like, params)  # dummy gradients
 
     params_mask = traverse_util.path_aware_map(
-      lambda path, x: 'w' in path, params)
+        lambda path, x: 'w' in path, params
+    )
     tx = optax.masked(optax.sgd(1.0), params_mask)
     state = tx.init(params)
     updates, new_state = tx.update(gradients, state, params)
     new_params = optax.apply_updates(params, updates)
 
-
-    self.assertTrue(np.allclose(new_params['linear_1']['b'], gradients['linear_1']['b']))
-    self.assertTrue(np.allclose(new_params['linear_2']['b'], gradients['linear_2']['b']))
-    self.assertTrue(np.allclose(new_params['linear_1']['w'], -gradients['linear_1']['w']))
-    self.assertTrue(np.allclose(new_params['linear_2']['w'], -gradients['linear_2']['w']))
+    self.assertTrue(
+        np.allclose(new_params['linear_1']['b'], gradients['linear_1']['b'])
+    )
+    self.assertTrue(
+        np.allclose(new_params['linear_2']['b'], gradients['linear_2']['b'])
+    )
+    self.assertTrue(
+        np.allclose(new_params['linear_1']['w'], -gradients['linear_1']['w'])
+    )
+    self.assertTrue(
+        np.allclose(new_params['linear_2']['w'], -gradients['linear_2']['w'])
+    )
 
   def test_path_aware_map_with_empty_nodes(self):
     params_in = {'a': {'b': 10, 'c': 2}, 'b': {}}
     params_out = traverse_util.path_aware_map(
-      lambda path, x: x + 1 if 'b' in path else -x, params_in)
+        lambda path, x: x + 1 if 'b' in path else -x, params_in
+    )
 
     self.assertEqual(params_out, {'a': {'b': 11, 'c': -2}, 'b': {}})
 
 
 if __name__ == '__main__':
   absltest.main()
```

